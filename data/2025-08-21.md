<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli](https://arxiv.org/abs/2508.14214)
*Mattson Ogg,Chace Ashcraft,Ritwik Bose,Raphael Norman-Tenazas,Michael Wolmetz*

Main category: cs.AI

TL;DR: 这项研究通过让多个知名的大型语言模型对情感内容数据集进行评分，发现GPT-4o在情感评分任务中与人类参与者的响应非常相似。然而，在唤起程度评分方面存在较大差异，而幸福评分最为相关。总体来说，LLMs在五类情绪框架中的一致性要优于在二维组织中的一致性。LLM评分比人类评分更具一致性，这些结果开始描述LLM代理如何解释情绪刺激，并突出了生物与人工智能在关键行为领域中的相似之处和差异之处。


<details>
  <summary>Details</summary>
Motivation: 情绪对人类行为和认知产生巨大影响，包括在日常任务和高压任务中。讨论是否以及如何将大型语言模型（LLMs）整合到日常生活中，应该建立在对这些工具如何评估情感加载的刺激或情境有一定了解的基础上。

Method: 通过让多个知名的大型语言模型（LLMs）对之前由人类评分过情感内容的单词和图像数据集进行评分，来研究这些工具如何评估情感加载的刺激或情境。

Result: 研究发现，GPT-4o在进行相同的情感评分任务时，其响应与人类参与者非常相似，跨模态、刺激和大多数评分尺度上的相关系数很高。但是，唤起程度评分在人类和LLM评分者之间的一致性较差，而幸福评分最为相关。总的来说，LLMs在五类情绪框架中的一致性要优于在二维组织中的一致性。最后，LLM评分比人类评分更具一致性。

Conclusion: 通过对数据集的情感内容进行分析，我们发现GPT-4o在情感评分任务中与人类参与者的响应非常相似，尤其在多种刺激和大多数评分尺度上的相关系数高达0.9以上。然而，在唤起程度评分方面，人类和LLM评分者之间的一致性较差，而幸福评分是最高相符的。总体而言，LLMs在五类情绪框架（幸福、愤怒、悲伤、恐惧、厌恶）中的一致性要优于在二维（唤起程度和价值）组织中的一致性。最后，LLM评分比人类评分更具一致性。总之，这些结果开始描述了LLM代理如何解释情绪刺激，突出了在关键行为领域中生物和人工智能之间的相似之处和差异之处。

Abstract: Emotions exert an immense influence over human behavior and cognition in both
commonplace and high-stress tasks. Discussions of whether or how to integrate
large language models (LLMs) into everyday life (e.g., acting as proxies for,
or interacting with, human agents), should be informed by an understanding of
how these tools evaluate emotionally loaded stimuli or situations. A model's
alignment with human behavior in these cases can inform the effectiveness of
LLMs for certain roles or interactions. To help build this understanding, we
elicited ratings from multiple popular LLMs for datasets of words and images
that were previously rated for their emotional content by humans. We found that
when performing the same rating tasks, GPT-4o responded very similarly to human
participants across modalities, stimuli and most rating scales (r = 0.9 or
higher in many cases). However, arousal ratings were less well aligned between
human and LLM raters, while happiness ratings were most highly aligned. Overall
LLMs aligned better within a five-category (happiness, anger, sadness, fear,
disgust) emotion framework than within a two-dimensional (arousal and valence)
organization. Finally, LLM ratings were substantially more homogenous than
human ratings. Together these results begin to describe how LLM agents
interpret emotional stimuli and highlight similarities and differences among
biological and artificial intelligence in key behavioral domains.

</details>


### [2] [Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions](https://arxiv.org/abs/2508.14294)
*Maria Leonor Pacheco,Fabio Somenzi,Dananjay Srinivas,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 该论文提出了一种神经符号方法，结合SAT求解器和大型语言模型，用于解释复杂决策序列，在Hitori难题上取得了良好的实验结果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于结合决策过程和语言模型解释复杂决策序列，利用Hitori难题的本地约束和连接性约束进行灵活组合和测试。

Method: 论文采用神经符号方法，结合SAT求解器和大型语言模型，实现对Hitori难题的解释。

Result: 作者实现了一个辅助人类解决Hitori难题的工具，并呈现了其有效性的实验证据。

Conclusion: 该论文提出了一种神经符号方法，结合决策过程和大型语言模型的优势，用于解释复杂决策序列。通过解释解决Hitori难题的方案，展示了这种方法的有效性。

Abstract: We propose a neurosymbolic approach to the explanation of complex sequences
of decisions that combines the strengths of decision procedures and Large
Language Models (LLMs). We demonstrate this approach by producing explanations
for the solutions of Hitori puzzles. The rules of Hitori include local
constraints that are effectively explained by short resolution proofs. However,
they also include a connectivity constraint that is more suitable for visual
explanations. Hence, Hitori provides an excellent testing ground for a flexible
combination of SAT solvers and LLMs. We have implemented a tool that assists
humans in solving Hitori puzzles, and we present experimental evidence of its
effectiveness.

</details>


### [3] [Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning](https://arxiv.org/abs/2508.14410)
*Beinuo Yang,Qishen Zhou,Junyi Li,Xingchen Su,Simon Hu*

Main category: cs.AI

TL;DR: 论文针对优化建模中的挑战，提出了LogiOR和ORThought模型，通过增强数据集、引入专家级别的建模原则和推理方式，自动化了OM过程。实验结果显示ORThought在复杂优化问题上优于现有方法，并提供了有价值的研究见解。


<details>
  <summary>Details</summary>
Motivation: 当前方法存在标注错误率高、评估范围狭窄和计算效率低等问题，通过引入LLM的自然语言理解和推理能力解决这些挑战。

Method: 增强数据集，引入LogiOR模型和ORThought框架，采用专家级别的优化建模原则和推理方式自动化OM过程。

Result: 经验评估表明ORThought在复杂优化问题上优于现有方法，提供了关键成功因素和失败模式的系统分析。

Conclusion: 提出了LogiOR和ORThought模型，通过系统纠正错误和更全面的注释增强现有数据集，ORThought在复杂优化问题上表现优异，并提供了对LLM-based optimization modeling的有价值见解。

Abstract: Optimization Modeling (OM) is essential for solving complex decision-making
problems. However, the process remains time-consuming and error-prone, heavily
relying on domain experts. While Large Language Models (LLMs) show promise in
addressing these challenges through their natural language understanding and
reasoning capabilities, current approaches face three critical limitations:
high benchmark labeling error rates reaching up to 42\%, narrow evaluation
scope that only considers optimal values, and computational inefficiency due to
heavy reliance on multi-agent systems or model fine-tuning. In this work, we
first enhance existing datasets through systematic error correction and more
comprehensive annotation. Additionally, we introduce LogiOR, a new optimization
modeling benchmark from the logistics domain, containing more complex problems
with standardized annotations. Furthermore, we present ORThought, a novel
framework that leverages expert-level optimization modeling principles through
chain-of-thought reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought outperforms existing
approaches, including multi-agent frameworks, with particularly significant
advantages on complex optimization problems. Finally, we provide a systematic
analysis of our method, identifying critical success factors and failure modes,
providing valuable insights for future research on LLM-based optimization
modeling.

</details>


### [4] [The Agent Behavior: Model, Governance and Challenges in the AI Digital Age](https://arxiv.org/abs/2508.14415)
*Qiang Zhang,Pei Yan,Yijia Xu,Chuanpo Fu,Yong Fang,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出了“网络行为生命周期”模型，比较了人类和代理人在行为上的区别，并引入了“A4A”范式和“HABD”模型。通过实际案例验证了模型的有效性，讨论了未来研究方向，旨在为人类和代理人的安全合作提供理论基础和技术路线图。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，网络环境中的代理人越来越像人类，导致人工和人类行为之间的界限变得模糊。这种转变带来了信任、责任、伦理、安全等方面的重大挑战。监督代理人行为的困难可能导致数据污染和责任不清晰。

Method: 提出了“网络行为生命周期”模型，划分了网络行为为6个阶段，并系统分析了人类和代理人在每个阶段的行为差异。引入了“A4A”范式和“HABD”模型，在5个维度上比较人类和代理人行为的基本区别。通过红队入侵和蓝队防御等实际案例验证了模型的有效性。

Result: 提出的模型在实际案例中验证了有效性，为未来研究动态认知治理架构、行为差异量化和元治理协议栈提供了方向。

Conclusion: 本文提出了“网络行为生命周期”模型，分析人类和代理人在行为上的差异，并引入了“A4A”范式和“HABD”模型。通过实际案例验证了模型的有效性，并讨论了未来研究方向，旨在为人类和代理人的安全合作提供理论基础和技术路线图。

Abstract: Advancements in AI have led to agents in networked environments increasingly
mirroring human behavior, thereby blurring the boundary between artificial and
human actors in specific contexts. This shift brings about significant
challenges in trust, responsibility, ethics, security and etc. The difficulty
in supervising of agent behaviors may lead to issues such as data contamination
and unclear accountability. To address these challenges, this paper proposes
the "Network Behavior Lifecycle" model, which divides network behavior into 6
stages and systematically analyzes the behavioral differences between humans
and agents at each stage. Based on these insights, the paper further introduces
the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity
(HABD)" model, which examine the fundamental distinctions between human and
agent behaviors across 5 dimensions: decision mechanism, execution efficiency,
intention-behavior consistency, behavioral inertia, and irrational patterns.
The effectiveness of the model is verified through real-world cases such as red
team penetration and blue team defense. Finally, the paper discusses future
research directions in dynamic cognitive governance architecture, behavioral
disparity quantification, and meta-governance protocol stacks, aiming to
provide a theoretical foundation and technical roadmap for secure and
trustworthy human-agent collaboration.

</details>


### [5] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: 最近的研究通过结构化示例提出了一种方法来改进LLM-based代理人的性能，但发现单独的结构化示例并不能实现一致的改进。LLM-based代理人在一些任务中表现出色，但在需要更深层次的认知能力的情况下面临挑战。因此，需要更多的工作来实现这些代理人的视角能力。


<details>
  <summary>Details</summary>
Motivation: 动机：最近LLM和推理框架的进展为提高自主智能体的视角能力开辟了新的可能性。然而，涉及主动感知、协作推理和角度视角的任务对当前基于LLM的系统提出了持续挑战。

Method: 方法：通过从Fast Downward规划器生成的转换解决方案图中提取的结构化示例，提出一种结构化解决方案处理流水线，生成三类不同类型的示例：最佳目标路径（G型）、信息节点路径（E型）和逐步最佳决策序列对比替代行动（L型）。

Result: 结果：L型示例略微减少澄清请求和整体行动步骤，但并未产生一致的改进。代理人在需要基本注意过滤的任务中成功，但在需要对遮挡空间进行心理演练或权衡认知行动成本的情景中表现困难。

Conclusion: 结论：结构化示例单独并不足以提高自主智能体的视角能力，强调在LLM基础上的代理人需要明确的信念跟踪、成本建模和更丰富的环境，以实现社交基础合作。

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [6] [LeanGeo: Formalizing Competitional Geometry problems in Lean](https://arxiv.org/abs/2508.14644)
*Chendong Song,Zihan Wang,Frederick Pu,Haiming Wang,Xiaohan Lin,Junqi Liu,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: 介绍了LeanGeo，它是一个统一的形式系统，用于解决竞赛级别的几何问题。提出了LeanGeo-Bench作为形式几何基准，评估了大型语言模型在该基准上的能力和局限性，强调了自动几何推理领域的发展需求。


<details>
  <summary>Details</summary>
Motivation: 现有的几何解决系统无法在统一框架内表达问题，难以与其他数学领域集成，几何证明依赖直观图表，验证问题尤为具有挑战性。为了解决这些问题，引入了LeanGeo。

Method: 引入了LeanGeo，它具有高级几何定理的全面库，配合Lean的基础逻辑，实现了严格的证明验证和与Mathlib的无缝集成。

Result: 评估了状态下最先进的大型语言模型在LeanGeo-Bench上的表现，展示了自动几何推理领域需进一步发展的必要性。

Conclusion: 介绍了LeanGeo，它是一个统一的形式系统，用于在Lean 4定理证明器中形式化和解决竞赛级别的几何问题。展示了LeanGeo-Bench，其中包括来自国际数学奥林匹克竞赛（IMO）和其他高级来源的问题，评估了最新的大型语言模型在该基准上的能力和局限性，强调了自动几何推理领域需要进一步的进展。

Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most
existing geometry solving systems cannot express problems within a unified
framework, thus are difficult to integrate with other mathematical fields.
Besides, since most geometric proofs rely on intuitive diagrams, verifying
geometry problems is particularly challenging. To address these gaps, we
introduce LeanGeo, a unified formal system for formalizing and solving
competition-level geometry problems within the Lean 4 theorem prover. LeanGeo
features a comprehensive library of high-level geometric theorems with Lean's
foundational logic, enabling rigorous proof verification and seamless
integration with Mathlib. We also present LeanGeo-Bench, a formal geometry
benchmark in LeanGeo, comprising problems from the International Mathematical
Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the
capabilities and limitations of state-of-the-art Large Language Models on this
benchmark, highlighting the need for further advancements in automated
geometric reasoning. We open source the theorem library and the benchmark of
LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.

</details>


### [7] [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](https://arxiv.org/abs/2508.14654)
*Peilin Ji,Xiao Xue,Simeng Wang,Wenhao Yan*

Main category: cs.AI

TL;DR: 该论文介绍了H-J层次化多智能体框架，用于增强城市洪水响应韧性。通过集成知识引导提示、熵约束生成和反馈驱动优化，H-J在实地实验中表现优异，优于基于规则和强化学习的方法。


<details>
  <summary>Details</summary>
Motivation: 城市洪水事件频率增加给应急调度系统带来了重大挑战，影响了公共安全和流动性。目前的方法无法解决竞争目标之间的权衡、动态策略、环境变化和多智能体协调等问题，需要一个集成的框架来解决这些挑战。

Method: 引入H-J层次化多智能体框架，集成知识引导提示、熵约束生成和反馈驱动优化。通过在实际城市地形和降雨数据下进行实验评估H-J在三种代表性条件下的表现。

Result: 实验证明H-J在交通流畅性、任务成功率和系统稳健性方面优于规则和强化学习基准方法。

Conclusion: 该论文介绍了一种层次化多智能体框架H-J，用于提高城市洪水响应的韧性。通过集成知识引导提示、熵约束生成和反馈驱动优化，H-J在实际城市地形和降雨数据上表现优异。研究结果表明，H-J在交通流畅性、任务成功率和系统稳健性方面优于基于规则和强化学习的基准方法。因此，这些发现突显了基于LLM的不确定性感知、知识约束方法在增强城市洪水响应韧性方面的潜力。

Abstract: In recent years, the increasing frequency of extreme urban rainfall events
has posed significant challenges to emergency scheduling systems. Urban
flooding often leads to severe traffic congestion and service disruptions,
threatening public safety and mobility. However, effective decision making
remains hindered by three key challenges: (1) managing trade-offs among
competing goals (e.g., traffic flow, task completion, and risk mitigation)
requires dynamic, context-aware strategies; (2) rapidly evolving environmental
conditions render static rules inadequate; and (3) LLM-generated strategies
frequently suffer from semantic instability and execution inconsistency.
Existing methods fail to align perception, global optimization, and multi-agent
coordination within a unified framework. To tackle these challenges, we
introduce H-J, a hierarchical multi-agent framework that integrates
knowledge-guided prompting, entropy-constrained generation, and feedback-driven
optimization. The framework establishes a closed-loop pipeline spanning from
multi-source perception to strategic execution and continuous refinement. We
evaluate H-J on real-world urban topology and rainfall data under three
representative conditions: extreme rainfall, intermittent bursts, and daily
light rain. Experiments show that H-J outperforms rule-based and
reinforcement-learning baselines in traffic smoothness, task success rate, and
system robustness. These findings highlight the promise of uncertainty-aware,
knowledge-constrained LLM-based approaches for enhancing resilience in urban
flood response.

</details>


### [8] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: MCP-Universe is a new benchmark for evaluating Large Language Models in realistic tasks with real-world MCP servers. Leading LLMs exhibit performance limitations, and challenges include long-context and unknown-tools scenarios. The benchmark includes execution-based evaluators and aims to foster innovation in the MCP ecosystem.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs are too simplistic and do not capture real application challenges. The motivation is to address this gap by introducing a benchmark that evaluates LLMs in realistic and hard tasks through interaction with real-world MCP servers.

Method: The paper introduces MCP-Universe, a comprehensive benchmark encompassing 6 core domains and 11 different MCP servers. It implements execution-based evaluators such as format evaluators, static evaluators, and dynamic evaluators to evaluate LLM performance. Real-time ground truth retrieval is used for temporally sensitive tasks.

Result: Leading LLMs, such as GPT-5, Grok-4, and Claude-4.0-Sonnet, show significant performance limitations when evaluated in the MCP-Universe benchmark. The benchmark poses challenges in long-context and unknown-tools scenarios for LLM agents. Enterprise-level agents like Cursor do not outperform standard ReAct frameworks in this evaluation.

Conclusion: MCP-Universe is introduced as a benchmark for evaluating Large Language Models (LLMs) in realistic and challenging tasks through interaction with real-world MCP servers. Leading LLMs, including GPT-5, Grok-4, and Claude-4.0-Sonnet, exhibit performance limitations in this benchmark. The benchmark includes execution-based evaluators to ensure rigorous evaluation.

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [9] [Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines](https://arxiv.org/abs/2508.14710)
*Swantje Plambeck,Ali Salamati,Eyke Huellermeier,Goerschwin Fey*

Main category: cs.AI

TL;DR: 本文提出了一种数据驱动方法，用于确定复杂系统在有限时间步骤内的安全概率。方法基于PAC学习范式和Mealy机离散抽象，通过主动学习收集新数据以提高确定概率的置信度。经过案例研究验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究表明，对于复杂的数字物理系统（CPS），缺乏适当的模型以进行验证、诊断或调试是一个挑战。本文的动机在于利用数据驱动方法解决CPS中的诊断和验证问题，通过建立离散逻辑与概率可达性分析之间的联系来提高对系统安全概率的判断置信度。

Method: 本文提出了一种基于Mealy机形式的离散抽象的数据驱动方法。该方法基于Probably Approximately Correct (PAC)学习范式，在系统的有限时间段内确定系统的安全概率。主动学习范式确保在收集初始学习数据集后，以引导方式采样新的学习数据。

Result: 通过提出的数据驱动方法，成功确定了系统在有限时间步骤内的安全概率，并验证了该方法在自动车道保持系统案例中的有效性。

Conclusion: 本文提出了一种基于数据驱动的方法，用于在有限时间步骤内确定系统的安全概率。通过可能近似正确（PAC）学习范式，建立了离散逻辑与系统的概率可达性分析之间的联系，并提供了对确定概率的额外置信度。使用主动学习范式进行学习过程，有效地收集新的学习数据。通过对自动车道保持系统的案例研究验证了该方法的有效性。

Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models
for tasks like verification, diagnosis, or debugging. Often, suitable models
are not available and manual extraction is difficult. Data-driven approaches
then provide a solution to, e.g., diagnosis tasks and verification problems
based on data collected from the system. In this paper, we consider CPS with a
discrete abstraction in the form of a Mealy machine. We propose a data-driven
approach to determine the safety probability of the system on a finite horizon
of n time steps. The approach is based on the Probably Approximately Correct
(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic
and probabilistic reachability analysis of systems, especially providing an
additional confidence on the determined probability. The learning process
follows an active learning paradigm, where new learning data is sampled in a
guided way after an initial learning set is collected. We validate the approach
with a case study on an automated lane-keeping system.

</details>


### [10] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: 本论文讨论了AI模型是否具有自省能力的问题，提出了更质实的自省定义。通过实验发现，LLMs在某些情况下可能仅表现出表面的自省能力，未能达到实质性自省标准。


<details>
  <summary>Details</summary>
Motivation: AI 模型是否具有自省能力是一个重要的实际问题，但关于自省的定义尚无定论。作者起初采用一种“轻量级”定义，但后来提出了更质实的定义。

Method: 在实验中让 LLMs 推理其内部温度参数，以展示其可能在外观上具有自省能力但并非按照提出定义进行实质性自省。

Result: 通过实验发现，LLMs 可能表现出轻量级的自省能力，但未能实质性自省。

Conclusion: 作者认为 AI 模型可能在外观上表现出自省能力，但在其提出的定义下，这种自省并不具有实质性意义。他们基于实验结果，提出 AI 自省不仅需要提供有关内部状态的信息，还需要比第三方获取相同或更低计算成本的信息更可靠的过程。

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>
