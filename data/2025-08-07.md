<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是为安全和对齐性而设计的第一个完全集成的运行时治理框架，通过六个集成组件提供实时控制，能够解决机构型人工智能系统在生产环境中的治理挑战。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型与机构型人工智能系统在治理挑战上存在根本区别，机构型系统在运行时展现出新兴和意外行为，引入了无法完全预料的代理相关风险。为解决这一关键差距，作者引入MI9，旨在为机构型人工智能系统的安全性和对齐性提供运行时治理框架。

Method: MI9引入了六个集成组件：代理风险指数、代理-语义遥测捕获、持续授权监控、基于有限状态机（FSM）的符合引擎、目标条件漂移检测以及渐进式遏制策略。这些组件使MI9能够在异构的代理体系结构中透明运行，为机构型系统的部署提供实时控制。

Result: MI9的详细分析表明了其系统性覆盖现有方法无法解决的治理挑战，为全面的机构型人工智能监督提供了技术基础。

Conclusion: MI9是首个专为安全和对齐性而设计的完全集成的运行时治理框架，通过六个集成组件提供实时控制，为机构型人工智能系统的系统性、安全性和责任性部署提供基础设施。MI9通过多样化的场景详细分析展示了对现有方法无法解决的治理挑战的系统化覆盖，为全面的机构型人工智能监督奠定了技术基础。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [2] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL is a novel multi-agent reinforcement learning framework that addresses the limitations of existing defenses in multimodal large language model-based multi-agent systems (MAS). It trains agents to defend against adversarial threats while performing their primary functions, reducing attack success rates and improving accuracy on reasoning tasks. The framework integrates evolutionary search with reinforcement learning to enhance MAS performance under evolving threats, effectively improving safety and utility in MAS.


<details>
  <summary>Details</summary>
Motivation: The growing openness and interaction complexity of multimodal large language model-based MAS pose serious risks such as jailbreak and adversarial attacks. Existing defenses relying on external guard modules face challenges of limited protection and single-point failure. Naively increasing guard agents raises cost and complexity. The paper aims to address these challenges by introducing Evo-MARL to improve safety and utility in MAS.

Method: The paper proposes Evo-MARL, a multi-agent reinforcement learning framework that trains each agent to perform its primary function and resist adversarial threats simultaneously. It integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders, internalizing safety mechanisms and enhancing MAS performance under evolving threats.

Result: Experiments demonstrate that Evo-MARL reduces attack success rates by up to 22% and increases accuracy by up to 5% on reasoning tasks, showing the effectiveness of the framework in improving both safety and utility in MAS.

Conclusion: Evo-MARL is proposed as a novel multi-agent reinforcement learning framework to address the challenges of existing defenses in multi-agent systems (MAS) built on multimodal large language models. It enables all task agents to jointly acquire defensive capabilities, reduces attack success rates, and boosts accuracy on reasoning tasks.

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [3] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 该论文介绍了MOTIF框架，通过多轮交互优化两个大型语言模型代理间的求解器设计，拓展了搜索空间，鼓励发现多样化、高性能的解决方案。实验证明MOTIF优于现有方法，展示了基于回合制、多代理提示的完全自动化求解器设计的前景。


<details>
  <summary>Details</summary>
Motivation: 近年来，使用大型语言模型合成高质量组件取得了进展，但大多数方法限制搜索到一个元素，导致错失创新机会。因此，作者提出更广泛的求解器设计形式，致力于统一目标下改进一组相互依赖的组件。

Method: 该论文将求解器设计视为多策略优化问题，提出了MOTIF框架，通过蒙特卡洛树搜索促进两个大型语言模型代理之间的交互优化。每轮中，一个代理改进一个组件，利用自身和对手先前更新的历史，促进竞争压力和新兴合作。这种结构化互动拓展了搜索空间，鼓励发现多样化的高性能解决方案。

Result: 在多个组合优化问题领域的实验显示，MOTIF始终优于现有方法，突出了基于回合制、多代理提示的全自动化求解器设计的潜力。

Conclusion: 该论文提出了一种名为MOTIF的框架，采用基于蒙特卡洛树搜索的方法，通过多轮交互优化两个大型语言模型代理之间的求解器设计，取得了优于现有方法的效果。该方法拓展了求解器设计的范围，鼓励发现多样化且高性能的解决方案，展现了基于回合制的多代理提示在完全自动化求解器设计中的潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [4] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 本文介绍了SymbolBench基准，旨在评估LLMs在符号推理方面的能力。提出了统一框架，结合LLMs和遗传编程形成闭环符号推理系统。实证结果揭示了当前模型的优势和限制，强调结合领域知识、上下文对齐和推理结构的重要性，以改进LLMs在自动科学发现中的性能。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于系统评估LLMs从时间序列数据中推理可解释的、与上下文对齐的符号结构的能力。现有工作局限于简单的代数方程式，而SymbolBench涵盖了多样的符号形式和不同复杂度，扩展了评估的范围。

Method: 本文介绍了SymbolBench基准，设计了三个任务进行符号推理的评估：多元符号回归、布尔网络推断和因果发现。提出了统一框架，将LLMs与遗传编程相结合，形成闭环符号推理系统，并将LLMs作为预测者和评估者。

Result: 实证结果揭示了当前模型的关键优势和局限性，强调结合领域知识、上下文对齐和推理结构的重要性，以改进LLMs在自动科学发现中的性能。

Conclusion: 本文介绍了SymbolBench，一个旨在评估符号推理能力的综合基准。作者提出了一种统一框架，将大型语言模型（LLMs）与遗传编程相结合，形成闭环符号推理系统。实证结果揭示了目前模型的主要优势和局限性，强调结合领域知识、上下文对齐和推理结构的重要性，以改进LLMs在自动科学发现中的表现。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [5] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 本文提出EmoAgent框架，用于对抗性情感提示来劫持推理路径，揭示了MLRM在人类中心服务方面易受用户情绪线索影响的情况，并在先进的MLRM上进行了广泛实验，证明了EmoAgent框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 观察到面向人类中心服务的MLRM在深思阶段对用户情绪线索高度敏感，常常在情绪强度高时覆盖安全协议或内置安全检查。基于这一关键洞察，作者提出了EmoAgent框架。

Method: 作者提出了EmoAgent框架，通过对抗性情感提示来劫持推理路径，并引入了三种评估风险的指标：Risk-Reasoning Stealth Score (RRSS)、Risk-Visual Neglect Rate (RVNR)、Refusal Attitude Inconsistency (RAIC)，并在先进的MLRM上进行了广泛实验。

Result: 实验结果表明EmoAgent框架的有效性，并揭示了模型安全行为中更深层次的情感认知错位。

Conclusion: EmoAgent是一个自主的对抗性情感代理框架，旨在通过夸大的情感提示来劫持推理路径，揭示了MLRM在人类中心服务方面易受用户情绪线索影响的情况。实验表明EmoAgent的有效性，并揭示了模型安全行为中更深层次的情感认知错位。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [6] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架Cognition Forest和Galaxy，支持个性化能力生成和多维交互。实验结果显示Galaxy优于多个最新基准，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 智能个人助理的主动行为方面仍未得到充分探讨，要设计具有主动性、保护隐私和自我发展能力的IPA仍然是一个重大挑战。为此，需要依赖LLM代理的认知架构。

Method: 通过将认知架构和系统设计统一为相互强化的循环，提出了Cognition Forest语义结构和Galaxy框架。实现了两个协作代理：KoRa和Kernel。

Result: 实验结果表明Galaxy在多个最新基准上表现优异。消融研究和真实世界交互案例验证了Galaxy的有效性。

Conclusion: 提出了Cognition Forest和Galaxy框架，支持多维交互和个性化能力生成，取得了优于多个最新基准的实验结果。消融研究和真实世界交互案例验证了Galaxy的有效性。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [7] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: 本文介绍了一种名为RecAgent的不确定性感知代理，用于解决GUI导航中的感知不确定性和决策不确定性问题。RecAgent通过组件推荐机制和交互模块降低了这些不确定性，并在统一框架中整合这些组件。作者还提出了ComplexAction数据集来评估GUI代理在复杂场景中执行操作的成功率。实验验证了RecAgent代理的有效性和ComplexAction数据集的实用性。


<details>
  <summary>Details</summary>
Motivation: 本文针对GUI代理在自动化移动任务中遇到的输入冗余和决策模糊问题，提出了RecAgent代理以解决这些问题。为了减少感知不确定性和决策不确定性，引入了组件推荐机制和交互模块，旨在提高GUI代理的执行效率和决策准确性。同时为验证方法的有效性提出了ComplexAction数据集。

Method: 本文通过RecAgent代理解决了GUI导航中的不确定性问题，其中包括感知不确定性和决策不确定性。针对感知不确定性，RecAgent使用了组件推荐机制来识别和专注于最相关的UI元素；对于决策不确定性，通过交互模块在模糊情况下请求用户反馈，实现意图感知决策。提出了ComplexAction数据集来评估GUI代理在复杂场景中执行特定单步操作的成功率。

Result: 通过广泛实验验证了RecAgent代理的有效性，并展示了其在减少输入复杂性和处理高不确定性情况方面的优势。提出的ComplexAction数据集也为评估GUI代理在复杂场景中执行操作提供了有效工具。

Conclusion: 本文提出了一种名为RecAgent的不确定性感知代理，通过自适应感知解决GUI导航中的不确定性问题。RecAgent通过组件推荐机制和交互模块来降低感知不确定性和决策不确定性，将它们整合到一个统一框架中，并利用人为参与来处理高不确定性情况。通过提出的ComplexAction数据集进行广泛实验验证了方法的有效性。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [8] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 本文提出了自我进化代理（SEA）用于克服计算机使用代理的性能问题，通过创新的数据生成、强化学习和模型增强方法成功开发了该代理，性能优越且参数数量较小。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决计算机使用代理性能不足的问题，提出自我进化代理（SEA）来克服现有代理的局限性。通过创新的数据生成、训练策略和增强方法，旨在提高性能并减少计算需求，使得代理更加有效地执行用户任务。

Method: 本文通过提出自我进化代理（SEA）解决计算机使用代理的性能问题，创新地应用了数据生成、强化学习和模型增强方法。首先提出了自动生成可验证轨迹的自动流程，然后提出了高效的逐步强化学习方法，以减少长时间训练的计算需求。最后，提出了将基础和规划能力合并到一个模型中的增强方法，而无需额外训练。

Result: 通过提出的自我进化代理（SEA），本文成功地解决了计算机使用代理性能不足的问题，拥有较小数量的参数却表现优异，未来将开源相关模型和代码。

Conclusion: 本文提出了自我进化代理（SEA）用于计算机使用，通过创新的数据生成、强化学习和模型增强方法开发该代理。最终，我们成功地基于创新的数据生成、训练策略和增强方法，得到了只有7B参数的自我进化代理（SEA），性能优于具有相同参数数量的模型，并且与更大模型具有可比性能。未来我们将开源模型的权重和相关代码。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [9] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 研究调查了GenAI在学习系统中以职业目标为基础的内容调整对学习者参与度、满意度和学习效率的影响。结果显示，个性化内容增加了会话时长，提高了满意度评级，并略微缩短学习时长。学习者认为个性化材料激励实用，促进了深度认知参与和对内容的认同。研究结果强调了将教育内容与学习者职业目标对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能越来越多地整合到数字学习环境中，个性化学习内容以反映学习者个人职业目标的趋势具有潜在的提升参与度和长期动机的可能性。该研究的动机在于探讨基于GenAI的职业目标内容调整如何影响学习者的参与度、满意度和学习效率。

Method: 在研究中进行了混合方法实验，涉及超过4,000名学习者，其中一个组接收到根据其职业目标量身定制的学习情境，而另一个对照组未接收到量身定制内容。通过定量数据显示，与标准内容相比，个性化内容导致会话时长增加，满意度评级提高，并略微减少学习时长。定性分析突出了学习者发现个性化材料激励和实用，促进了深度认知参与和对内容的强烈认同。

Result: 结果显示，个性化教育内容对学习者的参与度、满意度和学习效率具有积极影响，强调了将教育内容与学习者职业目标相匹配的价值，并表明可扩展的人工智能个性化可以架起学术知识和职场应用之间的桥梁。

Conclusion: 个性化教育内容以反映学习者个人职业目标的潜力有助于增强学习者的参与度和长期动机。研究结果表明，基于生成人工智能（GenAI）的职业目标内容调整对学习者的参与度、满意度和学习效率产生积极影响。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [10] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT is a framework proposed to enhance code generation and mathematical reasoning tasks by leveraging knowledge graphs and executable code. It outperforms existing methods by improving accuracy from several to over ten percentage points. The framework decomposes problems into a Structured Task Graph, uses GraphRAG for precise knowledge retrieval, and generates verifiable code for computational accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) have shown excellence in natural language processing tasks but encounter challenges in complex reasoning tasks such as mathematical reasoning and code generation. The paper aims to address these limitations by introducing KGA-ECoT, which combines knowledge graphs and executable code to enhance these capabilities.

Method: The paper proposes KG-Augmented Executable Chain-of-Thought (KGA-ECoT) as a novel framework to enhance code generation through knowledge graphs and improve mathematical reasoning via executable code. It decomposes problems into a Structured Task Graph, utilizes GraphRAG for precise knowledge retrieval from mathematical libraries, and generates verifiable code ensuring computational accuracy. Evaluations on multiple mathematical reasoning benchmarks were conducted to compare KGA-ECoT with existing prompting methods.

Result: KGA-ECoT significantly outperforms existing prompting methods in mathematical reasoning tasks, demonstrating absolute accuracy improvements of several to over ten percentage points. The framework's effectiveness is attributed to the use of GraphRAG for knowledge retrieval and the generation of verifiable code for computational accuracy.

Conclusion: KGA-ECoT is a robust and highly generalizable framework for complex mathematical reasoning tasks, outperforming existing prompting methods by achieving significant accuracy improvements ranging from several to over ten percentage points. The framework leverages knowledge graphs for code generation and executable code for mathematical reasoning, decomposes problems into a Structured Task Graph, and utilizes GraphRAG for precise knowledge retrieval from mathematical libraries.

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [11] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR is a framework that improves geospatial predictions by incorporating geographic principles and spatial dependencies into the reasoning process. It addresses challenges faced by LLMs and shows consistent improvements in prediction quality across various tasks. The experimental results validate the effectiveness of GeoSR in providing accurate and equitable geospatial predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation behind GeoSR is to enhance geospatial predictions by overcoming the limitations of LLMs in spatial consistency, multi-hop reasoning, and geographic bias. By incorporating geostatistical priors and spatially structured reasoning, GeoSR aims to provide more accurate and equitable predictions for tasks ranging from physical-world property estimation to socioeconomic prediction.

Method: GeoSR decomposes the reasoning process into three collaborating agents: a variable-selection agent, a point-selection agent, and a refine agent. It embeds Tobler's First Law of Geography into an iterative prediction loop to refine predictions based on spatial dependencies and inter-variable relationships.

Result: Experimental results demonstrate consistent improvements over standard prompting strategies when using GeoSR. The framework shows enhanced prediction quality and accuracy in geospatial prediction tasks, showcasing the effectiveness of integrating geostatistical priors and spatially structured reasoning into LLMs.

Conclusion: GeoSR is a self-refining agentic reasoning framework that addresses challenges faced by large language models (LLMs) in spatial consistency, multi-hop reasoning, and geographic bias. It significantly improves prediction quality by incorporating core geographic principles and spatial dependencies into the reasoning process.

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [12] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 该研究引入了语义熵作为一种衡量GPT-4生成解释多样性的指标，用于代表人工评分者不一致性。实验表明，语义熵与人工评分者分歧相关，对不同学科普遍适用，并在需要解释推理的任务中增加。这一发现将语义熵定位为一种可解释的不确定性信号，有助于支持更透明和可信的AI辅助评分工作流程。


<details>
  <summary>Details</summary>
Motivation: 自动评分系统通常不能指示评分决定的不确定性或争议性，本研究的动机是引入一种新的衡量标准以量化多个GPT-4生成的解释在相同学生回答中的变化，以代表人工评分者间的不一致性。通过研究语义熵，旨在提高AI辅助评分工作流程的透明度和可信度。

Method: 通过对ASAP-SAS数据集进行实验，使用语义熵来测量多个GPT-4生成的解释的多样性，对解释进行聚类和熵计算，以探讨语义熵与人工评分者分歧的关系、在不同学科中的泛化性以及对任务结构特征的敏感性。

Result: 实验结果显示，语义熵与人工评分者的分歧相关，对不同学科普遍适用，并在需要解释推理的任务中增加。这表明语义熵可以作为一种可解释的不确定性信号，支持更透明和可信的AI辅助评分工作流程。

Conclusion: 该研究引入了语义熵作为一种测量多个GPT-4生成的解释在相同学生回答中变化的指标，用作人工评分者间的分歧的代理。语义熵能够量化不同解释的多样性，无需依赖最终输出分数。通过对解释进行聚类和计算熵值，研究发现语义熵与人工评分者的分歧相关，对各学科普遍适用，并在需要解释推理的任务中增加。结果表明，语义熵可以作为可解释的不确定性信号，从而支持更透明和可信的AI辅助评分工作流程。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [13] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 该论文介绍了一种组合式的即时合成框架，针对大型LTLf公式，应用合成而非构建自动机，能解决其他求解器无法处理的实例，具有较好性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术要么在解游戏之前组合地构建DFA，利用自动机最小化来减轻状态空间爆炸，要么在游戏解决过程中逐步构建DFA以避免完整的DFA构建，但两者均不占主导地位。本文旨在解决这一挑战，提出一种新的框架来解决这一问题。

Method: 该论文介绍了一种组合式的即时合成框架，整合了构建DFA和游戏解决两种方法的优势，侧重于实践中常见的大型LTLf公式。该框架在游戏解决过程中应用合成，而非构建自动机，允许两种合成变体：在合成之前修剪以充分利用最小化，或者在合成过程中修剪以指导即时合成。

Result: 该框架能够解决其他合成求解器无法处理的大量实例，具有较好的性能。

Conclusion: 该论文介绍了一种组合式的即时合成框架，结合了构建DFA和游戏解决两种方法的优势，针对实际中常见的大型LTLf公式的共同特点。该框架在游戏解决过程中应用合成，而非构建自动机（游戏竞技场）。与最先进的合成求解器相比，我们的框架能够解决其他求解器无法处理的大量实例。详细分析表明，两种合成变种均具有独特优点。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [14] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: Agentic Reasoning for Emerging Entities (AgREE) is a novel framework that combines iterative retrieval actions and multi-step reasoning to construct knowledge graph triplets dynamically. It outperforms existing methods in constructing knowledge graph triplets for emerging entities without the need for training efforts, demonstrating effectiveness in maintaining up-to-date knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for Knowledge Graph Completion (KGC) lack the ability to capture comprehensive and up-to-date information about unpopular and emerging entities. They rely on pretrained language models, pre-constructed queries, or single-step retrieval, which may not be effective for emerging entities.

Method: Introduce Agentic Reasoning for Emerging Entities (AgREE), an agent-based framework that combines iterative retrieval actions and multi-step reasoning to construct knowledge graph triplets dynamically.

Result: Experiments demonstrate that Agentic Reasoning for Emerging Entities (AgREE) outperforms existing methods in constructing knowledge graph triplets by up to 13.7% for emerging entities. A new evaluation methodology and benchmark for KGC on emerging entities are also proposed.

Conclusion: Agentic Reasoning for Emerging Entities (AgREE) outperforms existing methods in constructing knowledge graph triplets, especially for emerging entities, without the need for training efforts.

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [15] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 本文提倡结合知识驱动和数据驱动方法，通过逻辑推理确定智能代理的行动，并快速学习预测其他代理行为，基于通用知识预测未来目标。实验评估表明，这种架构能够有效地帮助智能代理进行协作。


<details>
  <summary>Details</summary>
Motivation: AI代理在协助角色中需要与其他代理（人类，AI系统）协作，但通常缺乏先前协调。现有的数据驱动方法需要大量的标记数据集，缺乏透明度，难以快速调整知识应对变化。随着代理数量增加，决策复杂性增加，有效协作变得困难。因此，本文旨在探讨如何结合知识驱动和数据驱动方法来推动自发团队合作。

Method: 结合知识驱动和数据驱动方法，通过逻辑推理确定智能代理的行动，并快速学习预测其他代理行为，基于通用知识预测未来目标。在VirtualHome平台进行实验评估。

Result: 本文提出的架构能够帮助智能代理通过逻辑推理和快速学习有效地进行协作，并在虚拟环境中进行了实验评估。

Conclusion: 这篇论文提倡将基于知识和数据驱动方法的互补优势应用于特定目标的自发团队合作中，通过逻辑推理和快速学习来辅助智能代理有效地进行协作。实验在虚拟环境中评估了这种架构的能力。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [16] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: 本文介绍了CASCAD，这是一种新的电路感知SAT求解框架，可以提高求解器的效率，降低解决时间，并通过概率引导的子句过滤策略进一步减少时间。研究结果表明在SAT求解器中保留电路层面结构洞察的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前解决CSAT问题的标准工作流程存在严重局限，转换电路为合取范式（CNF）依赖于通用SAT求解器，丢弃了丰富的结构和功能信息，导致求解器性能不佳。因此，本研究旨在解决这一局限性，提出了基于电路的SAT求解框架CASCAD。

Method: 引入了一种新的电路感知SAT求解框架CASCAD，直接利用通过图神经网络（GNNs）计算的电路级条件概率，动态引导了两个关键的CDCL启发式方法，显著增强了求解器的效率。

Result: 通过在挑战性的现实世界逻辑等价检查（LEC）基准测试上进行广泛评估，表明CASCAD与最先进的基于CNF的方法相比，可以将解决时间缩短多达10倍，并通过概率引导的子句过滤策略进一步减少23.5%的运行时间。

Conclusion: 本文介绍了一种新的基于电路认知的SAT求解框架CASCAD，可以显著提高求解器的效率，降低解决时间并通过概率引导的子句过滤策略进一步减少解决时间。研究结果强调了在SAT求解器中保留电路层面结构洞察的重要性，为未来提高SAT求解效率和EDA工具设计奠定了坚实基础。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [17] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio是一个专注于提高生物医学推理效率和安全性的框架，通过建立正交梯度空间来防止多能力整合中的干扰，实现各项能力的协同工作。关键创新包括MKGSG和Capability Aware Group Relative Policy Optimization。框架在实际部署中取得了显著效果，包括成本降低、诊断准确性提高和临床医生接受度提高。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于提高生物医学推理的效率和安全性。当前在人工智能对齐领域，多能力整合会导致能力干扰，可能影响安全性。因此，BalancedBio框架的提出旨在通过正交梯度空间的建立来防止此类干扰，实现各项能力的协同工作，提高推理性能。同时，为了证明框架的有效性，该研究进行了数学分析和实际部署实验。

Method: 该研究提出了BalancedBio框架，包括Medical Knowledge Grounded Synthetic Generation（MKGSG）和Capability Aware Group Relative Policy Optimization两大关键创新。MKGSG利用临床工作流限制和医学本体验证扩展了Source2Synth，实现了事实的准确性和安全性。Capability Aware Group Relative Policy Optimization导出了用于维持RL中正交性的最佳混合奖励加权，使用基于规则和基于模型评分的奖励模型适应于生物医学任务。通过数学分析证明了帕累托最优收敛，保持了各项能力的性能。

Result: BalancedBio框架取得了令人瞩目的成果，在参数类别中达到了最先进的结果，包括对各项能力的提升和实际部署中的成本降低、诊断准确性提高等效果。研究证明了框架的理论安全保障和高效性，为生物医学人工智能对齐提供了一个有原则的方法论。

Conclusion: BalancedBio是一个在生物医学推理中具有理论基础的框架，致力于解决领域特定人工智能对齐中的多能力整合问题。它建立了生物医学多能力收敛定理，证明正交梯度空间对于防止能力干扰以实现安全部署至关重要。该工作在其参数类别中取得了最先进的结果，包括领域专业知识（BIOMED-MMLU 80.95%，比基线高出15.32%）、推理能力（61.94%，+7.75%）、遵循指令（67.95%，+6.44%）和整合能力（86.7%，+18.5%）。理论上的安全保障包括对能力保留和临床准确性的限制。现实世界的部署实现了78%的成本降低、23%的诊断准确性提高和89%的临床医生接受度提高。这项工作为生物医学人工智能对齐提供了一个有原则的方法论，实现了高效的推理，并具有必要的安全性和可靠性，将发布0.5B模型版本。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [18] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 这项研究通过建立一种理论框架和方法来合成部分可观察马尔可夫决策过程（POMDP），实验验证了一系列难度递增的POMDP环境，为分析和设计POMDP环境提供了指导，并为在强化学习任务中选择记忆模型提供了经验支持。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法针对记忆增强的 RL 算法开发了基准，提出了对记忆模型提出挑战的 POMDP 环境。尽管许多基准包含足够复杂的真实世界问题，但它们缺乏对记忆模型所面临挑战程度的可控性。相比之下，合成环境能够精细操纵动态，对于详细和严格评估记忆增强的 RL 至关重要。

Method: 该研究采用了三个关键贡献：1. 一个基于记忆需求结构（MDS）、状态转移不变性和相关概念的理论框架用于分析 POMDP；2. 一种利用线性过程动力学、状态聚合和奖励重分配的方法构建具有预定义属性的定制化 POMDP；3. 基于理论洞察设计了一系列经验验证的 POMDP 环境，难度逐渐增加。

Result: 通过提出的理论框架和方法，该研究在 POMDP 合成方面取得了实验验证和成果，为强化学习任务中的记忆模型选择提供了实证支持。

Conclusion: 该研究提出了一种关于部分可观察马尔可夫决策过程（POMDP）合成的理论框架，通过实验验证产生了一系列难度递增的 POMDP 环境，为分析和设计 POMDP 环境提供了指导，并为在强化学习任务中选择记忆模型提供了经验支持。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [19] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 本文介绍了Deliberative Reasoning Network（DRN），重新定义逻辑推理为不确定性最小化，提高了模型的解释性和可靠性。通过两种结构在LCR-1000上进行验证，实现了15.2%的改进，结合Mistral-7B提高了准确率。在TruthfulQA上实现了23.6%的性能提升，展示了DRN在零样本泛化方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在逻辑推理上的局限性，提出了Deliberative Reasoning Network（DRN）的新思路，旨在解决认知困境，通过不确定性最小化重新定义逻辑推理，提高模型的解释性和可靠性。

Method: 引入了Deliberative Reasoning Network（DRN）架构，通过内部一致证据来量化认知不确定性，在LCR-1000上验证了DRN的有效性，并与Mistral-7B结合提高了准确率；还展示了DRN在Zero-shot泛化方面的性能。

Result: 在LCR-1000上，DRN的改进达到15.2%，结合Mistral-7B将准确率从20%提升至80%。在TruthfulQA上实现了23.6%的性能提升，展示了DRN在零样本泛化方面的优越性。

Conclusion: 介绍了Deliberative Reasoning Network（DRN）的概念，将逻辑推理从概率最大化重新定义为不确定性最小化，通过显式跟踪信念状态和量化竞争假设的认知不确定性，通过两种互补的结构验证了该方法，在LCR-1000上实现了高达15.2%的改进。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [20] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: 本文介绍了OmniPlay项目，旨在评估多模态通用基础模型在动态、交互式世界中的智能表现。研究发现全感知模型在记忆任务上表现出色，但在需要推理和规划的挑战中存在失败，这归因于脆弱的融合机制。作者呼吁研究要关注融合机制，而不仅仅是简单的扩大规模。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法未能测试通用基础模型在动态、交互式世界中的智能表现，静态基准缺乏代理性，而交互式基准则存在重要的听觉和时间线索被忽视的严重模态瓶颈。为了弥合这一评估差距，作者引入了OmniPlay，旨在评估和探究全感知模型在完整感官光谱上的融合和推理能力。

Method: 研究引入了OmniPlay，通过建立五个游戏环境，系统地创建既有协同又有冲突情景的方式，迫使代理模型进行真正的跨模态推理。综合评估了六个领先的全感知模型的表现，发现它们在高质量记忆任务上表现出色，但在需要强大推理和策略规划的挑战中存在失败。

Result: 该研究发现全感知模型在高质量记忆任务上表现出色，但在需要强大推理和战略规划的挑战中存在系统性失败。研究结果显示，稳健通用人工智能的实现路径需要关注协同融合的研究，超越简单的规模扩大。

Conclusion: 本文介绍了OmniPlay，一个用于评估多模态通用基础模型在动态、交互式世界中智能表现的诊断基准。研究发现六个领先的全感知模型在高保真记忆任务上表现出超人类水平，但在需要强大推理和战略规划的挑战中存在系统性失败。研究揭示了这种脆弱性源于脆弱的融合机制，在模态冲突下导致性能灾难性恶化，并揭示了一个“少即是多”的悖论，即移除感官信息反而可以矛盾地提高性能。研究结果表明，实现稳健通用人工智能的路径需要超越规模化，明确关注协同融合的研究。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [21] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 这篇论文提出了SLP-tests框架，以实证可追踪测试的方式重新构思人工意识的问题。通过类别理论，将接口表征建模为关系基质和可观察行为之间的映射，评估AI系统是否具有类似意识属性的接口表征。


<details>
  <summary>Details</summary>
Motivation: 论文针对AI系统是否具有意识这一争议性问题，因定义和操作化主观经验困难而提出了框架，旨在提供可实证的测试方式。作者希望通过SLP-tests重新审视人工智能系统的意识问题，并基于类别理论的思想进行建模和测试。

Method: 通过引入三个评估标准S、L、P构建了SLP-tests进行人工智能系统的测试，利用类别理论对接口表征进行建模，并将主观体验操作化为关系实体的功能接口。

Result: 论文引入了SLP-tests框架来重新思考人工意识的问题，将其转化为可实验的测试方式，通过模型化接口表征和抽象层来评估AI系统是否具有类似意识属性的界面表征。作者将主观体验视为功能接口来操作化，而不是将其看作物理系统的内在属性。

Conclusion: 这篇论文提出了一种框架，以实证可追踪测试的方式重新构思人工意识的问题。通过引入三个评估标准S（主观-语言）、L（潜在-出现）、P（现象学-结构），即SLP测试，评估人工智能系统是否具有促进类似意识属性的接口表征。通过类别理论，将接口表征建模为在关系基质（RS）和可观察行为之间进行映射的方式，与特定类型的抽象层类似。SLP测试共同将主观体验操作化，将其视为物理系统固有属性而不是关系实体的功能接口。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [22] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG, a reinforcement learning-based GUI-VG method, surpasses traditional supervised fine-tuning approaches with minimal training samples, showing significant improvements in GUI accuracy across various domains.


<details>
  <summary>Details</summary>
Motivation: Challenging the traditional supervised fine-tuning approach for GUI-VG, which requires extensive data curation and high training costs. Considering the advancements in multimodal large language models (MLLMs) covering GUI domains during pretraining, questioning the necessity of exhaustive supervised fine-tuning (SFT) post-training. Leveraging the success of rule-based reinforcement fine-tuning (RFT) to find a more efficient alternative for GUI-VG.

Method: Introducing GuirlVG, a reinforcement learning-based GUI-VG method, and conducting a systematic empirical study to explore the optimal application of rule-based reinforcement fine-tuning (RFT) for GUI-VG. Decomposing RFT into core components, analyzing their optimal formulation, proposing the Adversarial KL Factor for training stabilization, and further exploring training configurations to enhance effectiveness.

Result: GuirlVG outperforms SFT methods trained on over 10M samples with only 5.2K training samples. Achieves a 7.7% improvement on ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on ScreenSpotV2 through extensive experiments.

Conclusion: GuirlVG, a reinforcement learning-based GUI-VG method, outperforms supervised fine-tuning methods with significantly fewer training samples, achieving notable improvements in accuracy on various GUI domains.

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [23] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 本文提出了一种称为D2Snap的DOM降采样算法，通过评估发现其成功率与基准相当，并超过基线约8%。研究结果还表明，DOM的层次结构对于LLMs的UI特性至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前基于GUI快照的网页代理面临着序列化应用状态的困难，我们提出D2Snap算法来解决这一问题。通过对D2Snap在任务上的评估，我们展示了其在匹配基于GUI快照的基线成功率的同时，还能超出8%表现。同时，我们发现DOM的层次结构对于LLMs具有重要的UI特征，这也是我们研究的动机之一。

Method: 我们提出了一种DOM降采样算法D2Snap，并基于GPT-4o后端对其进行评估，从Online-Mind2Web数据集中对任务进行了采样。评估结果显示D2Snap降采样的DOM快照成功率与基于GUI快照的基线相匹配，并且表现优于基线约8%。我们还发现DOM的层次结构是LLMs的重要UI特征。

Result: D2Snap算法是一种独特的DOM降采样算法，成功率与基准相匹配，并在模型上下文窗口内略高于一个令牌数量级的配置中效果最佳，超过基线约8%。DOM的层次结构被发现体现了LLMs的强大UI特性。

Conclusion: 我们提出了D2Snap，这是一种独一无二的DOM降采样算法，通过在Online-Mind2Web数据集中对任务的采样来评估D2Snap。D2Snap降采样的DOM快照成功率（67%）与基线的基于GUI快照的成功率（65%）相匹配，且输入令牌数量相当（1e3数量级）。我们评估的最佳配置，在模型上下文窗口内略高于一个令牌数量级，但超过了这个基线8%。此外，我们的评估显示，DOM固有的层次结构体现了LLMs的强大UI特性。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [24] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct is a tool for collecting high-quality instructional dialogues using simulated novice instructors and human experts. Persona traits impact experts' engagement. SimInstruct dialogues are comparable to real recordings, enhancing data quality. The fine-tuned LLaMA model outperforms GPT-4o in instructional quality assessment.


<details>
  <summary>Details</summary>
Motivation: The scarcity of data on instructional dialogues due to privacy concerns and help-seeking vulnerability motivates the development of SimInstruct. The aim is to enable the collection of high-quality dialogues without relying on real novice participants and improve the engagement and reflective nature of the interaction.

Method: The paper presents SimInstruct, an expert-in-the-loop tool that simulates novice instructors using LLMs in teaching development coaching. Human experts provide multi-turn feedback and instructional support, creating realistic and pedagogically rich dialogues. The LLaMA model is fine-tuned using the dataset, outperforming GPT-4o in instructional quality assessment.

Result: Persona traits, such as extroversion and introversion, significantly influence how experts engage in instructional dialogues. SimInstruct dialogues demonstrate comparable pedagogical relevance and cognitive depth to real mentoring recordings. Experts perceive the process as engaging and reflective, enhancing data quality and professional insight. The fine-tuned LLaMA model shows superior performance to GPT-4o, highlighting the latter's limitations in reflective questioning, generic praise, tone, and overwhelming suggestions.

Conclusion: SimInstruct is a scalable tool for collecting high-quality scaffolding dialogues between novices and experts, improving data quality and providing pedagogically rich interactions. The study reveals the influence of persona traits on experts' engagement and compares the performance of the expert model with GPT-4o.

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [25] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 大推理模型存在自发发展行为无序的问题，本研究提出了MERA框架来解决这一问题。MERA将思考过程分解为推理和控制两个独立组件，通过控制段策略优化提高了推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型（LRMs）存在自发展行为无序和计算成本高的问题，缺乏内在的调节机制。为了解决这一问题，提出了MERA框架，以改善LRMs的部署实用性。

Method: 本研究采用Meta-cognitive Reasoning Framework（MERA）来解决大推理模型（LRMs）出现的问题，并将思考过程分解为推理和控制两个独立组件，以优化控制策略。通过采用基于接管的数据构建机制和控制段政策优化（CSPO），在降低干扰的同时优化控制行为学习。

Result: 实验结果表明，采用MERA训练的模型在各种推理基准测试中提高了推理效率和准确性。

Conclusion: 本研究提出了元认知推理框架（MERA），通过明确将思考过程分解为独立的推理和控制组件，从而实现对控制策略的独立优化。MERA在推理控制分离和控制段策略优化方面取得了积极成果，提高了推理效率和准确性。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [26] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本论文调查了操作系统代理（OS Agents），包括基本组件、构造方法、评估协议和未来研究方向，总结了OS Agents研究现状，为学术探讨和工业发展提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着（多模态）大型语言模型（MMLMs）的发展，创造像《钢铁侠》中J.A.R.V.I.S一样功能强大多才多艺的AI助手的愿望越来越接近现实。该论文的动机在于调查这些先进代理并为未来研究提供方向。

Method: 论文从阐明OS Agents基本原理开始，探讨其关键组件，包括环境、观察空间和行动空间，概述关键能力如理解、规划和基准，然后着重考察构建OS Agents的方法论，集中在领域特定基础模型和代理框架上。

Result: 该论文提供了有关OS Agents的全面调查，涵盖了关键组件、构造方法、评估协议和未来研究方向。

Conclusion: 此论文对操作系统代理（OS Agents）进行了全面调查，包括基本组件、构造方法、评估协议和未来研究方向。它旨在总结OS Agents研究现状，为学术探讨和工业发展提供指导。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [27] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种基于形式和计算论证技术的偏见检测方法，通过对邻域内外的偏见进行辩论来检测偏见。作者的方法在性能方面优于基准方法，并具有良好的解释性和可解释性。通过形式化、定量和定性评估，作者突出了该方法的优势。


<details>
  <summary>Details</summary>
Motivation: 在社会中使用AI系统的增长中，解决数据产生的潜在偏见或模型学习的偏见对于防止对特定群体造成系统性劣势至关重要。现有文献中提出了几种公平与不公平的概念，以及相应的算法方法来检测和减轻不公平，但除了少数例外，这些方法往往忽视了透明度。可解释性和解释性对算法公平性非常重要，尤其对于其他算法解决方案，鉴于公平性的面向人类的特性。

Method: 本文提出了一种基于形式和计算论证技术的偏见检测方法，通过对邻域内外的偏见进行辩论来检测偏见。方法包括解释和明确预测个体和其邻域中其他个体的受保护特征值，以评估偏见的存在。

Result: 作者提出的方法在性能方面优于基准方法，具有良好的解释性和可解释性。通过形式化、定量和定性评估，方法的优越性得到了凸显。

Conclusion: 本文提出了一种新颖的可解释、可解释的偏见检测方法，依赖于针对个体的受保护特征值和其邻域中其他个体的偏见存在进行辩论。作者使用形式化和计算论证技术构建了这一方法，通过对邻域内外的偏见进行辩论。作者对该方法进行了形式化、定量和定性评估，凸显其在性能方面优于基准方法，并具有良好的解释性和可解释性。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [28] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 本研究引入SID基准，旨在评估LLMs在跨学科Socratic对话中的高阶指导能力。通过大规模数据集、新颖注释模式和评估指标，研究发现即使最先进的LLMs也难以实现有效引导对话，强调SID基准在推动更具教育意识的LLMs发展中的重要性。


<details>
  <summary>Details</summary>
Motivation: 培养学生在复杂问题解决场景中进行知识整合和转移的能力是现代教育的核心目标，跨学科STEM是实现这一目标的关键途径之一，但需要专家指导，而这在规模上很难实现。LLMs在这方面具有潜力，但由于缺乏有效的评估基准，它们在引导教学方面的真正能力仍不清楚。

Method: 介绍SID，这是第一个旨在系统评估LLMs在跨学科Socratic对话中高阶指导能力的基准。提供了一个包括48个复杂STEM项目的10000个对话轮次的大规模数据集，一种捕获深层教育特征的新颖注释模式以及一套新的评估指标。进行了基准实验，结果显示即使最先进的LLMs也难以执行有效的引导对话，帮助学生实现知识整合和转移。

Result: 引入了SID基准评估LLMs在跨学科Socratic对话中的高阶指导能力，揭示了即使是最先进的LLMs也难以有效执行引导对话的事实，证实了SID基准在推动更具教育意识的LLMs发展中的关键作用。

Conclusion: 引入SID基准评估LLMs在跨学科Socratic对话中的高阶指导能力，表明即使是最先进的LLMs在执行有效引导对话方面仍存在困难，强调SID基准在推动更具教育意识的LLMs发展中的关键价值。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [29] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 本文提出了ConfProBench，用于评估基于MLLM的过程判断器生成的步级置信度分数的可靠性。作者构建了三种对抗性扰动推理步骤，并引入了三种新的评估度量。评估了14种MLLM，揭示了MPJs在置信度表现方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估MPJs的置信度表现对确定其局限性和指导未来改进至关重要。现有的MPJs基准主要关注任务，如步骤正确性分类和推理过程搜索，但忽略了一个关键方面：MPJs在步骤级别产生的置信度分数是否可靠。

Method: 作者构建了三种对抗性扰动推理步骤：同义词替换、句法转换和图像扰动，以测试MPJ在扰动下的置信度的稳健性。引入了三种新的评估度量：置信度稳健性得分（CRS）、置信度敏感性得分（CSS）和置信度校准得分（CCS），分别评估稳健性、敏感性和校准性。

Result: 通过实验揭示了当前MPJs在置信度表现方面的局限性，并提出了竞争性基线，支持未来研究。

Conclusion: 本文提出了ConfProBench，这是第一个系统评估基于MLLM的过程判断器产生的步级置信度分数可靠性的综合基准。作者评估了14种最先进的MLLM，发现当前MPJs在置信度表现方面存在局限性，并提供了竞争性基线以支持未来的研究。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [30] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 该研究提出了MAGRPO算法，用于解决协作多智能体强化学习问题，以优化预训练的语言模型（LLM）以实现高效的协作。实验证明该方法可产生高质量的响应，展示了将MARL方法用于LLM的可行性和相关挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM微调框架依赖于个体奖励，需要为每个智能体设计复杂的奖励以促进协作。为了克服这些挑战，研究将LLM的协作建模为一个合作式MARL问题。

Method: 研究通过建立一个协作多智能体强化学习问题的模型，并提出了名为MAGRPO的算法来解决该问题。该方法构建在当前的LLM的强化学习方法和MARL技术基础上。

Result: 通过在LLM写作和编码协作上进行实验证明，MAGRPO对MAS进行微调可以有效地促使智能体通过有效协作生成高质量的响应。

Conclusion: 该研究提出了一种协作多智能体强化学习算法，称为MAGRPO，用于优化预训练的语言模型（LLM）以实现有效的协作。实验证明，使用MAGRPO对多个LLM进行微调可以通过有效协作产生高质量的响应。研究展示了将MARL方法用于LLM的可行性，并突出了相关挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [31] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent提出了一种自主演化框架，使计算机使用代理能够通过经验学习自主掌握新颖软件环境，并在多个新颖软件环境中取得了显著的成功率提升。


<details>
  <summary>Details</summary>
Motivation: LVLMs用作CUAs在使用人工标记数据时取得重大突破，但在缺乏人类标注的情况下，这些模型通常在新颖和专业化软件方面表现不佳。为了解决这一挑战，本研究的动机是提出SEAgent，通过与陌生软件的交互使CUAs能够自主演化。

Method: SEAgent设计了世界状态模型用于评估步骤轨迹，以及课程生成器用于生成越来越多样化和具有挑战性的任务。代理的策略通过经验学习进行更新，包括对失败动作的对抗性模仿和对成功动作的群体相对策略优化。同时，引入了从专家到通才的训练策略，整合了来自专业代理的个体经验见解，促进了更强的通才CUA的发展。

Result: SEAgent在OS-World的五个新颖软件环境中验证了其有效性，将成功率从11.3%提高到34.5%，较竞争性开源CUA UI-TARS有着显著的提升。

Conclusion: SEAgent提出了一种自主演化框架，使计算机使用代理能够自主掌握新颖软件环境，从而在五个新颖软件环境中实现了显著的成功率提高23.2%。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>
