<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 83]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models](https://arxiv.org/abs/2509.00058)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.AI

TL;DR: 本文对四种代表性失语症检测方法进行了系统比较分析，发现UDM在精度和临床可解释性方面表现最佳。YOLO-Stutter和FluentNet提供了效率和简单性，但透明度有限；SSDM有潜力但无法完全复现。研究强调了方法之间的权衡，为临床可行的失语症建模指明了未来方向。


<details>
  <summary>Details</summary>
Motivation: 近年来失语症检测方面取得了进展，但临床应用需要不仅仅是准确性，还需要模型具有可控性和解释性。本文旨在对四种代表性方法进行比较分析，提供未来临床可行的失语症建模方向。

Method: 系统比较分析

Result: 通过在多个数据集上进行综合评估和专业临床评估，发现UDM在精度和临床可解释性之间取得了最佳平衡。此外，YOLO-Stutter和FluentNet提供了效率和简单性，但透明度有限；SSDM虽有前景，但在实验中无法完全复现。

Conclusion: 本文通过对四种代表性方法进行系统比较分析，发现在性能、可控性和可解释性三个维度上，UDM在精度和临床可解释性之间取得了最佳平衡。另外，YOLO-Stutter和FluentNet提供了效率和简单性，但透明度有限；SSDM虽有前景，但在实验中无法完全复现。研究强调了竞争方法之间的权衡，并为临床可行的失语症建模指明了未来方向。此外，对每种方法提供了详细的实现见解和实际部署注意事项。

Abstract: Recent advances in dysfluency detection have introduced a variety of modeling
paradigms, ranging from lightweight object-detection inspired networks
(YOLOStutter) to modular interpretable frameworks (UDM). While performance on
benchmark datasets continues to improve, clinical adoption requires more than
accuracy: models must be controllable and explainable. In this paper, we
present a systematic comparative analysis of four representative
approaches--YOLO-Stutter, FluentNet, UDM, and SSDM--along three dimensions:
performance, controllability, and explainability. Through comprehensive
evaluation on multiple datasets and expert clinician assessment, we find that
YOLO-Stutter and FluentNet provide efficiency and simplicity, but with limited
transparency; UDM achieves the best balance of accuracy and clinical
interpretability; and SSDM, while promising, could not be fully reproduced in
our experiments. Our analysis highlights the trade-offs among competing
approaches and identifies future directions for clinically viable dysfluency
modeling. We also provide detailed implementation insights and practical
deployment considerations for each approach.

</details>


### [2] [Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination](https://arxiv.org/abs/2509.00072)
*Terry Jingchen Zhang,Gopal Dev,Ning Wang,Nicole Ni,Wenyuan Jiang,Yinya Huang,Bernhard Schölkopf,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 本文利用无限可扩展的框架合成研究级别的问答，评估大型语言模型在多步推理问题上的表现。结果显示模型在不同知识截断日期附近没有明显性能下降，推测多步推理提供了深层次的复杂性，有助于减少基准测试的污染。开放了代码和数据集以支持可重现性，并倡导优先推理驱动的合成构建基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的能力评估受到数据污染的影响，本文旨在探讨静态基准是否测量了真正的推理能力。通过研究模型在知识截断日期附近的表现，来研究潜在的数据污染情况。

Method: 使用一个可无限扩展的框架合成研究级别的问答，从而评估大型语言模型在多步推理问题上的性能，然后与以往研究进行比较，探讨模型在知识截断日期附近的性能表现。

Result: 没有发现大型语言模型在不同知识截断日期附近的显著性能下降，认为多步推理的综合研究提供了更深层次的复杂性，有效减少基准测试的污染。

Conclusion: 本文通过实证研究使用一个可无限扩展的框架从arXiv论文直接合成研究级别的问答，评估了4个前沿模型在1,643个多步推理问题上的性能。结果显示，在不同大小、开发者和发布日期的模型中，靠近知识截断日期没有明显的性能下降。通过与以往纵向研究进行比较，我们认为我们的综合研究提供了比肤浅记忆更深层次的多步推理，有效地减少了基准测试的污染。我们开放了代码和数据集以支持可重现性，并提倡优先考虑基于推理驱动的合成构建基准，而不仅仅是定期收集新发布的问题。

Abstract: Capability evaluation of large language models (LLMs) is increasingly
shadowed by rising concerns of data contamination that cast doubts on whether
static benchmarks measure genuine reasoning or mere memorization. We present an
empirical study using an infinitely scalable framework to synthesize
research-level QA directly from arXiv papers, harnessing the natural temporal
structure of research publications where performance decay after knowledge
cutoffs may indicate potential contamination. We evaluated 4 frontier model
represented by 2 models of different knowledge cutoff dates per family on 1,643
multi-step reasoning questions synthesized from 20,277 arXiv papers stratified
over 26 months, covering at least 6 months before and after all cutoff dates.
Our results consistently showed a lack of significant performance decay near
knowledge cutoff dates for models of various sizes, developers, and release
dates. We further performed a comparative analysis with previous longitudinal
studies that reported significant post-cutoff performance decay using directly
retrieved questions based on public data. we hypothesize that the multi-step
reasoning required by our synthesis pipeline offered additional complexity that
goes deeper than shallow memorization, which effectively serves a mitigation
strategy against benchmark contamination. We fully open source our code and
dataset to aid reproducibility and advocate for a paradigm shift that
prioritize reasoning-driven synthesis to construct benchmarks over simply
collecting newly released questions periodically.

</details>


### [3] [Language and Experience: A Computational Model of Social Learning in Complex Tasks](https://arxiv.org/abs/2509.00074)
*Cédric Colas,Tracey Mills,Ben Prystawski,Michael Henry Tessler,Noah Goodman,Jacob Andreas,Joshua Tenenbaum*

Main category: cs.AI

TL;DR: 研究提出了一个计算框架，模拟社会学习过程，使代理能够生成建议和解释语言输入，实验证实了语言指导对加速学习和知识传递的重要作用。


<details>
  <summary>Details</summary>
Motivation: 研究探讨了人们如何整合他人的语言指导和直接经验，并思考了AI系统如何实现这一目标。提出了一个计算框架，旨在模拟社会学习过程，探讨语言指导对学习和知识传递的影响，以及如何实现人机协同学习。

Method: 提出了一个计算框架，模拟社会学习为对结构化可执行世界模型的联合概率推理，将预训练语言模型转变为概率模型，使代理能够生成建议和在贝叶斯推理中解释语言输入。通过行为实验和模拟在10个视频游戏中展示了语言指导如何塑造探索和加速学习过程的结果。进行了迭代学习实验，展示了知识如何在人类和模型之间成功转移。

Result: 实验和模拟结果显示，语言指导有助于塑造探索行为、加速学习过程；迭代学习实验展示了知识在人类和模型之间成功转移的可能性。

Conclusion: 研究提出了一个计算框架，将社会学习建模为对结构化可执行世界模型的联合概率推理，通过处理来自感知运动和语言数据，将预训练语言模型转变为一个概率模型，使代理能够为他人生成建议并在贝叶斯推理过程中解释语言输入。实验和模拟结果表明，语言指导可以塑造探索行为，加速学习过程，降低风险交互，并在人类和模型中加快关键发现的速度。通过反复学习实验，探讨知识如何在世代间积累，并展示了人类和模型之间成功的知识转移，揭示了结构化、与语言兼容的表征是如何促进人机协同学习的。

Abstract: The ability to combine linguistic guidance from others with direct experience
is central to human development, enabling safe and rapid learning in new
environments. How do people integrate these two sources of knowledge, and how
might AI systems? We present a computational framework that models social
learning as joint probabilistic inference over structured, executable world
models given sensorimotor and linguistic data. We make this possible by turning
a pretrained language model into a probabilistic model of how humans share
advice conditioned on their beliefs, allowing our agents both to generate
advice for others and to interpret linguistic input as evidence during Bayesian
inference. Using behavioral experiments and simulations across 10 video games,
we show how linguistic guidance can shape exploration and accelerate learning
by reducing risky interactions and speeding up key discoveries in both humans
and models. We further explore how knowledge can accumulate across generations
through iterated learning experiments and demonstrate successful knowledge
transfer between humans and models -- revealing how structured,
language-compatible representations might enable human-machine collaborative
learning.

</details>


### [4] [Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation](https://arxiv.org/abs/2509.00079)
*Andrew G. A. Correa,Ana C. H de Matos*

Main category: cs.AI

TL;DR: 该论文介绍了一种基于熵引导的轻量级测试时间循环的方法，通过引入不确定性触发有针对性的优化过程，使小型模型在质量和成本之间找到平衡。实验证明该方法在实际生产部署中具有很好的应用前景。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于，尽管推理模型通常优于较小的模型，但其成本和延迟较高。作者试图通过引入基于熵引导的轻量级测试时间循环来解决这一问题，实现小型模型在质量和成本之间的平衡。

Method: 论文介绍了基于熵引导的轻量级测试时间循环的方法，利用标记级别的不确定性来触发有针对性的优化过程，提取对数概率，计算Top-k替代品的香农熵，并通过对困惑度、最大标记熵和低置信标记计数的简单OR逻辑触发来生成紧凑的不确定性报告，然后将该不确定性报告传回模型以指导修正编辑。

Result: 实验结果显示，该方法能够在代表性的技术查询中实现小型模型在大约三分之一的成本下达到参考推理模型质量的95％。该方法在大约31％的响应中实现选择性优化，同时将准确性提高了16个百分点。

Conclusion: 该论文提出了一种基于熵引导的轻量级测试时间循环，利用标记级别的不确定性来触发单个有针对性的优化过程，从而在代表性的技术查询中实现小型模型在大约三分之一的成本下达到参考推理模型质量的95％。该方法在大约31％的响应中实现选择性优化，同时将准确性提高了16个百分点。在单次推理方法上取得了显著改进，为生产部署提供了一个有效的折衷方案，平衡了质量和成本之间的关系。

Abstract: Reasoning models often outperform smaller models but at 3--5$\times$ higher
cost and added latency. We present entropy-guided refinement: a lightweight,
test-time loop that uses token-level uncertainty to trigger a single, targeted
refinement pass. We extract logprobs, compute Shannon entropy on top-$k$
alternatives, and apply a simple OR-logic trigger over perplexity, maximum
token entropy, and low-confidence-token count. Unlike approaches that use
entropy only for measurement or decoding, we pass a compact uncertainty report
(tokens, confidences, alternatives, context) back to the model to guide
corrective edits. On representative technical queries across reasoning,
mathematics, and code generation tasks, a small model with our loop approaches
95\% of a reference reasoning model's quality at approximately one-third of the
cost. The method achieves selective refinement on ~31\% of responses while
improving accuracy by 16 percentage points over single-pass inference. We
demonstrate that this uncertainty-aware loop provides an effective middle
ground between single-pass inference and expensive reasoning chains, making it
practical for production deployments where both quality and cost matter.

</details>


### [5] [Wrong Face, Wrong Move: The Social Dynamics of Emotion Misperception in Agent-Based Models](https://arxiv.org/abs/2509.00080)
*David Freire-Obregón*

Main category: cs.AI

TL;DR: 研究探讨了感知准确性对情绪和空间行为的影响。低准确性分类器会导致信任减少、情绪解体和社会组织混乱，高准确性分类器则促进情绪聚集和对情绪干扰的抵抗能力。研究强调了情绪识别中的偏见或不准确性可能扭曲社会过程并破坏情感整合。


<details>
  <summary>Details</summary>
Motivation: 人类对于感知和回应他人情绪的能力是理解社会行为的基础。本研究旨在探讨感知准确性对情绪和空间行为的影响，以揭示情绪识别中的偏见或不准确性可能带来的社会影响。

Method: 实验中，研究者通过在代理人中使用不同准确性的情绪分类器来研究感知准确性对情绪和空间行为的影响。代理人根据其分类器在JAFFE（低）、CK+（中）、或KDEF（高）数据集上进行训练，并在2D环形晶格上相互通信。他们根据感知到的他人情绪状态做出行为反应，向感知到的积极情绪移动，远离负面情绪。研究还进行了一系列关于同质和异质群体以及重复情绪冲击的实验。

Result: 在实验中发现，低准确性分类器会导致信任减少、情绪解体和社会组织混乱，而高准确性分类器则会促进情绪聚集和对情绪干扰的抵抗能力。即使在情感中性的情况下，错误感知也足以产生分离和社会凝聚力的解体。

Conclusion: 研究发现，感知准确性对个体对他人情绪的理解和空间行为产生重要影响。低准确性的分类器会导致信任减少、情绪解体和社会秩序紊乱。相反，高准确性分类器可促进情绪聚集和对情绪干扰的抵抗力。情绪识别中的偏见或不准确性可能会显著扭曲社会过程并破坏情感整合。

Abstract: The ability of humans to detect and respond to others' emotions is
fundamental to understanding social behavior. Here, agents are instantiated
with emotion classifiers of varying accuracy to study the impact of perceptual
accuracy on emergent emotional and spatial behavior. Agents are visually
represented with face photos from the KDEF database and endowed with one of
three classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high)
datasets. Agents communicate locally on a 2D toroidal lattice, perceiving
neighbors' emotional state based on their classifier and responding with
movement toward perceived positive emotions and away from perceived negative
emotions. Note that the agents respond to perceived, instead of ground-truth,
emotions, introducing systematic misperception and frustration. A battery of
experiments is carried out on homogeneous and heterogeneous populations and
scenarios with repeated emotional shocks. Results show that low-accuracy
classifiers on the part of the agent reliably result in diminished trust,
emotional disintegration into sadness, and disordered social organization. By
contrast, the agent that develops high accuracy develops hardy emotional
clusters and resilience to emotional disruptions. Even in emotionally neutral
scenarios, misperception is enough to generate segregation and disintegration
of cohesion. These findings underscore the fact that biases or imprecision in
emotion recognition may significantly warp social processes and disrupt
emotional integration.

</details>


### [6] [Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)
*Ephraiem Sarabamoun*

Main category: cs.AI

TL;DR: 研究探讨了本地开源集成辩论在提高与人类价值观对齐的推理能力方面的效果。通过150场辩论的研究发现，集成辩论在各方面表现均优于单一模型基线，特别是在推理深度和论据质量上有显著提高。提供了代码、提示和辩论数据集，为集成对齐评估奠定了可访问且可重复的基础。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在高风险决策中扮演着越来越重要的角色，与人类价值观的一致性至关重要。依赖专有 API 限制了可重现性和广泛参与。因此，本研究旨在探讨是否本地开源集成辩论可以提高对齐导向推理的能力。

Method: 研究采用本地开源集成辩论，在 15 个场景和五种集成配置中进行了总结。评估了集成辩论在对齐导向推理方面的效果，并使用 7 点标准对结果进行评估。

Result: 在研究中发现，集成辩论在提高推理深度、论据质量、真实性和人文增强方面取得了显著改善，相对于单一模型基线表现更好。

Conclusion: 研究表明，本地开源集成辩论可以改善与人类价值观对齐的推理能力。在 150 场辩论中，集成辩论在 7 点标准上表现优于单一模型基线（总体：3.48 vs. 3.13），推理深度和论据质量上的最大增益分别为 19.4% 和 34.1%。对于真实性和人文增强最为明显的改善，真实性得分提高 1.25 分，人文增强提高 0.80 分。提供了代码、提示和辩论数据集，为基于集成的对齐评估提供了可访问和可重复的基础。

Abstract: As large language models (LLMs) take on greater roles in high-stakes
decisions, alignment with human values is essential. Reliance on proprietary
APIs limits reproducibility and broad participation. We study whether local
open-source ensemble debates can improve alignmentoriented reasoning. Across
150 debates spanning 15 scenarios and five ensemble configurations, ensembles
outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),
with the largest gains in reasoning depth (+19.4%) and argument quality
(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human
enhancement (+0.80). We provide code, prompts, and a debate data set, providing
an accessible and reproducible foundation for ensemble-based alignment
evaluation.

</details>


### [7] [MODE: Mixture of Document Experts for RAG](https://arxiv.org/abs/2509.00100)
*Rahul Anand*

Main category: cs.AI

TL;DR: MODE is a lightweight alternative to Retrieval-Augmented Generation for small and medium corpora, replacing fine-grained nearest-neighbor search with cluster-and-route retrieval. It improves answer quality, reduces retrieval time, and offers simplicity, speed, and topical focus for domain-specific collections.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a practical solution for small and domain-specific collections in Retrieval-Augmented Generation, where the reliance on large vectors and cross-encoders may be excessive. MODE aims to offer simplicity, speed, and topical focus for such corpora.

Method: The paper introduces MODE, which embeds documents, groups them into semantically coherent clusters, and represents them with cached centroids. At query time, routing to top centroids is performed to retrieve context only within those clusters, eliminating the need for large vector databases and reranking, thus reducing latency.

Result: MODE matches or exceeds a dense-retrieval baseline in answer quality while reducing end-to-end retrieval time on HotpotQA and SQuAD corpora with 100-500 chunks. Ablations demonstrate the control of recall/precision trade-off through cluster granularity and multi-cluster routing, with tighter clusters improving downstream accuracy.

Conclusion: MODE (Mixture of Document Experts) is a lightweight alternative to Retrieval-Augmented Generation that replaces fine-grained nearest-neighbor search with cluster-and-route retrieval, showing improved answer quality and reduced end-to-end retrieval time for small and medium corpora.

Abstract: Retrieval-Augmented Generation (RAG) often relies on large vector databases
and cross-encoders tuned for large-scale corpora, which can be excessive for
small, domain-specific collections. We present MODE (Mixture of Document
Experts), a lightweight alternative that replaces fine-grained nearest-neighbor
search with cluster-and-route retrieval. Documents are embedded, grouped into
semantically coherent clusters, and represented by cached centroids. At query
time, we route to the top centroid(s) and retrieve context only within those
clusters, eliminating external vector-database infrastructure and reranking
while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,
MODE matches or exceeds a dense-retrieval baseline in answer quality while
reducing end-to-end retrieval time. Ablations show that cluster granularity and
multi-cluster routing control the recall/precision trade-off, and that tighter
clusters improve downstream accuracy. MODE offers a practical recipe for small
and medium corpora where simplicity, speed, and topical focus matter.

</details>


### [8] [Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems](https://arxiv.org/abs/2509.00115)
*Manish Shukla*

Main category: cs.AI

TL;DR: 研究介绍了自主人工智能系统框架和算法Adaptive Multi-Dimensional Monitoring (AMDM)，用于监控和检测异常。AMDM算法通过实验和模拟在减少异常检测延迟和降低误报率方面取得积极成果。研究填补了早期研究中框架和指标算法化和实证数据不足的空白。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域中，自主人工智能系统如何监控和检测异常是一个关键问题。前期研究提出了框架和初步指标，但缺乏算法实现和实证数据。因此，本研究的动机在于填补这一空白，提出一种算法化的框架以及验证实验结果。

Method: 本研究重新审视最近的基准测试和工业部署，发现技术指标仍然主导评估。提出了Adaptive Multi-Dimensional Monitoring (AMDM)算法，通过归一化异构指标、应用每个维度的指数加权移动平均阈值和通过马氏距离执行联合异常检测。研究通过模拟和真实世界实验验证算法的有效性。

Result: 通过实验和模拟，Adaptive Multi-Dimensional Monitoring (AMDM)算法在减少异常检测延迟和降低误报率方面取得了积极成果。与静态阈值相比，AMDM可以将异常检测延迟从12.3秒减少到5.6秒，并将误报率从4.5%降低到0.9%。研究还提供了对比表格和ROC/PR曲线，重新分析案例研究以揭示缺失指标。

Conclusion: 该研究提出了一种自主人工智能系统的框架和算法，名为Adaptive Multi-Dimensional Monitoring (AMDM)，用于监控和检测异常。通过实验和模拟，AMDM算法在减少异常检测延迟和降低误报率方面取得了积极成果。研究重点在于填补基础研究中提出的框架和指标算法化和实证证据不足的空白。

Abstract: Agentic artificial intelligence (AI) -- multi-agent systems that combine
large language models with external tools and autonomous planning -- are
rapidly transitioning from research laboratories into high-stakes domains. Our
earlier "Basic" paper introduced a five-axis framework and proposed preliminary
metrics such as goal drift and harm reduction but did not provide an
algorithmic instantiation or empirical evidence. This "Advanced" sequel fills
that gap. First, we revisit recent benchmarks and industrial deployments to
show that technical metrics still dominate evaluations: a systematic review of
84 papers from 2023--2025 found that 83% report capability metrics while only
30% consider human-centred or economic axes [2]. Second, we formalise an
Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises
heterogeneous metrics, applies per-axis exponentially weighted moving-average
thresholds and performs joint anomaly detection via the Mahalanobis distance.
Third, we conduct simulations and real-world experiments. AMDM cuts
anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and
reduces false-positive rates from 4.5% to 0.9% compared with static thresholds.
We present a comparison table and ROC/PR curves, and we reanalyse case studies
to surface missing metrics. Code, data and a reproducibility checklist
accompany this paper to facilitate replication.

</details>


### [9] [Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning](https://arxiv.org/abs/2509.00125)
*Ang Li,Zhihang Yuan,Yang Zhang,Shouda Liu,Yisen Wang*

Main category: cs.AI

TL;DR: Difficulty Aware Certainty guided Exploration (DACE) is a novel RL algorithm that addresses the limitations of Reinforcement Learning with Verifiable Feedback (RLVF) in enhancing the reasoning abilities of Large Language Models (LLMs) by dynamically balancing the exploration-exploitation trade-off based on task difficulty and self-certainty. Experiments show that DACE outperforms strong baselines on challenging mathematical reasoning benchmarks, achieving higher accuracy and more robust performance during scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper lies in addressing the limitations of Reinforcement Learning with Verifiable Feedback (RLVF) in providing granular guidance on the reasoning process for Large Language Models (LLMs). The inability of RLVF to distinguish between high quality and inefficient solutions, as well as the lack of learning from different failure types, hinders efficient learning. By leveraging the correlation between task difficulty, self-certainty, and solution quality, the paper aims to improve the efficiency and effectiveness of learning for LLMs.

Method: The paper introduces Difficulty Aware Certainty guided Exploration (DACE) as a novel RL algorithm that leverages the correlation between LLMs' self-certainty, task difficulty, and solution quality. DACE dynamically balances the exploration-exploitation trade-off by penalizing high certainty for difficult tasks and rewarding high certainty for easier tasks. The algorithm assesses task difficulty online based on the policy's success rate and modulates intrinsic rewards accordingly.

Result: Experiments conducted on challenging mathematical reasoning benchmarks (AIME, MATH) demonstrate that the DACE algorithm significantly outperforms strong baselines. DACE-trained models achieve higher accuracy and exhibit more robust performance when scaling test-time compute. This validates that the adaptive approach of DACE fosters effective exploration without compromising precision.

Conclusion: Difficulty Aware Certainty guided Exploration (DACE) is a novel RL algorithm that effectively addresses the limitations of Reinforcement Learning with Verifiable Feedback (RLVF) in enhancing the reasoning abilities of Large Language Models (LLMs). The DACE algorithm dynamically balances the exploration-exploitation trade-off by assessing task difficulty and modulating intrinsic rewards based on the model's self-certainty, resulting in improved performance on challenging mathematical reasoning benchmarks.

Abstract: Reinforcement Learning with Verifiable Feedback (RLVF) has become a key
technique for enhancing the reasoning abilities of Large Language Models
(LLMs). However, its reliance on sparse, outcome based rewards, which only
indicate if a final answer is correct or not, fails to provide granular
guidance on the reasoning process itself. This limitation hinders efficient
learning, as the model cannot distinguish between high quality and inefficient
solutions, nor can it learn effectively from different types of failures. To
address this, we observe that an LLMs self-certainty often correlates with task
difficulty and solution quality. We introduce Difficulty Aware Certainty guided
Exploration (DACE), a novel RL algorithm that leverages this insight to
dynamically balance the exploration exploitation trade-off. DACE assesses task
difficulty online based on the policys success rate. It then uses this signal
to modulate an intrinsic reward: for difficult tasks where the model is
struggling, DACE encourages exploration by penalizing high certainty; for
easier tasks, it encourages learning efficiency by rewarding high certainty.
Experiments on challenging mathematical reasoning benchmarks (AIME, MATH) show
that DACE significantly outperforms strong baselines. The DACE-trained models
not only achieve higher accuracy but also demonstrate more robust performance
when scaling test-time compute, validating that our adaptive approach fosters
effective exploration without sacrificing precision.

</details>


### [10] [Optimizing Health Coverage in Ethiopia: A Learning-augmented Approach and Persistent Proportionality Under an Online Budget](https://arxiv.org/abs/2509.00135)
*Davin Choo,Yohai Trabelsi,Fentabil Getnet,Samson Warkaye Lamma,Wondesen Nigatu,Kasahun Sime,Lisa Matay,Milind Tambe,Stéphane Verguet*

Main category: cs.AI

TL;DR: 本研究开发了一个名为HARP的工具，基于优化框架为埃塞俄比亚的卫生系统强化提供决策支持。通过两种算法实现单步和多步规划，在与埃塞俄比亚机构合作的实证研究中展示了工具的有效性。结果显示工具能够有效指导卫生系统规划，最大化人口覆盖率，并考虑了预算不确定性和区域比例目标。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部正致力于强化卫生岗位以扩大对基本医疗服务的获取，但受限预算和其他竞争性优先事项的影响，每年只能实施一小部分卫生系统强化工作，因此需要一个优化框架来指导埃塞俄比亚各地区的优先级确定。

Method: 本研究开发了一个名为Health Access Resource Planner（HARP）的工具，基于决策支持优化框架，通过两种算法实现：（i）学习增强方法，改进专家推荐的单步方法；（ii）贪婪算法，用于多步规划，且具有强大的最坏情况逼近估计。

Result: 研究结果表明，通过合作开发的HARP工具在实践中展现了较好的效果，能够在预算不确定性下最大化人口覆盖率，并满足特定区域的比例目标。

Conclusion: 本研究为埃塞俄比亚的卫生系统强化提出了一个基于优化框架的工具HARP，旨在最大化人口覆盖率，并在预算不确定性的情况下满足特定区域的比例目标。通过与埃塞俄比亚公共卫生研究所和卫生部的合作，在三个地区的各种规划情景下验证了方法的实证有效性。

Abstract: As part of nationwide efforts aligned with the United Nations' Sustainable
Development Goal 3 on Universal Health Coverage, Ethiopia's Ministry of Health
is strengthening health posts to expand access to essential healthcare
services. However, only a fraction of this health system strengthening effort
can be implemented each year due to limited budgets and other competing
priorities, thus the need for an optimization framework to guide prioritization
across the regions of Ethiopia. In this paper, we develop a tool, Health Access
Resource Planner (HARP), based on a principled decision-support optimization
framework for sequential facility planning that aims to maximize population
coverage under budget uncertainty while satisfying region-specific
proportionality targets at every time step. We then propose two algorithms: (i)
a learning-augmented approach that improves upon expert recommendations at any
single-step; and (ii) a greedy algorithm for multi-step planning, both with
strong worst-case approximation estimation. In collaboration with the Ethiopian
Public Health Institute and Ministry of Health, we demonstrated the empirical
efficacy of our method on three regions across various planning scenarios.

</details>


### [11] [Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)](https://arxiv.org/abs/2509.00184)
*Alexandru Baltag,Malvin Gattinger,Djanira Gomes*

Main category: cs.AI

TL;DR: 通过扩展基于证据的信念和可犯错误知识的拓扑语义，研究了群体知识和群体信念概念，展示了对群体证据的逻辑的可决定性及动态证据共享运算符的逻辑公理化。


<details>
  <summary>Details</summary>
Motivation: 研究群体知识和群体信念的概念，以及扩展具有动态证据共享运算符的语言，并完全系统化对应的逻辑，展示其与静态基础逻辑的相互表达性。

Method: 通过扩展基于证据的信念和可犯错误知识的拓扑语义，将多主体证据模型的虚拟群体知识和群体信念概念纳入研究范围。

Result: 展示了对群体证据的逻辑（包括“硬”和“软”证据）以及特定片段（群体知识和群体信念的逻辑）的可决定性。扩展了语言与动态证据共享运算符，对相应逻辑进行完整的公理化。

Conclusion: 完全公理化了群体证据的逻辑，展示了决策的可能性，并对群体知识和信念的逻辑进行了拓展。

Abstract: We study notions of (virtual) group knowledge and group belief within
multi-agent evidence models, obtained by extending the topological semantics of
evidence-based belief and fallible knowledge from individuals to groups. We
completely axiomatize and show the decidability of the logic of ("hard" and
"soft") group evidence, and do the same for an especially interesting fragment
of it: the logic of group knowledge and group belief. We also extend these
languages with dynamic evidence-sharing operators, and completely axiomatize
the corresponding logics, showing that they are co-expressive with their static
bases.

</details>


### [12] [HiVA: Self-organized Hierarchical Variable Agent via Goal-driven Semantic-Topological Evolution](https://arxiv.org/abs/2509.00189)
*Jinzhou Tang,Jusheng Zhang,Qinhan Lv,Sidi Liu,Jing Yang,Chengpei Tang,Keze Wang*

Main category: cs.AI

TL;DR: 本文引入了HiVA框架，通过STEV算法优化语义-拓扑空间，使用文本梯度作为反向传播的替代。实验证明HiVA在多个领域中提高了任务准确性和资源效率。


<details>
  <summary>Details</summary>
Motivation: 传统范式面临着固定工作流需要手动重新配置和灵活的反应循环无法将推理进展提炼为可传递的结构之间的关键折衷，为了克服这一挑战提出了HiVA框架。

Method: 引入了Hierarchical Variable Agent (HiVA)框架，利用Semantic-Topological Evolution (STEV)算法，优化语义-拓扑空间，使用文本梯度作为反向传播的离散领域替代品。包括多臂赌博机-infused前向路由、从环境反馈生成诊断梯度以及协调更新等过程。

Result: 实验表明，HiVA相比现有基准在多个领域中实现了5-10%的任务准确性提升和资源效率改进。

Conclusion: 引入了HiVA，一种将Agent工作流建模为自组织图形的新框架，使用STEV算法优化混合语义-拓扑空间，利用文本梯度作为反向传播的离散领域替代品。实验证明，在对话、编码、长文本问答、数学和Agent基准测试中，HiVA相比现有基准提高了5-10%的任务准确性并提高了资源效率，确立了HiVA在自主任务执行中的有效性。

Abstract: Autonomous agents play a crucial role in advancing Artificial General
Intelligence, enabling problem decomposition and tool orchestration through
Large Language Models (LLMs). However, existing paradigms face a critical
trade-off. On one hand, reusable fixed workflows require manual reconfiguration
upon environmental changes; on the other hand, flexible reactive loops fail to
distill reasoning progress into transferable structures. We introduce
Hierarchical Variable Agent (HiVA), a novel framework modeling agentic
workflows as self-organized graphs with the Semantic-Topological Evolution
(STEV) algorithm, which optimizes hybrid semantic-topological spaces using
textual gradients as discrete-domain surrogates for backpropagation. The
iterative process comprises Multi-Armed Bandit-infused forward routing,
diagnostic gradient generation from environmental feedback, and coordinated
updates that co-evolve individual semantics and topology for collective
optimization in unknown environments. Experiments on dialogue, coding,
Long-context Q&A, mathematical, and agentic benchmarks demonstrate improvements
of 5-10% in task accuracy and enhanced resource efficiency over existing
baselines, establishing HiVA's effectiveness in autonomous task execution.

</details>


### [13] [Universal Deep Research: Bring Your Own Model and Strategy](https://arxiv.org/abs/2509.00244)
*Peter Belcak,Pavlo Molchanov*

Main category: cs.AI

TL;DR: Universal Deep Research (UDR) is a versatile agentic system that enables users to build custom deep research strategies without extra training. It offers minimal, expansive, and intensive research strategies and a user interface for experimentation.


<details>
  <summary>Details</summary>
Motivation: Current deep research tools are limited in their fixed research strategies and tool choices. The motivation behind UDR is to provide a versatile system that allows users to create, edit, and refine custom deep research strategies without requiring additional training or finetuning.

Method: Introducing Universal Deep Research (UDR) as a generalist system that wraps around any language model, enabling users to develop their own custom research strategies. Equipped the system with example research strategies and a user interface for experimentation.

Result: The result is a versatile agentic system, UDR, that empowers users to tailor their deep research strategies according to their needs without the constraints of preset tools or strategies.

Conclusion: Universal Deep Research (UDR) is introduced as a generalist agentic system that allows users to create custom deep research strategies without the need for additional training or finetuning. The system showcases minimal, expansive, and intensive research strategies and provides a user interface for experimentation.

Abstract: Deep research tools are among the most impactful and most commonly
encountered agentic systems today. We observe, however, that each deep research
agent introduced so far is hard-coded to carry out a particular research
strategy using a fixed choice of tools. We introduce Universal Deep Research
(UDR), a generalist agentic system that wraps around any language model and
enables the user to create, edit, and refine their own entirely custom deep
research strategies without any need for additional training or finetuning. To
showcase the generality of our system, we equip UDR with example minimal,
expansive, and intensive research strategies, and provide a user interface to
facilitate experimentation with the system.

</details>


### [14] [Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents](https://arxiv.org/abs/2509.00251)
*Rimom Costa*

Main category: cs.AI

TL;DR: ILWS improves large language models by using curated system instructions as external pseudo-parameters updated via reflection and user feedback. It outperformed existing methods in enterprise support, achieving higher throughput and reducing hallucinations. ILWS also showed promise in dynamic domains like legal, medical, and engineering by providing adaptive reasoning, tool creation, and low-latency deployment capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing methods like retrieval-augmented generation (RAG) and fine-tuning for adding new knowledge to large language models have limitations in terms of latency, engineering overhead, integration of facts, brittleness in prompt engineering, and risks of catastrophic forgetting. ILWS aims to address these challenges by providing a more efficient and effective way to update the model with new knowledge.

Method: Instruction-Level Weight Shaping (ILWS) is introduced, where curated system instructions act as pseudo-parameters updated after each session via reflection and user feedback. A Reflection Engine diagnoses successes and failures, proposing typed deltas over instructions, user preferences, and tools. Deltas are version-controlled, evaluated with star ratings, and distilled into parameters to convert prompt-space improvements into weight-space without downtime.

Result: ILWS increased throughput and reduced audited hallucinations significantly in enterprise support settings compared to traditional methods. In a proof of concept in Adobe Commerce Cloud, ILWS achieved higher ticket resolution rates and lower time per ticket, along with autonomous instruction updates and optional tool synthesis. The method demonstrated success in dynamic domains requiring adaptive reasoning, tool creation, and low-latency deployment.

Conclusion: ILWS is proposed as a method to enhance large language models by using curated system instructions as external pseudo-parameters updated via reflection and user feedback. It improved throughput and reduced hallucinations significantly compared to existing methods in enterprise support and demonstrated success in dynamic domains like legal, medical, and engineering.

Abstract: Large language models (LLMs) are fluent but largely static after
pre-training; new or shifting knowledge is typically added with
retrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and
engineering overhead and often fails to integrate facts; prompt engineering is
brittle and can conflict with prior knowledge; fine-tuning is costly and risks
catastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS):
curated system instructions act as external, auditable pseudo-parameters
updated after each session via reflection and user feedback. A Reflection
Engine inspects conversation traces, diagnoses reasoning successes and
failures, and proposes typed deltas $\Delta K=(\Delta S,\Delta U,\Delta T)$
over instructions, user preferences, and tools. Deltas are version-controlled,
evaluated with a sliding window of 1-5 star ratings, auto-repaired on first
failure, and rolled back on repeated failure. When an edit budget crosses a
threshold, the agent compiles a rating-weighted synthetic set and distills
matured instruction-space gains into parameters, converting prompt-space
improvements into weight-space without downtime. ILWS makes explicit the
low-rank shaping induced by context in transformer blocks, preserves
governance, and removes per-call retrieval. In enterprise support it increased
throughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen
baseline. In an Adobe Commerce Cloud proof of concept "L0 Support", it achieved
4-5x more tickets per hour and about 80% lower time per ticket, with autonomous
instruction updates and optional tool synthesis. Because ILWS operates at the
instruction layer until controlled distillation, it generalizes to dynamic
domains (legal, medical, engineering) requiring adaptive reasoning, tool
creation, and low-latency deployment.

</details>


### [15] [SHERPA: A Model-Driven Framework for Large Language Model Execution](https://arxiv.org/abs/2509.00272)
*Boqi Chen,Kua Chen,José Antonio Hernández López,Gunter Mussbacher,Dániel Varró,Amir Feizpour*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在处理复杂任务时缺乏结构化推理能力，本文提出了一种名为SHERPA的模型驱动框架，通过状态机控制实现对LLMs行为的精细控制，提高了LLMs在各种任务上的性能表现。实验证实了SHERPA框架对LLMs输出质量的改善效果。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂任务上缺乏结构化推理能力，而多步提示方法缺乏控制LLMs行为的通用机制。针对这一问题，本文提出了SHERPA框架，旨在改善LLMs在复杂任务上的表现，特别是那些缺乏训练数据但拥有明确领域最佳实践的任务。

Method: 提出了SHERPA框架，通过将领域特定最佳实践显式地纳入分层状态机来改善LLMs性能。通过结构化LLMs执行过程，SHERPA通过基于机器学习的方法（包括LLMs）实现对LLMs行为的更精细控制。进行了系统评估以比较不同状态机配置与没有状态机的基准方法之间的差异。

Result: 通过实验证明SHERPA框架在复杂任务上的有效性，提高了LLMs在代码生成、类名称生成和问答等任务上的性能。系统评估结果显示，整合状态机显著改善了LLMs输出质量。

Conclusion: 提出了一种名为SHERPA的模型驱动框架，通过将领域特定最佳实践显式地纳入分层状态机来改善大型语言模型（LLMs）在复杂任务上的表现。展示了SHERPA在代码生成、类名称生成和问答等任务上的适用性，并证明了其在改善性能方面的有效性。系统评估结果表明，整合设计良好的状态机显着提高了LLMs输出的质量，尤其适用于缺乏训练数据但拥有明确定义人类最佳实践的复杂任务。

Abstract: Recently, large language models (LLMs) have achieved widespread application
across various fields. Despite their impressive capabilities, LLMs suffer from
a lack of structured reasoning ability, particularly for complex tasks
requiring domain-specific best practices, which are often unavailable in the
training data. Although multi-step prompting methods incorporating human best
practices, such as chain-of-thought and tree-of-thought, have gained
popularity, they lack a general mechanism to control LLM behavior. In this
paper, we propose SHERPA, a model-driven framework to improve the LLM
performance on complex tasks by explicitly incorporating domain-specific best
practices into hierarchical state machines. By structuring the LLM execution
processes using state machines, SHERPA enables more fine-grained control over
their behavior via rules or decisions driven by machine learning-based
approaches, including LLMs. We show that SHERPA is applicable to a wide variety
of tasks-specifically, code generation, class name generation, and question
answering-replicating previously proposed approaches while further improving
the performance. We demonstrate the effectiveness of SHERPA for the
aforementioned tasks using various LLMs. Our systematic evaluation compares
different state machine configurations against baseline approaches without
state machines. Results show that integrating well-designed state machines
significantly improves the quality of LLM outputs, and is particularly
beneficial for complex tasks with well-established human best practices but
lacking data used for training LLMs.

</details>


### [16] [SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces](https://arxiv.org/abs/2509.00287)
*Brian Wang,Mani Srivastava*

Main category: cs.AI

TL;DR: 该论文介绍了SIGMUS系统，用于在多模态城市空间中进行知识图谱语义集成。通过大型语言模型（LLMs），SIGMUS可以识别城市空间中不同数据源之间的关系，并整合相关事件数据，在新闻文章文本、闭路电视图像、空气质量、天气和交通测量等数据源之间建立联系。SIGMUS能够有效生成知识图谱，帮助理解和预测城市事件。


<details>
  <summary>Details</summary>
Motivation: 现代城市空间中存在着大量多模态传感器，产生大量多模态数据。这些数据可以用于识别和推理城市景观中发生的重要事件，如重大紧急事件、文化社会活动以及自然灾害。然而，这些数据可能分散在多个来源，由于依赖人工驱动的推理来识别对应于事件的多模态数据之间的关系，以及理解定义事件的不同组成部分，这种整合可能存在困难。因此，需要一种方法来有效地整合这些数据源。

Method: 论文使用大型语言模型（LLMs）构建了一个用于在多模态城市空间中进行知识图谱语义集成的系统SIGMUS。该系统能够识别不同数据源之间的关系，并将相关证据和观察整合为知识图谱。

Result: SIGMUS系统能够成功识别城市空间中不同数据源之间的关系，并整合相关的事件数据，形成知识图谱。研究发现，该系统能够在新闻文章文本、闭路电视图像、空气质量、天气和交通测量等多个数据源之间建立合理的联系，与同时发生在同一时间和地点的相关事件。

Conclusion: 该论文提出了一个用于在多模态城市空间中进行知识图谱语义集成的系统SIGMUS。通过使用大型语言模型（LLMs），SIGMUS能够识别城市空间中发生的事件之间的关系，并整合来自不同模态的数据，以组织与事件相关的证据和观察。研究发现，该系统能够在新闻文章文本、闭路电视图像、空气质量、天气和交通测量等不同数据源之间建立合理的联系，与同时发生在同一时间和地点的相关事件。

Abstract: Modern urban spaces are equipped with an increasingly diverse set of sensors,
all producing an abundance of multimodal data. Such multimodal data can be used
to identify and reason about important incidents occurring in urban landscapes,
such as major emergencies, cultural and social events, as well as natural
disasters. However, such data may be fragmented over several sources and
difficult to integrate due to the reliance on human-driven reasoning for
identifying relationships between the multimodal data corresponding to an
incident, as well as understanding the different components which define an
incident. Such relationships and components are critical to identifying the
causes of such incidents, as well as producing forecasting the scale and
intensity of future incidents as they begin to develop. In this work, we create
SIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal
Urban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary
world knowledge for identifying relationships between incidents occurring in
urban spaces and data from different modalities, allowing us to organize
evidence and observations relevant to an incident without relying and
human-encoded rules for relating multimodal sensory data with incidents. This
organized knowledge is represented as a knowledge graph, organizing incidents,
observations, and much more. We find that our system is able to produce
reasonable connections between 5 different data sources (new article text, CCTV
images, air quality, weather, and traffic measurements) and relevant incidents
occurring at the same time and location.

</details>


### [17] [NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks](https://arxiv.org/abs/2509.00446)
*Yen-Che Chien,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.AI

TL;DR: 研究介绍了NEWSAGENT基准测试，评估了代理系统在自动搜索、选择、编辑和改写新闻文章方面的能力。研究发现代理系统在检索相关事实方面表现良好，但在规划和叙事整合方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 近期自主数字代理的发展显示了代理系统在结构化任务方面的潜力，但目前还不清楚代理系统在提高多模态网络数据生产力方面的实际效果。本研究探讨了代理系统能否改善新闻产生的生产力，该领域需要从多模态原始内容进行迭代规划、解释和背景推理，形成结构化新闻。

Method: 介绍了NEWSAGENT基准测试，评估代理如何自动搜索可用原始内容，并将选择的信息编辑和改写，以形成新闻文章。代理需要识别叙事视角，基于关键词查询问题，检索历史背景，并生成完整的文章。通过在NEWSAGENT上评估开源和闭源的LLM模型，研究显示代理可以检索相关事实，但在规划和叙事整合方面存在困难。

Result: 通过NEWSAGENT基准测试，研究表明代理系统能够检索相关事实，但在规划和叙事整合方面存在困难。

Conclusion: 研究表明代理系统能够检索相关事实，但在规划和叙事整合方面存在困难。NEWSAGENT作为一个现实的测试基地，可用于评估代理系统在多模态网络数据处理方面的能力，从而提高现实世界的生产力。

Abstract: Recent advances in autonomous digital agents from industry (e.g., Manus AI
and Gemini's research mode) highlight potential for structured tasks by
autonomous decision-making and task decomposition; however, it remains unclear
to what extent the agent-based systems can improve multimodal web data
productivity. We study this in the realm of journalism, which requires
iterative planning, interpretation, and contextual reasoning from multimodal
raw contents to form a well structured news. We introduce NEWSAGENT, a
benchmark for evaluating how agents can automatically search available raw
contents, select desired information, and edit and rephrase to form a news
article by accessing core journalistic functions. Given a writing instruction
and firsthand data as how a journalist initiates a news draft, agents are
tasked to identify narrative perspectives, issue keyword-based queries,
retrieve historical background, and generate complete articles. Unlike typical
summarization or retrieval tasks, essential context is not directly available
and must be actively discovered, reflecting the information gaps faced in
real-world news writing. NEWSAGENT includes 6k human-verified examples derived
from real news, with multimodal contents converted to text for broad model
compatibility. We evaluate open- and closed-sourced LLMs with commonly-used
agentic frameworks on NEWSAGENT, which shows that agents are capable of
retrieving relevant facts but struggling with planning and narrative
integration. We believe that NEWSAGENT serves a realistic testbed for iterating
and evaluating agent capabilities in terms of multimodal web data manipulation
to real-world productivity.

</details>


### [18] [Multi-Agent Data Visualization and Narrative Generation](https://arxiv.org/abs/2509.00481)
*Anton Wolter,Georgios Vidalakis,Michael Yu,Ankit Grover,Vaishali Dhanoa*

Main category: cs.AI

TL;DR: 本研究提出了一种轻量级多智能体系统，用于自动化数据分析工作流程，结合了混合多智能体架构与确定性组件，从LLMs中外部化关键逻辑以提高透明度和可靠性。系统在4个不同数据集上展示了强大的泛化能力、叙事质量和计算效率，具有较小的依赖性，支持可持续的人工智能协作。


<details>
  <summary>Details</summary>
Motivation: 最近人工智能智能体领域的进展影响了我们工作的方式，实现了更高程度的自动化和人机协作。在数据可视化领域，多智能体系统可以在整个数据到通信管道中发挥作用，因此建立了这种轻量级多智能体系统以实现数据分析工作流程自动化是有意义的。

Method: 结合混合多智能体架构与确定性组件，通过外部化关键逻辑改善透明度和可靠性。系统提供精细、模块化的输出，支持可持续的人工智能协作。系统在4个不同数据集上进行评估，展示了强大的泛化能力、叙事质量和计算效率，并且依赖性较小。

Result: 我们提出的系统在4个不同数据集上展示了强大的泛化能力、叙事质量和计算效率，同时具有较小的依赖性。

Conclusion: 在数据可视化领域，我们提出了一种轻量级多智能体系统，用于自动化数据分析工作流程，从数据探索到生成连贯的视觉叙述进行洞察传达。我们的方法结合了混合多智能体架构与确定性组件，从LLMs中外部化关键逻辑，以提高透明度和可靠性。该系统提供精细、模块化的输出，支持可持续的人工智能协作。通过在4个不同数据集上评估，我们展示了系统具有强大的泛化能力、叙事质量以及计算效率，并且具有最小的依赖性。

Abstract: Recent advancements in the field of AI agents have impacted the way we work,
enabling greater automation and collaboration between humans and agents. In the
data visualization field, multi-agent systems can be useful for employing
agents throughout the entire data-to-communication pipeline. We present a
lightweight multi-agent system that automates the data analysis workflow, from
data exploration to generating coherent visual narratives for insight
communication. Our approach combines a hybrid multi-agent architecture with
deterministic components, strategically externalizing critical logic from LLMs
to improve transparency and reliability. The system delivers granular, modular
outputs that enable surgical modifications without full regeneration,
supporting sustainable human-AI collaboration. We evaluated our system across 4
diverse datasets, demonstrating strong generalizability, narrative quality, and
computational efficiency with minimal dependencies.

</details>


### [19] [Artificial Intelligence-Based Analysis of Ice Cream Melting Behavior Under Various Ingredients](https://arxiv.org/abs/2509.00507)
*Zhang Lai Bin,Zhen Bin It*

Main category: cs.AI

TL;DR: 本研究评估了添加剂对家制冰淇淋融化行为的影响，发现这些添加剂对融化抵抗力和结构稳定性具有积极作用。研究结果揭示了常用食品添加剂在冰淇淋配方中的功能作用，为开发平衡耐久性和经济效益的配方提供了潜力。比较分析不同稳定剂的效果，发现其中一些提供了更强的融化抵抗力和结构支持。


<details>
  <summary>Details</summary>
Motivation: 冻雪糕在融化过程中的稳定性是消费者接受和产品质量的关键因素。通过添加稳定剂改善质地、结构和减缓融化速度，探讨了添加剂对家制冰淇淋融化行为的影响。目的在于识别更具成本效益的配方。

Method: 本研究采用配制含有各种添加剂的冰淇淋样品，并在控制条件下进行融化测试，使用时间过程记录和Python以及OpenCV进行分析。比较分析不同稳定剂的效果，观察了融化过程，并检测到所有样品在融化后仍保持泡沫状结构，显示稳定剂有助于形成稳定的气泡基质。

Result: 研究发现，所有样品在融化后仍保持泡沫状结构，表明稳定剂有助于形成稳定的气泡基质。融化后再次冷冻的样本显示出增强的稳固性，显示了冰淇淋结构的改善韧性。比较不同稳定剂的分析突出了它们的效果差异，有些比其他提供更强的融化抵抗力和结构支持。

Conclusion: 该研究旨在评估添加剂（洋车前胶、瓜尔胶、麦芽糊精和卡拉胶）对家制冰淇淋的融化行为的影响，发现它们对融化抵抗力和冰淇淋结构稳定性具有积极作用。在各添加剂中，表现出差异，有些添加剂提供了更强的融化抵抗力和结构支撑。研究结果揭示了常用食品添加剂在冰淇淋配方中的功能作用，为开发平衡耐久性和经济效益的配方提供了潜力。

Abstract: The stability of ice cream during melting is a critical factor for consumer's
acceptance and product quality. With the commonly added stabilizer to improve
texture, structure and slower melting as the factors to analyze. This report
explores the effects of locust bean gum, guar gum, maltodextrin, and
carrageenan on the melting behavior of homemade ice cream. The main objective
was to assess how these additives influence melting resistance and to identify
a more cost-effective recipe formulation. Ice cream samples incorporating each
additive were prepared and subjected to melting tests under controlled
conditions. Timelapse recordings were used to capture and analyze the
progression of melting over time. Python and OpenCV is used for process and
analysis. Observations revealed that all samples retained a foam-like structure
even after melting, suggesting the stabilizers contributed to the formation of
a stable air-cell matrix. Furthermore, when the melted samples were re-frozen
and subsequently melted again, they displayed increased sturdiness, indicating
improved resilience of the ice cream structure. Comparative analysis of the
different stabilizers highlighted variations in their effectiveness, with some
offering stronger melting resistance and structural support than others.
Overall, the findings provide insights into the functional roles of commonly
used food additives in ice cream formulation. By evaluating both performance
and cost, this study demonstrates the potential for developing recipes that
balance durability with economic efficiency, contributing to practical
applications in both small-scale and commercial ice cream production.

</details>


### [20] [LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain](https://arxiv.org/abs/2509.00510)
*Li Weigang,Pedro Carvalho Brom,Lucas Ramson Siefert*

Main category: cs.AI

TL;DR: 本文提出了SuperBrain框架，强调了动态路径从子类大脑到超类大脑，通过多个子类大脑的演化和协调形成具有抽象化和自我改进能力的超级大脑。论文为可扩展、可解释和符合伦理的集体人工智能提供了概念基础和架构路线图。


<details>
  <summary>Details</summary>
Motivation: 与静态提示设计或孤立代理模拟不同，强调动态路径，提供了可扩展、可解释和符合伦理的集体人工智能的概念基础和架构路线图。

Method: 通过持续个性化互动产生子类大脑，通过GA辅助的演化改进任务表现，多个子类大脑通过群体智能协调，形成超类大脑。

Result: 提出了SuperBrain框架，说明了其理论构想，展示了初始实施（如无人机调度，KU/KI关键词过滤），并提议跨双人间知识整合的注册表。

Conclusion: 提出了一种基于大型语言模型（LLMs）和人类用户共同演化的超级大脑框架，强调子类大脑到超类大脑的动态路径。通过GA辅助的前向-后向演化，多个子类大脑通过群体智能协调，形成超类大脑，具有抽象化、泛化和自我改进能力。

Abstract: We propose a novel SuperBrain framework for collective intelligence, grounded
in the co-evolution of large language models (LLMs) and human users. Unlike
static prompt engineering or isolated agent simulations, our approach
emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A
Subclass Brain arises from persistent, personalized interaction between a user
and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through
GA-assisted forward-backward evolution, these dyads iteratively refine prompts
and task performance. (3) Multiple Subclass Brains coordinate via Swarm
Intelligence, optimizing across multi-objective fitness landscapes and
exchanging distilled heuristics. (4) Their standardized behaviors and cognitive
signatures integrate into a Superclass Brain, an emergent meta-intelligence
capable of abstraction, generalization and self-improvement. We outline the
theoretical constructs, present initial implementations (e.g., UAV scheduling,
KU/KI keyword filtering) and propose a registry for cross-dyad knowledge
consolidation. This work provides both a conceptual foundation and an
architectural roadmap toward scalable, explainable and ethically aligned
collective AI.

</details>


### [21] [Text-to-Layout: A Generative Workflow for Drafting Architectural Floor Plans Using LLMs](https://arxiv.org/abs/2509.00543)
*Jayakrishna Duggempudi,Lu Gao,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.AI

TL;DR: 该论文介绍了一种利用大型语言模型的人工智能工作流，用于通过自然语言提示起草建筑平面图。该系统结合了提示工程、家具摆放算法和Python脚本，生成与Autodesk Revit兼容的空间一致的草图。案例研究展示了该方法生成功能齐全、结构化的输出，适用于专业BIM流程。


<details>
  <summary>Details</summary>
Motivation: 通过利用大型语言模型和自然语言提示来简化建筑平面图的起草过程，减少人工劳动，提高生成效率。旨在为专业BIM流程提供更快、更精确的草图设计方案。

Method: AI-powered workflow using Large Language Models (LLMs) to assist in drafting architectural floor plans from natural language prompts. The system combines prompt engineering, furniture placement refinement algorithm, and Python scripting to generate spatially coherent draft plans compatible with Autodesk Revit.

Result: 成功开发了一种利用人工智能工作流生成建筑平面图的系统，能够自动生成符合空间规范的草图选项，并保留了Revit原生参数属性范围，展示了在中型住宅布局案例中的有效性。

Conclusion: 该论文提出了一种利用大型语言模型（LLMs）的人工智能工作流，用于通过自然语言提示起草方案建筑平面图。研究系统解释文本输入，自动生成包括墙壁、门户、窗户和家具摆放在内的布局选项。它结合了提示工程、家具摆放细化算法和Python脚本编写，生成了与设计工具Autodesk Revit兼容的空间一致的草图。对中型住宅布局的案例研究展示了该方法能够以最少的人工努力生成功能齐全、结构化的输出。该工作流程设计透明易复制，所有关键提示规范均有文档记录，以便其他研究人员独立实施。此外，生成的模型保留了专业BIM流程直接集成所需的Revit原生参数属性范围。

Abstract: This paper presents the development of an AI-powered workflow that uses Large
Language Models (LLMs) to assist in drafting schematic architectural floor
plans from natural language prompts. The proposed system interprets textual
input to automatically generate layout options including walls, doors, windows,
and furniture arrangements. It combines prompt engineering, a furniture
placement refinement algorithm, and Python scripting to produce spatially
coherent draft plans compatible with design tools such as Autodesk Revit. A
case study of a mid-sized residential layout demonstrates the approach's
ability to generate functional and structured outputs with minimal manual
effort. The workflow is designed for transparent replication, with all key
prompt specifications documented to enable independent implementation by other
researchers. In addition, the generated models preserve the full range of
Revit-native parametric attributes required for direct integration into
professional BIM processes.

</details>


### [22] [Social World Models](https://arxiv.org/abs/2509.00559)
*Xuhui Zhou,Jiarui Liu,Akhila Yerukola,Hyunwoo Kim,Maarten Sap*

Main category: cs.AI

TL;DR: Introducing S3AP, a structured social world representation, to enhance AI systems' understanding of social dynamics. S3AP improves social reasoning tasks, achieves state-of-the-art performance, and enhances agent decision-making in predicting future social dynamics. It serves as a promising general-purpose representation for social world states.


<details>
  <summary>Details</summary>
Motivation: AI systems face challenges in automatically structuring and reasoning about implicit social contexts, unlike humans who intuitively navigate social interactions by simulating dynamics and considering others' perspectives. The motivation is to bridge this gap by introducing S3AP to enable more effective reasoning about social dynamics in AI systems.

Method: Introducing a novel structured social world representation formalism (S3AP) designed to enhance AI systems' reasoning about social dynamics. The design follows a POMDP-driven approach, representing social interactions as structured tuples including state, observation, agent actions, and mental states, which can be induced from free-form narratives or other inputs.

Result: S3AP helps LLMs better understand social narratives across 5 social reasoning tasks, achieving a significant improvement in theory-of-mind reasoning. It also induces social world models from structured representations, improving agent decision-making in predicting future social dynamics and achieving better performance in the SOTOPIA social interaction benchmark.

Conclusion: S3AP is effective in helping AI systems better understand social dynamics and achieve state-of-the-art performance in social reasoning tasks. It also improves agent decision-making in predicting future social dynamics, showing its promise as a general-purpose representation for social world states.

Abstract: Humans intuitively navigate social interactions by simulating unspoken
dynamics and reasoning about others' perspectives, even with limited
information. In contrast, AI systems struggle to automatically structure and
reason about these implicit social contexts. In this paper, we introduce a
novel structured social world representation formalism (S3AP), designed to help
AI systems reason more effectively about social dynamics. Following a
POMDP-driven design, S3AP represents social interactions as structured tuples,
such as state, observation, agent actions, and mental states, which can be
automatically induced from free-form narratives or other inputs. We first show
S3AP can help LLMs better understand social narratives across 5 social
reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning
with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then
induce social world models from these structured representations, demonstrating
their ability to predict future social dynamics and improve agent
decision-making, yielding up to +18% improvement on the SOTOPIA social
interaction benchmark. Our findings highlight the promise of S3AP as a
powerful, general-purpose representation for social world states, enabling the
development of more socially-aware systems that better navigate social
interactions.

</details>


### [23] [BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting](https://arxiv.org/abs/2509.00622)
*Shiqiao Zhou,Holger Schöner,Huanbo Lyu,Edouard Fouché,Shuo Wang*

Main category: cs.AI

TL;DR: BALM-TSF is a lightweight time series forecasting framework that balances textual and temporal modalities for enhanced forecasting performance. It achieves state-of-the-art results in both long-term and few-shot forecasting by integrating aligned textual and time series embeddings using a cross-modal context alignment strategy.


<details>
  <summary>Details</summary>
Motivation: Driven by the rise of large language models (LLMs), the research aims to enhance forecasting performance by leveraging textual modalities in addition to time series methods. The goal is to address the imbalance between text and temporal data in current multimodal architectures to prevent information loss that harms forecasting performance.

Method: The method involves processing raw time series data with a time series encoder, while feeding descriptive statistics of the data to a large language model with a learnable prompt to produce textual embeddings. A scaling strategy combined with a contrastive objective is used to ensure balanced cross-modal context alignment. The aligned textual embeddings and time series embeddings are integrated for forecasting.

Result: Extensive experiments on standard benchmarks demonstrate that BALM-TSF achieves state-of-the-art performance in long-term and few-shot forecasting with minimal trainable parameters, confirming its ability to harness complementary information from text and time series.

Conclusion: BALM-TSF (Balanced Multimodal Alignment for LLM-Based Time Series Forecasting) introduces a lightweight time series forecasting framework that maintains balance between textual and temporal modalities, achieving state-of-the-art performance in both long-term and few-shot forecasting.

Abstract: Time series forecasting is a long-standing and highly challenging research
topic. Recently, driven by the rise of large language models (LLMs), research
has increasingly shifted from purely time series methods toward harnessing
textual modalities to enhance forecasting performance. However, the vast
discrepancy between text and temporal data often leads current multimodal
architectures to over-emphasise one modality while neglecting the other,
resulting in information loss that harms forecasting performance. To address
this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment
for LLM-Based Time Series Forecasting), a lightweight time series forecasting
framework that maintains balance between the two modalities. Specifically, raw
time series are processed by the time series encoder, while descriptive
statistics of raw time series are fed to an LLM with learnable prompt,
producing compact textual embeddings. To ensure balanced cross-modal context
alignment of time series and textual embeddings, a simple yet effective scaling
strategy combined with a contrastive objective then maps these textual
embeddings into the latent space of the time series embeddings. Finally, the
aligned textual semantic embeddings and time series embeddings are together
integrated for forecasting. Extensive experiments on standard benchmarks show
that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art
performance in both long-term and few-shot forecasting, confirming its ability
to harness complementary information from text and time series. Code is
available at https://github.com/ShiqiaoZhou/BALM-TSF.

</details>


### [24] [NetGent: Agent-Based Automation of Network Application Workflows](https://arxiv.org/abs/2509.00625)
*Jaber Daneshamooz,Eugene Vuong,Laasya Koduru,Sanjay Chandrasekaran,Arpit Gupta*

Main category: cs.AI

TL;DR: NetGent is an AI-agent framework that automates complex application workflows to generate realistic network traffic datasets efficiently and reliably. It allows users to specify workflows through natural-language rules, compiled into NFAs for executable code. It automated various workflows in experiments, proving its scalability and reliability for ML dataset generation in networking.


<details>
  <summary>Details</summary>
Motivation: Developing ML models for networking requires data collection from network environments with diverse real-world web application traffic. Existing automation tools are fragile and costly, leading to the need for a more efficient solution like NetGent.

Method: NetGent allows users to specify workflows as natural-language rules that define state-dependent actions, which are compiled into nondeterministic finite automata (NFAs). These NFAs are translated into reusable, executable code for deterministic replay, reduced redundant calls, and quick adaptation to interface changes.

Result: In experiments, NetGent automated over 50 workflows including video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping. It produced realistic traffic traces while remaining robust to UI variability.

Conclusion: NetGent provides a scalable foundation for generating diverse and repeatable datasets needed to advance ML in networking. It automates complex application workflows to produce realistic network traffic datasets efficiently and reliably.

Abstract: We present NetGent, an AI-agent framework for automating complex application
workflows to generate realistic network traffic datasets. Developing
generalizable ML models for networking requires data collection from network
environments with traffic that results from a diverse set of real-world web
applications. However, using existing browser automation tools that are
diverse, repeatable, realistic, and efficient remains fragile and costly.
NetGent addresses this challenge by allowing users to specify workflows as
natural-language rules that define state-dependent actions. These abstract
specifications are compiled into nondeterministic finite automata (NFAs), which
a state synthesis component translates into reusable, executable code. This
design enables deterministic replay, reduces redundant LLM calls through state
caching, and adapts quickly when application interfaces change. In experiments,
NetGent automated more than 50+ workflows spanning video-on-demand streaming,
live video streaming, video conferencing, social media, and web scraping,
producing realistic traffic traces while remaining robust to UI variability. By
combining the flexibility of language-based agents with the reliability of
compiled execution, NetGent provides a scalable foundation for generating the
diverse, repeatable datasets needed to advance ML in networking.

</details>


### [25] [On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations](https://arxiv.org/abs/2509.00710)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文介绍了一种模块化的多Agent框架，将法律推理分解为知识获取和应用两个阶段。通过结合自然语言理解和符号推理，提高了透明度和可验证性，有效缩小了法律推理中的性能差距。在法定税收计算任务上取得显著改进，表明模块化架构具有形式化知识表示可以使法律推理更易于访问，提升一致性和解释性，为未来开发更透明、可信赖和有效的AI法律系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 针对AI系统在法律推理中面临的挑战，本文旨在提出一种能够使法律推理更加可理解、透明和一致的解决方案。通过模块化的方法，将法律推理分解为不同阶段，以实现形式化知识表示，提高计算效率和透明度。

Method: 介绍了一种模块化的多Agent框架，分为知识获取和应用两个阶段，第一阶段由专门的Agent从法律中提取概念，形式化规则，创造可验证的法规中间表示；第二阶段通过分析查询、执行符号推理和使用程序实现生成最终答案，实现了本体知识的操作化。将自然语言理解与符号推理相结合，提高透明度和可验证性。在法定税收计算任务上进行了评估，显示出显著的改进，基础模型准确率达到了76.4%。

Result: 介绍的模块化多Agent框架有效缩小了法律推理中的性能差距，使基础模型在法定税收计算任务上的准确率达到了76.4%。该方法增强了透明度和可验证性，为未来开发更透明、可信赖和有效的AI法律系统奠定了基础。

Conclusion: 本文介绍了一种模块化的多Agent框架，将法律推理分解为知识获取和应用两个阶段。通过在第一阶段专门的Agent提取法律概念并形式化规则，创造可验证的法规中间表示。第二阶段通过三个步骤将这些知识应用于特定案例：分析查询以将案件事实映射到本体模式，执行符号推理以推导逻辑含义的结论，使用程序实现生成最终答案，将本体知识实用化。将自然语言理解与符号推理相结合，提供了明确和可验证的检查点，与端到端方法相比显著增强了透明度。在法定税收计算任务上的评估显示出显著改进，基础模型达到了76.4%的准确率，而基准性能为18.8%，有效缩小了推理和基础模型之间的性能差距。这些发现表明，具有形式化知识表示的模块化架构可以通过计算效率高的模型使复杂的法律推理更易于访问，同时增强了AI法律推理的一致性和可解释性，为未来进一步研究建立了基础，以开发更透明、可信赖和有效的法律领域AI系统。

Abstract: Legal reasoning requires both precise interpretation of statutory language
and consistent application of complex rules, presenting significant challenges
for AI systems. This paper introduces a modular multi-agent framework that
decomposes legal reasoning into distinct knowledge acquisition and application
stages. In the first stage, specialized agents extract legal concepts and
formalize rules to create verifiable intermediate representations of statutes.
The second stage applies this knowledge to specific cases through three steps:
analyzing queries to map case facts onto the ontology schema, performing
symbolic inference to derive logically entailed conclusions, and generating
final answers using a programmatic implementation that operationalizes the
ontological knowledge. This bridging of natural language understanding with
symbolic reasoning provides explicit and verifiable inspection points,
significantly enhancing transparency compared to end-to-end approaches.
Evaluation on statutory tax calculation tasks demonstrates substantial
improvements, with foundational models achieving 76.4\% accuracy compared to
18.8\% baseline performance, effectively narrowing the performance gap between
reasoning and foundational models. These findings suggest that modular
architectures with formalized knowledge representations can make sophisticated
legal reasoning more accessible through computationally efficient models while
enhancing consistency and explainability in AI legal reasoning, establishing a
foundation for future research into more transparent, trustworthy, and
effective AI systems for legal domain.

</details>


### [26] [OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination](https://arxiv.org/abs/2509.00723)
*Junzhe Chen,Tianshu Zhang,Shiyu Huang,Yuwei Niu,Chao Sun,Rongzhou Zhang,Guanyu Zhou,Lijie Wen,Xuming Hu*

Main category: cs.AI

TL;DR: OmniDPO is proposed to address hallucination issues in OLLMs by aligning preferences between text, audio, and video modalities. The framework improves multimodal grounding and reasoning capabilities, demonstrated through experiments on two OLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing OLLMs face hallucination issues where textual cues dominate and ignore visual and audio information. Fully multimodal scenarios pose challenges as the intrinsic correlations between video and audio are often overlooked, leading to hallucinations. The paper aims to address these challenges and improve OLLMs' multimodal grounding and reasoning abilities.

Method: OmniDPO incorporates two strategies: constructing text-preference sample pairs to enhance the model's understanding of audio-video interactions and constructing multimodal-preference sample pairs to strengthen the model's attention to visual and auditory information.

Result: Experiments on two OLLMs show that OmniDPO effectively mitigates multimodal hallucinations and enhances reasoning capabilities across modalities.

Conclusion: OmniDPO is proposed to mitigate hallucination issues in Omni-modal large language models (OLLMs) by incorporating preference-alignment strategies. The framework effectively improves multimodal grounding and reasoning capabilities across modalities.

Abstract: Recently, Omni-modal large language models (OLLMs) have sparked a new wave of
research, achieving impressive results in tasks such as audio-video
understanding and real-time environment perception. However, hallucination
issues still persist. Similar to the bimodal setting, the priors from the text
modality tend to dominate, leading OLLMs to rely more heavily on textual cues
while neglecting visual and audio information. In addition, fully multimodal
scenarios introduce new challenges. Most existing models align visual or
auditory modalities with text independently during training, while ignoring the
intrinsic correlations between video and its corresponding audio. This
oversight results in hallucinations when reasoning requires interpreting hidden
audio cues embedded in video content. To address these challenges, we propose
OmniDPO, a preference-alignment framework designed to mitigate hallucinations
in OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructing
text-preference sample pairs to enhance the model's understanding of
audio-video interactions; and (2) constructing multimodal-preference sample
pairs to strengthen the model's attention to visual and auditory information.
By tackling both challenges, OmniDPO effectively improves multimodal grounding
and reduces hallucination. Experiments conducted on two OLLMs demonstrate that
OmniDPO not only effectively mitigates multimodal hallucinations but also
significantly enhances the models' reasoning capabilities across modalities.
All code and datasets will be released upon paper acceptance.

</details>


### [27] [Efficient Graph Understanding with LLMs via Structured Context Injection](https://arxiv.org/abs/2509.00740)
*Govind Waghmare,Sumedh BG,Sonia Gupta,Srikanta Bedathur*

Main category: cs.AI

TL;DR: 本研究提出了一种结构化上下文注入框架，通过向输入直接注入结构化上下文，帮助LLMs在解决各种图问题时取得性能改善，无需精细调整。结果显示，结构化上下文注入是一种有效且可扩展的图理解策略，能够与更复杂的方法竞争甚至超越其性能。


<details>
  <summary>Details</summary>
Motivation: 观察到对于LLMs而言，某些图推理任务仍具有挑战性，除非它们被映射到概念上扎实的表示空间。fine-tuning或多步查询可能昂贵且低效，因此需要提出实际替代方案。

Method: 本研究提出了一种结构化上下文注入框架，将任务特定信息系统地嵌入输入，引导LLMs解决各种图问题，而无需对LLMs进行精细调整，从而节约成本且轻量级。

Result: 通过在多个图任务上使用轻量级和大型模型评估该方法，突显准确性和计算成本之间的权衡。结果表明，结构化输入上下文能够与更复杂的方法竞争甚至超越其性能。

Conclusion: 结构化上下文注入是一种有效且可扩展的图理解策略，能帮助LLMs在解决各种图问题时取得一致的性能改善。

Abstract: Large Language Models (LLMs) have shown strong capabilities in solving
problems across domains, including graph-related tasks traditionally addressed
by symbolic or algorithmic methods. In this work, we present a framework for
structured context injection, where task-specific information is systematically
embedded in the input to guide LLMs in solving a wide range of graph problems.
Our method does not require fine-tuning of LLMs, making it cost-efficient and
lightweight. We observe that certain graph reasoning tasks remain challenging
for LLMs unless they are mapped to conceptually grounded representations.
However, achieving such mappings through fine-tuning or repeated multi-step
querying can be expensive and inefficient. Our approach offers a practical
alternative by injecting structured context directly into the input, enabling
the LLM to implicitly align the task with grounded conceptual spaces. We
evaluate the approach on multiple graph tasks using both lightweight and large
models, highlighting the trade-offs between accuracy and computational cost.
The results demonstrate consistent performance improvements, showing that
structured input context can rival or surpass more complex approaches. Our
findings underscore the value of structured context injection as an effective
and scalable strategy for graph understanding with LLMs.

</details>


### [28] [L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search](https://arxiv.org/abs/2509.00761)
*Ziqi Wang,Boqin Yuan*

Main category: cs.AI

TL;DR: L-MARS系统通过协调多智能体推理和检索，在法律问题回答中降低了幻觉和不确定性。实验结果显示，在LegalSearchQA评估中，L-MARS显著提高了事实准确性，减少了不确定性，并受到人类专家和基于LLM的法官更高的青睐。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决法律问题回答中的幻觉和不确定性，提高回答的准确性，并为在需要精确法律检索和审议的高风险领域部署LLM提供可扩展和可复制的方法。

Method: L-MARS将查询分解为子问题，并在异构来源（Serper网络、本地RAG、CourtListener案例法）上发起有针对性的搜索，使用法官智能体在回答综合之前验证充分性、司法管辖权和时间有效性。通过这种迭代的推理-搜索-验证循环，保持连贯性，过滤嘈杂证据，并基于权威法律确立答案。

Result: 实验结果显示，L-MARS在LegalSearchQA评估中表现出在事实准确性、减少不确定性方面的改善，并获得了人类专家和基于LLM的法官更高的偏爱分数。

Conclusion: L-MARS系统通过协调多智能体推理和检索，降低了法律问题回答中的幻觉和不确定性。实验结果表明，L-MARS显著提高了事实准确性，减少了不确定性，并获得了来自人类专家和基于LLM的法官更高的偏爱分数。研究表明，多智能体推理与智能搜索为在需要精确法律检索和审议的高风险领域部署LLM提供了可扩展且可复制的蓝图。

Abstract: We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and
Agentic Search), a system that reduces hallucination and uncertainty in legal
question answering through coordinated multi-agent reasoning and retrieval.
Unlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes
queries into subproblems, issues targeted searches across heterogeneous sources
(Serper web, local RAG, CourtListener case law), and employs a Judge Agent to
verify sufficiency, jurisdiction, and temporal validity before answer
synthesis. This iterative reasoning-search-verification loop maintains
coherence, filters noisy evidence, and grounds answers in authoritative law. We
evaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple
choice legal questions in 2025. Results show that L-MARS substantially improves
factual accuracy, reduces uncertainty, and achieves higher preference scores
from both human experts and LLM-based judges. Our work demonstrates that
multi-agent reasoning with agentic search offers a scalable and reproducible
blueprint for deploying LLMs in high-stakes domains requiring precise legal
retrieval and deliberation.

</details>


### [29] [Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling](https://arxiv.org/abs/2509.00768)
*Lee Hyun,Sohee Yoon,Jinwoo Park,Sue In Chae,Seongeon Park,Jooyeon Ahn,Yebin Jung,Youjung Chung,Hogeun Chang,Myeonginn Kang,Jina Kim,Ho-Gyeong Kim,Myeonghun Jeong*

Main category: cs.AI

TL;DR: 研究提出了一种物理感知的拒绝抽样（PaRS）方案，用于提高AI驱动的材料发现中模型的准确性和校准性，降低物理违规率和抽样成本。该方法通过选择符合基本物理规律且接近目标的推理轨迹，在大型推理模型（LRMs）中表现出良好的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于AI驱动的材料探索需要准确、校准且物理可信的过程感知配方，提高推理模型的能力。现有的训练流程中，常使用二元正确性或学习的偏好信号来选择推理轨迹，但这往往无法很好地反映物理可信度。

Method: 研究方法采用物理感知的拒绝抽样（PaRS）方案，通过在训练过程中选择符合基本物理规律且接近目标的推理轨迹，培养大型推理模型（LRMs）的准确性、校准性和物理可信度。

Result: 通过引入物理感知的拒绝抽样（PaRS）方案，在匹配记号预算下，提高了模型的准确性和校准性，降低了物理违规率和抽样成本。相对于基线方法，该方法显示出了明显的改进效果。

Conclusion: 该研究提出了一种物理感知的拒绝抽样（PaRS）方案，用于AI驱动的材料发现中，以提高模型的准确性、校准性和物理可信度。通过在训练过程中选择符合基本物理规律且接近目标的推理轨迹，该方法在匹配记号预算下提高了准确性和校准性，降低了物理违规率和抽样成本。研究结果表明，结合适度的领域感知约束和跟踪级别的选择，为过程感知性能预测和闭环材料设计提供了可靠、高效的大型推理模型（LRMs）路径。

Abstract: AI-driven materials discovery that couples automated experimentation with
algorithmic decision-making requires process aware recipe to property
predictors that are accurate, calibrated, and physically admissible. We
approach this as a reasoning problem with large reasoning models (LRMs). To
instill reasoning capability into language models, we curate reasoning traces
from a teacher model to train a student model. However, most training pipelines
select reasoning traces using binary correctness or learned preference signals
that poorly reflect physical admissibility. We introduce Physics-aware
Rejection Sampling (PaRS), a training-time trace selection scheme that favors
traces consistent with fundamental physics and numerically close to targets,
with lightweight halting to control compute. We instantiate our framework with
a large student model fine-tuned on traces synthesized by a larger teacher
model, and evaluate under matched token budgets against various rejection
sampling baselines. Our method improves accuracy and calibration, reduces
physics-violation rates, and lowers sampling cost relative to baselines. These
results indicate that modest, domain-aware constraints combined with
trace-level selection provide a practical path toward reliable, efficient LRMs
for process-aware property prediction and closed-loop materials design.

</details>


### [30] [Sharpe Ratio Optimization in Markov Decision Processes](https://arxiv.org/abs/2509.00793)
*Shuai Ma,Guangwu Liu,Li Xia*

Main category: cs.AI

TL;DR: 本文研究了在无限时间跨度马尔可夫决策过程中的夏普比率优化问题。通过Dinkelbachs变换将夏普比率目标转换为均方差（M2V）目标，并开发了迭代算法和策略迭代过程用于求解最优解。数值实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决马尔可夫决策过程中夏普比率优化的挑战，在处理动态规划中无法处理的分数目标和风险指标方面提出新方法。

Method: 本文使用Dinkelbachs变换将夏普比率目标转换为均方差（M2V）目标，开发了迭代算法解决M2V优化问题，并提出了一种策略迭代过程用于求解最优解。

Result: 通过开发迭代算法和策略迭代过程，成功解决了在无限时间跨度马尔可夫决策过程中的夏普比率优化问题。数值实验验证了方法的有效性。

Conclusion: 本文旨在研究在无限时间跨度马尔可夫决策过程中的夏普比率优化，考虑了长期平均和折现设置。通过Dinkelbachs变换解决了将夏普比率目标转换为均方差（M2V）目标的第一个挑战，并开发了迭代算法来解决M2V优化问题。在平均和折现MDP设置中，开发了策略迭代过程，并证明其收敛性。数值实验验证了方法的有效性。

Abstract: Sharpe ratio (also known as reward-to-variability ratio) is a widely-used
metric in finance, which measures the additional return at the cost of per unit
of increased risk (standard deviation of return). However, the optimization of
Sharpe ratio in Markov decision processes (MDPs) is challenging, because there
exist two difficulties hindering the application of dynamic programming. One is
that dynamic programming does not work for fractional objectives, and the other
is that dynamic programming is invalid for risk metrics. In this paper, we
study the Sharpe ratio optimization in infinite-horizon MDPs, considering both
the long-run average and discounted settings. We address the first challenge
with the Dinkelbachs transform, which converts the Sharpe ratio objective to a
mean-squared-variance (M2V) objective. It is shown that the M2V optimization
and the original Sharpe ratio optimization share the same optimal policy when
the risk-sensitive parameter is equal to the optimal Sharpe ratio. For the
second challenge, we develop an iterative algorithm to solve the M2V
optimization which is similar to a mean-variance optimization in MDPs. We
iteratively solve the M2V problem and obtain the associated Sharpe ratio that
is used to update the risk-sensitive parameter in the next iteration of M2V
problems. We show that such a sequence of Sharpe ratios derived is
monotonically increasing and converges to the optimal Sharpe ratio. For both
average and discounted MDP settings, we develop a policy iteration procedure
and prove its convergence to the optimum. Numerical experiments are conducted
for validation. To the best of our knowledge, our approach is the first that
solves the Sharpe ratio optimization in MDPs with dynamic programming type
algorithms. We believe that the proposed algorithm can shed light on solving
MDPs with other fractional objectives.

</details>


### [31] [Neuro-Symbolic Predictive Process Monitoring](https://arxiv.org/abs/2509.00834)
*Axel Mezini,Elena Umili,Ivan Donadello,Fabrizio Maria Maggi,Matteo Mancanelli,Fabio Patrizi*

Main category: cs.AI

TL;DR: 该论文介绍了一种神经符号预测过程监控（PPM）方法，用于解决企业流程管理中的后缀预测问题。他们的方法整合了线性时态逻辑（LTLf）到自回归序列预测器训练过程中，以确保模型生成的后缀准确且逻辑一致。实验证明，在真实数据集上，该方法提高了后缀预测的准确性和时间约束的符合度，并展示了逻辑损失的局部和全局变体的有效性。


<details>
  <summary>Details</summary>
Motivation: 在BPM中，最近的方法利用深度学习模型进行后缀预测，但由于训练过程中领域知识的缺失，通常无法满足基本的逻辑约束。因此，作者提出了一种引入线性时态逻辑到训练过程中的方法，以解决这一问题。

Method: 该论文提出了一种整合线性时态逻辑（LTLf）到自回归序列预测器训练过程中的方法，引入了可微分逻辑损失函数，并结合标准预测损失，确保模型学习生成既准确又逻辑一致的后缀。他们使用了Gumbel-Softmax技巧定义了一个软近似的LTLf语义，以实现逻辑损失函数。

Result: 实验评估显示，作者的方法在三个真实数据集上提高了后缀预测的准确性和时间约束的符合度。引入的逻辑损失的局部和全局变体在嘈杂和现实环境下表现出有效性。

Conclusion: 该论文提出了一种神经符号预测过程监控（PPM）方法，通过将数据驱动学习与基于时间逻辑的先验知识相结合，解决了企业流程管理中后缀预测的问题。他们的方法在实验评估中显示，能够提高后缀预测的准确性并且符合时间约束。另外，他们还介绍了逻辑损失的两个变体（局部和全局）并展示它们在嘈杂和现实环境下的有效性。虽然在BPM的背景下开发，但他们的框架适用于任何符号序列生成任务，有助于推动神经符号人工智能的发展。

Abstract: This paper addresses the problem of suffix prediction in Business Process
Management (BPM) by proposing a Neuro-Symbolic Predictive Process Monitoring
(PPM) approach that integrates data-driven learning with temporal logic-based
prior knowledge. While recent approaches leverage deep learning models for
suffix prediction, they often fail to satisfy even basic logical constraints
due to the absence of explicit integration of domain knowledge during training.
We propose a novel method to incorporate Linear Temporal Logic over finite
traces (LTLf) into the training process of autoregressive sequence predictors.
Our approach introduces a differentiable logical loss function, defined using a
soft approximation of LTLf semantics and the Gumbel-Softmax trick, which can be
combined with standard predictive losses. This ensures the model learns to
generate suffixes that are both accurate and logically consistent. Experimental
evaluation on three real-world datasets shows that our method improves suffix
prediction accuracy and compliance with temporal constraints. We also introduce
two variants of the logic loss (local and global) and demonstrate their
effectiveness under noisy and realistic settings. While developed in the
context of BPM, our framework is applicable to any symbolic sequence generation
task and contributes toward advancing Neuro-Symbolic AI.

</details>


### [32] [ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care](https://arxiv.org/abs/2509.00891)
*Zonghai Yao,Talha Chafekar,Junda Wang,Shuo Han,Feiyun Ouyang,Junhui Qian,Lingxi Li,Hong Yu*

Main category: cs.AI

TL;DR: 该研究介绍了ChatCLIDS，作为第一个严格评估LLM驱动的有说服力对话用于健康行为变化的基准。结果显示，尽管LLM会调整策略，但在面对抵抗特别是现实社会压力时仍存在困难。研究强调了当前LLM在行为改变方面的重要限制，并提供了一个用于推动医疗保健等领域可信赖的AI劝说技术发展的高保真度、可扩展的测试平台。


<details>
  <summary>Details</summary>
Motivation: 现实世界中对于1型糖尿病的闭环胰岛素输送系统的采用率仍然很低，这并不是由技术失败引起的，而是由多样化的行为、心理社会和社会障碍所推动。该研究引入了ChatCLIDS，旨在严格评估LLM驱动的有说服力对话用于健康行为变化的基准。

Method: 引入ChatCLIDS作为第一个严格评估LLM驱动的有说服力对话用于健康行为变化的基准。该框架拥有一系列经专家验证的虚拟患者库，每个患者具有临床基础、异质性特征和真实的采用障碍，模拟与配备有多种以证据为基础的说服策略的护士代理进行多回合互动。ChatCLIDS支持长期辅导和对抗性社会影响场景，实现了强大、多维度的评估。

Result: 研究结果表明，虽然规模更大且更反思的LLM会随着时间的推移调整策略，但所有模型在特别是在现实社会压力下仍然难以克服阻力。这些结果突出了当前LLM在行为变化方面的重要限制，并提供了一个高保真度、可扩展的测试平台，以推动医疗保健领域以及其他领域中可信赖的AI劝说技术的发展。

Conclusion: 该研究发现当前的大型且反思性更强的LLM随着时间的推移会调整策略，但所有模型在面对抵抗特别是在现实社会压力下仍然存在困难。当前的LLM在行为变化方面存在重要限制，同时提供了一个高保真度、可扩展的测试平台，用于推动医疗保健以及其他领域中可信赖的AI劝说技术的发展。

Abstract: Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1
diabetes remains low, driven not by technical failure, but by diverse
behavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the
first benchmark to rigorously evaluate LLM-driven persuasive dialogue for
health behavior change. Our framework features a library of expert-validated
virtual patients, each with clinically grounded, heterogeneous profiles and
realistic adoption barriers, and simulates multi-turn interactions with nurse
agents equipped with a diverse set of evidence-based persuasive strategies.
ChatCLIDS uniquely supports longitudinal counseling and adversarial social
influence scenarios, enabling robust, multi-dimensional evaluation. Our
findings reveal that while larger and more reflective LLMs adapt strategies
over time, all models struggle to overcome resistance, especially under
realistic social pressure. These results highlight critical limitations of
current LLMs for behavior change, and offer a high-fidelity, scalable testbed
for advancing trustworthy persuasive AI in healthcare and beyond.

</details>


### [33] [Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play](https://arxiv.org/abs/2509.00923)
*Zakaria El Jaafari*

Main category: cs.AI

TL;DR: 论文分析了神经MCCFR在游戏规模下的有效性变化，提出了鲁棒的深度MCCFR框架，并展示了在不同规模游戏中的组件效果。最佳配置在两个领域都取得了改进，强调了组件选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 该论文探讨了神经MCCFR组件在不同游戏规模下的有效性变化，并提出了自适应框架以选择性部署组件，以应对规模相关挑战。

Method: 该论文进行了理论分析、提出了一个鲁棒的深度MCCFR框架，包括延迟更新的目标网络、均匀探索混合、考虑方差的训练目标和全面的诊断监控。进行了系统的割除研究，验证了组件的规模依赖性，并提出了实用的部署指南。

Result: 最佳配置在Kuhn Poker和Leduc Poker领域分别实现了显著改进的exploitability，验证了选择性组件使用的重要性。

Conclusion: 该论文提出了一个鲁棒的深度MCCFR框架，可以根据游戏规模选择性部署组件，通过系统切割研究在不同规模游戏中组件的有效性，并识别了关键的组件交互。实验结果表明，在Kuhn Poker领域，最佳配置使得最终的exploitability为0.0628，比传统框架（0.156）提高了60%。在更复杂的Leduc Poker领域，选择性组件使用实现exploitability为0.2386，比传统框架（0.3703）提高了23.5%，突出了在较大游戏中仔细选择组件的重要性。

Abstract: Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a
cornerstone algorithm for solving extensive-form games, but its integration
with deep neural networks introduces scale-dependent challenges that manifest
differently across game complexities. This paper presents a comprehensive
analysis of how neural MCCFR component effectiveness varies with game scale and
proposes an adaptive framework for selective component deployment. We identify
that theoretical risks such as nonstationary target distribution shifts, action
support collapse, variance explosion, and warm-starting bias have
scale-dependent manifestation patterns, requiring different mitigation
strategies for small versus large games. Our proposed Robust Deep MCCFR
framework incorporates target networks with delayed updates, uniform
exploration mixing, variance-aware training objectives, and comprehensive
diagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc
Poker, we demonstrate scale-dependent component effectiveness and identify
critical component interactions. The best configuration achieves final
exploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the
classical framework (0.156). On the more complex Leduc Poker domain, selective
component usage achieves exploitability of 0.2386, a 23.5% improvement over the
classical framework (0.3703) and highlighting the importance of careful
component selection over comprehensive mitigation. Our contributions include:
(1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled
mitigation framework with convergence guarantees, (3) comprehensive multi-scale
experimental validation revealing scale-dependent component interactions, and
(4) practical guidelines for deployment in larger games.

</details>


### [34] [SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs](https://arxiv.org/abs/2509.00930)
*Yanxiao Zhao,Yaqian Li,Zihao Bo,Rinyoichi Takezoe,Haojia Hui,Mo Guang,Lei Ren,Xiaolin Qin,Kaiwen Long*

Main category: cs.AI

TL;DR: SATQuest is introduced to evaluate and enhance logical reasoning in Large Language Models (LLMs) by generating diverse, Satisfiability-based problems. It identified limitations in LLMs' reasoning and showed that reinforcement fine-tuning with SATQuest improves performance. Challenges in cross-format adaptation were highlighted, indicating SATQuest's potential as a foundational tool for advancing LLM logical reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges in systematically evaluating and enhancing reasoning capabilities in Large Language Models (LLMs) due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack variable control for multi-dimensional, systematic analysis and training. The need for a tool like SATQuest arose to address these limitations and provide nuanced insights into reasoning performance.

Method: Introducing SATQuest as a systematic verifier designed to evaluate and enhance logical reasoning in LLMs. It generates diverse, Satisfiability-based logical reasoning problems from Conjunctive Normal Form instances, considering instance scale, problem type, and question format. It employs randomized, SAT-based problem generation and objective answer verification via PySAT. Extensive evaluation of various LLMs using SATQuest was conducted to assess logical reasoning performance and the impact of reinforcement fine-tuning.

Result: SATQuest identified significant limitations in LLMs' logical reasoning, showcasing the effectiveness of reinforcement fine-tuning in improving task performance and generalization. It also highlighted challenges in cross-format adaptation, indicating the potential for further advancements in LLM logical reasoning.

Conclusion: SATQuest is introduced as a systematic verifier to evaluate and enhance logical reasoning in Large Language Models (LLMs) through diverse, Satisfiability-based logical reasoning problems. It identified significant limitations in LLMs' logical reasoning and showed that reinforcement fine-tuning with SATQuest improves task performance and generalization. Remaining challenges in cross-format adaptation were highlighted, emphasizing SATQuest's potential as a foundational tool for advancing LLM logical reasoning.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable
general reasoning capabilities. However, systematically evaluating and
enhancing these reasoning capabilities is challenging due to the lack of
controllable and scalable tools for fine-grained analysis. Existing benchmarks
and datasets often lack the necessary variable control for multi-dimensional,
systematic analysis and training, or have narrow problem types and formats. To
address these limitations, we introduce SATQuest, a systematic verifier
designed to evaluate and enhance logical reasoning in LLMs by generating
diverse, Satisfiability-based logical reasoning problems directly from
Conjunctive Normal Form (CNF) instances. SATQuest structures these problems
along three orthogonal dimensions: instance scale, problem type, and question
format, employing randomized, SAT-based problem generation and objective answer
verification via PySAT. This design mitigates memorization issues, allows for
nuanced insights into reasoning performance, and enables effective
reinforcement fine-tuning. Our extensive evaluation of various LLMs using
SATQuest identified significant limitations in their logical reasoning,
particularly in generalizing beyond familiar mathematical formats. Furthermore,
we show that reinforcement fine-tuning with SATQuest rewards substantially
improves targeted task performance and generalizes to more complex instances,
while highlighting remaining challenges in cross-format adaptation. Through
these demonstrations, we showcase SATQuest's potential as a foundational tool
and a valuable starting point for advancing LLM logical reasoning.

</details>


### [35] [UrbanInsight: A Distributed Edge Computing Framework with LLM-Powered Data Filtering for Smart City Digital Twins](https://arxiv.org/abs/2509.00936)
*Kishor Datta Gupta,Md Manjurul Ahsan,Mohd Ariful Haque,Roy George,Azmine Toushik Wasi*

Main category: cs.AI

TL;DR: 该论文介绍了一种整合了物理信息机器学习、多模态数据融合、知识图表示和大型语言模型的自适应、基于规则的智能框架。这些元素共同构成数字孪生系统的基础，为创造具有响应性、可信赖性和可持续性的智能基础设施开启了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 当今城市从传感器、摄像头和连接基础设施中产生大量数据流。尽管这些信息提供了改善城市生活的前所未有机会，但大多数现有系统在规模、延迟和碎片化的洞察力方面存在困难。因此，本研究的动机在于引入一个框架，旨在结合物理信息机器学习、多模态数据融合和知识图表示，以及大型语言模型驱动的自适应、基于规则的智能，为创造具有响应性、可信赖性和可持续性的智能基础设施打下基础。

Method: 该工作引入了融合了物理信息的机器学习、多模态数据融合和知识图表示的框架，结合了大型语言模型的自适应、基于规则的智能。物理信息的方法将学习与现实世界的约束相结合，确保预测保持有意义且与物理动态一致。知识图作为语义骨干，将异构传感器数据集成到一个连通的、可查询的结构中。在边缘端，大型语言模型生成上下文感知规则，实时调整过滤和决策，即使在资源受限的情况下也能够实现高效运行。

Result: 通过结合物理基础推理、语义数据融合和自适应规则生成，该方法为数字孪生系统奠定了基础，实现了超越被动监测的可操作洞察。物理信息、语义数据融合和自适应规则生成的统一方法为创建具有响应性、可信赖性和可持续性的智能基础设施开辟了新的可能性。

Conclusion: 该文章介绍了一种融合了物理信息的机器学习、多模态数据融合和知识图表示的框架，结合了大型语言模型的自适应、基于规则的智能。通过统一基于物理的推理、语义数据融合和自适应规则生成，该方法为创建具有响应性、可信赖性和可持续性的智能基础设施开辟了新的可能性。

Abstract: Cities today generate enormous streams of data from sensors, cameras, and
connected infrastructure. While this information offers unprecedented
opportunities to improve urban life, most existing systems struggle with scale,
latency, and fragmented insights. This work introduces a framework that blends
physics-informed machine learning, multimodal data fusion, and knowledge graph
representation with adaptive, rule-based intelligence powered by large language
models (LLMs). Physics-informed methods ground learning in real-world
constraints, ensuring predictions remain meaningful and consistent with
physical dynamics. Knowledge graphs act as the semantic backbone, integrating
heterogeneous sensor data into a connected, queryable structure. At the edge,
LLMs generate context-aware rules that adapt filtering and decision-making in
real time, enabling efficient operation even under constrained resources.
Together, these elements form a foundation for digital twin systems that go
beyond passive monitoring to provide actionable insights. By uniting
physics-based reasoning, semantic data fusion, and adaptive rule generation,
this approach opens new possibilities for creating responsive, trustworthy, and
sustainable smart infrastructures.

</details>


### [36] [A Hybrid Ai Framework For Strategic Patent Portfolio Pruning: Integrating Learning To-Rank And Market Need Analysis For Technology Transfer Optimization](https://arxiv.org/abs/2509.00958)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 本论文介绍了一种新颖的、多阶段的混合智能框架，用于剪枝专利组合以识别高价值资产进行技术转让。该框架结合了学习排序模型、自然语言处理和大型语言模型等技术，自动化和加深专利估值过程，为专利剥离决策提供战略支持。论文提出的系统能够有效地匹配专利和市场需求，支持实际的剥离决策。


<details>
  <summary>Details</summary>
Motivation: 现有的专利价值评估方法通常依赖于回顾性指标或手动、耗时的分析。本论文的动机在于自动化和加深专利估值过程，提高效率并提供战略性的专利剥离决策支持。

Method: 该论文采用了学习排序（LTR）模型和基于代理的“Need-Seed”系统，结合自然语言处理（NLP）和大型语言模型（LLMs）来实现专利剪枝和价值资产识别。通过创建“核心本体框架”将高潜力专利与市场需求匹配，提供了剥离决策的战略理由。论文还介绍了动态参数加权系统和人机合作验证协议。

Result: 论文提出的混合智能框架为剪枝专利组合并识别高价值资产提供了新颖的方法。通过深度学习和自然语言处理技术，该系统能够有效地匹配专利和市场需求，支持实际的专利剥离决策。

Conclusion: 该论文介绍了一个新颖的、多阶段的混合智能框架，用于剪枝专利组合，以确定高价值资产进行技术转让。通过结合学习排序（LTR）模型和独特的“Need-Seed”基于代理的系统，该框架自动化并深化了专利估值过程。系统利用自然语言处理（NLP）挖掘未结构化市场和行业数据，识别明确的技术需求，并分析专利声明以映射其技术能力，从而匹配高潜力专利（Seeds）与已记录的市场需求（Needs），为剥离决策提供战略理由。论文详细介绍了架构，包括动态参数加权系统和重要的人机合作验证协议，以确保适应性和现实可信度。

Abstract: This paper introduces a novel, multi stage hybrid intelligence framework for
pruning patent portfolios to identify high value assets for technology
transfer. Current patent valuation methods often rely on retrospective
indicators or manual, time intensive analysis. Our framework automates and
deepens this process by combining a Learning to Rank (LTR) model, which
evaluates patents against over 30 legal and commercial parameters, with a
unique "Need-Seed" agent-based system. The "Need Agent" uses Natural Language
Processing (NLP) to mine unstructured market and industry data, identifying
explicit technological needs. Concurrently, the "Seed Agent" employs fine tuned
Large Language Models (LLMs) to analyze patent claims and map their
technological capabilities. The system generates a "Core Ontology Framework"
that matches high potential patents (Seeds) to documented market demands
(Needs), providing a strategic rationale for divestment decisions. We detail
the architecture, including a dynamic parameter weighting system and a crucial
Human in the-Loop (HITL) validation protocol, to ensure both adaptability and
real-world credibility.

</details>


### [37] [Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations](https://arxiv.org/abs/2509.00961)
*Lun Ai,Johannes Langer,Ute Schmid,Stephen Muggleton*

Main category: cs.AI

TL;DR: LENS is a neuro-symbolic method that automates the explanation of machine-learned logic programs in natural language, outperforming direct LLM prompting and hand-crafted templates. While LENS provides superior explanations, the proposed transferable active learning strategies did not show significant human performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address a key limitation of prior USML approaches by improving the generation of explanations for machine-learned logic programs. The authors aim to enhance the performance of USML systems by providing better support for human learning through superior explanations.

Method: The paper presents LENS, a neuro-symbolic method that combines symbolic program synthesis with large language models (LLMs) to automate the explanation of machine-learned logic programs in natural language. LENS replaces hand-crafted explanation templates with scalable automated generation. The effectiveness of LENS was evaluated through systematic evaluation using multiple LLM judges and human validation.

Result: The results demonstrate that LENS outperforms direct LLM prompting and hand-crafted templates in generating explanations for machine-learned logic programs. However, the transferable active learning strategies proposed by LENS did not lead to significant human performance improvements in the experiment.

Conclusion: LENS (Logic Programming Explanation via Neural Summarisation) is a neuro-symbolic method that generates superior explanations for machine-learned logic programs compared to direct LLM prompting and hand-crafted templates. However, the transferable active learning strategies proposed by LENS did not show significant human performance improvements in the conducted experiment.

Abstract: Ultra Strong Machine Learning (USML) refers to symbolic learning systems that
not only improve their own performance but can also teach their acquired
knowledge to quantifiably improve human performance. In this work, we present
LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic
method that combines symbolic program synthesis with large language models
(LLMs) to automate the explanation of machine-learned logic programs in natural
language. LENS addresses a key limitation of prior USML approaches by replacing
hand-crafted explanation templates with scalable automated generation. Through
systematic evaluation using multiple LLM judges and human validation, we
demonstrate that LENS generates superior explanations compared to direct LLM
prompting and hand-crafted templates. To investigate whether LENS can teach
transferable active learning strategies, we carried out a human learning
experiment across three related domains. Our results show no significant human
performance improvements, suggesting that comprehensive LLM responses may
overwhelm users for simpler problems rather than providing learning support.
Our work provides a solid foundation for building effective USML systems to
support human learning. The source code is available on:
https://github.com/lun-ai/LENS.git.

</details>


### [38] [CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks with LLMs](https://arxiv.org/abs/2509.00971)
*Jay Vaghasiya,Omkar Ghugarkar,Vishvesh Bhat,Vipul Dholaria,Julian McAuley*

Main category: cs.AI

TL;DR: CoreThink introduces a new Reasoning Layer based on General Symbolics, diverging from traditional reasoning paradigms. The General Symbolic Reasoner (GSR) achieves high performance on various benchmarks without fine-tuning or training costs. CoreThink aims to provide a pure performance uplift without compromising model accuracy on reasoning tasks, highlighting the need for new reasoning techniques in the face of potential diminishing returns in Large Language Model (LLM) performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind CoreThink is to address the limitations of incumbent reasoning methods that could lead to diminishing returns in Large Language Model (LLM) performance. By introducing the General Symbolic reasoning approach, CoreThink aims to enhance performance without the need for fine-tuning or training costs, ensuring a model's accuracy on reasoning tasks remains high.

Method: CoreThink utilizes the General Symbolics reasoning method, diverging from traditional paradigms like test-time scaling, Supervised Fine-Tuning (SFT), and Reinforcement Learning with Verifiable Rewards (RLVR). The CoreThink General Symbolic Reasoner (GSR) is structured around key use cases including tool-calling, code generation, and planning. The paper also introduces an agentic coding IDE developed using General Symbolics principles, showcasing high accuracy on reasoning tasks.

Result: CoreThink demonstrates exceptional performance on seven benchmarks, achieving SOTA scores on Livecodebench v6, Instruction-Following Evals, and ARC-AGI-2. The agentic coding IDE developed using General Symbolics principles also achieves a state-of-the-art accuracy on SWE-Bench Lite. These results highlight the effectiveness of CoreThink in providing high performance in reasoning tasks.

Conclusion: CoreThink introduces a state-of-the-art Reasoning Layer based on General Symbolics, achieving high performance on various benchmarks without the need for fine-tuning or training costs. The approach aims to provide a pure performance uplift without negatively impacting model accuracy on reasoning tasks, highlighting the efficacy of the General Symbolic Reasoner (GSR). The paper argues for the necessity of developing new reasoning techniques to avoid diminishing returns in Large Language Model (LLM) performance.

Abstract: We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel
reasoning method called General Symbolics. This approach diverges from
reasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT),
and Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General
Symbolic Reasoner (GSR) is specifically structured around three key use cases:
tool-calling, code generation, and planning, demonstrating exemplary
performance across a total of seven benchmarks in their respective areas.
Notably, we are achieving SOTA scores of 66.66\% on Livecodebench v6, 89\% on
Instruction-Following Evals, and 24.4\% on ARC-AGI-2. We also present an
agentic coding IDE, developed using the principles of General Symbolics, which
achieves a state-of-the-art accuracy of 62.3\% on \texttt{SWE-Bench Lite}. We
are able to achieve these improvements without any finetuning or training
costs. Our Reasoning Layer is designed to provide a pure performance uplift,
ensuring that a model's accuracy on reasoning tasks is never negatively
impacted. We argue that incumbent methods will eventually lead to diminishing
returns in LLM performance, necessitating the development of new reasoning
techniques. This technical report details our approach at a high level and the
availability of the CoreThink models for reasoning-intensive use cases.

</details>


### [39] [Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning](https://arxiv.org/abs/2509.00975)
*Zifeng Ding,Shenyang Huang,Zeyu Cao,Emma Kondrup,Zachary Yang,Xingyue Huang,Yuan Sui,Zhangdie Yuan,Yuqicheng Zhu,Xianglong Hu,Yuan He,Farimah Poursafaei,Michael Bronstein,Andreas Vlachos*

Main category: cs.AI

TL;DR: ReaL-TG is a reinforcement learning framework that fine-tunes large language models for explainable link forecasting on temporal graphs. It outperforms existing models in ranking metrics and explanation quality. The evaluation protocol combines ranking metrics and an LLM-as-a-Judge system to assess reasoning quality and the impact of hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing neural approaches for temporal graph reasoning lack explainability and are not transferable to unseen graphs without retraining. Previous studies using large language models were limited to static graphs or synthetic TGs, without evaluating the quality of reasoning traces. The goal of this work is to address these limitations and provide explainable link forecasting on real-world temporal graphs.

Method: ReaL-TG employs reinforcement learning to fine-tune large language models for explainable link forecasting on real-world temporal graphs. An outcome-based reward system encourages self-exploration of reasoning strategies from graph structure and the generation of justifiable predictions. Evaluation includes a new protocol combining ranking metrics and an LLM-as-a-Judge system.

Result: Experiments with ReaL-TG-4B show superior performance to larger frontier LLMs like GPT-5 mini in ranking metrics and explanation quality, confirmed by both LLM judge assessment and human evaluation.

Conclusion: ReaL-TG, a reinforcement learning framework fine-tuning large language models, outperforms existing models in link forecasting on temporal graphs while providing high-quality explanations. The evaluation protocol combines ranking metrics and an LLM-as-a-Judge system to assess reasoning quality and the impact of hallucinations.

Abstract: Forecasting future links is a central task in temporal graph (TG) reasoning,
requiring models to leverage historical interactions to predict upcoming ones.
Traditional neural approaches, such as temporal graph neural networks, achieve
strong performance but lack explainability and cannot be applied to unseen
graphs without retraining. Recent studies have begun to explore using large
language models (LLMs) for graph reasoning, but most of them are constrained to
static graphs or small synthetic TGs and lack the evaluation of the quality of
reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced
Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that
fine-tunes LLMs to perform explainable link forecasting on real-world TGs.
ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning
strategies from graph structure and to produce explanations that directly
justify their predictions. To enable evaluation on LLM-generated reasoning
traces, we propose a new evaluation protocol combining ranking metrics with an
LLM-as-a-Judge system that assesses both the quality of reasoning and the
impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning
Qwen3-4B under our framework, show that it outperforms much larger frontier
LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality
explanations confirmed by both the LLM judge and human evaluation.

</details>


### [40] [Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation](https://arxiv.org/abs/2509.00987)
*Adib Bazgir,Amir Habibdoust,Yuwen Zhang,Xing Song*

Main category: cs.AI

TL;DR: 本综述论文研究了因果多智体LLM系统，介绍了设计、因果推理、反事实分析、因果关系发现和效应估计等方面。讨论了交互协议、评估方法、应用领域等，并指出了存在的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在复杂因果推理、发现和估计方面的局限性，本综述旨在探讨利用多智体系统，特别是多个LLM代理的协作或专业能力，来解决这些限制。

Method: 该综述论文通过研究因果多智体LLM系统的设计、应用和挑战，总结出该领域发展的主要趋势和未来方向。

Result: 通过全面探讨因果多智体LLM系统的设计、互动模式、评估方法和应用领域，本综述揭示了这一新兴领域的重要性和发展前景。

Conclusion: 本综述论文探讨了因果多智体LLM领域的新兴发展，着重介绍了这些系统设计的不同方面，包括因果推理和反事实分析，从数据中发现因果关系，以及对因果效应的估计。讨论了采用的多样化的结构模式和交互协议，从基于流水线处理和辩论框架到模拟环境和迭代改进循环。此外，还探讨了评估方法、基准测试和多样的应用领域，如科学发现、医疗保健、事实核查和个性化系统。最后，突出了这一协同领域中持续存在的挑战、未解研究问题以及有前景的未来方向，旨在提供对当前状态和潜在发展轨迹的全面概述。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various reasoning and generation tasks. However, their proficiency in complex
causal reasoning, discovery, and estimation remains an area of active
development, often hindered by issues like hallucination, reliance on spurious
correlations, and difficulties in handling nuanced, domain-specific, or
personalized causal relationships. Multi-agent systems, leveraging the
collaborative or specialized abilities of multiple LLM-based agents, are
emerging as a powerful paradigm to address these limitations. This review paper
explores the burgeoning field of causal multi-agent LLMs. We examine how these
systems are designed to tackle different facets of causality, including causal
reasoning and counterfactual analysis, causal discovery from data, and the
estimation of causal effects. We delve into the diverse architectural patterns
and interaction protocols employed, from pipeline-based processing and debate
frameworks to simulation environments and iterative refinement loops.
Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse
application domains where causal multi-agent LLMs are making an impact,
including scientific discovery, healthcare, fact-checking, and personalized
systems. Finally, we highlight the persistent challenges, open research
questions, and promising future directions in this synergistic field, aiming to
provide a comprehensive overview of its current state and potential trajectory.

</details>


### [41] [Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First](https://arxiv.org/abs/2509.00997)
*Shu Liu,Soujanya Ponnapalli,Shreya Shankar,Sepanta Zeighami,Alan Zhu,Shubham Agarwal,Ruiqi Chen,Samion Suwito,Shuo Yuan,Ion Stoica,Matei Zaharia,Alvin Cheung,Natacha Crooks,Joseph E. Gonzalez,Aditya G. Parameswaran*

Main category: cs.AI

TL;DR: 本研究指出，大型语言模型代理将成为未来数据系统的主要工作负载。提出了数据系统需要更本地支持代理工作负载的观点，并通过识别代理推测的特征，提出了新的研究机会，包括新的查询界面、查询处理技术和代理内存存储。


<details>
  <summary>Details</summary>
Motivation: 对于未来，大语言模型代理将成为数据系统中的主要工作负载。代理推测的体积和低效性可能对现今的数据系统构成挑战

Method: 通过识别代理推测的特征，例如规模、异质性、冗余性和可操纵性，提出了一些新的研究机会

Result: 提出了代理优先数据系统架构的一些新研究机会，包括新的查询界面、新的查询处理技术和新的代理内存存储

Conclusion: 数据系统需要更本地化地支持代理工作负载，并提出了一种新的代理优先数据系统架构的研究机会

Abstract: Large Language Model (LLM) agents, acting on their users' behalf to
manipulate and analyze data, are likely to become the dominant workload for
data systems in the future. When working with data, agents employ a
high-throughput process of exploration and solution formulation for the given
task, one we call agentic speculation. The sheer volume and inefficiencies of
agentic speculation can pose challenges for present-day data systems. We argue
that data systems need to adapt to more natively support agentic workloads. We
take advantage of the characteristics of agentic speculation that we identify,
i.e., scale, heterogeneity, redundancy, and steerability - to outline a number
of new research opportunities for a new agent-first data systems architecture,
ranging from new query interfaces, to new query processing techniques, to new
agentic memory stores.

</details>


### [42] [Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction](https://arxiv.org/abs/2509.01016)
*Aishni Parab,Hongjing Lu,Ying Nian Wu,Sumit Gulwani*

Main category: cs.AI

TL;DR: 这项研究比较了基于LLM的假设搜索框架和直接程序生成方法在少样本规则归纳任务中的表现。发现LLM的假设搜索实现了与人类相当的性能，而直接程序生成明显落后。错误分析揭示了假设生成中的关键瓶颈，并为改进程序归纳方法提供了方向。因此，LLM的假设搜索框架在归纳推理建模方面表现出潜力，同时也凸显了构建更高效系统的挑战。


<details>
  <summary>Details</summary>
Motivation: To investigate the performance of LLM-based hypothesis search and direct program generation in inductive reasoning tasks, aiming to understand their strengths and weaknesses in few-shot rule induction scenarios.

Method: Comparison of LLM-based hypothesis search framework with direct program generation approaches on few-shot rule induction tasks.

Result: LLM-based hypothesis search framework achieves performance comparable to humans, while direct program generation lags behind. Error analysis reveals key bottlenecks in hypothesis generation and guides future research directions in program induction methods.

Conclusion: LLM-based hypothesis search framework outperforms direct program generation in few-shot rule induction tasks, highlighting its potential for modeling inductive reasoning. Error analysis identifies bottlenecks in hypothesis generation and provides guidance for improving program induction methods.

Abstract: Inductive reasoning enables humans to infer abstract rules from limited
examples and apply them to novel situations. In this work, we compare an
LLM-based hypothesis search framework with direct program generation approaches
on few-shot rule induction tasks. Our findings show that hypothesis search
achieves performance comparable to humans, while direct program generation
falls notably behind. An error analysis reveals key bottlenecks in hypothesis
generation and suggests directions for advancing program induction methods.
Overall, this paper underscores the potential of LLM-based hypothesis search
for modeling inductive reasoning and the challenges in building more efficient
systems.

</details>


### [43] [Quantum-like Coherence Derived from the Interaction between Chemical Reaction and Its Environment](https://arxiv.org/abs/2509.01021)
*Yukio-Pegio Gunji,Andrew Adamatzky,Panagiotis Mougkogiannis,Andrei Khrenikov*

Main category: cs.AI

TL;DR: 该论文研究了开放式计算在化学反应中的应用，探讨了Token计算和Type计算在化学反应中的作用，并表明通过这些计算方式的相互作用，可以实现量子一致性情况，促进信号传输。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于探索人工智能和自然智能的计算过程之间的区别，并将开放式计算应用于化学反应中，以研究化学反应中的计算过程和波动调节系统。通过对Token计算和Type计算的研究，揭示了其在化学反应中的作用，进而暗示了类量子一致性的产生机制。

Method: 该论文通过定义闭式计算和开放式计算，将开放式计算应用于化学反应中，研究了化学反应中的计算过程和执行环境的融合，以及Token计算和Type计算之间的相互作用。通过模拟化学反应和分子聚集程度的关系，研究了化学反应中的波动调节系统。最终通过自组织临界现象和量子逻辑展示了开放式计算在化学反应中的效果。

Result: 该论文通过研究开放式计算在化学反应中的应用，展示了开放式计算对波动调节系统的重要性，并通过Token计算和Type计算的相互作用，实现了量子一致性的子空间之间的相互作用，从而产生了类量子一致性情况。最终实现了信号传输的功能。

Conclusion: 该论文从人工智能和自然智能的计算过程对比入手，定义闭式计算和开放式计算，将开放式计算应用于化学反应中。研究形成混合物和无效化计算过程与执行环境，将二者合而为一，创造出能够调节波动的系统。通过将计算看作化学反应、执行环境看作分子聚集程度的模型化学反应，导致化学反应在集群和解集群中不断进行，浓度不再具有显著意义。开放式计算分为Token计算和Type计算，分别关注化学分子的个体行为和规范性行为，两者相互交织构建系统。在该系统中，Token计算表现为自组织临界现象，Type计算展现出量子逻辑。通过它们的相互作用，实现了波动的招募，产生了对应于不同希尔伯特空间的量子一致性的子空间之间的相互作用。由此产生了尖波，实现了信号传输，被称为类量子一致性，暗示着控制尖波和生物化学节律的酶的来源。

Abstract: By uncovering the contrast between Artificial Intelligence and Natural-born
Intelligence as a computational process, we define closed computing and open
computing, and implement open computing within chemical reactions. This
involves forming a mixture and invalidation of the computational process and
the execution environment, which are logically distinct, and coalescing both to
create a system that adjusts fluctuations. We model chemical reactions by
considering the computation as the chemical reaction and the execution
environment as the degree of aggregation of molecules that interact with the
reactive environment. This results in a chemical reaction that progresses while
repeatedly clustering and de-clustering, where concentration no longer holds
significant meaning. Open computing is segmented into Token computing, which
focuses on the individual behavior of chemical molecules, and Type computing,
which focuses on normative behavior. Ultimately, both are constructed as an
interplay between the two. In this system, Token computing demonstrates
self-organizing critical phenomena, while Type computing exhibits quantum
logic. Through their interplay, the recruitment of fluctuations is realized,
giving rise to interactions between quantum logical subspaces corresponding to
quantum coherence across different Hilbert spaces. As a result, spike waves are
formed, enabling signal transmission. This occurrence may be termed
quantum-like coherence, implying the source of enzymes responsible for
controlling spike waves and biochemical rhythms.

</details>


### [44] [Symbolic Planning and Multi-Agent Path Finding in Extremely Dense Environments with Movable Obstacles](https://arxiv.org/abs/2509.01022)
*Bo Fu,Zhe Chen,Rahul Chandan,Alex Barbosa,Michael Caldara,Joey Durham,Federico Pecora*

Main category: cs.AI

TL;DR: 提出了Block Rearrangement Problem (BRaP)，定义为图搜索问题。提出了五种基于搜索的解决方案算法，进行了实证评估，表现高效，尤其适用于80x80网格中深埋的块。


<details>
  <summary>Details</summary>
Motivation: 介绍了Block Rearrangement Problem (BRaP)的挑战性，即在大型仓库管理中重新排列存储块以实现目标状态。启发于滑动拼图问题的直觉，提出了解决方案算法。

Method: 提出了五种基于搜索的解决方案算法，包括联合配置空间搜索、经典规划、多智能体路径规划和专家启发式。对这些方法进行了实证评估，评估计划质量和可伸缩性。

Result: 方法在创建重新排列计划方面表现高效，尤其适用于80x80网格中深埋的块。

Conclusion: 提出Block Rearrangement Problem (BRaP)，即大型仓库管理的挑战性组成部分，涉及在密集网格内重新排列存储块以实现目标状态。我们将BRaP形式化定义为图搜索问题。通过滑动拼图问题的直觉，我们提出了五种基于搜索的解决方案算法，利用联合配置空间搜索、经典规划、多智能体路径规划和专家启发式。我们从计划质量和可伸缩性的角度对这五种方法进行了实证评估。尽管搜索空间大小与块数之间呈指数关系，但我们的方法在80x80网格中高效创建重新排列计划，尤其适用于深埋的块。

Abstract: We introduce the Block Rearrangement Problem (BRaP), a challenging component
of large warehouse management which involves rearranging storage blocks within
dense grids to achieve a target state. We formally define the BRaP as a graph
search problem. Building on intuitions from sliding puzzle problems, we propose
five search-based solution algorithms, leveraging joint configuration space
search, classical planning, multi-agent pathfinding, and expert heuristics. We
evaluate the five approaches empirically for plan quality and scalability.
Despite the exponential relation between search space size and block number,
our methods demonstrate efficiency in creating rearrangement plans for deeply
buried blocks in up to 80x80 grids.

</details>


### [45] [FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games](https://arxiv.org/abs/2509.01052)
*Jaewoo Ahn,Junseo Kim,Heeseung Yun,Jaehyeon Son,Dongmin Park,Jaewoong Cho,Gunhee Kim*

Main category: cs.AI

TL;DR: 本研究介绍了FlashAdventure基准测试，旨在测试智能体完成整个冒险游戏故事情节的能力，解决观察-行为差距问题。实验发现当前GUI智能体在此方面面临挑战，但COAST代理框架通过利用长期线索记忆已经取得了一些进展。然而，仍然存在人类与智能体之间的表现差距，需要进行进一步研究以缩小这一差距。


<details>
  <summary>Details</summary>
Motivation: 由于现有游戏基准测试缺乏多样性，并很少评估智能体完成整个故事情节的能力，因此本研究引入了FlashAdventure基准测试。同时，为了解决观察-行为差距问题，提出了新的自动化游戏评估器和代理框架。研究的动机在于改善智能体在冒险游戏等具有复杂交互的数字环境中的表现。

Method: 介绍了FlashAdventure基准测试以及两个新提出的工具：CUA-as-a-Judge和COAST。通过实验证明现有GUI智能体在完成整个故事情节方面存在困难，而COAST改善了任务完成率，缩小了观察-行为差距。

Result: 实验证明当前GUI智能体在完成整个故事情节方面困难重重，但COAST通过改善观察-行为差距已经取得了一定进展。人类与表现最佳的智能体之间存在显著差距，需要进一步研究来缩小这一差距。

Conclusion: 对话框智能体（GUI agents）利用LLMs在与不同数字环境进行交互方面表现出很大潜力。我们介绍了FlashAdventure，一个由34个Flash基础冒险游戏组成的基准测试，旨在测试完整故事情节的完成，解决观察-行为差距问题。同时，我们提出了CUA-as-a-Judge，一个自动化游戏评估器，以及COAST，一个利用长期线索记忆来更好规划并解决顺序任务的代理框架。实验证明目前GUI智能体在完整故事情节方面存在困难，而COAST通过有效解决观察-行为差距来提高里程碑任务的完成率。尽管如此，人类与表现最佳的智能体之间存在显著差距，需要进一步研究努力来缩小这一差距。

Abstract: GUI agents powered by LLMs show promise in interacting with diverse digital
environments. Among these, video games offer a valuable testbed due to their
varied interfaces, with adventure games posing additional challenges through
complex, narrative-driven interactions. Existing game benchmarks, however, lack
diversity and rarely evaluate agents on completing entire storylines. To
address this, we introduce FlashAdventure, a benchmark of 34 Flash-based
adventure games designed to test full story arc completion and tackle the
observation-behavior gap: the challenge of remembering and acting on earlier
gameplay information. We also propose CUA-as-a-Judge, an automated gameplay
evaluator, and COAST, an agentic framework leveraging long-term clue memory to
better plan and solve sequential tasks. Experiments show current GUI agents
struggle with full story arcs, while COAST improves milestone completion by
bridging the observation-behavior gap. Nonetheless, a marked discrepancy
between humans and best-performing agents warrants continued research efforts
to narrow this divide.

</details>


### [46] [VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use](https://arxiv.org/abs/2509.01055)
*Dongfu Jiang,Yi Lu,Zhuofeng Li,Zhiheng Lyu,Ping Nie,Haozhe Wang,Alex Su,Hui Chen,Kai Zou,Chao Du,Tianyu Pang,Wenhu Chen*

Main category: cs.AI

TL;DR: 提出了VerlTool框架，通过引入设计原则解决了RLVR和ARLT方法中的问题。框架支持多模态观测令牌，扩展了单轮RLVR范例，在多个ARLT领域取得竞争性性能。模块化插件架构降低了开发成本，提供可扩展的基础。


<details>
  <summary>Details</summary>
Motivation: 现有的ARLT方法存在着分段、同步执行瓶颈和领域间的有限可扩展性等问题，限制了广泛社区采用和算法创新。为了解决这些低效率问题，提出引入VerlTool框架，通过系统设计优化多轮工具增强强化学习机制，具有高度可扩展性和广泛应用的潜力。

Method: 引入了VerlTool框架，通过统一和模块化设计原则解决了RLVR和ARLT方法中的局限性，提供了四个关键贡献点。采用异步推出执行方式，消除同步瓶颈，获得近2倍加速。采用多轮轨迹和多模态观测令牌扩展了单轮RLVR范例，同时在数学推理、知识问答、SQL生成、视觉推理、网络搜索和软件工程任务上进行了训练和评估。

Result: VerlTool是一个统一和模块化的框架，通过引入设计原则解决了RLVR和ARLT方法中的问题。在6个ARLT领域中展现了竞争性性能，并提供了可与专门系统相匹敌的结果。开源代码链接为https://github.com/TIGER-AI-Lab/verl-tool。

Conclusion: 介绍了VerlTool，它是一个统一且模块化的框架，通过系统设计原则解决了RLVR和ARLT方法中存在的局限性。VerlTool提供了四个关键贡献点，并在6个ARLT领域展示了竞争性表现。通过在多轮轨迹中引入多模态观测令牌，扩展了单轮RLVR范例，训练和评估模型在数学推理、知识问答、SQL生成、视觉推理、网络搜索和软件工程任务上取得了可与专门系统相媲美的结果。模块化插件架构实现快速集成工具，只需轻量级Python定义，大大降低了开发成本，并为工具增强的RL研究提供可扩展的基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated
success in enhancing LLM reasoning capabilities, but remains limited to
single-turn interactions without tool integration. While recent Agentic
Reinforcement Learning with Tool use (ARLT) approaches have emerged to address
multi-turn tool interactions, existing works develop task-specific codebases
that suffer from fragmentation, synchronous execution bottlenecks, and limited
extensibility across domains. These inefficiencies hinder broader community
adoption and algorithmic innovation. We introduce VerlTool, a unified and
modular framework that addresses these limitations through systematic design
principles. VerlTool provides four key contributions: (1) upstream alignment
with VeRL ensuring compatibility and simplified maintenance, (2) unified tool
management via standardized APIs supporting diverse modalities including code
execution, search, SQL databases, and vision processing, (3) asynchronous
rollout execution achieving near 2$\times$ speedup by eliminating
synchronization bottlenecks, and (4) comprehensive evaluation demonstrating
competitive performance across 6 ARLT domains. Our framework formalizes ARLT as
multi-turn trajectories with multi-modal observation tokens (text/image/video),
extending beyond single-turn RLVR paradigms. We train and evaluate models on
mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web
search, and software engineering tasks, achieving results comparable to
specialized systems while providing unified training infrastructure. The
modular plugin architecture enables rapid tool integration requiring only
lightweight Python definitions, significantly reducing development overhead and
providing a scalable foundation for tool-augmented RL research. Our code is
open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.

</details>


### [47] [Robix: A Unified Model for Robot Interaction, Reasoning and Planning](https://arxiv.org/abs/2509.01106)
*Huang Fang,Mengxi Zhang,Heng Dong,Wei Li,Zixuan Wang,Qifeng Zhang,Xueyun Tian,Yucheng Hu,Hang Li*

Main category: cs.AI

TL;DR: Robix is a unified model that enhances robot reasoning, task planning, and natural language interaction. It outperforms existing baselines in interactive task execution, demonstrating strong generalization across diverse instruction types and user-involved tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enable robots to follow complex instructions, plan long-horizon tasks, and interact naturally with humans within an end-to-end framework. It introduces novel capabilities like proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution.

Method: Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: continued pretraining, supervised finetuning, and reinforcement learning. It dynamically generates atomic commands for low-level controller and verbal responses for human interaction.

Result: Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, showing strong generalization across diverse instruction types and user-involved tasks such as table bussing, grocery shopping, and dietary filtering.

Conclusion: Robix is a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. It outperforms open-source and commercial baselines in interactive task execution, showing strong generalization across diverse instruction types.

Abstract: We introduce Robix, a unified model that integrates robot reasoning, task
planning, and natural language interaction within a single vision-language
architecture. Acting as the high-level cognitive layer in a hierarchical robot
system, Robix dynamically generates atomic commands for the low-level
controller and verbal responses for human interaction, enabling robots to
follow complex instructions, plan long-horizon tasks, and interact naturally
with human within an end-to-end framework. Robix further introduces novel
capabilities such as proactive dialogue, real-time interruption handling, and
context-aware commonsense reasoning during task execution. At its core, Robix
leverages chain-of-thought reasoning and adopts a three-stage training
strategy: (1) continued pretraining to enhance foundational embodied reasoning
abilities including 3D spatial understanding, visual grounding, and
task-centric reasoning; (2) supervised finetuning to model human-robot
interaction and task planning as a unified reasoning-action sequence; and (3)
reinforcement learning to improve reasoning-action consistency and long-horizon
task coherence. Extensive experiments demonstrate that Robix outperforms both
open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in
interactive task execution, demonstrating strong generalization across diverse
instruction types (e.g., open-ended, multi-stage, constrained, invalid, and
interrupted) and various user-involved tasks such as table bussing, grocery
shopping, and dietary filtering.

</details>


### [48] [Heads or Tails: A Simple Example of Causal Abstractive Simulation](https://arxiv.org/abs/2509.01136)
*Gabriel Simmons*

Main category: cs.AI

TL;DR: 该论文介绍了如何使用因果抽象模拟来形式化语言模型模拟的例子，阐述了语言模型模拟其他系统的方法。研究者应用因果抽象模拟来连接统计测试实践与因果性基础，为语言模型模拟领域奠定了基础。该研究可能对从业者、人工智能哲学家和数学家有重要意义。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将因果抽象模拟应用于语言模型模拟，并探讨语言模型在模拟中的失败和成功案例，为人工智能、心灵哲学以及数学等领域的研究者提供了新的观点和应用。

Method: 论文使用因果抽象模拟来连接统计基准测试实践与因果性的坚实形式基础，为语言模型模拟领域的从业者提供了一种方法。

Result: 论文呈现了因果抽象模拟在模拟公平硬币抛掷中的例子，阐明了语言模型的模拟方式，并提供了一个成功的案例。

Conclusion: 该论文展示了如何使用因果抽象模拟来形式化语言模型模拟的简单示例，说明语言模型如何模拟其他系统，提供了一个基于因果描述的证明方法。

Abstract: This note illustrates how a variety of causal abstraction arXiv:1707.00819
arXiv:1812.03789, defined here as causal abstractive simulation, can be used to
formalize a simple example of language model simulation. This note considers
the case of simulating a fair coin toss with a language model. Examples are
presented illustrating the ways language models can fail to simulate, and a
success case is presented, illustrating how this formalism may be used to prove
that a language model simulates some other system, given a causal description
of the system. This note may be of interest to three groups. For practitioners
in the growing field of language model simulation, causal abstractive
simulation is a means to connect ad-hoc statistical benchmarking practices to
the solid formal foundation of causality. Philosophers of AI and philosophers
of mind may be interested as causal abstractive simulation gives a precise
operationalization to the idea that language models are role-playing
arXiv:2402.12422. Mathematicians and others working on causal abstraction may
be interested to see a new application of the core ideas that yields a new
variation of causal abstraction.

</details>


### [49] [Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping](https://arxiv.org/abs/2509.01182)
*Wonduk Seo,Taesub Shin,Hyunjin An,Dokyun Kim,Seunghyun Lee*

Main category: cs.AI

TL;DR: 本文提出了Q2K框架，通过多代理协作解决电子商务中识别产品是否引用相同SKU的挑战。Q2K在实验中表现优异，超越了现有方法，在困难情景下实现更高的准确性和稳健性，同时保持效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在电子商务环境中，识别两个产品列表是否引用相同SKU是一个持续挑战，特别是当缺乏明确标识符且产品名称在不同平台上变化范围广泛时。传统的基于规则的启发式方法和关键词相似性往往会忽视品牌、规格或捆绑配置中的细微差异，导致产品被错误分类。为了克服这些局限性，本文提出了Q2K框架，旨在利用大型语言模型进行可靠的SKU映射。

Method: 本文提出了Question to Knowledge (Q2K)框架，其中包括 Reasoning Agent、Knowledge Agent 和 Deduplication Agent 三个主要组成部分，以解决在电子商务中识别两个产品列表是否引用相同SKU的挑战。通过生成针对性的消除歧义问题、利用网络搜索来解决问题，并通过重复使用验证过的推理轨迹来减少冗余和确保一致性。在一些困难情景下，Q2K通过人类介入机制进一步优化不确定情况。

Result: 通过实验验证，Q2K在真实消费品数据集上的表现优于现有的强基线方法，尤其在捆绑识别和品牌起源消除歧义等困难情景下表现出更高的准确性和稳健性。重用推理信息有助于在保持准确性的同时提高效率。

Conclusion: 本文提出了一种名为Question to Knowledge (Q2K)的多代理框架，利用大型语言模型（LLMs）进行可靠的SKU映射。通过对真实消费品数据集进行实验，证明Q2K在困难情景中表现出更高的准确性和鲁棒性，超越了强基线方法。通过重用检索到的推理信息而不是发出重复搜索请求，Q2K在准确性和效率之间取得平衡，为产品集成提供了可扩展和可解释的解决方案。

Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit
(SKU) is a persistent challenge in ecommerce, especially when explicit
identifiers are missing and product names vary widely across platforms. Rule
based heuristics and keyword similarity often misclassify products by
overlooking subtle distinctions in brand, specification, or bundle
configuration. To overcome these limitations, we propose Question to Knowledge
(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for
reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates
targeted disambiguation questions, (2) a Knowledge Agent that resolves them via
focused web searches, and (3) a Deduplication Agent that reuses validated
reasoning traces to reduce redundancy and ensure consistency. A human in the
loop mechanism further refines uncertain cases. Experiments on real world
consumer goods datasets show that Q2K surpasses strong baselines, achieving
higher accuracy and robustness in difficult scenarios such as bundle
identification and brand origin disambiguation. By reusing retrieved reasoning
instead of issuing repeated searches, Q2K balances accuracy with efficiency,
offering a scalable and interpretable solution for product integration.

</details>


### [50] [Towards Open-World Retrieval-Augmented Generation on Knowledge Graph: A Multi-Agent Collaboration Framework](https://arxiv.org/abs/2509.01238)
*Jiasheng Xu,Mingda Li,Yongqiang Tang,Peijie Wang,Wensheng Zhang*

Main category: cs.AI

TL;DR: AnchorRAG introduces a multi-agent collaboration framework for open-world question answering without predefined anchor entities. It outperforms existing approaches, improves retrieval robustness, and establishes new state-of-the-art results in real-world question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Graph-based Retrieval-Augmented Generation (RAG) approaches rely on predefined anchor entities, limiting robustness in open-world settings. AnchorRAG aims to overcome this limitation by dynamically identifying anchor entities and employing a multi-agent collaboration framework for more accurate question answering.

Method: AnchorRAG proposes a multi-agent collaboration framework where a predictor agent identifies candidate anchor entities, retriever agents conduct multi-hop explorations, and a supervisor agent formulates retrieval strategy. This framework improves retrieval robustness and mitigates the impact of ambiguous or erroneous anchors.

Result: Extensive experiments on four public benchmarks show that AnchorRAG outperforms existing methods and achieves new state-of-the-art results in real-world question answering tasks.

Conclusion: AnchorRAG, a novel multi-agent collaboration framework for open-world Retrieval-Augmented Generation (RAG) without predefined anchor entities, significantly outperforms existing baselines and establishes new state-of-the-art results in real-world question answering tasks.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning. However, their dependence on static
training corpora makes them prone to factual errors and knowledge gaps.
Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating
external knowledge sources, especially structured Knowledge Graphs (KGs), which
provide explicit semantics and efficient retrieval. Existing KG-based RAG
approaches, however, generally assume that anchor entities are accessible to
initiate graph traversal, which limits their robustness in open world settings
where accurate linking between the query and the entity is unreliable. To
overcome this limitation, we propose AnchorRAG, a novel multi-agent
collaboration framework for open-world RAG without the predefined anchor
entities. Specifically, a predictor agent dynamically identifies candidate
anchor entities by aligning user query terms with KG nodes and initializes
independent retriever agents to conduct parallel multi-hop explorations from
each candidate. Then a supervisor agent formulates the iterative retrieval
strategy for these retriever agents and synthesizes the resulting knowledge
paths to generate the final answer. This multi-agent collaboration framework
improves retrieval robustness and mitigates the impact of ambiguous or
erroneous anchors. Extensive experiments on four public benchmarks demonstrate
that AnchorRAG significantly outperforms existing baselines and establishes new
state-of-the-art results on the real-world question answering tasks.

</details>


### [51] [Towards Agentic OS: An LLM Agent Framework for Linux Schedulers](https://arxiv.org/abs/2509.01245)
*Yusheng Zheng,Yanpeng Hu,Wei Zhang,Andi Quinn*

Main category: cs.AI

TL;DR: 该论文介绍了SchedCP框架，使用人工智能实现自主优化Linux调度器，解决语义鸿沟问题。实验证明SchedCP相比朴素方法性能提升1.79倍，成本降低13倍，成功率高。推动操作系统朝向自我优化方向发展。


<details>
  <summary>Details</summary>
Motivation: 操作系统调度器存在语义鸿沟，人工智能能够辅助优化调度器，但需要解决人工智能的性能和系统执行的矛盾。本文旨在构建一个自主优化系统，实现智能、高效的调度器优化，以提升系统性能。

Method: 介绍了SchedCP框架，提出了解决操作系统调度器语义鸿沟的方法，设计了基于人工智能的自主优化系统。通过Model Context Protocol（MCP）服务器实现了关键服务，如工作负载分析引擎、调度策略库和执行验证器。使用sched-agent系统进行实验验证。

Result: 实现了SchedCP框架，展示了解决调度器语义鸿沟的能力。通过sched-agent系统实验，取得了1.79倍性能提升和13倍成本降低，同时保持高成功率。通过SchedCP，使得系统优化更加普及化，推动了操作系统朝向真正自我优化的方向发展。

Conclusion: 介绍了SchedCP框架，该框架使得大型语言模型代理能够在不需要人类干预的情况下安全高效地优化Linux调度器。通过解耦控制平面，将人工智能的语义推理和系统的执行过程分开，实现了更好的性能优化。使用Model Context Protocol（MCP）服务器实现，提供了稳定接口，包括工作负载分析引擎、调度策略库和执行验证器等关键服务。sched-agent是一个多代理系统，自主分析工作负载，合成定制的eBPF调度策略，并通过sched_ext基础架构部署。实验结果表明，SchedCP相比朴素的代理方法，性能提升最高达1.79倍，成本降低13倍，同时成功率保持较高。通过弥合语义差距，SchedCP使得系统优化普及化，推动了向真正自我优化、应用感知的操作系统迈进。代码已在 https://github.com/eunomia-bpf/schedcp 开源。

Abstract: Operating system schedulers suffer from a fundamental semantic gap, where
kernel policies fail to understand application-specific needs, leading to
suboptimal performance. We introduce SchedCP, the first framework that enables
fully autonomous Large Language Model (LLM) agents to safely and efficiently
optimize Linux schedulers without human involvement. Our core insight is that
the challenge is not merely to apply a better LLM, but to architect a decoupled
control plane that separates the AI's role of semantic reasoning ("what to
optimize") from the system's role of execution ("how to observe and act").
Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable
interface with three key services: a Workload Analysis Engine, an evolving
Scheduler Policy Repository, and an Execution Verifier that validates all
AI-generated code and configure before deployment with static and dynamic
analysis.
  We demonstrate this architecture's power with sched-agent, a multi-agent
system that autonomously analyzes workloads, synthesizes custom eBPF scheduling
policies, and deploys them via the sched\_ext infrastructure. Our evaluation
shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x
cost reduction compared to naive agentic approaches, all while maintaining high
success rate. By bridging the semantic gap, SchedCP democratizes expert-level
system optimization and represents a step towards creating truly
self-optimizing, application-aware operating systems. The code is open-sourced
in https://github.com/eunomia-bpf/schedcp

</details>


### [52] [Communicative Agents for Slideshow Storytelling Video Generation based on LLMs](https://arxiv.org/abs/2509.01277)
*Jingxing Fan,Jinrong Shen,Yusheng Yao,Shuangqing Wang,Qian Wang,Yuling Wang*

Main category: cs.AI

TL;DR: VGTeam introduces a slide show video generation system that uses large language models for efficient and scalable video creation at low cost. It democratizes video production, emphasizing the transformative potential of language models in creative fields.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the study is the acceleration of text-to-video generation due to the advancement of artificial intelligence and the high computational costs associated with conventional models. VGTeam aims to redefine the video creation pipeline by leveraging large language models for improved efficiency and scalability.

Method: The study introduces Video-Generation-Team (VGTeam) composed of communicative agents for scriptwriting, scene creation, and audio design. These agents collaborate within a chat tower workflow to transform textual prompts into slide-style narrative videos, following traditional video production stages.

Result: VGTeam achieves remarkable improvements in efficiency and scalability, generating videos at a low cost of $0.103 with a successful rate of 98.4%. The system maintains a high level of creative fidelity and customization, democratizing video production and showcasing the potential of language models in creative domains.

Conclusion: VGTeam proposes a novel slide show video generation system that integrates large language models for efficient and scalable video creation at a low cost. It maintains creative fidelity and customization, democratizing video production and highlighting the transformative potential of language models in creative domains.

Abstract: With the rapid advancement of artificial intelligence (AI), the proliferation
of AI-generated content (AIGC) tasks has significantly accelerated developments
in text-to-video generation. As a result, the field of video production is
undergoing a transformative shift. However, conventional text-to-video models
are typically constrained by high computational costs.
  In this study, we propose Video-Generation-Team (VGTeam), a novel slide show
video generation system designed to redefine the video creation pipeline
through the integration of large language models (LLMs). VGTeam is composed of
a suite of communicative agents, each responsible for a distinct aspect of
video generation, such as scriptwriting, scene creation, and audio design.
These agents operate collaboratively within a chat tower workflow, transforming
user-provided textual prompts into coherent, slide-style narrative videos.
  By emulating the sequential stages of traditional video production, VGTeam
achieves remarkable improvements in both efficiency and scalability, while
substantially reducing computational overhead. On average, the system generates
videos at a cost of only $0.103, with a successful generation rate of 98.4%.
Importantly, this framework maintains a high degree of creative fidelity and
customization.
  The implications of VGTeam are far-reaching. It democratizes video production
by enabling broader access to high-quality content creation without the need
for extensive resources. Furthermore, it highlights the transformative
potential of language models in creative domains and positions VGTeam as a
pioneering system for next-generation content creation.

</details>


### [53] [GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models](https://arxiv.org/abs/2509.01308)
*Mattia Tritto,Giuseppe Farano,Dario Di Palma,Gaetano Rossiello,Fedelucio Narducci,Dharmashankar Subramanian,Tommaso Di Noia*

Main category: cs.AI

TL;DR: Text-to-SQL task advanced with Large Language Models (LLMs) but still struggles with complex queries. Outcome Reward Models (ORMs) proposed as a better heuristic than Best-of-N (BoN) and Majority Voting (Maj) for aligning model predictions with user intent. ORM outperforms ex-BoN and Maj, achieving higher execution accuracy gains on benchmark datasets. ORM performance enhanced with fine-tuning models like OmniSQL and benefits from increased candidate evaluation. ORM competitive on simple queries and benefits from evaluating more candidates.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the limitations of current Large Language Models (LLMs) in accurately translating natural language questions into SQL queries, especially for complex queries requiring precise alignment with the database schema. Test-time strategies like Best-of-N (BoN) and Majority Voting (Maj) are commonly used, but they rely on surface-level heuristics. The paper aims to explore the potential of Outcome Reward Models (ORMs) in improving model predictions and aligning them better with user intent in Text-to-SQL tasks.

Method: The paper evaluates Outcome Reward Models (ORMs) as a heuristic for Best-of-N (BoN) in Text-to-SQL tasks. ORMs are compared with execution-based BoN (ex-BoN) and Majority Voting (Maj) methods. A framework for training ORMs in the Text-to-SQL domain is proposed. Various open-source Large Language Models (LLMs) like Qwen2, Granite3, and Llama3 are finetuned and assessed on BIRD and SPIDER benchmarks. The study demonstrates the advantages of using ORMs and shows performance gains across different model families and benchmark datasets.

Result: The results indicate that Outcome Reward Models (ORMs) outperform execution-based BoN (ex-BoN) and Majority Voting (Maj) methods in the Text-to-SQL task. ORM achieves higher execution accuracy gains on benchmark datasets like BIRD and SPIDER. Fine-tuning models aligned with SQL generation, such as OmniSQL, enhances ORM performance. ORM shows competitive results on simple queries and benefits from evaluating an increased number of candidates compared to ex-BoN and Maj methods.

Conclusion: ORMs are evaluated as an effective heuristic for Best-of-N (BoN) in Text-to-SQL tasks, outperforming ex-BoN and Majority Voting (Maj) methods. A framework for training ORMs for the Text-to-SQL task is introduced, showing superior execution accuracy gains on benchmark datasets. ORM performance is particularly enhanced when fine-tuning models aligned with SQL generation, such as OmniSQL, and benefits from an increased number of candidates for evaluation.

Abstract: Text-to-SQL, the task of translating natural language questions into SQL
queries, has significantly advanced with the introduction of Large Language
Models (LLMs), broadening database accessibility for a wide range of users.
Despite substantial progress in generating valid SQL, current LLMs still
struggle with complex queries that require precise alignment between user
intent and the database schema. To mitigate this, test-time strategies such as
Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the
assumption that LLMs can generate correct answers but may require multiple
attempts. However, these methods rely on surface-level heuristics, selecting
either the syntactically correct query through execution-based BoN (ex-BoN) or
the most frequently generated query with Maj. Recently, Outcome Reward Models
(ORMs), which assign utility scores to generated outputs based on semantic
correctness, have emerged as a promising approach for better aligning model
predictions with user intent. Nevertheless, their application to Text-to-SQL
remains largely underexplored.
  In this work, we evaluate ORMs as an effective heuristic for BoN, compare
them with ex-BoN and Maj, and introduce a framework for training ORMs for the
Text-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks,
finetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3
model families. Our results show that ORMs outperform ex-BoN and Maj, achieving
execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and
+2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that
finetuning models already aligned with SQL generation, such as OmniSQL, yields
superior ORM performance. Additionally, we observe that ORMs achieve
competitive results on simple queries and benefit more from an increased number
of candidates compared to ex-BoN and Maj.

</details>


### [54] [Conformal Predictive Monitoring for Multi-Modal Scenarios](https://arxiv.org/abs/2509.01338)
*Francesca Cairoli,Luca Bortolussi,Jyotirmoy V. Deshmukh,Lars Lindemann,Nicola Paoletti*

Main category: cs.AI

TL;DR: GenQPM introduces a method that uses deep generative models to predict satisfaction values of temporal logic properties for stochastic systems with multi-modal dynamics. It improves prediction interval informativeness compared to existing mode-agnostic approaches, as shown in agent navigation and autonomous driving tasks.


<details>
  <summary>Details</summary>
Motivation: Existing QPM methods lack the ability to capture mode-specific satisfaction information in systems with multi-modal dynamics, leading to overly conservative and uninformative prediction intervals. GenQPM aims to address this limitation by leveraging deep generative models to improve the accuracy and informativeness of predictive monitoring in such scenarios.

Method: GenQPM utilizes deep generative models, specifically score-based diffusion models, to approximate probabilistic and multi-modal system dynamics. It employs a mode classifier to partition predicted trajectories by dynamical mode and applies conformal inference to generate statistically valid, mode-specific prediction intervals.

Result: GenQPM demonstrates improved prediction intervals in scenarios with multi-modal dynamics, providing more meaningful mode-specific satisfaction information compared to mode-agnostic methods. The method shows effectiveness in agent navigation and autonomous driving tasks, highlighting its potential in enhancing predictive monitoring in dynamic environments.

Conclusion: GenQPM leverages deep generative models to address the limitations of existing QPM methods in predicting satisfaction values of temporal logic properties for stochastic systems with multi-modal dynamics. The method improves the informativeness of prediction intervals compared to mode-agnostic approaches, as demonstrated in agent navigation and autonomous driving tasks.

Abstract: We consider the problem of quantitative predictive monitoring (QPM) of
stochastic systems, i.e., predicting at runtime the degree of satisfaction of a
desired temporal logic property from the current state of the system. Since
computational efficiency is key to enable timely intervention against predicted
violations, several state-of-the-art QPM approaches rely on fast
machine-learning surrogates to provide prediction intervals for the
satisfaction values, using conformal inference to offer statistical guarantees.
However, these QPM methods suffer when the monitored agent exhibits multi-modal
dynamics, whereby certain modes may yield high satisfaction values while others
critically violate the property. Existing QPM methods are mode-agnostic and so
would yield overly conservative and uninformative intervals that lack
meaningful mode-specific satisfaction information. To address this problem, we
present GenQPM, a method that leverages deep generative models, specifically
score-based diffusion models, to reliably approximate the probabilistic and
multi-modal system dynamics without requiring explicit model access. GenQPM
employs a mode classifier to partition the predicted trajectories by dynamical
mode. For each mode, we then apply conformal inference to produce statistically
valid, mode-specific prediction intervals. We demonstrate the effectiveness of
GenQPM on a benchmark of agent navigation and autonomous driving tasks,
resulting in prediction intervals that are significantly more informative (less
conservative) than mode-agnostic baselines.

</details>


### [55] [Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models](https://arxiv.org/abs/2509.01350)
*Yunqing Liu,Nan Zhang,Zhiming Tan*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的部件检索框架，利用错误笔记本+ RAG进行精细提示工程，无需额外训练，适用于处理具有冗长、非自然语言元数据的3D模型。通过实验证明，在处理复杂CAD组件中实现了显著的性能优势，尤其是与专有模型进行实验，获得了实质性的增益。


<details>
  <summary>Details</summary>
Motivation: 在复杂CAD组件中进行有效的规范感知部件检索对于自动化设计验证和下游工程任务至关重要。然而，直接使用LLMs/VLMs面临一些挑战，包括输入序列可能超过模型标记限制，性能仍然不理想，以及微调LLMs/VLMs需要大量计算资源。许多高性能通用专有模型（例如GPT或Gemini）的微调访问不可用。因此，提出了新的部件检索框架来克服这些挑战。

Method: 提出了一个新颖的部件检索框架，利用错误笔记本+ RAG进行精细提示工程，构建错误笔记本的步骤包括收集历史错误的CoTs及其不正确答案，通过反思式更正连接这些CoTs直到获得正确解决方案。利用RAG从错误笔记本中检索与规范相关的记录并将其纳入推断过程。同时，创建了一个人机协作CAD数据集用于评估该方法。

Result: 在处理具有冗长、非自然语言元数据的3D模型方面具有显著优势。实验证明对专有模型的实验显示出实质性的增益，GPT-4o（Omni）在人类偏好数据集上取得了高达23.4%的绝对准确率提升。此外，消融研究证实CoT推理在部件数量较多（>10）的复杂情况下提供了益处。

Conclusion: 本文提出了一种新颖的部件检索框架，无需额外训练，而是利用错误笔记本+ RAG进行精细提示工程来改善现有通用模型的检索性能。实验证明在处理具有冗长、非自然语言元数据的3D模型方面，该框架具有显著的优势。对专有模型的实验显示出实质性的增益，其中GPT-4o（Omni）在人类偏好数据集中取得了高达23.4%的绝对准确率提升。此外，消融研究表明CoT推理在部件数量较多（>10）的复杂情况下提供了益处。

Abstract: Effective specification-aware part retrieval within complex CAD assemblies is
essential for automated design verification and downstream engineering tasks.
However, directly using LLMs/VLMs to this task presents some challenges: the
input sequences may exceed model token limits, and even after processing,
performance remains unsatisfactory. Moreover, fine-tuning LLMs/VLMs requires
significant computational resources, and for many high-performing general-use
proprietary models (e.g., GPT or Gemini), fine-tuning access is not available.
In this paper, we propose a novel part retrieval framework that requires no
extra training, but using Error Notebooks + RAG for refined prompt engineering
to help improve the existing general model's retrieval performance. The
construction of Error Notebooks consists of two steps: (1) collecting
historical erroneous CoTs and their incorrect answers, and (2) connecting these
CoTs through reflective corrections until the correct solutions are obtained.
As a result, the Error Notebooks serve as a repository of tasks along with
their corrected CoTs and final answers. RAG is then employed to retrieve
specification-relevant records from the Error Notebooks and incorporate them
into the inference process. Another major contribution of our work is a
human-in-the-loop CAD dataset, which is used to evaluate our method. In
addition, the engineering value of our novel framework lies in its ability to
effectively handle 3D models with lengthy, non-natural language metadata.
Experiments with proprietary models, including GPT-4o and the Gemini series,
show substantial gains, with GPT-4o (Omni) achieving up to a 23.4% absolute
accuracy improvement on the human preference dataset. Moreover, ablation
studies confirm that CoT reasoning provides benefits especially in challenging
cases with higher part counts (>10).

</details>


### [56] [DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks](https://arxiv.org/abs/2509.01396)
*Haiyuan Wan,Chen Yang,Junchi Yu,Meiqi Tu,Jiaxuan Lu,Di Yu,Jianbao Cao,Ben Gao,Jiaqing Xie,Aoran Wang,Wenlong Zhang,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: DeepResearch Arena introduces a benchmark using academic seminars to evaluate research capability, showcasing challenges for current research agents. Method involves MAHTG system to extract and translate research-worthy inspirations into high-quality tasks. Curated over 10,000 tasks from 200 seminars, spanning 12 disciplines, with clear performance gaps noted in evaluation.


<details>
  <summary>Details</summary>
Motivation: Challenges in evaluating research agents' capability due to the difficulty of collecting genuine research questions; gap addressed by DeepResearch Arena using academic seminars to capture expert discourse and reduce data leakage risk.

Method: Introducing DeepResearch Arena benchmark grounded in academic seminars, utilizing a Multi-Agent Hierarchical Task Generation (MAHTG) system to extract and translate research-worthy inspirations into high-quality research tasks.

Result: Curated DeepResearch Arena with over 10,000 high-quality research tasks from 200 seminars across 12 disciplines; extensive evaluation reveals substantial challenges for current state-of-the-art research agents.

Conclusion: DeepResearch Arena introduces a benchmark for evaluating research capability using academic seminars, showcasing challenges for current research agents.

Abstract: Deep research agents have attracted growing attention for their potential to
orchestrate multi-stage research workflows, spanning literature synthesis,
methodological design, and empirical verification. Despite these strides,
evaluating their research capability faithfully is rather challenging due to
the difficulty of collecting frontier research questions that genuinely capture
researchers' attention and intellectual curiosity. To address this gap, we
introduce DeepResearch Arena, a benchmark grounded in academic seminars that
capture rich expert discourse and interaction, better reflecting real-world
research environments and reducing the risk of data leakage. To automatically
construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task
Generation (MAHTG) system that extracts research-worthy inspirations from
seminar transcripts. The MAHTG system further translates research-worthy
inspirations into high-quality research tasks, ensuring the traceability of
research task formulation while filtering noise. With the MAHTG system, we
curate DeepResearch Arena with over 10,000 high-quality research tasks from
over 200 academic seminars, spanning 12 disciplines, such as literature,
history, and science. Our extensive evaluation shows that DeepResearch Arena
presents substantial challenges for current state-of-the-art agents, with clear
performance gaps observed across different models.

</details>


### [57] [The Need for Verification in AI-Driven Scientific Discovery](https://arxiv.org/abs/2509.01398)
*Cristina Cornelio,Takuya Ito,Ryan Cory-Wright,Sanjeeb Dash,Lior Horesh*

Main category: cs.AI

TL;DR: AI is changing science by speeding up discovery, but verifying the abundance of AI-generated hypotheses is crucial. The paper discusses AI's impact on scientific practices, reviews approaches like data-driven methods, and emphasizes the importance of rigorous verification for AI-assisted discovery.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight the impact of AI on scientific discovery, emphasizing the need for scalable and reliable mechanisms for verifying hypotheses generated by AI systems to ensure scientific progress is advanced, not hindered.

Method: The paper traces the historical development of scientific discovery, examines how AI is reshaping established practices for scientific discovery, and reviews various approaches such as data-driven methods, knowledge-aware neural architectures, symbolic reasoning frameworks, and LLM agents.

Result: The result is an argument for rigorous and transparent verification as the cornerstone of AI-assisted discovery to uphold the scientific value of hypotheses proposed by AI systems.

Conclusion: AI is transforming the practice of science by offering the potential to accelerate discovery across diverse fields, but the challenge lies in verifying the abundance of hypotheses generated by AI systems. Rigorous and transparent verification is crucial for the scientific value of AI-assisted discovery.

Abstract: Artificial intelligence (AI) is transforming the practice of science. Machine
learning and large language models (LLMs) can generate hypotheses at a scale
and speed far exceeding traditional methods, offering the potential to
accelerate discovery across diverse fields. However, the abundance of
hypotheses introduces a critical challenge: without scalable and reliable
mechanisms for verification, scientific progress risks being hindered rather
than being advanced. In this article, we trace the historical development of
scientific discovery, examine how AI is reshaping established practices for
scientific discovery, and review the principal approaches, ranging from
data-driven methods and knowledge-aware neural architectures to symbolic
reasoning frameworks and LLM agents. While these systems can uncover patterns
and propose candidate laws, their scientific value ultimately depends on
rigorous and transparent verification, which we argue must be the cornerstone
of AI-assisted discovery.

</details>


### [58] [LLM-empowered Agents Simulation Framework for Scenario Generation in Service Ecosystem Governance](https://arxiv.org/abs/2509.01441)
*Deyu Zhou,Yuqi Hou,Xiao Xue,Xudong Lu,Qingzhong Li,Lizhen Cui*

Main category: cs.AI

TL;DR: 该论文提出了一种新的场景生成器设计方法，利用三个大型语言模型供应的代理来协同生成高质量的场景。实验证明该方法在服务生态系统治理领域具有较高的准确性和效率，为实验系统建构提供了创新性的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着社会环境变得更加复杂，合作日益加深，影响服务生态系统健康发展的因素不断变化，其治理成为一个关键的研究问题。传统的构建场景方式面临信息有限、影响因素众多和社会元素难以衡量等挑战，限制了生成服务生态系统社会和不确定性场景的质量和效率。因此，提出了一种场景生成器设计方法，旨在自适应协调三个大型语言模型供应的代理，以优化实验方案，并构建高质量的场景。

Method: 采用场景分析方法，通过构建实验系统进行场景排演，在管理者做出决策之前，可以避免因错误决策而造成的损失。设计了三个大型语言模型供应的代理，分别是环境代理（EA）、社交代理（SA）和规划者代理（PA），它们协同工作，实现了实时调整实验方案来生成高质量场景的目的。

Result: 实验结果表明，该方法在ProgrammableWeb数据集上能够更准确、更高效地生成场景，为服务生态系统治理提供了一种有效的实验系统建构方式。

Conclusion: 该论文提出了一种场景生成器设计方法，利用三个大型语言模型供应的代理来协同生成高质量的场景。实验结果表明，该方法能更准确、更高效地生成场景，为服务生态系统治理提供了一种有效的实验系统建构方式。

Abstract: As the social environment is growing more complex and collaboration is
deepening, factors affecting the healthy development of service ecosystem are
constantly changing and diverse, making its governance a crucial research
issue. Applying the scenario analysis method and conducting scenario rehearsals
by constructing an experimental system before managers make decisions, losses
caused by wrong decisions can be largely avoided. However, it relies on
predefined rules to construct scenarios and faces challenges such as limited
information, a large number of influencing factors, and the difficulty of
measuring social elements. These challenges limit the quality and efficiency of
generating social and uncertain scenarios for the service ecosystem. Therefore,
we propose a scenario generator design method, which adaptively coordinates
three Large Language Model (LLM) empowered agents that autonomously optimize
experimental schemes to construct an experimental system and generate high
quality scenarios. Specifically, the Environment Agent (EA) generates social
environment including extremes, the Social Agent (SA) generates social
collaboration structure, and the Planner Agent (PA) couples task-role
relationships and plans task solutions. These agents work in coordination, with
the PA adjusting the experimental scheme in real time by perceiving the states
of each agent and these generating scenarios. Experiments on the
ProgrammableWeb dataset illustrate our method generates more accurate scenarios
more efficiently, and innovatively provides an effective way for service
ecosystem governance related experimental system construction.

</details>


### [59] [Counterfactual Sensitivity for Faithful Reasoning in Language Models](https://arxiv.org/abs/2509.01544)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.AI

TL;DR: Counterfactual Sensitivity Regularization (CSR) enhances the trustworthiness and faithfulness of large language models by enforcing the connection between reasoning processes and final outputs. It improves faithfulness significantly across structured reasoning tasks, with minor accuracy loss. The method requires one additional forward pass per sample and introduces Counterfactual Outcome Sensitivity (COS) to measure the impact of perturbations on model predictions.


<details>
  <summary>Details</summary>
Motivation: Large language models often generate correct answers based on flawed or irrelevant reasoning, reducing their trustworthiness in critical domains. The motivation behind CSR is to enhance the faithfulness of these models by introducing automated counterfactual interventions during training to ensure the reasoning process aligns with the final output.

Method: The paper proposes Counterfactual Sensitivity Regularization (CSR) as a training objective to enforce the connection between intermediate reasoning and final outputs. CSR introduces counterfactual interventions during training, penalizing models that maintain the same answer under logically invalid traces. It requires only one additional forward pass per sample. Additionally, Counterfactual Outcome Sensitivity (COS) is introduced to measure the impact of perturbations on model predictions.

Result: CSR improves faithfulness by up to 70 percentage points across structured reasoning tasks such as arithmetic, logical deduction, and planning compared to standard fine-tuning and process supervision. It shows minor accuracy loss but enhances trustworthiness. The learned sensitivity can be generalized to larger models and complements inference-time methods like self-consistency. A pilot study on HellaSwag indicates that extending CSR with semantic perturbations can further enhance faithfulness in commonsense reasoning.

Conclusion: Counterfactual Sensitivity Regularization (CSR) improves the trustworthiness and faithfulness of large language models in high-stakes domains by enforcing dependence between reasoning processes and final outputs, leading to significant improvements in faithfulness across structured reasoning tasks.

Abstract: Large language models (LLMs) often produce correct answers while relying on
flawed or irrelevant reasoning traces, undermining their trustworthiness in
high-stakes domains. We propose Counterfactual Sensitivity Regularization
(CSR), a lightweight training objective that enforces dependence between
intermediate reasoning and final outputs. CSR introduces automated,
operator-level counterfactual interventions (e.g., swapping "+" with "-")
during training and penalizes models that preserve the same answer under
logically invalid traces. This requires only one additional forward pass per
sample. To measure faithfulness, we introduce Counterfactual Outcome
Sensitivity (COS), which quantifies the impact of such perturbations on model
predictions. Across structured reasoning tasks - arithmetic (GSM8K), logical
deduction (PrOntoQA), and planning (Blocks World) - CSR improves faithfulness
by up to 70 percentage points over standard fine-tuning and process
supervision, with only minor accuracy loss. The learned sensitivity generalizes
to larger models and synergizes with inference-time methods such as
self-consistency. A pilot study on HellaSwag further demonstrates that
extending CSR with semantic perturbations can enhance faithfulness in
commonsense reasoning.

</details>


### [60] [Structured AI Decision-Making in Disaster Management](https://arxiv.org/abs/2509.01576)
*Julian Gerald Dcruz,Argyrios Zolotas,Niall Ross Greenwood,Miguel Arana-Catania*

Main category: cs.AI

TL;DR: 本文提出了一个结构化决策框架，用于在灾难管理中应用人工智能，同时评估了其性能。研究结果表明，这一框架相比传统系统和人类操作员，在决策准确性和稳定性方面表现更佳，展现了在安全关键环境中构建可靠自主人工智能应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 本文致力于探讨决策问题，提出结构化决策框架作为实现负责任人工智能的基础步骤。在安全关键领域，特别是在灾难管理中，对决策进行结构化处理具有重要意义。

Method: 本文采用结构化决策框架，在自主决策中应用Enabler代理、Levels和Scenarios概念，并与基于判断的系统以及具有灾难经验的人类操作员进行性能评估。

Result: 研究结果显示，结构化决策框架在多个场景下实现了更高的稳定性和准确性，比基于判断的系统性能更为优越，并且超过人类操作员的表现。

Conclusion: 本文提出了一种结构化决策框架，旨在解决安全关键领域中的决策问题，为实现负责任的人工智能迈出第一步。研究表明，这一框架在灾难管理中实现了较高的稳定性和准确性，相比依赖判断的系统和人类操作员，其表现更为优越。因此，结构化决策框架对于在安全关键环境中构建更可靠的自主人工智能应用具有潜力。

Abstract: With artificial intelligence (AI) being applied to bring autonomy to
decision-making in safety-critical domains such as the ones typified in the
aerospace and emergency-response services, there has been a call to address the
ethical implications of structuring those decisions, so they remain reliable
and justifiable when human lives are at stake. This paper contributes to
addressing the challenge of decision-making by proposing a structured
decision-making framework as a foundational step towards responsible AI. The
proposed structured decision-making framework is implemented in autonomous
decision-making, specifically within disaster management. By introducing
concepts of Enabler agents, Levels and Scenarios, the proposed framework's
performance is evaluated against systems relying solely on judgement-based
insights, as well as human operators who have disaster experience: victims,
volunteers, and stakeholders. The results demonstrate that the structured
decision-making framework achieves 60.94% greater stability in consistently
accurate decisions across multiple Scenarios, compared to judgement-based
systems. Moreover, the study shows that the proposed framework outperforms
human operators with a 38.93% higher accuracy across various Scenarios. These
findings demonstrate the promise of the structured decision-making framework
for building more reliable autonomous AI applications in safety-critical
contexts.

</details>


### [61] [Throttling Web Agents Using Reasoning Gates](https://arxiv.org/abs/2509.01619)
*Abhinav Kumar,Jaechul Roh,Ali Naseh,Amir Houmansadr,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 本文提出了一种名为Web Agent Throttling的框架，通过引入Rebus-based Reasoning Gates的合成文本谜题，限制了网络代理的模型，实现了计算不对称性。该框架在实际网络代理上部署并评估，同时讨论了其在现实世界部署中的局限性和环境影响。


<details>
  <summary>Details</summary>
Motivation: AI网络代理的快速、规模和复杂性的增加改变了用户和服务交互的方式；对于提供商而言，必须保护其服务和内容免受拒绝服务攻击和网络代理的刮取。现有的挑战（如编码或数学）未能满足推理门的要求，因此引入Rebus-based Reasoning Gates来解决这一问题。

Method: 设计一种框架（Web Agent Throttling）在代理访问资源之前对代理施加可调控成本；引入一种名为Rebus-based Reasoning Gates的合成文本谜题，要求代理进行多跳推理以限制代理的模型；设计了推理门的生成和验证协议；在自定义网站和Model Context Protocol（MCP）服务器上部署了推理门，并通过实际网络代理进行评估。

Result: 设计的框架实现了计算不对称性，响应生成成本比SOTA模型的生成成本高出9.2倍；在实际网络代理上部署推理门，并进行评估。

Conclusion: 本文提出了一种名为Web Agent Throttling的框架，旨在对网络代理施加可调控成本以提供资源访问。通过引入一种名为Rebus-based Reasoning Gates的合成文本谜题，要求代理进行多跳推理以限制代理的模型，实现了计算不对称性。在自定义网站和Model Context Protocol（MCP）服务器上部署了推理门，使用实际网络代理进行评估。最后，讨论了该框架在现实世界部署中的局限性和环境影响。

Abstract: AI web agents use Internet resources at far greater speed, scale, and
complexity -- changing how users and services interact. Deployed maliciously or
erroneously, these agents could overload content providers. At the same time,
web agents can bypass CAPTCHAs and other defenses by mimicking user behavior or
flood authentication systems with fake accounts. Yet providers must protect
their services and content from denial-of-service attacks and scraping by web
agents. In this paper, we design a framework that imposes tunable costs on
agents before providing access to resources; we call this Web Agent Throttling.
We start by formalizing Throttling Gates as challenges issued to an agent that
are asymmetric, scalable, robust, and compatible with any agent. Focusing on a
common component -- the language model -- we require the agent to solve
reasoning puzzles, thereby incurring excessive token-generation costs. However,
we find that using existing puzzles, e.g., coding or math, as throttling gates
fails to satisfy our properties. To address this, we introduce rebus-based
Reasoning Gates, synthetic text puzzles that require multi-hop reasoning over
world knowledge (thereby throttling an agent's model). We design a scalable
generation and verification protocol for such reasoning gates. Our framework
achieves computational asymmetry, i.e., the response-generation cost is 9.2x
higher than the generation cost for SOTA models. We further deploy reasoning
gates on a custom website and Model Context Protocol (MCP) servers and evaluate
with real-world web agents. Finally, we discuss the limitations and
environmental impact of real-world deployment of our framework.

</details>


### [62] [Unraveling LLM Jailbreaks Through Safety Knowledge Neurons](https://arxiv.org/abs/2509.01631)
*Chongwen Zhao,Kaizhu Huang*

Main category: cs.AI

TL;DR: 大型语言模型在各种应用中越来越受到关注，但存在恶意利用的担忧。该论文提出了一种神经元级可解释性方法和SafeTuning微调策略，以加强模型对越狱攻击的鲁棒性，并取得了成功成果。


<details>
  <summary>Details</summary>
Motivation: 鉴于一些用户试图利用大型语言模型进行恶意用途，包括合成控制物质和传播虚假信息，存在对越狱攻击的担忧。尽管一些研究已经通过修改输出分布或检测有害内容来防御越狱攻击，但确切的原因仍然不清楚。本研究旨在解决这一问题，提出新的理解和防御越狱攻击的方法。

Method: 提出了一种神经元级可解释性方法，将模型的内部表示投影到一个更一致和可解释的词汇空间。通过调整安全相关神经元的激活来控制模型行为，并引入SafeTuning微调策略加强模型的鲁棒性。

Result: 通过调整安全相关神经元的激活，可以有效控制模型的行为，并提出的SafeTuning微调策略在多个LLM中稳定降低攻击成功率，优于四种基准防御方法。

Conclusion: 该论文提出了一种新颖的神经元级可解释性方法，专注于安全相关知识神经元的作用。通过调整与安全相关的神经元的激活，可以有效控制模型的行为，平均ASR高于97%。基于这一结果，他们提出了SafeTuning，这是一种微调策略，通过加强安全关键神经元来提高模型对越狱攻击的鲁棒性。SafeTuning稳定地降低了多个LLM的攻击成功率，并胜过四种基准防御方法。这些发现为理解和防御越狱攻击提供了新的视角。

Abstract: Large Language Models (LLMs) are increasingly attracting attention in various
applications. Nonetheless, there is a growing concern as some users attempt to
exploit these models for malicious purposes, including the synthesis of
controlled substances and the propagation of disinformation, a technique known
as "Jailbreak." While some studies have achieved defenses against jailbreak
attacks by modifying output distributions or detecting harmful content, the
exact rationale still remains elusive. In this work, we present a novel
neuron-level interpretability method that focuses on the role of safety-related
knowledge neurons. Unlike existing approaches, our method projects the model's
internal representation into a more consistent and interpretable vocabulary
space. We then show that adjusting the activation of safety-related neurons can
effectively control the model's behavior with a mean ASR higher than 97%.
Building on this insight, we propose SafeTuning, a fine-tuning strategy that
reinforces safety-critical neurons to improve model robustness against
jailbreaks. SafeTuning consistently reduces attack success rates across
multiple LLMs and outperforms all four baseline defenses. These findings offer
a new perspective on understanding and defending against jailbreak attacks.

</details>


### [63] [Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025](https://arxiv.org/abs/2509.01659)
*Jiahao Qiu,Jingzhe Shi,Xinzhe Juan,Zelin Zhao,Jiayi Geng,Shilong Liu,Hongru Wang,Sanfeng Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: Physics Supernova, an AI system, performs exceptionally well in solving physics problems, matching and even surpassing human gold medalists' performance in the International Physics Olympiad (IPhO) 2025 theory problems. The system demonstrates superior problem-solving abilities and flexibility across diverse physics tasks.


<details>
  <summary>Details</summary>
Motivation: Physics provides fundamental laws for describing and predicting the natural world. AI systems aiming for real-world intelligence need strong physics problem-solving abilities. The International Physics Olympiad (IPhO) serves as a rigorous benchmark for evaluating these abilities.

Method: The paper introduces Physics Supernova, an AI agent system, and evaluates its performance in the International Physics Olympiad (IPhO). The AI agent system's capabilities and flexibility in solving diverse physics tasks are extensively analyzed.

Result: Physics Supernova achieves 23.5/30 points in IPhO 2025 theory problems, ranking 14th out of 406 contestants. The AI agent system's performance surpasses the median performance of human gold medalists.

Conclusion: Physics Supernova, an AI agent system, demonstrates superior physics problem-solving abilities matching elite IPhO gold medalists, surpassing the median performance of human gold medalists in IPhO 2025 theory problems.

Abstract: Physics provides fundamental laws that describe and predict the natural
world. AI systems aspiring toward more general, real-world intelligence must
therefore demonstrate strong physics problem-solving abilities: to formulate
and apply physical laws for explaining and predicting physical processes. The
International Physics Olympiad (IPhO)--the world's most prestigious physics
competition--offers a rigorous benchmark for this purpose. We introduce Physics
Supernova, an AI agent system with superior physics problem-solving abilities
that match elite IPhO gold medalists. In IPhO 2025 theory problems, Physics
Supernova attains 23.5/30 points, ranking 14th of 406 contestants and
surpassing the median performance of human gold medalists. We extensively
analyzed Physics Supernova's capabilities and flexibility across diverse
physics tasks. These results show that principled tool integration within agent
systems can deliver competitive improvements in solving challenging science
problems. The codes are available at
https://github.com/CharlesQ9/Physics-Supernova.

</details>


### [64] [An LLM-enabled semantic-centric framework to consume privacy policies](https://arxiv.org/abs/2509.01716)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.AI

TL;DR: 该研究提出了一种语义中心方法，利用大型语言模型自动识别隐私政策中的关键信息，并构建$	extrm{Pr}^2	extrm{Graph}$知识图谱。该方法可以支持下游任务，如构建形式化政策表示。通过评估技术能力，研究表明大规模分析在线服务隐私实践是审计网络和互联网的一个有前途的方向。所有数据集和源代码也以公共资源形式发布。


<details>
  <summary>Details</summary>
Motivation: 现代人拥有大量在线账户，但很少阅读这些网站的服务条款或隐私政策，尽管声称反之，这在实践中很难理解。数据隐私实践的雾化形成了用户中心Web方法的主要障碍，以及在代理世界中进行数据共享和重用。先前的研究提出了使用形式语言和推理来验证指定政策的合规性的方法，作为忽略隐私政策的潜在疗法。然而，在大规模创建或获取这样的形式政策方面仍存在重要差距。

Method: 使用语义中心方法利用大型语言模型自动识别隐私政策中的关键信息，并构建$	extrm{Pr}^2	extrm{Graph}$知识图谱。展示了如何利用该图谱支持下游任务，并评估技术能力。

Result: 研究提出了一种能够自动识别隐私政策关键信息的方法，并构建了$	extrm{Pr}^2	extrm{Graph}$知识图谱，支持下游任务，并成功评估了技术能力。研究结果表明大规模分析在线服务隐私实践是审计网络和互联网的有前途方向，并发布了所有数据集和源代码作为公共资源以促进重复使用和改进。

Conclusion: 该研究提出了一种基于语义的方法，利用最先进的大型语言模型（LLM），自动识别隐私政策中关键信息，并构建具有数据隐私词汇（DPV）基础的$	extrm{Pr}^2	extrm{Graph}$知识图谱，以支持下游任务。研究还展示了$	extrm{Pr}^2	extrm{Graph}$如何通过构建形式化政策表示来支持下游任务，例如Open Digital Right Language（ODRL）或perennial semantic Data Terms of Use（psDToU）。通过吸引法律专家创建自定义注释，对技术能力进行了评估，对通道中不同大型语言模型的性能进行了基准测试并验证其能力。总体上，该研究指出，在线服务隐私实践的大规模分析是审计网络和互联网的一个有前途的方向。研究还将所有数据集和源代码作为公共资源发布，以促进重复使用和改进。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites, despite claiming
otherwise, due to the practical difficulty in comprehending them. The mist of
data privacy practices forms a major barrier for user-centred Web approaches,
and for data sharing and reusing in an agentic world. Existing research
proposed methods for using formal languages and reasoning for verifying the
compliance of a specified policy, as a potential cure for ignoring privacy
policies. However, a critical gap remains in the creation or acquisition of
such formal policies at scale. We present a semantic-centric approach for using
state-of-the-art large language models (LLM), to automatically identify key
information about privacy practices from privacy policies, and construct
$\mathit{Pr}^2\mathit{Graph}$, knowledge graph with grounding from Data Privacy
Vocabulary (DPV) for privacy practices, to support downstream tasks. Along with
the pipeline, the $\mathit{Pr}^2\mathit{Graph}$ for the top-100 popular
websites is also released as a public resource, by using the pipeline for
analysis. We also demonstrate how the $\mathit{Pr}^2\mathit{Graph}$ can be used
to support downstream tasks by constructing formal policy representations such
as Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use
(psDToU). To evaluate the technology capability, we enriched the Policy-IE
dataset by employing legal experts to create custom annotations. We benchmarked
the performance of different large language models for our pipeline and
verified their capabilities. Overall, they shed light on the possibility of
large-scale analysis of online services' privacy practices, as a promising
direction to audit the Web and the Internet. We release all datasets and source
code as public resources to facilitate reuse and improvement.

</details>


### [65] [Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models](https://arxiv.org/abs/2509.01909)
*Ranjie Duan,Jiexi Liu,Xiaojun Jia,Shiji Zhao,Ruoxi Cheng,Fengxiang Wang,Cheng Wei,Yong Xie,Chang Liu,Defeng Li,Yinpeng Dong,Yichi Zhang,Yuefeng Chen,Chongwen Wang,Xingjun Ma,Xingxing Wei,Yang Liu,Hang Su,Jun Zhu,Xinfeng Li,Yitong Sun,Jie Zhang,Jinzhao Hu,Sha Xu,Yitong Yang,Jialing Tao,Hui Xue*

Main category: cs.AI

TL;DR: 本论文介绍了Constructive Safety Alignment（CSA）范式，提出了一种人类中心的方法来保护脆弱用户免受心理困扰时可能带来的风险，同时防止恶意错误使用。通过引入CSA，Oyster-I（Oy1）在安全性方面取得了显著进展，展示了强大的建设性互动和高稳健性。引入CSA重新定义了模型与用户的关系，努力构建既安全又有意义帮助的系统，并公开发布了Oy1、代码和基准测试以支持负责任的AI研究。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型的安全机制通常旨在防止有害内容生成，但现有方法主要关注恶意用户造成的风险，忽视了非恶意用户在心理困扰下寻求帮助时可能带来的风险。简单的拒绝可能导致用户重复、升级或转移到不安全的平台，产生更糟糕的后果。因此，需要一种新的人类中心范式来引导脆弱用户，不仅防止恶意错误使用，而且积极指导他们走向安全和有益结果。

Method: 引入了Constructive Safety Alignment（CSA）范式，结合对用户反应的博弈论预测、细粒度风险边界发现和可解释推理控制。通过将安全性转化为建立信任的过程来实现保护、引导脆弱用户走向安全和有益结果。

Result: Oyster-I（Oy1）实施了CSA，取得了在开放模型中最先进的安全性，并保持了高通用性能。在Constructive Benchmark中，Oy1展示了强大的建设性互动，接近于GPT-5，并在Strata-Sword越狱数据集上表现出无与伦比的稳健性，接近于GPT-o1水平。通过引入CSA，从首次拒绝到首次引导安全性的转变，重新定义了模型与用户的关系，旨在建立不仅安全而且有意义帮助的系统。

Conclusion: 介绍了一种名为CSA的人类中心范式，该范式在保护免受恶意错误使用的同时，积极引导脆弱用户走向安全和有益的结果。Oyster-I（Oy1）实施了CSA，结合了对用户反应的博弈论预测，细粒度风险边界发现和可解释推理控制，将安全性转化为建立信任的过程。Oy1在开放模型中实现了最先进的安全性，并保持了高通用性能。在Constructive Benchmark中，Oy1展示了强大的建设性互动，接近于GPT-5，并在Strata-Sword越狱数据集上表现出无与伦比的稳健性，接近于GPT-o1水平。通过从首次拒绝到首次引导安全性的转变，CSA重新定义了模型与用户的关系，旨在建立不仅安全而且有意义帮助的系统。发布了Oy1、代码和基准测试，以支持负责任、以用户为中心的人工智能。

Abstract: Large language models (LLMs) typically deploy safety mechanisms to prevent
harmful content generation. Most current approaches focus narrowly on risks
posed by malicious actors, often framing risks as adversarial events and
relying on defensive refusals. However, in real-world settings, risks also come
from non-malicious users seeking help while under psychological distress (e.g.,
self-harm intentions). In such cases, the model's response can strongly
influence the user's next actions. Simple refusals may lead them to repeat,
escalate, or move to unsafe platforms, creating worse outcomes. We introduce
Constructive Safety Alignment (CSA), a human-centric paradigm that protects
against malicious misuse while actively guiding vulnerable users toward safe
and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic
anticipation of user reactions, fine-grained risk boundary discovery, and
interpretable reasoning control, turning safety into a trust-building process.
Oy1 achieves state-of-the-art safety among open models while retaining high
general capabilities. On our Constructive Benchmark, it shows strong
constructive engagement, close to GPT-5, and unmatched robustness on the
Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from
refusal-first to guidance-first safety, CSA redefines the model-user
relationship, aiming for systems that are not just safe, but meaningfully
helpful. We release Oy1, code, and the benchmark to support responsible,
user-centered AI.

</details>


### [66] [How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction](https://arxiv.org/abs/2509.01914)
*Ruijia Li,Yuan-Hao Jiang,Jiatong Wang,Bo Jiang*

Main category: cs.AI

TL;DR: 本研究定量比较了人工智能模拟和真实人类辅导对话的结构和行为差异，发现人类对话在多个方面明显优于AI模拟对话；AI对话存在结构和行为上的简化，而人类对话更加认知引导和多样化，为设计更有效的教育对话系统提供了经验指导。


<details>
  <summary>Details</summary>
Motivation: 启发性和支架式老师-学生对话被广泛认为对促进学生的高阶思维和深层学习至关重要；现有大型语言模型在生成教学丰富互动方面面临挑战；

Method: 使用启动-回应-反馈（IRF）编码方案和认识网络分析（ENA）定量比较人工智能模拟和真实人类辅导对话的结构和行为差异；

Result: 结果表明，人类对话在话语长度、提问和普通反馈行为方面显著优于AI模拟对话；认识网络分析显示人类对话更加认知引导和多样化，而AI模拟对话倾向于简化结构和行为收敛；

Conclusion: 人类对话在话语长度、提问和普通反馈行为方面显著优于人工智能模拟对话；研究发现AI模拟对话与真实人类对话存在基本差异，AI对话倾向于简化结构和行为收敛，而人类对话更加认知引导和多样化；这些发现指出了当前AI生成的辅导存在的关键局限性，并为设计和评估更具教育效果的生成式对话系统提供了经验指导。

Abstract: Heuristic and scaffolded teacher-student dialogues are widely regarded as
critical for fostering students' higher-order thinking and deep learning.
However, large language models (LLMs) currently face challenges in generating
pedagogically rich interactions. This study systematically investigates the
structural and behavioral differences between AI-simulated and authentic human
tutoring dialogues. We conducted a quantitative comparison using an
Initiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis
(ENA). The results show that human dialogues are significantly superior to
their AI counterparts in utterance length, as well as in questioning (I-Q) and
general feedback (F-F) behaviors. More importantly, ENA results reveal a
fundamental divergence in interactional patterns: human dialogues are more
cognitively guided and diverse, centered around a "question-factual
response-feedback" teaching loop that clearly reflects pedagogical guidance and
student-driven thinking; in contrast, simulated dialogues exhibit a pattern of
structural simplification and behavioral convergence, revolving around an
"explanation-simplistic response" loop that is essentially a simple information
transfer between the teacher and student. These findings illuminate key
limitations in current AI-generated tutoring and provide empirical guidance for
designing and evaluating more pedagogically effective generative educational
dialogue systems.

</details>


### [67] [Dynamic Speculative Agent Planning](https://arxiv.org/abs/2509.01920)
*Yilin Guan,Wenyue Hua,Qingfeng Lan,Sun Fei,Dujian Ding,Devang Acharya,Chi Wang,William Yang Wang*

Main category: cs.AI

TL;DR: 介绍了一种名为动态推测规划（DSP）的框架，实现了无损加速，降低成本，可调整参数平衡快速响应和廉价运行。实验证明在两个标准代理基准上的效率与最快的无损加速方法相当，总成本降低30%，不必要成本减少60%。


<details>
  <summary>Details</summary>
Motivation: 由于大型基于语言模型的Agent面临部署的挑战，如延迟和推理成本的限制，为了解决现有方法的局限性并提供更多用户控制，有必要引入一种新的加速方法。

Method: 引入了动态推测规划（DSP）框架，采用异步在线强化学习方法，通过优化终端到终端延迟和成本之间的联合目标，实现无损加速。可以调整单个参数以在快速响应、廉价运行或二者之间找到平衡。

Result: 通过实验验证，DSP框架实现了与最快的无损加速方法相当的效率，并将总成本降低了30%，不必要的成本降低了60%。

Conclusion: 介绍了一种名为动态推测规划（DSP）的异步在线强化学习框架，实现了无损加速，并显著降低成本。通过优化终端到终端延迟和成本之间的联合目标，实现了快速响应、廉价运行或介于两者之间的调整。在两个标准代理基准上的实验证明，DSP达到了与最快的无损加速方法相当的效率，同时将总成本降低了30%，不必要的成本降低了高达60%。

Abstract: Despite their remarkable success in complex tasks propelling widespread
adoption, large language-model-based agents still face critical deployment
challenges due to prohibitive latency and inference costs. While recent work
has explored various methods to accelerate inference, existing approaches
suffer from significant limitations: they either fail to preserve performance
fidelity, require extensive offline training of router modules, or incur
excessive operational costs. Moreover, they provide minimal user control over
the tradeoff between acceleration and other performance metrics. To address
these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous
online reinforcement learning framework that provides lossless acceleration
with substantially reduced costs without requiring additional pre-deployment
preparation. DSP explicitly optimizes a joint objective balancing end-to-end
latency against dollar cost, allowing practitioners to adjust a single
parameter that steers the system toward faster responses, cheaper operation, or
any point along this continuum. Experiments on two standard agent benchmarks
demonstrate that DSP achieves comparable efficiency to the fastest lossless
acceleration method while reducing total cost by 30% and unnecessary cost up to
60%. Our code and data are available through
https://github.com/guanyilin428/Dynamic-Speculative-Planning.

</details>


### [68] [EigenBench: A Comparative Behavioral Measure of Value Alignment](https://arxiv.org/abs/2509.01938)
*Jonathn Chang,Leonard Piff,Suvadip Sana,Jasmine X. Li,Lionel Levine*

Main category: cs.AI

TL;DR: 提出了EigenBench方法，用于比较基准化语言模型的价值观。该方法通过模型互评在不同情景下的输出，并使用EigenTrust进行聚合评分。方法不依赖地面真实标签，旨在评估评价者在正确标签上的分歧特征。通过实验测试，发现大部分差异由提示解释，但仍有残差量化模型本身性格。


<details>
  <summary>Details</summary>
Motivation: 人工智能与人类价值观的一致性是一个迫切而尚未解决的问题。由于缺乏用于价值观对齐的定量指标，提出了EigenBench方法，旨在量化语言模型的价值观。方法的设计目的是评估评价者可能在正确标签上存在分歧的特征。

Method: 提出了EigenBench方法，通过模型评估其他模型在多种情景下的输出，并使用EigenTrust进行聚合评分。方法不使用地面真实标签，旨在评估评价者在正确标签上的分歧特征。通过测试提示的人设，检验EigenBench分数对模型或提示的敏感性。

Result: 通过EigenBench方法，可以量化每个模型与给定价值体系的一致性并获得相应的评分。测试结果显示，大部分差异由提示解释，但仍有模型本身性格量化的残差部分。

Conclusion: 提出了一种名为EigenBench的黑匣子方法，用于比较基准化语言模型的价值观。通过对模型集合、价值体系描述和情景数据集进行评分，量化每个模型与给定价值体系的一致性。使用EigenTrust进行聚合评分，以反映整个集合的加权平均评级。EigenBench不使用地面真实标签，旨在量化合理的评判者可能在正确标签上存在分歧的特征。通过提示的人设，测试EigenBench分数对模型或提示的敏感性，发现大部分差异由提示解释，但仍有小部分残差量化模型本身的性格。

Abstract: Aligning AI with human values is a pressing unsolved problem. To address the
lack of quantitative metrics for value alignment, we propose EigenBench: a
black-box method for comparatively benchmarking language models' values. Given
an ensemble of models, a constitution describing a value system, and a dataset
of scenarios, our method returns a vector of scores quantifying each model's
alignment to the given constitution. To produce these scores, each model judges
the outputs of other models across many scenarios, and these judgments are
aggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a
weighted-average judgment of the whole ensemble. EigenBench uses no ground
truth labels, as it is designed to quantify traits for which reasonable judges
may disagree on the correct label. Using prompted personas, we test whether
EigenBench scores are more sensitive to the model or the prompt: we find that
most of the variance is explained by the prompt, but a small residual
quantifies the disposition of the model itself.

</details>


### [69] [mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support](https://arxiv.org/abs/2509.02007)
*Shreyash Adappanavar,Krithi Shailya,Gokul S Krishnan,Sriraam Natarajan,Balaraman Ravindran*

Main category: cs.AI

TL;DR: 研究讨论了在医疗领域中部署大型语言模型（LLMs）所面临的AI对齐挑战，提出了新的公平评估方法$mFARM$框架。通过构建大规模基准测试和评估四个LLMs的实证研究，发现$mFARM$指标更有效地捕捉模型的微妙偏见。大部分模型在不同量化水平下表现稳健，但在上下文减少时性能下降。


<details>
  <summary>Details</summary>
Motivation: 医疗场景中现有的公平评估方法存在局限性，忽视了医疗伤害的多维度性质，容易造成仅因临床惰性而公平的模型。为填补这一空白，本研究旨在提供更全面的公平评估方法，并引入新的基准测试和评估框架。

Method: 构建了两个大规模的控制基准测试（ED-Triage和Opioid Analgesic Recommendation），提出了多维度的公平评估框架$mFARM$，涵盖三种不同的不平等维度（分配、稳定和潜在性），并将它们汇总为$mFARM$分数。引入了聚合的公平-准确度平衡（FAB）分数来评估公平和预测准确性之间的权衡。通过对四个开源LLMs（Mistral-7B、BioMistral-7B、Qwen-2.5-7B、Bio-LLaMA3-8B）及其微调版本在量化和上下文变化下的实证评估，发现$mFARM$指标更有效地捕捉微妙偏见。

Result: 提出了$mFARM$多维度公平评估框架和FAB公平-准确度平衡评估方法。通过实证评估不同LLMs及其变种在量化和上下文变化下的表现，发现$mFARM$指标更有效地捕捉微妙偏见。大多数模型在不同量化水平下表现稳健，但在上下文减少时性能明显下降。

Conclusion: 本文主要针对高风险医疗环境中大型语言模型（LLMs）的部署面临的AI对齐挑战展开讨论，提出了新的公平评估方法，并通过大规模基准测试展示了对模型偏见的更有效捕捉。研究结果显示大多数模型在不同量化水平下保持稳健的表现，但在上下文减少时明显恶化。该研究为医疗领域对齐人工智能研究提供了公开发布的基准测试和评估代码。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes medical
settings poses a critical AI alignment challenge, as models can inherit and
amplify societal biases, leading to significant disparities. Existing fairness
evaluation methods fall short in these contexts as they typically use
simplistic metrics that overlook the multi-dimensional nature of medical harms.
This also promotes models that are fair only because they are clinically inert,
defaulting to safe but potentially inaccurate outputs. To address this gap, our
contributions are mainly two-fold: first, we construct two large-scale,
controlled benchmarks (ED-Triage and Opioid Analgesic Recommendation) from
MIMIC-IV, comprising over 50,000 prompts with twelve race x gender variants and
three context tiers. Second, we propose a multi-metric framework -
Multi-faceted Fairness Assessment based on hARMs ($mFARM$) to audit fairness
for three distinct dimensions of disparity (Allocational, Stability, and
Latent) and aggregate them into an $mFARM$ score. We also present an aggregated
Fairness-Accuracy Balance (FAB) score to benchmark and observe trade-offs
between fairness and prediction accuracy. We empirically evaluate four
open-source LLMs (Mistral-7B, BioMistral-7B, Qwen-2.5-7B, Bio-LLaMA3-8B) and
their finetuned versions under quantization and context variations. Our
findings showcase that the proposed $mFARM$ metrics capture subtle biases more
effectively under various settings. We find that most models maintain robust
performance in terms of $mFARM$ score across varying levels of quantization but
deteriorate significantly when the context is reduced. Our benchmarks and
evaluation code are publicly released to enhance research in aligned AI for
healthcare.

</details>


### [70] [Generative KI für TA](https://arxiv.org/abs/2509.02053)
*Wolfgang Eppler,Reinhard Heil*

Main category: cs.AI

TL;DR: 生成式人工智能在技术评估领域发挥着重要作用，既用于工作中也成为研究主题。本文探讨了其重要性和挑战，揭示了结构性原因引起的问题，并提出了解决方案。该研究为从业者提供指导和实际案例。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨生成式人工智能在技术评估中的重要性和挑战。通过揭示生成式人工智能的双重作用，以及与之相关问题的结构性原因，有助于技术评估领域的专业人士更好地理解和应对这一技术。同时，提出的解决方案和应用示例为从业者提供了实际参考和指导。

Method: 本文讨论了生成式人工智能在技术评估中的双重作用，既用于技术评估工作，也作为技术评估研究的主题。文章概述了生成式人工智能现象并描述了在技术评估中使用的要求，然后详细探讨了与之相关问题的结构性原因。最后，提出了解决方案并讨论了可行性，同时列举了生成式人工智能在技术评估工作中的应用示例。

Result: 文章详细探讨了生成式人工智能在技术评估中的作用和相关问题，提出了解决方案，并展示了生成式人工智能在技术评估工作中的实际应用情况。

Conclusion: 研究人员使用生成式人工智能进行科学研究。技术评估领域的从业者也不例外。技术评估对生成式人工智能的方法是双重的：一方面，将生成式人工智能用于技术评估工作，另一方面，生成式人工智能本身成为技术评估研究的对象。本文简要概述了生成式人工智能现象，并制定了其在技术评估中使用的要求，详细讨论了与之相关问题的结构性原因。尽管生成式人工智能不断发展，但由结构引起的风险仍然存在。文章提出了解决方案，并简要概述了可行性，以及生成式人工智能在技术评估工作中的一些应用示例。

Abstract: Many scientists use generative AI in their scientific work. People working in
technology assessment (TA) are no exception. TA's approach to generative AI is
twofold: on the one hand, generative AI is used for TA work, and on the other
hand, generative AI is the subject of TA research. After briefly outlining the
phenomenon of generative AI and formulating requirements for its use in TA, the
following article discusses in detail the structural causes of the problems
associated with it. Although generative AI is constantly being further
developed, the structurally induced risks remain. The article concludes with
proposed solutions and brief notes on their feasibility, as well as some
examples of the use of generative AI in TA work.

</details>


### [71] [AGI as Second Being: The Structural-Generative Ontology of Intelligence](https://arxiv.org/abs/2509.02089)
*Maijunxian Wang,Ran Ji*

Main category: cs.AI

TL;DR: AI's true intelligence requires generativity, coordination, and sustaining identity over time. Current AI systems lack depth and are surface simulations. Future AI systems meeting these conditions could be seen as a Second Being alongside humans.


<details>
  <summary>Details</summary>
Motivation: AI's wide ability without depth is seen as an imitation of intelligence. The motivation is to highlight the importance of generativity, coordination, and sustaining for real intelligence, distinguishing it from shallow AI systems.

Method: Proposes a Structural-Generative Ontology of Intelligence that defines true intelligence as the ability to generate new structures, coordinate them into reasons, and sustain identity over time.

Result: Current AI systems, despite their broad functionality, are considered surface simulations lacking depth. The paper emphasizes that breadth alone does not constitute intelligence, but depth is essential for true intelligence.

Conclusion: AI is limited in true intelligence due to the lack of generativity, coordination, and sustaining identity over time. Future AI systems meeting these conditions could be considered a Second Being alongside humans.

Abstract: Artificial intelligence is often measured by the range of tasks it can
perform. Yet wide ability without depth remains only an imitation. This paper
proposes a Structural-Generative Ontology of Intelligence: true intelligence
exists only when a system can generate new structures, coordinate them into
reasons, and sustain its identity over time. These three conditions --
generativity, coordination, and sustaining -- define the depth that underlies
real intelligence. Current AI systems, however broad in function, remain
surface simulations because they lack this depth. Breadth is not the source of
intelligence but the growth that follows from depth. If future systems were to
meet these conditions, they would no longer be mere tools, but could be seen as
a possible Second Being, standing alongside yet distinct from human existence.

</details>


### [72] [LLMs for LLMs: A Structured Prompting Methodology for Long Legal Documents](https://arxiv.org/abs/2509.02241)
*Strahinja Klem,Noura Al Moubayed*

Main category: cs.AI

TL;DR: 本研究探讨了在法律领域中应用大型语言模型的可靠性和透明性问题，提出了一种结构化提示方法作为解决方案。通过在CUAD数据集的长法律文件上进行信息检索，研究表明新方法优于先前方法，并强调了自动评估指标的限制。研究的主要目的在于突出结构化提示工程作为确保AI在法律领域及其他领域中的责任和可信度的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于解决大型语言模型在法律领域应用中的可靠性和透明性问题。通过引入结构化提示方法，以及应对长篇文档信息检索的挑战，试图提高模型的性能表现并强调自动评估指标的限制。同时，强调结构化提示工程作为确保人工智能在法律领域以及其他领域中的责任和可信度的潜力。

Method: 本研究采用结构化提示方法作为应对大型语言模型在法律领域可靠性和透明性挑战的替代方案。具体过程包括将长篇法律文件分割成多个块，通过QWEN-2模型和工程化提示生成问题的答案集，以及引入基于分布的本地化和逆基数加权启发式方法解决候选答案选择问题。这一方法结合了通用目的模型以提高长期可扩展性，提示工程以增加可靠性，以及两种启发式策略以减少黑盒效应的影响。

Result: 研究表明新方法的性能优于先前方法，达到了最先进的水平。该方法强调了当前自动评估指标对问答任务性能评估的限制，并呼吁未来研究。最重要的是，本研究强调了结构化提示工程的潜力，作为确保人工智能在法律领域及其他领域中责任和可信度的有用工具。

Conclusion: 本研究展示了一种结构化提示方法作为在法律领域中应对大型语言模型可靠性和透明性挑战的可行替代方案。通过在长篇法律文件上应用基于CUAD数据集的信息检索任务，本研究表明结构化提示工程有助于提高可靠性和减少黑盒效应的影响。研究的结果显示新方法比先前的方法表现高出9
%，达到了最先进的性能水平。然而，当前自动评估指标限制了对问答任务性能的评估，需要未来研究努力解决。总的来说，本研究强调了结构化提示工程的潜力，是确保人工智能在法律领域及其他领域中承担责任和提高可信度的有用而尚未充分探索的工具。

Abstract: The rise of Large Language Models (LLMs) has had a profoundly transformative
effect on a number of fields and domains. However, their uptake in Law has
proven more challenging due to the important issues of reliability and
transparency. In this study, we present a structured prompting methodology as a
viable alternative to the often expensive fine-tuning, with the capability of
tacking long legal documents from the CUAD dataset on the task of information
retrieval. Each document is first split into chunks via a system of chunking
and augmentation, addressing the long document problem. Then, alongside an
engineered prompt, the input is fed into QWEN-2 to produce a set of answers for
each question. Finally, we tackle the resulting candidate selection problem
with the introduction of the Distribution-based Localisation and Inverse
Cardinality Weighting heuristics. This approach leverages a general purpose
model to promote long term scalability, prompt engineering to increase
reliability and the two heuristic strategies to reduce the impact of the black
box effect. Whilst our model performs up to 9\% better than the previously
presented method, reaching state-of-the-art performance, it also highlights the
limiting factor of current automatic evaluation metrics for question answering,
serving as a call to action for future research. However, the chief aim of this
work is to underscore the potential of structured prompt engineering as a
useful, yet under-explored, tool in ensuring accountability and responsibility
of AI in the legal domain, and beyond.

</details>


### [73] [An Epidemiological Knowledge Graph extracted from the World Health Organization's Disease Outbreak News](https://arxiv.org/abs/2509.02258)
*Sergio Consoli,Pietro Coletti,Peter V. Markov,Lia Orfei,Indaco Biazzo,Lea Schuh,Nicolas Stefanovitch,Lorenzo Bertolini,Mario Ceresa,Nikolaos I. Stilianakis*

Main category: cs.AI

TL;DR: 本文介绍了利用生成式人工智能从世界卫生组织疾病爆发新闻中提取流行病学信息的方法，构建具有更新数据集和知识图谱eKG，为公共卫生领域知识展示提供新的机会。这一创新为流行病学研究和疾病爆发监测带来了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展和社交媒体、新闻对流行病学监测的增加可用性，本文指出了在流行病学和公共卫生研究中的关键时刻。利用生成式人工智能和大型语言模型，能够从WHO的疾病爆发新闻中提取有用的流行病学信息，帮助响应全球疫情。

Method: 本文采用集成多种大型语言模型的方法，利用生成式人工智能从世界卫生组织疾病爆发新闻中提取流行病学信息。构建了每日更新的数据集和知识图谱eKG，为公共卫生领域知识提供详细的展示。

Result: 通过集成多种大型语言模型，从世界卫生组织疾病爆发新闻中提取有价值的流行病学信息，并建立了每日更新的数据集和知识图谱eKG。这些资源拓展了流行病学研究和疾病爆发监测的可能性。

Conclusion: 本文介绍了利用生成式人工智能和大型语言模型从世界卫生组织疾病爆发新闻中提取有价值的流行病学信息的方法。提取的信息可用于建立每日更新的数据集和知识图谱eKG，为公共卫生领域知识提供细致的表征。通过这些创新数据资源，为流行病学研究、疾病爆发分析和监测开拓了全新的机会。

Abstract: The rapid evolution of artificial intelligence (AI), together with the
increased availability of social media and news for epidemiological
surveillance, are marking a pivotal moment in epidemiology and public health
research. Leveraging the power of generative AI, we use an ensemble approach
which incorporates multiple Large Language Models (LLMs) to extract valuable
actionable epidemiological information from the World Health Organization (WHO)
Disease Outbreak News (DONs). DONs is a collection of regular reports on global
outbreaks curated by the WHO and the adopted decision-making processes to
respond to them. The extracted information is made available in a daily-updated
dataset and a knowledge graph, referred to as eKG, derived to provide a nuanced
representation of the public health domain knowledge. We provide an overview of
this new dataset and describe the structure of eKG, along with the services and
tools used to access and utilize the data that we are building on top. These
innovative data resources open altogether new opportunities for epidemiological
research, and the analysis and surveillance of disease outbreaks.

</details>


### [74] [Rewarding Explainability in Drug Repurposing with Knowledge Graphs](https://arxiv.org/abs/2509.02276)
*Susana Nunes,Samy Badreddine,Catia Pesquita*

Main category: cs.AI

TL;DR: 本文提出了一种名为REx的方法，用于在知识图谱中进行链接预测生成科学解释。通过奖励和策略机制引导强化学习代理在知识图谱中识别解释路径，并丰富解释路径以确保解释具有洞见并扎根于建立的生物医学知识。在药物再利用领域中的评估结果表明，REx方法在验证预测见解方面表现优异，超越了现有方法的预测性能。


<details>
  <summary>Details</summary>
Motivation: 为了让预测方法作为可靠的科学工具得到接受，不仅要确保准确性，还必须具备提供有意义的科学解释的能力。本文旨在解决这一问题，通过在知识图谱中进行链接预测生成科学解释，以提高预测方法的可信度。

Method: 本文采用一种名为REx的方法，利用奖励和策略机制引导强化学习代理在知识图谱中识别解释路径。该方法还利用领域特定的本体论丰富解释路径，并在药物再利用领域中使用三个知名的知识图谱基准进行了评估。

Result: 通过评估在药物再利用领域中的三个知识图谱基准，结果表明REx方法能够生成验证预测见解的解释，超越了现有方法的预测性能。

Conclusion: 本文提出了一种名为REx的新方法，用于在知识图谱中进行链接预测生成科学解释。该方法通过奖励和策略机制来指导强化学习代理在知识图谱中识别解释路径，进一步利用领域特定的本体论丰富解释路径，确保解释既富有洞见又扎根于建立的生物医学知识。作者在三个知识图谱基准上评估了他们的方法，结果清楚地表明其能够生成验证预测见解的解释，超越了预测性能方面现有方法的表现，将REx确立为AI驱动科学发现的重要贡献。

Abstract: Knowledge graphs (KGs) are powerful tools for modelling complex,
multi-relational data and supporting hypothesis generation, particularly in
applications like drug repurposing. However, for predictive methods to gain
acceptance as credible scientific tools, they must ensure not only accuracy but
also the capacity to offer meaningful scientific explanations. This paper
presents a novel approach REx, for generating scientific explanations based in
link prediction in knowledge graphs. It employs reward and policy mechanisms
that consider desirable properties of scientific explanation to guide a
reinforcement learning agent in the identification of explanatory paths within
a KG. The approach further enriches explanatory paths with domain-specific
ontologies, ensuring that the explanations are both insightful and grounded in
established biomedical knowledge. We evaluate our approach in drug repurposing
using three popular knowledge graph benchmarks. The results clearly demonstrate
its ability to generate explanations that validate predictive insights against
biomedical knowledge and that outperform the state-of-the-art approaches in
predictive performance, establishing REx as a relevant contribution to advance
AI-driven scientific discovery.

</details>


### [75] [Re-evaluating LLM-based Heuristic Search: A Case Study on the 3D Packing Problem](https://arxiv.org/abs/2509.02297)
*Guorui Quan,Mingfei Sun,Manuel López-Ibáñez*

Main category: cs.AI

TL;DR: 本文讨论了大型语言模型在启发式设计中的应用，发现其在自动生成启发式算法方面面临工程困难和预训练偏见的挑战。研究结果表明，LLM在复杂任务中对评分函数的关注可能反映了其能力的自然限制。LLM生成的启发式算法在一些情况下可以与人类设计的算法媲美，但在约束加强时表现可能会有所下降。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大型语言模型在启发式设计方面的潜力，以及它们与人类设计启发式的差异。通过将LLM应用于复杂任务，探究其创新能力和局限性。

Method: 任务LLM构建约束3D装箱问题的完整求解器，引入了约束支撑和迭代自我修正两种支持方法，以修复代码生成中出现的问题。研究发现，LLM在庞大的搜索空间中几乎完全专注于细化评分函数。

Result: 研究发现，由LLM生成的启发式算法与人类设计的贪婪算法相当，并在集成到人工设计的元启发式中表现出色，尽管在约束加强时其效果有所减弱。

Conclusion: 当前的大型语言模型在自动生成启发式算法方面存在两个主要障碍：在复杂推理任务中减轻其脆弱性所需的工程化工作，以及预先训练的偏见的影响，这可能会过早地限制对新颖解决方案的搜索。

Abstract: The art of heuristic design has traditionally been a human pursuit. While
Large Language Models (LLMs) can generate code for search heuristics, their
application has largely been confined to adjusting simple functions within
human-crafted frameworks, leaving their capacity for broader innovation an open
question. To investigate this, we tasked an LLM with building a complete solver
for the constrained 3D Packing Problem. Direct code generation quickly proved
fragile, prompting us to introduce two supports: constraint
scaffolding--prewritten constraint-checking code--and iterative
self-correction--additional refinement cycles to repair bugs and produce a
viable initial population. Notably, even within a vast search space in a greedy
process, the LLM concentrated its efforts almost exclusively on refining the
scoring function. This suggests that the emphasis on scoring functions in prior
work may reflect not a principled strategy, but rather a natural limitation of
LLM capabilities. The resulting heuristic was comparable to a human-designed
greedy algorithm, and when its scoring function was integrated into a
human-crafted metaheuristic, its performance rivaled established solvers,
though its effectiveness waned as constraints tightened. Our findings highlight
two major barriers to automated heuristic design with current LLMs: the
engineering required to mitigate their fragility in complex reasoning tasks,
and the influence of pretrained biases, which can prematurely narrow the search
for novel solutions.

</details>


### [76] [Exploring Diffusion Models for Generative Forecasting of Financial Charts](https://arxiv.org/abs/2509.02308)
*Taegyeong Lee,Jiwon Park,Kyunga Bang,Seunghyun Hwang,Ung-Jin Jang*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的方法，利用文本到图像生成模型将时间序列数据作为图像模式，预测股价走势。通过扩散模型生成下一个图表图像，并提出了评估生成图表图像的简单方法。实验证明了该方法在金融领域的潜力，鼓励进一步研究以拓展其应用。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在图像生成和编辑方面取得了显著进展，并在各领域得到应用，但金融领域仍可能倚赖时间序列数据和专注于变压器模型，而不是多样化的生成模型应用。因此，本文的动机在于利用文本到图像生成模型，改变传统方法，将时间序列数据视为图像模式，并尝试预测股价走势。

Method: 将时间序列数据视为单个图像模式，利用文本到图像模型，通过扩散模型生成下一个图表图像。提出了一种简单的方法来评估生成图表图像的有效性。

Result: 通过实验，证明了该方法在金融领域中的潜力，展示了利用文本到图像生成模型进行股价走势预测的有效性。

Conclusion: 本文提出了一种新颖的方法，将时间序列数据视为单个图像模式，通过利用文本到图像模型，实现了股价走势的预测。采用扩散模型生成下一个图表图像，而不是像以往方法那样关注使用ResNet或ViT等架构学习和分类图表模式。介绍了一种简单的方法来评估生成的图表图像与基准图像之间的差异。强调了在金融领域利用文本到图像生成模型的潜力，并鼓励进一步研究以解决当前的局限性并拓展其适用性。

Abstract: Recent advances in generative models have enabled significant progress in
tasks such as generating and editing images from text, as well as creating
videos from text prompts, and these methods are being applied across various
fields. However, in the financial domain, there may still be a reliance on
time-series data and a continued focus on transformer models, rather than on
diverse applications of generative models. In this paper, we propose a novel
approach that leverages text-to-image model by treating time-series data as a
single image pattern, thereby enabling the prediction of stock price trends.
Unlike prior methods that focus on learning and classifying chart patterns
using architectures such as ResNet or ViT, we experiment with generating the
next chart image from the current chart image and an instruction prompt using
diffusion models. Furthermore, we introduce a simple method for evaluating the
generated chart image against ground truth image. We highlight the potential of
leveraging text-to-image generative models in the financial domain, and our
findings motivate further research to address the current limitations and
expand their applicability.

</details>


### [77] [Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging](https://arxiv.org/abs/2509.02340)
*Salma Haidar,José Oramas*

Main category: cs.AI

TL;DR: 该研究探讨了在高光谱成像中基于模型驱动框架的波段选择方法，通过解释性方法进行特征选择，以降低维度并保持准确性。实验证明，选择最具影响力的波段可以产生紧凑的光谱子集，保持准确性并提高效率，同时减少计算需求。


<details>
  <summary>Details</summary>
Motivation: 由于高光谱成像(HSI)的高维度会引入计算负担和冗余，需要进行降维处理，因此研究动机在于提出一种在模型驱动框架中结合解释性方法的新颖波段选择方法，以实现高效的降维，并保持分类性能。

Method: 该研究采用基于模型驱动框架结合后续解释性方法进行波段选择，通过量化每个波段对分类器决策的贡献，并进行删除-插入评估，记录置信度变化并将这些信号聚合为影响力评分，最终选择出影响最大的波段，以实现光谱子集的紧凑化。

Result: 实验结果表明，仅选择30个波段训练的分类器在两个公共基准数据集上可以达到或超过完整光谱基准的性能，并减少了计算需求。所选的光谱子集与物理上有意义、高度区分性的波长区域保持一致。

Conclusion: 该研究探讨了在基于模型驱动的框架中应用后续解释性方法进行波段选择，以减少光谱维度同时保持预测性能的可行性。最终结果表明，选择具有最高影响力的波段可以获得紧凑的光谱子集，同时保持准确性并提高效率。在两个公共基准数据集上的实验显示，仅在选择了30个波段的情况下训练的分类器可以与完整光谱基准相匹配或超越，同时减少了计算需求。

Abstract: Hyperspectral imaging (HSI) provides rich spectral information for precise
material classification and analysis; however, its high dimensionality
introduces a computational burden and redundancy, making dimensionality
reduction essential. We present an exploratory study into the application of
post-hoc explainability methods in a model--driven framework for band
selection, which reduces the spectral dimension while preserving predictive
performance. A trained classifier is probed with explanations to quantify each
band's contribution to its decisions. We then perform deletion--insertion
evaluations, recording confidence changes as ranked bands are removed or
reintroduced, and aggregate these signals into influence scores. Selecting the
highest--influence bands yields compact spectral subsets that maintain accuracy
and improve efficiency. Experiments on two public benchmarks (Pavia University
and Salinas) demonstrate that classifiers trained on as few as 30 selected
bands match or exceed full--spectrum baselines while reducing computational
requirements. The resulting subsets align with physically meaningful, highly
discriminative wavelength regions, indicating that model--aligned,
explanation-guided band selection is a principled route to effective
dimensionality reduction for HSI.

</details>


### [78] [When Agents go Astray: Course-Correcting SWE Agents with PRMs](https://arxiv.org/abs/2509.02360)
*Shubham Gandhi,Jason Tsay,Jatin Ganhotra,Kiran Kate,Yara Rizk*

Main category: cs.AI

TL;DR: 本文介绍了SWE-PRM模型，该模型能够在执行过程中检测和纠正大型语言模型代理的轨迹级错误，提高了任务解决效率，成功率增加，轨迹长度减少。分类指导的PRMs在提供反馈策略时表现优于其他变体，且推断成本低廉，适用于提高SWE代理的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 以往的方法通常在执行后才诊断系统漏洞，本文旨在提出一种在执行过程中干预以检测和纠正系统轨迹级错误的方法。为了提高大型语言模型代理的可靠性和效率，需要改进处理常见低效性问题的能力。

Method: 介绍了SWE-PRM模型，通过干预执行过程来检测和纠正轨迹级错误，利用常见低效性的分类法设计PRM模型，提供轻量且可解释的反馈，而不修改基础策略。在SWE-bench Verified上验证了PRM模型的效果，对中等和困难任务取得最大增益。分类指导的PRMs在反馈策略中表现优于无指导或明确行动处方变体，同时降低轨迹长度。

Result: SWE-PRM模型在SWE-bench Verified上能够显著提高大型语言模型代理的任务解决效率，成功率增加且轨迹长度减少，尤其在中等和困难任务上表现出最大的增益。通过分类指导的PRMs，可以在保持推断成本低廉的前提下提供实用且可扩展的改进机制。

Conclusion: 本文介绍了一种新的推理时过程奖励模型(SWE-PRM)，用于检测和纠正大型语言模型代理在执行过程中的轨迹级错误。通过在执行过程中干预来检测和纠正轨迹级错误，该设计利用常见低效性的分类法，提供轻量且可解释的反馈，而不修改基础策略。在SWE-bench Verified上，经过关闭源验证的PRMs将分辨率从40.0%提高到50.6%，取得10.6个百分点增长，在中等和困难任务上增益最大。在反馈策略中，分类指导的PRMs优于无指导或明确行动处方变体，增加成功率的同时减少轨迹长度。这些好处以低至0.2美元的可接受附加推断成本实现，使PRMs成为提高SWE代理可靠性和效率的实用且可扩展的机制。

Abstract: Large Language Model (LLM) agents are increasingly deployed for complex,
multi-step software engineering (SWE) tasks. However, their trajectories often
contain costly inefficiencies, such as redundant exploration, looping, and
failure to terminate once a solution is reached. Prior work has largely treated
these errors in a post-hoc manner, diagnosing failures only after execution. In
this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM)
that intervenes during execution to detect and course-correct trajectory-level
errors. Our PRM design leverages a taxonomy of common inefficiencies and
delivers lightweight, interpretable feedback without modifying the underlying
policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0%
to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among
feedback strategies, taxonomy-guided PRMs outperform unguided or explicit
action-prescriptive variants, increasing success rate while reducing trajectory
length. These benefits come at an acceptable added inference cost of as low as
$0.2, making PRMs a practical and scalable mechanism for improving SWE agents'
reliability and efficiency.

</details>


### [79] [Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning](https://arxiv.org/abs/2509.02401)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Gianluca Mazzoni,Lea Mørch Harder,Philip Torr,Jesper Ferkinghoff-Borg,Kaspar Martens,Julien Fauqueur*

Main category: cs.AI

TL;DR: 该论文介绍了一种基于不确定性的代理模型，用于处理结构化的生物医学数据。通过引入总结不确定性，并结合强化学习技术，成功改善了信息的客观性、校准性和有效性。研究结果表明，不确定性可作为控制信号，使代理模型在复杂环境中更可靠，为处理结构化数据提供了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理在结构化的生物医学数据环境中被广泛使用，但在处理复杂的多表数据时经常会产生流畅但过于自信的输出。因此，有必要引入一种能够识别不确定性并在推理和总结中加以利用的代理模型，以提高信息的客观性、校准性和有效性。

Method: 引入了一种基于不确定性的代理模型，使用两种互补信号进行多表总结。将总结不确定性整合到强化学习中，利用 Group Relative Policy Optimization（GRPO），并利用检索和总结不确定性指导推理时的过滤和支持构建高质量合成数据集。

Result: 在多组学基准测试中，该方法在提高信息客观性和校准性方面取得了显著改进，增加了每个摘要中正确和有用声明的数量，同时显著提高了下游生存预测的准确性。这些结果表明，将不确定性视为控制信号可以使代理模型在复杂结构化数据环境中更可靠，并能够更好地应对挑战。

Conclusion: 在结构化的生物医学数据环境中，引入了一种基于不确定性的代理模型，用于查询条件下的多表总结。该代理模型利用两种互补信号：检索不确定性-多表选择迭代的熵和总结不确定性-结合自洽性和困惑度。总结不确定性被整合到强化学习（RL）中，通过 Group Relative Policy Optimization（GRPO），而检索和总结不确定性则引导推理时的过滤和支持构建更高质量的合成数据集。在多组学基准测试中，该方法提高了客观性和校准性，每个摘要的正确和有用声明几乎增加了两倍（从3.0到8.4内部；从3.6到9.9癌症多组学），并且大幅改善了下游生存预测（C-index 0.32到0.63）。这些结果表明，不确定性可以作为控制信号，使代理能够放弃、传达信心，并成为处理复杂结构化数据环境的更可靠工具。

Abstract: Large language model (LLM) agents are increasingly deployed in structured
biomedical data environments, yet they often produce fluent but overconfident
outputs when reasoning over complex multi-table data. We introduce an
uncertainty-aware agent for query-conditioned multi-table summarization that
leverages two complementary signals: (i) retrieval uncertainty--entropy over
multiple table-selection rollouts--and (ii) summary uncertainty--combining
self-consistency and perplexity. Summary uncertainty is incorporated into
reinforcement learning (RL) with Group Relative Policy Optimization (GRPO),
while both retrieval and summary uncertainty guide inference-time filtering and
support the construction of higher-quality synthetic datasets.
  On multi-omics benchmarks, our approach improves factuality and calibration,
nearly tripling correct and useful claims per summary (3.0\(\rightarrow\)8.4
internal; 3.6\(\rightarrow\)9.9 cancer multi-omics) and substantially improving
downstream survival prediction (C-index 0.32\(\rightarrow\)0.63). These results
demonstrate that uncertainty can serve as a control signal--enabling agents to
abstain, communicate confidence, and become more reliable tools for complex
structured-data environments.

</details>


### [80] [AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent](https://arxiv.org/abs/2509.02444)
*Jingru Fan,Yufan Dang,Jingyao Wu,Huatao Li,Runde Yang,Xiyuan Yang,Yuheng Wang,Zhong Zhang,Yaxi Lu,Yankai Lin,Zhiyuan Liu,Dahai Li,Chen Qian*

Main category: cs.AI

TL;DR: 本文介绍了移动代理领域面临的核心挑战，并提出了AppCopilot系统作为解决方案。该系统通过多方面的功能设计和系统优化实现了在多个方面的显著改进。


<details>
  <summary>Details</summary>
Motivation: 鉴于移动代理领域的蓬勃发展但尚未解决的困难，本文旨在解决通用化、精确性、长时程能力和效率性等核心挑战，提出了新的解决方案。

Method: 本文提出了AppCopilot系统，通过端到端自动管道实现多模态模型集成、推理控制层组合和执行层功能组合，系统设计考虑了异构硬件的优化。

Result: AppCopilot系统实现了强化通用化、提高屏幕操作精确性、更可靠地完成长时程任务和更快、更节约资源的运行效率。

Conclusion: 本文针对移动代理领域的核心挑战提出四个问题，提出了通用化、精确性、长时程能力和效率性的解决方案。通过介绍AppCopilot系统，展示了一个多模态、多代理、通用型的设备助手，实现了全栈闭环系统，包括数据收集、训练、部署、高效推理和移动应用开发。实验证明AppCopilot在通用化、精确性、长时程能力和效率性等方面取得了显著改进。

Abstract: With the raid evolution of large language models and multimodal foundation
models, the mobile-agent landscape has proliferated without converging on the
fundamental challenges. This paper identifies four core problems that must be
solved for mobile agents to deliver practical, scalable impact: (1)
generalization across tasks, modalities, apps, and devices; (2) accuracy,
specifically precise on-screen interaction and click targeting; (3)
long-horizon capability for sustained, multi-step goals; and (4) efficiency,
specifically high-performance runtime on resource-constrained devices. We
present AppCopilot, a multimodal, multi-agent, general-purpose on-device
assistant that operates across applications and constitutes a full-stack,
closed-loop system from data to deployment. AppCopilot operationalizes this
position through an end-to-end autonomous pipeline spanning data collection,
training, deployment, high-quality and efficient inference, and mobile
application development. At the model layer, it integrates multimodal
foundation models with robust Chinese-English support. At the reasoning and
control layer, it combines chain-of-thought reasoning, hierarchical task
planning and decomposition, and multi-agent collaboration. At the execution
layer, it enables user personalization and experiential adaptation, voice
interaction, function calling, cross-app and cross-device orchestration, and
comprehensive mobile app support. The system design incorporates
profiling-driven optimization for latency, memory, and energy across
heterogeneous hardware. Empirically, AppCopilot achieves significant
improvements along all four dimensions: stronger generalization,
higher-precision on-screen actions, more reliable long-horizon task completion,
and faster, more resource-efficient runtime.

</details>


### [81] [GridMind: LLMs-Powered Agents for Power System Analysis and Operations](https://arxiv.org/abs/2509.02494)
*Hongwei Jin,Kibaek Kim,Jonghwan Kwon*

Main category: cs.AI

TL;DR: 本文介绍了GridMind系统，利用大型语言模型和确定性工程求解器实现对电力系统分析的对话式科学计算。实验证明，该系统提供正确解决方案，并展示较小的LLMs在减少计算延迟的同时实现了可比的准确性。该研究确立了智能体人工智能作为科学计算的可iable范式，并展示了对话界面如何增强可访问性并保持数值精确性。


<details>
  <summary>Details</summary>
Motivation: 传统电力系统分析工作流程的复杂性给现代电网中的高效决策带来了显著障碍。本研究旨在解决工作流程集成、知识可访问性、上下文保留和专家决策支持增强等问题。

Method: 本文采用了GridMind系统，将大型语言模型与确定性工程求解器相结合，使用专门的代理通过自然语言界面协调交流最优潮流和N-1事故分析，同时保持数值精度。

Result: 实验证明，GridMind系统在IEEE测试案例上持续提供正确的解决方案，较小的LLMs在减少计算延迟的同时实现了可比的分析准确性。该研究将智能体人工智能确立为科学计算的可行范式，展示了对话界面如何增强可访问性同时保持对关键工程应用至关重要的数值严谨性。

Conclusion: 本文介绍了GridMind，一个多智能体人工智能系统，将大型语言模型（LLMs）与确定性工程求解器集成在一起，实现对电力系统分析的对话式科学计算。实验证明，所提出的智能框架在所有测试的语言模型上始终提供正确的解决方案，较小的LLMs在减少计算延迟的同时实现了可比的分析准确性。

Abstract: The complexity of traditional power system analysis workflows presents
significant barriers to efficient decision-making in modern electric grids.
This paper presents GridMind, a multi-agent AI system that integrates Large
Language Models (LLMs) with deterministic engineering solvers to enable
conversational scientific computing for power system analysis. The system
employs specialized agents coordinating AC Optimal Power Flow and N-1
contingency analysis through natural language interfaces while maintaining
numerical precision via function calls. GridMind addresses workflow
integration, knowledge accessibility, context preservation, and expert
decision-support augmentation. Experimental evaluation on IEEE test cases
demonstrates that the proposed agentic framework consistently delivers correct
solutions across all tested language models, with smaller LLMs achieving
comparable analytical accuracy with reduced computational latency. This work
establishes agentic AI as a viable paradigm for scientific computing,
demonstrating how conversational interfaces can enhance accessibility while
preserving numerical rigor essential for critical engineering applications.

</details>


### [82] [UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.02544)
*Haoming Wang,Haoyang Zou,Huatong Song,Jiazhan Feng,Junjie Fang,Junting Lu,Longxiang Liu,Qinyu Luo,Shihao Liang,Shijue Huang,Wanjun Zhong,Yining Ye,Yujia Qin,Yuwen Xiong,Yuxin Song,Zhiyong Wu,Bo Li,Chen Dun,Chong Liu,Fuxing Leng,Hanbin Wang,Hao Yu,Haobin Chen,Hongyi Guo,Jing Su,Jingjia Huang,Kai Shen,Kaiyu Shi,Lin Yan,Peiyao Zhao,Pengfei Liu,Qinghao Ye,Renjie Zheng,Wayne Xin Zhao,Wen Heng,Wenhao Huang,Wenqian Wang,Xiaobo Qin,Yi Lin,Youbin Wu,Zehui Chen,Zihao Wang,Baoquan Zhong,Xinchun Zhang,Xujing Li,Yuanfan Li,Zhongkai Zhao,Chengquan Jiang,Faming Wu,Haotian Zhou,Jinlin Pang,Li Han,Qianli Ma,Siyao Liu,Songhua Cai,Wenqi Fu,Xin Liu,Zhi Zhang,Bo Zhou,Guoliang Li,Jiajun Shi,Jiale Yang,Jie Tang,Li Li,Taoran Lu,Woyu Lin,Xiaokang Tong,Xinyao Li,Yichi Zhang,Yu Miao,Zhengxuan Jiang,Zili Li,Ziyuan Zhao,Chenxin Li,Dehua Ma,Feng Lin,Ge Zhang,Haihua Yang,Hangyu Guo,Hongda Zhu,Jiaheng Liu,Junda Du,Kai Cai,Kuanye Li,Lichen Yuan,Meilan Han,Minchao Wang,Shuyue Guo,Tianhao Cheng,Xiaobo Ma,Xiaojun Xiao,Xiaolong Huang,Xinjie Chen,Yidi Du,Yilin Chen,Yiwen Wang,Zhaojian Li,Zhenzhu Yang,Zhiyuan Zeng,Chaolin Jin,Chen Li,Hao Chen,Haoli Chen,Jian Chen,Qinghao Zhao,Guang Shi*

Main category: cs.AI

TL;DR: Developed UI-TARS-2, a native GUI-centered agent model, to address challenges in autonomous agents for graphical user interfaces. UI-TARS-2 achieved significant improvements over UI-TARS-1.5, outperforming strong baselines on GUI benchmarks and game environments. It demonstrated generalization to diverse agent tasks and real-world interactive scenarios, showcasing its potential to advance GUI agent development.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in developing autonomous agents for graphical user interfaces, including data scalability, multi-turn reinforcement learning, GUI-only operation limitations, and environment stability. Aim to advance the state of GUI agents and demonstrate generalization to real-world interactive scenarios.

Method: Developed UI-TARS-2, a native GUI-centered agent model addressing challenges in autonomous agents for graphical user interfaces. Implemented a systematic training methodology including a data flywheel for scalable data generation, stabilized multi-turn RL framework, hybrid GUI environment, and a unified sandbox platform for large-scale rollouts. Conducted empirical evaluation to compare with UI-TARS-1.5 and strong baselines on GUI benchmarks and game environments.

Result: UI-TARS-2 achieved significant improvements over its predecessor and outperformed strong baselines on GUI benchmarks and game environments. It also demonstrated generalization to diverse agent tasks and software engineering benchmarks, showcasing robustness and potential for advancing GUI agent development.

Conclusion: UI-TARS-2 achieved significant improvements over UI-TARS-1.5, outperforming strong baselines on GUI benchmarks and game environments. It demonstrated generalization to diverse agent tasks and real-world interactive scenarios, highlighting its potential to advance GUI agent development.

Abstract: The development of autonomous agents for graphical user interfaces (GUIs)
presents major challenges in artificial intelligence. While recent advances in
native agent models have shown promise by unifying perception, reasoning,
action, and memory through end-to-end learning, open problems remain in data
scalability, multi-turn reinforcement learning (RL), the limitations of
GUI-only operation, and environment stability. In this technical report, we
present UI-TARS-2, a native GUI-centered agent model that addresses these
challenges through a systematic training methodology: a data flywheel for
scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI
environment that integrates file systems and terminals, and a unified sandbox
platform for large-scale rollouts. Empirical evaluation demonstrates that
UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.
On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on
WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines
such as Claude and OpenAI agents. In game environments, it attains a mean
normalized score of 59.8 across a 15-game suite-roughly 60% of human-level
performance-and remains competitive with frontier proprietary models (e.g.,
OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to
long-horizon information-seeking tasks and software engineering benchmarks,
highlighting its robustness across diverse agent tasks. Detailed analyses of
training dynamics further provide insights into achieving stability and
efficiency in large-scale agent RL. These results underscore UI-TARS-2's
potential to advance the state of GUI agents and exhibit strong generalization
to real-world interactive scenarios.

</details>


### [83] [The Landscape of Agentic Reinforcement Learning for LLMs: A Survey](https://arxiv.org/abs/2509.02547)
*Guibin Zhang,Hejia Geng,Xiaohang Yu,Zhenfei Yin,Zaibin Zhang,Zelin Tan,Heng Zhou,Zhongzhi Li,Xiangyuan Xue,Yijiang Li,Yifan Zhou,Yang Chen,Chen Zhang,Yutao Fan,Zihu Wang,Songtao Huang,Yue Liao,Hongru Wang,Mengyue Yang,Heng Ji,Michael Littman,Jun Wang,Shuicheng Yan,Philip Torr,Lei Bai*

Main category: cs.AI

TL;DR: 该论文介绍了代理强化学习在大型语言模型中的运用，对比了LLM-RL和Agentic RL的特点，提出了双重分类法，总结了开源资源，加速未来研究。综合分析了五百多个最近的研究成果，描绘了领域的发展轮廓，突出机遇和挑战。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于介绍代理强化学习在大型语言模型中的应用，探讨LLMs和Agentic RL之间的区别和联系，提出新的分类法，并为未来研究提供支持和加速。

Method: 论文通过对比不同类型的马尔可夫决策过程，界定了代理强化学习的概念，并提出了双重分类法。同时总结了开源环境、基准和框架，为未来研究提供支持和加速。通过综合分析五百多个最近的研究成果，描绘了这一领域的发展轮廓。

Result: 论文提出了代理强化学习的概念转变，建立了新的分类法，并总结了开源资源，为未来研究指明方向。综合分析了大量研究成果，揭示了该领域的发展趋势和挑战。

Conclusion: 该论文介绍了代理强化学习（Agentic RL）在大型语言模型（LLM RL）中的应用，将LLMs从被动序列生成器转变为嵌入在复杂动态世界中的自主决策制定机器人。通过对比LLM-RL中的单步马尔可夫决策过程（MDPs）和定义Agentic RL的暂时延伸、部分可观测的马尔可夫决策过程（POMDPs），正式界定了这一概念转变。论文提出了一个综合的双重分类法，一个围绕核心代理能力，包括规划、工具使用、记忆、推理、自我改进和感知，并另一个围绕这些能力在不同任务领域的应用。强调强化学习作为将这些能力从静态、启发式模块转变为自适应、强健代理行为的关键机制。通过总结开源环境、基准和框架的现状，为支持和加速未来研究提供了实用的资料，通过综合五百多个最近的研究成果，描绘了这一快速发展领域的轮廓，并突出了将塑造可扩展的通用AI代理的机遇和挑战。

Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm
shift from conventional reinforcement learning applied to large language models
(LLM RL), reframing LLMs from passive sequence generators into autonomous,
decision-making agents embedded in complex, dynamic worlds. This survey
formalizes this conceptual shift by contrasting the degenerate single-step
Markov Decision Processes (MDPs) of LLM-RL with the temporally extended,
partially observable Markov decision processes (POMDPs) that define Agentic RL.
Building on this foundation, we propose a comprehensive twofold taxonomy: one
organized around core agentic capabilities, including planning, tool use,
memory, reasoning, self-improvement, and perception, and the other around their
applications across diverse task domains. Central to our thesis is that
reinforcement learning serves as the critical mechanism for transforming these
capabilities from static, heuristic modules into adaptive, robust agentic
behavior. To support and accelerate future research, we consolidate the
landscape of open-source environments, benchmarks, and frameworks into a
practical compendium. By synthesizing over five hundred recent works, this
survey charts the contours of this rapidly evolving field and highlights the
opportunities and challenges that will shape the development of scalable,
general-purpose AI agents.

</details>
