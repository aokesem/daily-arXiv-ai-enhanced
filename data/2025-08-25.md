<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 该论文提出了神经符号框架T-ILR，旨在将线性时态逻辑规范直接融入深度学习架构中，提高了图像序列分类任务的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前集成符号知识与深度学习架构的方法在静态领域取得了良好结果，但处理时态逻辑规范的方法仍未得到充分探索。现有方法依赖于有限状态自动机的显式表示，而本论文旨在将线性时态逻辑规范直接整合到深度学习架构中，以应对时序任务。

Method: 在神经符号框架中扩展了迭代局部细化（ILR）算法，利用模糊LTLf解释，将线性时态逻辑规范直接融入深度学习架构，从而提出了Temporal Iterative Local Refinement（T-ILR）方法。

Result: T-ILR在图像序列分类的任务中展现了比现有方法更高的准确性和计算效率。

Conclusion: 该论文提出了一种神经符号框架，称为Temporal Iterative Local Refinement（T-ILR），旨在将线性时态逻辑规范直接融入基于序列的深度学习架构中。通过在现有基准上评估T-ILR，在处理带有时态知识的图像序列分类任务时，相较于现有方法，T-ILR表现出更高的准确性和计算效率。

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [2] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: 该论文介绍了一个生成对抗性心电图（CoFE）框架，用于展示特征如何影响AI-ECG模型的预测决策。通过两个案例研究证明框架的适用性，揭示心电图信号中特征的变化与临床知识的一致性，预期提高AI-ECG模型的可解释性及支持更有效的临床决策。


<details>
  <summary>Details</summary>
Motivation: 为了实现对AI-ECG模型的可解释性，帮助其成功融入临床实践，介绍了CoFE框架。

Method: 介绍了生成对抗性心电图（CoFE）框架，用于生成CoFE以说明特征如何影响模型的预测决策，展示了两个案例研究以证明框架的适用性。

Result: 通过揭示心电图信号中特征的变化与临床知识的一致性，预期增强AI-ECG模型的可解释性，并支持更有效的临床决策。

Conclusion: 该论文介绍了一个生成对抗性心电图（CoFE）框架，用于说明特定特征如振幅和间隔如何影响模型的预测决策。通过展示CoFE的适用性，包括两个案例研究：房颤分类和钾水平回归模型，揭示了心电图信号中特征变化与临床知识的一致性。通过阐明心电图中有效特征的出现位置以及它们如何影响模型预测，预期该框架将增强AI-ECG模型的可解释性，支持更有效的临床决策。

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [3] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 该论文提出了一个训练免费的框架，由自适应规划图引导，用于多模态多跳问题回答。实验表明，该方法在MultimodalQA和WebQA上与依赖训练的现有模型相匹配甚至表现更好。框架包括规划、检索和推理模块，规划模块分析自适应规划图的当前状态，确定下一步操作和图的扩展位置，实现推理路径的动态灵活探索。设计了适应不同数据类型的模态特定策略，以处理文本到未指定目标模态的检索。作者的方法在实验中表现出色，显示出与依赖训练的现有模型相匹配甚至优于其表现。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于顺序检索和推理，这种单一路径范式使它们容易受到错误的影响。此外，开发多模态模型往往需要大量的训练，计算成本昂贵。因此，作者提出了一个训练免费的框架，以解决这些限制。

Method: 框架包括规划、检索和推理模块，规划模块分析自适应规划图的当前状态，确定下一步操作和图的扩展位置，实现推理路径的动态灵活探索。设计了适应不同数据类型的模态特定策略，以处理文本到未指定目标模态的检索。

Result: 作者的方法在实验中表现出色，显示出与依赖训练的现有模型相匹配甚至优于其表现。

Conclusion: 该论文提出了一个训练免费的框架，由自适应规划图引导，用于多模态多跳问题回答。实验表明，该方法在MultimodalQA和WebQA上与依赖训练的现有模型相匹配甚至表现更好。

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [4] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: 该研究介绍了Generative Deep Patient（GDP），一个多模态基础模型，与当前大多数方法不同，GDP能够有效处理结构化电子病历时间序列和非结构化临床数据。该模型经过两个阶段的训练，在临床预测和临床叙述生成方面展现出优异性能，同时在MIMIC-IV数据集上取得了显著成果。GDP展示了灵活的架构能够扩展到其他数据模态。


<details>
  <summary>Details</summary>
Motivation: 电子病历（EHRs）是丰富的临床数据源，但也是复杂的患者数据存储库，包括结构化元素（人口统计学、生命体征、实验室结果、编码）、非结构化临床笔记和其他数据模态。利用这种异质性对于改善患者预后至关重要。近期大型语言模型（LLMs）的进展使得可以学习多种数据模态并支持临床任务的基础模型成为可能。然而，大多数当前方法仅将数值型EHR数据序列化为文本，存在遗失时间和数量细节的风险。

Method: 研究引入了Generative Deep Patient（GDP）多模态基础模型，该模型通过CNN-Transformer编码器对结构化电子病历时间序列进行编码，并通过交叉模态注意力融合非结构化电子病历，采用LLaMA-based解码器。GDP训练分为两个阶段：（1）生成性预训练，学习从原始患者时间线中产生临床叙述并执行掩蔽特征预测（MFP）和下一个时间步预测（NTP）以捕获时间动态；（2）多任务微调用于临床有意义的预测（如心力衰竭、2型糖尿病、30天再入院）。

Result: 在MIMIC-IV上，GDP在临床预测中表现出优越性能，心力衰竭AUROC = 0.923、2型糖尿病AUROC = 0.817、30天再入院AUROC = 0.627。在叙述生成方面，GDP实现了ROUGE-L = 0.135和BERTScore-F1 = 0.545。在盲人评估中，GDP-Instruct在忠实度、流畅度和整体临床实用性方面得分最高，表明减少医院文件工作量而不牺牲准确性。

Conclusion: 该研究提出的Generative Deep Patient（GDP）多模态基础模型在临床预测和临床叙述生成方面表现出优异性能。通过对结构化电子病历时间序列进行编码并与非结构化病历进行跨模态注意融合，GDP在临床任务中展现出卓越的表现。该模型不仅可以预测临床可操作事件，还可以生成高质量的临床叙述，减少了医院文件工作量而不影响准确性。GDP还展现了灵活的架构，可以扩展到其他数据模态。

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [5] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 研究探讨了城市舒适性的理论解释和方法论，强调了多维分析、数据支持和人工智能辅助等关键维度。采用计算方法评估绿化覆盖、热舒适和步行便利性等因素。


<details>
  <summary>Details</summary>
Motivation: 城市规划的基本目标之一是确保生活质量和舒适度。先前的研究已经使用了计算方法来评估与城市舒适性相关的因素，但对城市舒适性的明确定义和综合评估框架仍然模糊。

Method: 采用计算方法来评估城市舒适性，包括绿化覆盖、热舒适和步行便利性等因素。

Result: 综合评估城市舒适性的理论解释和方法论，突出了多维分析、数据支持和人工智能辅助等三个关键维度。

Conclusion: 研究探讨了在数字规划中评估城市舒适性的理论解释和方法论，强调了多维分析、数据支持和人工智能辅助等三个关键维度。

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [6] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: 本文提出了Multi-layer Steerable Embedding Fusion (MSEF)框架，实现了LLMs在所有深度直接访问时间序列模式，从而改善了在深层中时间序列表示的渐进丢失问题。实验结果显示MSEF在七项基准测试中相较基线方法有显著性能提升，均方误差平均降低31.8%。


<details>
  <summary>Details</summary>
Motivation: 在现有方法中，LLMs通常仅在输入层浅层次访问时间序列表示，这导致时间序列表示在更深层次逐渐减弱，最终导致文本嵌入和时间序列表示之间的有效适应性降低。因此，需要一种方法来解决这一问题，以利用LLMs在时间序列预测中的潜力。

Method: 本文使用了Multi-layer Steerable Embedding Fusion (MSEF)框架，该框架利用现成的时间序列基础模型提取语义丰富的嵌入，通过层特定的引导向量将这些嵌入与LLM层之间的中间文本表示进行融合。这些引导向量旨在不断优化时间序列和文本模态之间的对齐，促进层特定的适应机制，确保高效的小样本学习能力。

Result: 在七项基准测试中，MSEF相对于基线方法表现出明显的性能改进，均方误差平均降低了31.8%。

Conclusion: 本文提出了一种新颖的框架Multi-layer Steerable Embedding Fusion (MSEF)，使大型语言模型 (LLMs) 能够直接在所有深度访问时间序列模式，从而减轻了在深层中时间序列表示的渐进丢失。通过实验结果，在七项基准测试中，MSEF 相对基线方法表现出显著的性能改进，均方误差平均降低了31.8%。

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [7] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: The paper introduces InMind, a framework to evaluate whether LLMs can adopt personalized reasoning styles in social deduction games, highlighting the limitations of general-purpose LLMs like GPT-4o in individualized reasoning. Reasoning-enhanced LLMs, such as DeepSeek-R1, show promise in style-sensitive reasoning. InMind paves the way for cognitively aligned human-AI interaction.


<details>
  <summary>Details</summary>
Motivation: Previous evaluations of LLMs focused on human-centric reasoning tasks but neglected individualized reasoning styles. Social deduction games offer a testbed for evaluating diverse reasoning strategies. The motivation is to understand if LLMs can adapt to different players' reasoning styles in identical conditions.

Method: The paper introduces InMind, a framework for evaluating whether LLMs can capture and apply personalized reasoning styles in social deduction games. InMind collects round-level strategy traces and post-game reflections in Observer and Participant modes to assess static alignment and dynamic adaptation. The framework supports four cognitively motivated tasks for evaluation.

Result: The study applied InMind to evaluate 11 LLMs in the game Avalon. General-purpose LLMs, including GPT-4o, struggled with temporal anchoring and evolving strategies, relying on lexical cues. Reasoning-enhanced LLMs like DeepSeek-R1 exhibited style-sensitive reasoning early on. These results reveal the limitations of current LLMs in personalized, adaptive reasoning.

Conclusion: LLMs, including state-of-the-art ones such as GPT-4o, struggle with capturing and applying personalized reasoning styles in social deduction games. Reasoning-enhanced LLMs, like DeepSeek-R1, show potential in style-sensitive reasoning. The study highlights the limitations of current LLMs in individualized, adaptive reasoning and proposes InMind as a framework for cognitively aligned human-AI interaction.

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [8] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: IR-Agent is a multi-agent framework for molecular structure elucidation from IR spectra, designed to emulate expert-driven IR analysis and improve accuracy. It enhances performance on experimental IR spectra and shows adaptability to diverse chemical information.


<details>
  <summary>Details</summary>
Motivation: Existing approaches in IR spectroscopy lack flexibility in incorporating diverse types of chemical knowledge essential in real-world analytical scenarios. The paper aims to address this gap by introducing a framework that enhances performance and adaptability in molecular structure elucidation from IR spectra.

Method: The paper proposes the IR-Agent framework designed to emulate expert-driven IR analysis procedures with a multi-agent approach. Each agent specializes in a specific aspect of IR interpretation, enabling integrated reasoning for improved accuracy of structure elucidation.

Result: Extensive experiments demonstrate that IR-Agent enhances baseline performance on experimental IR spectra and exhibits strong adaptability to various forms of chemical information.

Conclusion: IR-Agent is a novel multi-agent framework for molecular structure elucidation from IR spectra that improves baseline performance and shows strong adaptability to various forms of chemical information.

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [9] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为Food Claim-Traceability Network（FCN）的方法，通过FKG.in知识图谱和Reddit数据以及大型语言模型实现食品声称及其可追溯性的建模。作者采用本体设计和半自动化知识整理工作流程，成功开发了FCN，并展示了其在印度食品知识图谱上的应用。该方法的应用不可知，适用于其他地理、烹饪或监管环境，旨在为食品知识生态系统的透明性和可靠性做出贡献。


<details>
  <summary>Details</summary>
Motivation: 全球食品领域充斥着关于食物的科学、文化和商业主张，包括对其属性、功效以及不应该或不能做的事情的各种主张。尽管这些主张对人们具有广泛影响，但追踪、验证和背景化这些主张的基础设施仍然零散和不完善。因此，作者的动机是提出一种基于知识图谱和语言模型的解决方案，以结构化、验证和可解释的方式建模食品声称及其可追溯性，从而支持更透明和负责任的食品知识生态系统。

Method: 作者提出了Food Claim-Traceability Network（FCN）的方法作为一种解决方案，使用FKG.in知识图谱和Reddit数据以及大型语言模型进行概念验证。他们采用本体设计和半自动化知识整理工作流程，结合结构化架构和溯源意识管道，实现食品相关声明的提取和验证。该方法是应用不可知的，并适用于其他地理、烹饪或监管环境。

Result: 通过本论文提出的方法，作者成功地开发了Food Claim-Traceability Network（FCN）作为印度食品知识图谱FKG.in的扩展，并利用Reddit数据和大型语言模型进行了概念验证。他们展示了本体设计和半自动化知识整理工作流程，实现了食品相关声明的提取和验证，并提出了应用不可知的方法适用于其他地理、烹饪或监管环境。

Conclusion: 在这篇论文中，作者提出了一种名为Food Claim-Traceability Network（FCN）的方法，作为印度食品知识图谱FKG.in的扩展。他们展示了本体设计和半自动化知识整理工作流程，利用Reddit数据和大型语言模型开发了FKG.in-FCN的概念验证。FCN整合了策划数据输入、结构化架构和具有溯源意识的食品相关声明抽取和验证流程。作者旨在通过以结构化、可验证和可解释的方式建模食品声明及其可追溯性，为更透明和负责任的食品知识生态系统做出贡献，支持研究人员、政策制定者以及日常消费者。

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [10] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: 该论文介绍了引入了 MM-Retinal-Reason 数据集和 OphthaReason 模型，通过 Uncertainty-Aware Dynamic Thinking（UADT）方法取得了领先的性能表现。提出的模型在基本和复杂推理任务上优于其他多模态大语言模型和医学领域的模型。


<details>
  <summary>Details</summary>
Motivation: 针对现有医学领域的多模态推理模型仅专注于基本推理的不足，提出了一种结合异质临床信息和多模态医学成像数据的推理过程，并希望通过 MM-Retinal-Reason 数据集以及 OphthaReason 模型来填补这一空白。

Method: 设计了 OphthaReason 模型，并介绍了 Uncertainty-Aware Dynamic Thinking（UADT）方法用于估计样本级不确定性和动态调节模型的探索深度。

Result: 通过实验结果，该模型在基本和复杂推理任务上的表现均优于通用 MLLMs、医学 MLLMs、RL-based 医学 MLLMs 和眼科 MLLMs 至少 24.92％、15.00％、21.20％ 和 17.66％。

Conclusion: 该论文介绍了 MM-Retinal-Reason 数据集和 OphthaReason 模型，通过 Uncertainty-Aware Dynamic Thinking（UADT）方法实现了对基本和复杂推理任务的灵活适应，取得了领先的性能表现。

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [11] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: This paper introduces the Preference Chain method to improve simulation of human behavior in transportation systems using Graph Retrieval-Augmented Generation and Large Language Models. Experiments show its superiority over standard LLM, with applications in urban mobility modeling and personalized travel behavior analysis.


<details>
  <summary>Details</summary>
Motivation: Collecting accurate behavioral data in newly developed areas is challenging. Existing generative agents struggle with generating consistent, context-sensitive, and realistic behavioral outputs. This paper aims to address these limitations by introducing a novel method for simulating human behavior in data-scarce environments.

Method: Introducing the Preference Chain method, which integrates Graph Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) to enhance context-aware simulation of human behavior in transportation systems.

Result: Experiments on the Replica dataset show that the Preference Chain method aligns better with real-world transportation mode choices compared to standard LLM. The Mobility Agent developed based on this method demonstrates potential applications in urban mobility modeling, personalized travel behavior analysis, and dynamic traffic forecasting.

Conclusion: Preference Chain method outperforms standard LLM in simulating human behavior in transportation systems, with potential applications in urban mobility modeling and personalized travel behavior analysis.

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [12] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: M2N2 is an evolutionary algorithm designed to enhance model merging by dynamically adjusting boundaries, preserving model diversity, and identifying promising model pairs for fusion. It evolves models from scratch, achieves comparable performance to CMA-ES with greater computational efficiency, and scales to merge specialized language and image generation models, maintaining crucial model capabilities. The code is available on GitHub at https://github.com/SakanaAI/natural_niches.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods require manually partitioning model parameters into fixed groups, limiting the exploration of potential combinations and overall performance. To overcome these limitations, M2N2 is introduced to enable the evolution of models entirely from scratch, improve computational efficiency, and merge specialized language and image generation models.

Method: The proposed method, Model Merging of Natural Niches (M2N2), is an evolutionary algorithm with three key features: dynamic adjustment of merging boundaries, diversity preservation mechanism, and a heuristic-based attraction metric. These features enable exploring a broader range of parameter combinations, maintaining a diverse population of high-performing models, and identifying promising model pairs for fusion.

Result: Experimental results demonstrate the effectiveness of M2N2 in evolving models from scratch, achieving performance comparable to CMA-ES but with higher computational efficiency. M2N2 is also successful in merging specialized language and image generation models, showcasing state-of-the-art performance and preserving essential model capabilities beyond those explicitly optimized by the fitness function.

Conclusion: Model Merging of Natural Niches (M2N2) is proposed as an evolutionary algorithm to address the limitations of existing model merging methods. It dynamically adjusts merging boundaries, preserves diversity in the model population, and uses a heuristic-based attraction metric to identify promising model pairs for fusion. Experimental results show that M2N2 can evolve models from scratch with performance comparable to CMA-ES, with greater computational efficiency. It also scales to merge specialized language and image generation models, achieving state-of-the-art performance and preserving crucial model capabilities.

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [13] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: 本研究旨在通过GROW-AI框架扩展人工智能评估，探讨机器是否可以“成长”。方法基于六个主要标准进行评估，通过特定的“游戏”探索人类维度和在人工智能领域的转移。结果显示，该方法允许对不同类型（机器人、软件代理、LLMs）的AI实体的“成长”水平进行一致且可比较的评估，突出了优势和脆弱领域，保证了评估的可追溯性和可复制性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是扩展对人工智能评估的框架，通过GROW-AI来探讨机器是否可以“成长”，并为了解决这一问题而设计。旨在将“成长”的概念从人类世界转移到人工智能领域，结合跨学科的视角进行整合测试，以测量AI实体的成熟水平和进化路径。

Method: 该研究方法基于GROW-AI框架，应用六个主要标准进行评估，通过特定的“游戏”探索人类维度和在人工智能领域的转移。实体的决策和行动在AI日志中记录，评估使用先前专家方法确定初始权重，并计算全球分数（成长指数）为六个分数的算术平均值。

Result: 结果表明，该方法允许对AI实体的“成长”水平进行一致且可比较的评估，无论其类型是机器人、软件代理还是LLMs。多游戏结构突出了优势和脆弱领域，使用统一日志保证了评估的可追溯性和可复制性。

Conclusion: 该研究拓展了评估人工智能的框架，称为GROW-AI（成长与实现自主智慧），旨在回答“机器可以成长吗？”这一自然延伸图灵测试的问题。方法应用基于六个主要标准（C1-C6）的系统，每个标准通过特定的“游戏”进行评估，分为探索人类维度及其在人工智能中的转移的四个竞技场。实体的所有决策和行动都记录在标准化的AI日志中，这是计算复合分数的主要来源。评估使用先前专家方法建立初始权重，全球分数——成长指数——计算为六个分数的算术平均值，并对成熟阈值进行解释。结果显示，该方法允许对AI实体的“成长”水平进行一致且可比较的评估，无论其类型是机器人、软件代理还是LLMs。多游戏结构突出了优势和脆弱领域，使用统一日志保证了评估的可追溯性和可复制性。研究的独创性在于将从人类世界到人工智能世界的“成长”过程概念转移，以结合心理学、机器人技术、计算机科学和伦理学的视角进行综合测试格式。通过这种方法，GROW-AI不仅衡量性能，还捕捉了AI实体向成熟发展的进化路径。

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [14] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope enhances tool-based interactions for building agentic applications by introducing foundational components, unified interfaces, and extensible modules. It grounds agent behaviors in the ReAct paradigm, integrates built-in agents, and offers robust engineering support. The system includes a scalable evaluation module and a runtime sandbox for safe execution, providing a practical foundation for scalable and effective agentic applications.


<details>
  <summary>Details</summary>
Motivation: Driven by the advancements of Large Language Models (LLMs) empowering agents to combine intrinsic knowledge with dynamic tool use. Aim to enhance agent capacity in addressing real-world tasks and improving human-agent and agent-agent interaction patterns. Provide developer-friendly experiences and enable the development of long-trajectory agentic applications in a manageable way.

Method: Introducing foundational components, unified interfaces, and extensible modules to facilitate leveraging new models and functionalities. Grounding agent behaviors in the ReAct paradigm and design systematic asynchronous processes for improved interactions and execution efficiency. Integrating built-in agents tailored for specific scenarios and providing robust engineering support. Offering a scalable evaluation module with visual studio interface and a runtime sandbox for safe agent execution and rapid deployment.

Result: AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications with major improvements in tool-based interactions, foundational components, advanced infrastructure, and safe execution environment.

Conclusion: AgentScope introduces major improvements in supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. It provides foundational components, unified interfaces, and extensible modules to leverage the latest progress. The ReAct paradigm grounds agent behaviors and advanced agent-level infrastructure enhances interaction patterns and execution efficiency. AgentScope includes built-in agents for practical scenarios, robust engineering support, scalable evaluation module, and a runtime sandbox for safe agent execution and rapid deployment.

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [15] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: IVA framework proposed to enhance VLAs' handling of false-premise instructions, improving detection accuracy and successful response rates in such scenarios.


<details>
  <summary>Details</summary>
Motivation: Investigating how VLAs can handle false-premise instructions in robotic tasks, aiming to improve the robustness of interpreting user intent in the presence of inaccurate requests.

Method: Proposed Instruct-Verify-and-Act (IVA) framework to address false-premise instructions in Vision-Language-Action (VLA) models, which includes detection of unexecutable instructions, language-based clarification, and grounding plausible alternatives. Constructed a large-scale instruction tuning setup and trained a VLA model with structured language prompts. Leveraged a contextually augmented, semi-synthetic dataset for robust detection and natural language correction.

Result: IVA framework significantly improves false premise detection accuracy and successful responses in false-premise scenarios.

Conclusion: IVA framework improves false premise detection accuracy by 97.56% over baselines and increases successful responses in false-premise scenarios by 50.78%.

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [16] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 本文提出了一种因果感知的深度学习框架，用于mmWave多输入多输出系统的波束对准。通过因果发现来优化波束选择过程，减少输入选择时间和波束扫描开销，以提高性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的波束对准方法往往忽略输入和输出之间的潜在因果关系，导致解释性有限、泛化能力差以及不必要的波束扫描开销。在6G及以后的时代，通信需快速、适应性强且对现实世界中的不确定性具有韧性，高效可靠的波束对准对于mmWave多输入多输出系统至关重要。

Method: 本文提出了一种因果感知的深度学习框架，通过引入因果发现学习贝叶斯图来捕获接收功率输入和最佳波束之间的依赖关系，然后指导因果特征选择以供深度学习分类器使用。

Result: 模拟结果显示，所提出的因果波束选择方法在减少输入选择时间和波束扫描开销的同时，能够与传统方法实现相当的性能。

Conclusion: 本文提出了一种因果感知的深度学习框架，将因果发现集成到波束管理流程中。通过引入一种新颖的两阶段因果波束选择算法，识别出波束预测的相关输入的最小集合，实现了与传统方法相当的性能，并将输入选择时间和波束扫描开销分别减少了94.4%和59.4%。

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [17] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: GLARE is a legal reasoning framework that dynamically acquires legal knowledge to enhance reasoning in Legal Judgment Prediction tasks. Experiments validate its effectiveness on real-world datasets, offering improved interpretability and practical applications.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) lack legal knowledge, leading to insufficient reasoning in Legal Judgment Prediction tasks. The paper aims to address this issue by developing GLARE to enhance reasoning depth and breadth in LJP.

Method: Introduce GLARE, an agentic legal reasoning framework that acquires key legal knowledge dynamically by invoking different modules for improved reasoning. Conduct experiments on real-world dataset to validate the method's effectiveness.

Result: The experiments verify the effectiveness of GLARE in improving reasoning in Legal Judgment Prediction tasks. The reasoning chain generated adds interpretability and opens possibilities for practical applications.

Conclusion: GLARE is an agentic legal reasoning framework that dynamically acquires legal knowledge to enhance reasoning in Legal Judgment Prediction (LJP) tasks. The experiments demonstrate the effectiveness of GLARE on real-world datasets, improving interpretability and offering practical applications.

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [18] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: 该论文介绍了一种名为MoDER的方法，通过训练多个文本专家并利用模块化框架存储它们，在推理时将这些专家组合从而改进Vision-Language Models的零样本分类能力。该方法在实验中表现出了良好的效果，提高了VLMs在未见类别上的分类性能。


<details>
  <summary>Details</summary>
Motivation: 在先前的持续学习方法中，主要集中于保留VLMs的零样本能力，本研究则旨在提高这种能力。通过MoDER方法，试图将零样本分类能力的保持转变为增强，以提高VLMs在未见类别的分类性能。

Method: 研究利用MoDular Embedding Recomposition（MoDER）方法，通过训练多个文本专家，并在推理时从存储的专家中检索来提高预训练的Vision-Language Models（VLMs）的零样本分类能力。

Result: 研究结果表明MoDER方法在两种零样本增量学习协议下展现出了有效性，并在14个数据集上取得了良好的分类性能。

Conclusion: 这篇论文提出了一种名为MoDular Embedding Recomposition（MoDER）的方法，该方法通过引入一个模块化框架，训练多个文本专家，每个专家专注于一个单独的已见类，并将它们存储在一个基础枢纽中。该方法在推理时，针对每个未见类别从枢纽中查询和组合检索的专家，以合成一个改进的原型，从而提高分类性能。研究表明这种方法在两种常用的零样本增量学习协议Class-IL和MTIL上表现出了有效性，涵盖了总共14个数据集。

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [19] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 该论文探讨了如何利用扩散模型进行神经符号学习，通过两阶段训练策略培养基本推理能力和系统学习逻辑约束，取得了在符号推理基准测试中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 神经网络学习复杂的逻辑约束和实现符号推理是一个关键挑战，需要将神经网络的输出分布引导到符号约束附近，此研究旨在填补这一间隙，利用扩散模型的强大架构进行神经符号学习。

Method: 采用扩散模型进行神经符号学习，通过两阶段训练策略，第一阶段培养基本推理能力，第二阶段着重于系统学习逻辑约束，利用马尔可夫决策过程制定扩散推理器，利用改进的近端策略优化算法进行创新微调，采用基于规则的奖励信号并灵活优化策略。

Result: 该方法在一些经典符号推理基准测试中取得了出色的准确性和逻辑一致性。

Conclusion: 该论文研究了如何让神经网络学习复杂的逻辑约束和实现符号推理，提出了一种基于扩散模型的管道，通过两阶段训练策略来实现神经符号学习，并在符号推理基准测试中取得了优异的准确性和逻辑一致性表现。

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>


### [20] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: 该论文描述了一个竞争对手发现组件在代理AI系统中的应用，使用LLM-based AI系统处理数据并引入新的评估基准和验证代理。作者成功提高了召回率，降低了误报率，并在生产中取得成功。


<details>
  <summary>Details</summary>
Motivation: 由于当前的LLM-based AI系统无法可靠地检索所有竞争性药物名称，并且针对此任务没有公认的公开基准，作者受到评估缺失的启发。他们希望解决这一问题，并改进竞争对手发现的效率。

Method: 作者使用LLM-based AI系统来处理多模式、非结构化的尽职调查备忘录数据，将其转换为结构化的评估语料库。他们还引入了一个验证竞争对手的LLM-as-a-judge代理来提高准确性，最大化精度和抑制虚假成果。

Result: 通过引入新的评估基准和验证竞争对手的代理，作者成功地解决了当前LLM-based AI系统的问题，提高了召回率并降低了误报率。在生产环境中，他们取得了显著的成功，并加快了竞争分析的处理速度。

Conclusion: 该论文描述了一个在代理AI系统中用于快速药物资产尽职调查的竞争对手发现组件，并进行了基准测试。他们使用LLM-based AI系统来解决当前存在的问题，该系统可靠地检索所有竞争性药物名称，并为此任务引入了一个新的评估基准。他们的竞争对手发现代理在基准测试中实现了83%的召回率，超过了OpenAI Deep Research的65%和Perplexity Labs的60%。系统已部署在企业用户中，并在生产中取得成功。在与生物技术风险投资基金的案例研究中，分析师的处理时间从2.5天降低到约3小时，提高了20倍。

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>
