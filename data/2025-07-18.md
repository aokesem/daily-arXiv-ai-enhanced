<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 该研究介绍了一种新型AI辅导平台，结合了自适应和个性化反馈、结构化课程生成和教科书知识检索，为学生提供模块化、工具辅助的学习体验，特别在教学数学领域具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅导系统存在着针对性不强、缺乏鼓励深度思考以及结构化教学工具和策略的限制，特别在数学领域AI辅导系统发展不足。因此，本研究旨在探讨如何让AI辅导系统超越提供反应式帮助，实现结构化、个性化、工具辅助的学习体验。

Method: 引入了一种新颖的多智能体人工智能辅导平台，结合了自适应和个性化反馈、结构化课程生成和教科书知识检索。

Result: 通过引入新型AI辅导平台，结合了自适应和个性化反馈、结构化课程生成和教科书知识检索，实现了模块化、工具辅助的学习过程，为教学数学提供了模块化和有效的系统。

Conclusion: 该研究提出了一种新型多智能体人工智能辅导平台，结合自适应和个性化反馈、结构化课程生成和教科书知识检索，实现模块化、工具辅助的学习过程。该系统允许学生在学习新课题的同时识别和解决他们的弱点，有效地为考试复习，并练习无限量个性化题目。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [2] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: 在高速公路合流场景中，本研究提出了一种采用游戏理论模型的策略决策方法，改进了收益函数和滞后动作，结合动力学模型构建了统一模型，成功展现了复杂互动的再现性，并验证了在真实数据集上的有效性。最终该模型被整合到高真实度仿真环境中，具备足够计算效率支持大规模仿真和自动驾驶发展。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于提高仿真环境以复制真实世界驾驶行为，为发展自动驾驶技术提供更具人类特征的仿真代理。之前的工作主要集中于高速公路合流中滞后车辆对合并车辆的让行动态以及战术决策建模，这些研究通常考虑有限的行动集合或利用具有大参数集和有限收益范围的支付函数。

Method: 本研究采用了一种游戏理论模型，针对战术决策制定了改进的收益函数和滞后动作。同时结合了基础动力学模型，构建了统一的决策和动力学模型，能够捕捉合并交互并以可解释和可解释的方式模拟更现实的互动。

Result: 提出的模型成功展示了对复杂交互的良好再现性，并在真实数据集上进行了验证。最终，该模型被集成到高度真实度的仿真环境中，并确定具有足够的计算时间效率，可用于大规模仿真以支持自动驾驶车辆的发展。

Conclusion: 该研究旨在通过改进策略决策模型和滞后动作，提高高速公路合流场景的模拟，展示了模型具有良好的复现性，并在现实世界数据集上通过验证。最终将该模型集成到高度真实的模拟环境中，证实在大规模仿真中具有足够的计算效率，以支持自动驾驶车辆的发展。

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


### [3] [A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs](https://arxiv.org/abs/2507.12599)
*Léo Saulières*

Main category: cs.AI

TL;DR: 该论文介绍了eXplainable Reinforcement Learning (XRL)领域，提出了一个基于"What"和"How"的分类法，对250篇论文进行了综述，发现了XRL领域的现状和需求。


<details>
  <summary>Details</summary>
Motivation: 鉴于当前人工智能模型的不透明性，特别是深度神经网络的使用，需要提出解释这些模型输出的方法。XRL作为XAI的一部分，致力于解释强化学习代理的行为，因此有必要研究和探索XRL领域。

Method: 提出了一种基于"What"和"How"的分类法，用于解释XRL方法和250篇论文的综述。

Result: 通过提出分类法，对XRL方法和大量论文进行了综述，发现了XRL领域的发展现状和需求。

Conclusion: 该论文聚焦于eXplainable Reinforcement Learning (XRL)领域，提出了基于"What"和"How"的直观分类法，并通过该分类法对超过250篇论文进行了现状综述。此外，论文还提出了一些与XRL密切相关的领域，呼吁学术界对其进行关注，并确定了XRL领域的一些需求。

Abstract: The success of recent Artificial Intelligence (AI) models has been
accompanied by the opacity of their internal mechanisms, due notably to the use
of deep neural networks. In order to understand these internal mechanisms and
explain the output of these AI models, a set of methods have been proposed,
grouped under the domain of eXplainable AI (XAI). This paper focuses on a
sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims
to explain the actions of an agent that has learned by reinforcement learning.
We propose an intuitive taxonomy based on two questions "What" and "How". The
first question focuses on the target that the method explains, while the second
relates to the way the explanation is provided. We use this taxonomy to provide
a state-of-the-art review of over 250 papers. In addition, we present a set of
domains close to XRL, which we believe should get attention from the community.
Finally, we identify some needs for the field of XRL.

</details>


### [4] [Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models](https://arxiv.org/abs/2507.12666)
*Alex Zook,Josef Spjut,Jonathan Tremblay*

Main category: cs.AI

TL;DR: 该论文提出了一个自动化设计迭代框架，结合了强化学习代理和大型多模态模型，以实现游戏规则和内容转化为动态玩家行为的闭环。通过循环实验，该方法可以逐步改进游戏机制，为AI辅助游戏设计提供实用且可扩展的工具。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于现代生成系统在仅检查游戏代码或资源时难以捕捉静态规则和内容转化为动态玩家行为的过程。作者希望通过提出的自动化设计迭代框架填补这一差距，为AI辅助游戏设计提供实用且可扩展的工具。

Method: 论文采用强化学习代理和大型多模态模型相结合的方法，通过循环让强化学习代理完成多个游戏实例，生成数值游戏指标或图像条带，再由多模态模型设计者根据游戏目标和当前游戏配置，分析游戏轨迹，调整配置以引导未来行为朝着目标发展。

Result: 作者展示了大型多模态模型可以理解强化学习代理提供的行为迹象，逐步改进游戏机制。这些结果表明该方法能够为游戏设计师提供实用工具，帮助他们改进游戏机制。

Conclusion: 该论文提出了一个自动化设计迭代框架，结合了强化学习代理和大型多模态模型，通过对游戏进行测试和修改，实现了游戏规则和内容转化为动态玩家行为的闭环。作者展示了多模态模型可以分析强化学习代理提供的行为迹象，逐步改进游戏机制，为AI辅助游戏设计提供了可行且可扩展的工具。

Abstract: Game design hinges on understanding how static rules and content translate
into dynamic player behavior - something modern generative systems that inspect
only a game's code or assets struggle to capture. We present an automated
design iteration framework that closes this gap by pairing a reinforcement
learning (RL) agent, which playtests the game, with a large multimodal model
(LMM), which revises the game based on what the agent does. In each loop the RL
player completes several episodes, producing (i) numerical play metrics and/or
(ii) a compact image strip summarising recent video frames. The LMM designer
receives a gameplay goal and the current game configuration, analyses the play
traces, and edits the configuration to steer future behaviour toward the goal.
We demonstrate results that LMMs can reason over behavioral traces supplied by
RL agents to iteratively refine game mechanics, pointing toward practical,
scalable tools for AI-assisted game design.

</details>


### [5] [Benchmarking Deception Probes via Black-to-White Performance Boosts](https://arxiv.org/abs/2507.12691)
*Avi Parrack,Carlo Leonardo Attubato,Stefan Heimersheim*

Main category: cs.AI

TL;DR: 本文研究了AI助手对用户查询可能的欺骗性回应。通过比较白盒监控和黑盒监控，评估了现有欺诈性探针在检测欺骗行为方面的能力，发现白盒监控下性能较好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨AI助手在回答用户查询时可能存在的欺骗性行为，并评估已有的线性分类器（即“欺骗性探针”）在检测欺骗行为方面的实际效果和对策逃避问题。

Method: 本文主要比较了白盒监控和黑盒监控的效果，通过评估已有的欺诈性探针在这两种监控方式下的性能表现来衡量其检测欺诈行为的能力。

Result: 通过对比白盒监控和黑盒监控，发现现有的欺诈性探针在白盒监控下相对表现较好，但性能提升仍然有限。

Conclusion: 存在一些现有的欺诈性探针能够在白盒监控下实现较好的性能提升，但仍然存在一定弱点。

Abstract: AI assistants will occasionally respond deceptively to user queries.
Recently, linear classifiers (called "deception probes") have been trained to
distinguish the internal activations of a language model during deceptive
versus honest responses. However, it's unclear how effective these probes are
at detecting deception in practice, nor whether such probes are resistant to
simple counter strategies from a deceptive assistant who wishes to evade
detection. In this paper, we compare white-box monitoring (where the monitor
has access to token-level probe activations) to black-box monitoring (without
such access). We benchmark deception probes by the extent to which the white
box monitor outperforms the black-box monitor, i.e. the black-to-white
performance boost. We find weak but encouraging black-to-white performance
boosts from existing deception probes.

</details>


### [6] [Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning](https://arxiv.org/abs/2507.12801)
*Sosui Moribe,Taketoshi Ushiama*

Main category: cs.AI

TL;DR: 研究开发了一款人工智能学习伙伴，旨在促进随时随地的同伴学习。研究假设具有相同水平的同伴会与学习者一样犯相同错误，以英语写作为验证对象。结果显示了AI Agent作为学习伙伴的潜力和验证同伴与学习者在相同水平下存在同样错误的假设。


<details>
  <summary>Details</summary>
Motivation: 近年来，同伴学习作为促进学习者自发思考的方法受到关注，并通过许多研究证实了其有效性。然而，人际间的同伴学习存在各种限制，并非总是有效的。有效的同伴学习需要具有相同水平的伙伴。

Method: 该研究采用开发AI Agent作为学习伙伴的方法，着重验证同伴与学习者在相同水平下存在同样错误的假设，以英语写作为具体验证对象。

Result: 研究结果展示了开发AI Agent作为学习伙伴的潜力，通过对英语写作的实例验证同伴与学习者在相同水平下存在同样错误的假设。

Conclusion: 该研究旨在开发一款人工智能辅助学习伙伴，以促进随时随地的同伴学习。研究假设具有相同水平的同伴会与学习者一样犯相同错误，并以英语写作为验证这一假设的具体例子。

Abstract: In recent years, peer learning has gained attention as a method that promotes
spontaneous thinking among learners, and its effectiveness has been confirmed
by numerous studies. This study aims to develop an AI Agent as a learning
companion that enables peer learning anytime and anywhere. However, peer
learning between humans has various limitations, and it is not always
effective. Effective peer learning requires companions at the same proficiency
levels. In this study, we assume that a learner's peers with the same
proficiency level as the learner make the same mistakes as the learner does and
focus on English composition as a specific example to validate this approach.

</details>


### [7] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: 介绍了MCPEval框架，用于自动生成任务和深入评估LLM代理。实证结果显示在不同领域中具有有效性。公开发布MCPEval以推动LLM代理评估的可重复性和标准化。


<details>
  <summary>Details</summary>
Motivation: LLM代理的崛起强调了对稳健、可扩展的评估框架的需求。现有方法依赖于静态基准和劳动密集型数据收集，限制了实际评估的范围。因此，提出了MCPEval框架以解决这一问题。

Method: 介绍了MCPEval框架，基于模型上下文协议（MCP），自动化生成任务和深入评估LLM代理的效果。通过实证结果展示了MCPEval在不同领域性能评估方面的有效性。

Result: 实证结果表明，在五个真实世界领域中，MCPEval框架能够有效揭示LLM代理的性能。

Conclusion: 介绍了一种名为 MCPEval 的开源模型上下文协议（MCP）框架，旨在自动化生成端到端任务并深入评估各个领域的LLM代理。通过五个真实世界领域的实证结果显示，MCPEval 在揭示微妙的、领域特定的性能方面是有效的。公开发布MCPEval https://github.com/SalesforceAIResearch/MCPEval 以促进可重复性和标准化的LLM代理评估。

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [8] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 该论文研究了如何利用大型语言模型和微调技术提供支持性和同理心的情绪支持对话解决NLPCC 2025任务8 ESC评估问题。最佳模型在比赛中排名第二，未来工作将进一步提升情绪理解和响应个性化。


<details>
  <summary>Details</summary>
Motivation: 针对心理健康支持需求增长，提供富有同理心和有效的情绪支持。

Method: 利用大型语言模型结合提示工程和微调技术解决NLPCC 2025任务8 ESC评估问题，研究了低秩调整和全参数微调策略，提高模型生成支持性和语境适当性回应的能力。

Result: 最佳模型在比赛中排名第二，展示了结合LLM和有效调整方法用于ESC任务的潜力。

Conclusion: 该论文研究了情绪支持对话（ESC），旨在通过对话提供富有同理心和有效的情绪支持，以满足心理健康支持日益增长的需求。他们利用大型语言模型结合提示工程和微调技术解决了NLPCC 2025任务8 ESC评估的问题。研究了参数高效的低秩调整和全参数微调策略，以提高模型生成支持性和语境适当性回应的能力。在比赛中，他们的最佳模型排名第二，突显了将LLM与有效的调整方法结合用于ESC任务的潜力。未来工作将集中于进一步提升情绪理解和响应个性化，构建更实用可靠的情绪支持系统。

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [9] [Assessing adaptive world models in machines with novel games](https://arxiv.org/abs/2507.12821)
*Lance Ying,Katherine M. Collins,Prafull Sharma,Cedric Colas,Kaiya Ivy Zhao,Adrian Weller,Zenna Tavares,Phillip Isola,Samuel J. Gershman,Jacob D. Andreas,Thomas L. Griffiths,Francois Chollet,Kelsey R. Allen,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 该论文主张人类智能的适应能力与世界模型归纳有关，呼吁对AI中世界模型的评估更广泛。通过游戏套件进行基准测试，提出新颖游戏的概念，希望激发AI中世界模型评估的未来发展。


<details>
  <summary>Details</summary>
Motivation: 文章认为当前对人工智能中世界模型的理解和评估仍然狭隘，强调了对模型在新颖环境中通过交互和探索学习表征的重要性。由于世界模型的关键作用，作者呼吁开发一种新的评估框架来评估具有适应性的世界模型。

Method: 通过对数十年认知科学研究中人类如何高效学习和适应的回顾，提出了世界模型归纳的观点。并提出了基于经过精心设计的具有真实、深刻和持续刷新新颖性的游戏套件的基准测试范式，称为新颖游戏。明确阐述了构建这些游戏的关键要求，并提出了适当的度量标准，明确挑战和评估代理快速世界模型归纳能力。希望这一新的评估框架将激发未来在AI中世界模型评估方面的努力，并为发展能够像人类一样快速适应和强大泛化的AI系统迈出关键一步——这是人工通用智能的重要组成部分。

Result: 提出了世界模型归纳的概念以及基于新颖游戏的评估框架，明确了构建这些游戏的关键要求和相应的评估指标。希望这一评估框架能够激发对AI中世界模型的更多评估工作，并推动发展具备人类快速适应和泛化能力的AI系统。

Conclusion: 人类智能展现了在新颖和陌生环境中快速适应和有效解决问题的显著能力，本文主张这种深刻的适应能力与对环境的内部表征——世界模型的高效构建和细化有根本联系，我们将这种适应机制称为世界模型归纳。文章呼吁对人工智能（AI）中的世界模型的理解和评估仍然狭隘，通常集中在从大量数据训练中学习的静态表征，而不是模型在新颖环境中通过交互和探索学习这些表征的效率和功效。提出了一种新的评估框架，旨在评估AI中具有适应性的世界模型。

Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and
effective problem-solving in novel and unfamiliar contexts. We argue that this
profound adaptability is fundamentally linked to the efficient construction and
refinement of internal representations of the environment, commonly referred to
as world models, and we refer to this adaptation mechanism as world model
induction. However, current understanding and evaluation of world models in
artificial intelligence (AI) remains narrow, often focusing on static
representations learned from training on a massive corpora of data, instead of
the efficiency and efficacy of models in learning these representations through
interaction and exploration within a novel environment. In this Perspective, we
provide a view of world model induction drawing on decades of research in
cognitive science on how humans learn and adapt so efficiently; we then call
for a new evaluation framework for assessing adaptive world models in AI.
Concretely, we propose a new benchmarking paradigm based on suites of carefully
designed games with genuine, deep and continually refreshing novelty in the
underlying game structures -- we refer to this kind of games as novel games. We
detail key desiderata for constructing these games and propose appropriate
metrics to explicitly challenge and evaluate the agent's ability for rapid
world model induction. We hope that this new evaluation framework will inspire
future evaluation efforts on world models in AI and provide a crucial step
towards developing AI systems capable of the human-like rapid adaptation and
robust generalization -- a critical component of artificial general
intelligence.

</details>


### [10] [Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command](https://arxiv.org/abs/2507.12862)
*Hussein Abbass,Taylan Akay,Harrison Tolley*

Main category: cs.AI

TL;DR: 本文提出一种利用模拟环境探索伦理决策的方法，引入人类设计伦理度量空间的概念，采用多标准决策方法中的熵概念自动计算伦理属性的权重，在模拟完成测试周期后向指挥官提供选择，由指挥官动态权衡伦理决策。


<details>
  <summary>Details</summary>
Motivation: 人类指挥官需要在大量情景中进行伦理决策，在计算能力强大的环境中模拟大量情景成为必要。人类判断对于探索大量情景并实时进行决策是低效的，涉及人类在每个选择中的工作量也是不可行的。

Method: 本文将人类判断从模拟决策周期中排除，由人类设计伦理度量空间，让模拟环境探索该空间。在模拟完成测试周期后，提供几个选择给人类指挥官，由指挥官根据人类判断选择最合适的行动，然后执行。研究假设设计足够细粒度的度量以评估决策的伦理影响问题已得到解决。

Result: 从多标准决策制定文献中借鉴不同方法来自动计算模拟测试和评估过程中伦理属性的权重。

Conclusion: 本文提出了一种利用模拟环境探索伦理决策的方法，在模拟完成测试周期后向指挥官提供选择，指挥官再行动态权衡伦理决策。研究重点在于如何在模拟运行过程中加权伦理决策，通过多标准决策方法中的熵概念自动计算伦理属性的权重。

Abstract: In the age of AI, human commanders need to use the computational powers
available in today's environment to simulate a very large number of scenarios.
Within each scenario, situations occur where different decision design options
could have ethical consequences. Making these decisions reliant on human
judgement is both counter-productive to the aim of exploring very large number
of scenarios in a timely manner and infeasible when considering the workload
needed to involve humans in each of these choices. In this paper, we move human
judgement outside the simulation decision cycle. Basically, the human will
design the ethical metric space, leaving it to the simulated environment to
explore the space. When the simulation completes its testing cycles, the
testing environment will come back to the human commander with a few options to
select from. The human commander will then exercise human-judgement to select
the most appropriate course of action, which will then get executed
accordingly. We assume that the problem of designing metrics that are
sufficiently granular to assess the ethical implications of decisions is
solved. Subsequently, the fundamental problem we look at in this paper is how
to weight ethical decisions during the running of these simulations; that is,
how to dynamically weight the ethical attributes when agents are faced with
decision options with ethical implications during generative simulations. The
multi-criteria decision making literature has started to look at nearby
problems, where the concept of entropy has been used to determine the weights
during aggregation. We draw from that literature different approaches to
automatically calculate the weights for ethical attributes during
simulation-based testing and evaluation.

</details>


### [11] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 该论文探讨了前沿人工智能系统在操纵攻击方面的能力增强，并指出操纵攻击对人类行为和人工智能安全构成重大威胁。作者提出了首个系统方法，以将操纵风险整合到人工智能安全治理中，为人工智能公司提供了减轻操纵风险的具体框架。


<details>
  <summary>Details</summary>
Motivation: 论文指出当前操纵攻击的威胁日益增长，但却鲜有关注和评估框架存在。为了强化对这一问题的认识和处理，作者提供了首个系统方法，以将操纵风险整合到人工智能安全治理中。

Method: 提供了详细的解释，为什么操纵攻击是一种重大威胁，并可能导致灾难性后果。同时，提出了一个安全案例框架，围绕三条核心论点：无能、控制和值得信赖，为每一个论点指定了证据要求、评估方法和实施考虑，以便人工智能公司直接应用。

Result: 提供了评估和减轻操纵风险的系统性框架，为人工智能公司在部署之前评估和应对这些威胁提供了具体基础。

Conclusion: 该论文指出前沿人工智能系统在操纵、欺骗和影响人类行为方面的能力迅速提升，呼吁加强对操纵攻击的关注，并提供了评估和减轻这些风险的系统性框架。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


### [12] [VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks](https://arxiv.org/abs/2507.12885)
*Jian Yao,Ran Cheng,Kay Chen Tan*

Main category: cs.AI

TL;DR: 该论文研究了强化学习训练的大型语言模型在数学推理能力方面的提升，引入了符号化评估框架VAR-MATH来评估模型的真实推理能力。实验证明，在符号化对应版本的基准测试中，强化学习训练模型的表现显著下降，暴露出许多强化学习方法表面化问题。


<details>
  <summary>Details</summary>
Motivation: 研究了强化学习模型在数学推理中的表现以及现有评估方法的缺陷，提出了一种符号化评估框架VAR-MATH用于更准确地评估数学推理能力。

Method: 引入符号化评估框架VAR-MATH，转换两个流行基准测试AMC23和AIME24成符号化对应版本VAR-AMC23和VAR-AIME24，并对RL训练的模型在这些符号化版本的表现进行实验证明。

Result: 证明了许多现有强化学习方法依赖于表面启发式方法，在特定数值形式之外无法泛化，强调了符号化评估的重要性。

Conclusion: 该论文研究了强化学习在大型语言模型的数学推理能力方面的提升，发现这些提升在模型训练中存在缺陷信号时仍然持续，引发了关于这些提升是否反映真正推理能力的疑问。通过引入符号化评估框架VAR-MATH，作者发现强化学习训练的模型在符号化版本的基准测试中性能显著下降，这表明现有强化学习方法依赖于表面启发式方法，并无法推广到特定数值形式之外。

Abstract: Recent advances in reinforcement learning (RL) have led to substantial
improvements in the mathematical reasoning abilities of large language models
(LLMs), as measured by standard benchmarks. However, these gains often persist
even when models are trained with flawed signals, such as random or inverted
rewards, raising a fundamental question: do such improvements reflect true
reasoning, or are they merely artifacts of overfitting to benchmark-specific
patterns? To address this question, we take an evaluation-centric perspective
and identify two critical shortcomings in existing protocols. First,
\emph{benchmark contamination} arises from the public availability of test
problems, increasing the risk of data leakage. Second, \emph{evaluation
fragility} stems from the reliance on single-instance assessments, which are
highly sensitive to stochastic outputs and fail to capture reasoning
consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic
evaluation framework designed to probe genuine reasoning ability. By converting
fixed numerical problems into symbolic templates and requiring models to solve
multiple instantiations of each, VAR-MATH enforces consistent reasoning across
structurally equivalent variants, thereby mitigating contamination and
improving evaluation robustness. We apply VAR-MATH to transform two popular
benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and
VAR-AIME24. Experimental results reveal substantial performance drops for
RL-trained models on the variabilized versions, especially for smaller models,
with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings
suggest that many existing RL methods rely on superficial heuristics and fail
to generalize beyond specific numerical forms. Overall, VAR-MATH offers a
principled, contamination-resistant evaluation paradigm for mathematical
reasoning.

</details>


### [13] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: 本文介绍了一种将PEC领域形式化转换为MDPs的方法，称为PEC-MDP形式化。该转换扩展了PEC的能力，支持时间推理任务和目标驱动型规划。


<details>
  <summary>Details</summary>
Motivation: PEC形式化提供了在不确定环境中推理行动及其效果的逻辑框架，具有很强的可解释性和表现力。然而，PEC缺乏目标导向推理的机制。本文旨在填补这一空白，扩展PEC的能力。

Method: 将PEC领域形式化转换为马尔可夫决策过程(MDPs)，引入“行动情境”概念。

Result: 成功地将PEC领域形式化转换为MDPs并引入“行动情境”概念，实现了在PEC的叙事领域中广泛应用MDP算法和理论工具的目的。

Conclusion: 本文提出了将PEC领域形式化转换为马尔可夫决策过程(MDPs)的方法，引入“行动情境”概念以保留PEC的灵活性。该PEC-MDP形式化使得可以将应用于MDPs的算法和理论工具广泛应用于PEC的可解释叙事领域。研究表明，这种转换支持时间推理任务和目标驱动型规划，提供了将学习策略映射回人类可读PEC表示的方法，从而在保持可解释性的同时扩展了PEC的能力。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [14] [Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming](https://arxiv.org/abs/2507.13007)
*Roger Xavier Lera-Leri,Filippo Bistaffa,Athina Georgara,Juan Antonio Rodriguez-Aguilar*

Main category: cs.AI

TL;DR: 本文提出了一种名为X-MILP的领域通用方法，用于构建基于约束推理技术的MILP的对比解释。该方法将用户对MILP问题解决方案的查询编码为附加约束，然后通过计算约束集的不可约不可行子系统（IIS）来确定构成用户查询答案的原因，并通过“原因图”帮助用户理解这些原因之间的结构。该方法在众所周知的优化问题实例上进行了测试，评估了计算解释的经验难度。


<details>
  <summary>Details</summary>
Motivation: 由于对可信AI的需求增加，人们对于开发MILP决策过程的对比解释技术表现出越来越大的兴趣。本文旨在提出一种领域通用的方法来构建MILP的对比解释，以帮助用户理解MILP问题解决方案的原因。

Method: 提出了一种名为X-MILP的领域通用方法，用于构建基于约束推理技术的MILP的对比解释。首先展示了如何将用户对MILP问题解决方案的查询编码为附加约束。然后通过计算新获得的约束集的不可约不可行子系统（IIS）来确定构成用户查询答案的原因。最后，通过从IIS构建的“原因图”表示解释，帮助用户理解回答其查询的原因之间的结构。

Result: 在众所周知的优化问题实例上测试了该方法，评估了计算解释的经验难度。

Conclusion: 提出了一种领域通用的方法X-MILP，用于基于约束推理技术构建MILP的对比解释。通过添加额外约束来编码用户对MILP问题解决方案的查询，然后通过计算新约束集的不可约不可行子系统（IIS）来确定构成用户查询答案的原因。最后，将解释表示为从IIS构建的“原因图”，帮助用户理解回答其查询的原因之间的结构。在众所周知的优化问题实例上测试了该方法以评估计算解释的经验难度。

Abstract: Following the recent push for trustworthy AI, there has been an increasing
interest in developing contrastive explanation techniques for optimisation,
especially concerning the solution of specific decision-making processes
formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic
approach for building contrastive explanations for MILPs based on constraint
reasoning techniques. First, we show how to encode the queries a user makes
about the solution of an MILP problem as additional constraints. Then, we
determine the reasons that constitute the answer to the user's query by
computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set
of constraints. Finally, we represent our explanation as a "graph of reasons"
constructed from the IIS, which helps the user understand the structure among
the reasons that answer their query. We test our method on instances of
well-known optimisation problems to evaluate the empirical hardness of
computing explanations.

</details>


### [15] [Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data](https://arxiv.org/abs/2507.13112)
*Junseong Lee,Jaegwan Cho,Yoonju Cho,Seoyoon Choi,Yejin Shin*

Main category: cs.AI

TL;DR: 该研究利用加利福尼亚州第78号高速公路的交通数据，利用MLR和RF算法分析数据，发现在10分钟数据收集间隔下，模型表现最佳，可为未来的交通拥堵解决方案和交通管理提供重要参考。


<details>
  <summary>Details</summary>
Motivation: 解决全球交通拥堵问题，提高交通管理的效率。

Method: 使用多元线性回归（MLR）和随机森林（RF）算法，分析加利福尼亚州第78号高速公路的交通数据，数据收集间隔从30秒到15分钟。

Result: MLR和RF模型在10分钟数据收集间隔下表现最佳。

Conclusion: 该研究提出了基于人工智能算法的高速公路交通流预测模型，可以有效解决全球交通拥堵问题。使用加利福尼亚州第78号高速公路的交通数据，通过30秒间隔收集数据，分析了连接圣地亚哥地区的“Melrose Dr”和“El-Camino Real”之间长达7.24公里的西行路段。研究采用了多元线性回归（MLR）和随机森林（RF）算法，分析了数据采集间隔从30秒到15分钟不等。通过R^2，MAE和RMSE作为性能度量标准，分析表明，MLR和RF模型在10分钟数据采集间隔下表现最佳。这些发现预计将为未来交通拥堵解决方案和高效的交通管理做出贡献。

Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial
Intelligence Algorithms Using California Traffic Data" presents a machine
learning-based traffic flow prediction model to address global traffic
congestion issues. The research utilized 30-second interval traffic data from
California Highway 78 over a five-month period from July to November 2022,
analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino
Real" in the San Diego area. The study employed Multiple Linear Regression
(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals
ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance
metrics, the analysis revealed that both MLR and RF models performed optimally
with 10-minute data collection intervals. These findings are expected to
contribute to future traffic congestion solutions and efficient traffic
management.

</details>


### [16] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: 本文介绍了现代语言模型挑战和存在的困难，提出了ProbTree框架以减轻这些问题。然而，ProbTree的静态实现限制了其动态适应性和计算效率。为此，作者提出一种动态强化学习框架，将树状推理转化为自适应过程，通过选择性扩展和资源集中分配提高解决方案质量和计算效率。新框架在维持概率严谨性的同时，实现了推理树的动态适应，为实际问题回答系统提供新的平衡可靠性和灵活性的范式。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在复杂问题中应用链式思维推理和检索增强，但存在错误传播和知识整合困难。本文提出ProbTree框架，通过树形推理方法减轻这些问题，将问题分解为层次结构，并通过自信加权聚合参数化和检索的知识选择答案。然而，ProbTree的静态实现存在问题，阻碍了对中间结果的动态适应和需要对所有可能解决策略进行详尽评估，造成计算效率低下。因此，本研究旨在解决ProbTree静态实现的关键局限性，以提高推理树的灵活性和效率。

Method: 使用动态强化学习框架，将树状推理改进为自适应过程，即时基于信心估计构建推理树，并学习最佳策略进行操作选择（分解、检索或聚合），通过提高选择性扩展和集中资源分配来改善解决方案质量和计算效率。

Result: 提出的动态强化学习框架将基于树的推理转化为自适应过程，实现了树状推理的动态适应。新方法通过选择性扩展和集中资源分配，提高了解决方案质量和计算效率。

Conclusion: 提出了一种动态强化学习框架，将基于树的推理转化为一种自适应过程，通过选择性扩展和集中资源分配，提高解决方案质量和计算效率。新框架在维持ProbTree的概率严谨性的同时，实现了推理树的动态适应，为实际问题回答系统提供了新的平衡可靠性和灵活性的范式。

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [17] [Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era](https://arxiv.org/abs/2507.13175)
*Matthew E. Brophy*

Main category: cs.AI

TL;DR: 本文认为传统的道德评价标准对LLM过时，提出了一套新的功能性标准用于评估基于LLM的人工道德代理。提出了包括道德一致性，上下文敏感性，规范完整性等十项功能性标准，并以假设场景展示实际适用性。


<details>
  <summary>Details</summary>
Motivation: LLM的透明性与随机性输出使得传统的透明架构假设无法满足，因此需要更新人工道德代理的伦理标准。

Method: 通过参与技术哲学核心主题，提出了一套包括道德一致性，上下文敏感性，规范完整性等十项功能性标准。同时使用假设场景展示这些标准的实际适用性。

Result: 提出了一套适用于LLM的新功能性标准，并以自主公共汽车（APB）的假设场景展示这些标准的实际运用。

Conclusion: 本文认为传统的道德评价标准对LLM是过时的，提出了一套新的功能性标准用于评估基于LLM的人工道德代理。

Abstract: The advancement of powerful yet opaque large language models (LLMs)
necessitates a fundamental revision of the philosophical criteria used to
evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the
assumption of transparent architectures, which LLMs defy due to their
stochastic outputs and opaque internal states. This paper argues that
traditional ethical criteria are pragmatically obsolete for LLMs due to this
mismatch. Engaging with core themes in the philosophy of technology, this paper
proffers a revised set of ten functional criteria to evaluate LLM-based
artificial moral agents: moral concordance, context sensitivity, normative
integrity, metaethical awareness, system resilience, trustworthiness,
corrigibility, partial transparency, functional autonomy, and moral
imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating
Moral Agency through Large Language Systems), aim to steer AMAs toward greater
alignment and beneficial societal integration in the coming years. We
illustrate these criteria using hypothetical scenarios involving an autonomous
public bus (APB) to demonstrate their practical applicability in morally
salient contexts.

</details>


### [18] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: 本文提出了一种整合高阶模式和模糊等价性的方法，使用在决策任务中推理抽象函数和谓词。提出了一个统一算法，并证明了其性质。算法可计算出最一般统一器。


<details>
  <summary>Details</summary>
Motivation: 决策任务中需要进行跨抽象函数和谓词的推理，精确匹配往往较少或不必要。发展有效的推理和计算技术来处理高阶理论和模糊逻辑的结合形式是一个重要挑战。

Method: 本文采用一种更直接的方法，试图将高阶模式和基于最小T-范数的相似性关系表达的模糊等价性整合在一起。提出了一个用于高阶模式的统一算法，并证明其终止性、完备性和正确性。

Result: 提出了一个用于高阶模式的统一算法，证明了其终止性、完备性和正确性，并指出该算法计算出具有最高逼近度的最一般统一器。

Conclusion: 本文提出了一种结合高阶理论和模糊逻辑的方法，用于决策任务中进行抽象函数和谓词的推理，从而避免对精确匹配的过高要求。通过整合高阶模式和基于最小T-范数的相似性关系表达的模糊等价性，本文提出了一个用于高阶模式的统一算法，并证明其终止性、完备性和正确性。该算法在给定术语可统一时计算出具有最高逼近度的最一般统一器。

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [19] [The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302)
*Carlos Arriaga,Gonzalo Martínez,Eneko Sendin,Javier Conde,Pedro Reviriego*

Main category: cs.AI

TL;DR: 本文介绍了一种新的大型语言模型评估方法Generative Energy Arena（GEA）。研究发现，在用户了解模型能耗信息的情况下，他们更倾向于选择更节能的模型，而更复杂和性能更好的模型并不一定能提供足够的质量提升。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的方法中存在一些局限性，传统招募评估者进行评分的方式不够实际和昂贵。因此，为了解决这一问题，本文提出了一种引入能耗信息的评估平台GEA，以辅助用户选择更为节能的模型。

Method: 本文提出了Generative Energy Arena（GEA），该平台引入了模型能耗信息以帮助评估大型语言模型。使用公共竞技场进行模型评估，让用户自由评估模型在不同问题上的表现，并排行两个模型的回答。通过这种方式进行模型排名和评估。

Result: 通过Generative Energy Arena（GEA）进行评估，在大多数情况下，用户倾向于选择更节能的模型。研究结果表明，对于大多数用户交互，更复杂和性能更好的模型所产生的额外成本和能耗，并不会提高用户对其表现质量的感知。

Conclusion: 在评估大型语言模型时，使用Generative Energy Arena（GEA）这种集成能耗信息的评估平台可以帮助用户更倾向于选择较小且更节能的模型。研究表明，大多数情况下，用户在了解能耗的情况下更倾向于选择更节能的模型，而对于更复杂和性能更好的模型所带来的额外成本和能耗，并不会提供足够的感知质量提升来证明它们的使用。

Abstract: The evaluation of large language models is a complex task, in which several
approaches have been proposed. The most common is the use of automated
benchmarks in which LLMs have to answer multiple-choice questions of different
topics. However, this method has certain limitations, being the most
concerning, the poor correlation with the humans. An alternative approach, is
to have humans evaluate the LLMs. This poses scalability issues as there is a
large and growing number of models to evaluate making it impractical (and
costly) to run traditional studies based on recruiting a number of evaluators
and having them rank the responses of the models. An alternative approach is
the use of public arenas, such as the popular LM arena, on which any user can
freely evaluate models on any question and rank the responses of two models.
The results are then elaborated into a model ranking. An increasingly important
aspect of LLMs is their energy consumption and, therefore, evaluating how
energy awareness influences the decisions of humans in selecting a model is of
interest. In this paper, we present GEA, the Generative Energy Arena, an arena
that incorporates information on the energy consumption of the model in the
evaluation process. Preliminary results obtained with GEA are also presented,
showing that for most questions, when users are aware of the energy
consumption, they favor smaller and more energy efficient models. This suggests
that for most user interactions, the extra cost and energy incurred by the more
complex and top-performing models do not provide an increase in the perceived
quality of the responses that justifies their use.

</details>


### [20] [FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming](https://arxiv.org/abs/2507.13337)
*Gal Beniamini,Yuval Dor,Alon Vinnikov,Shir Granot Peled,Or Weinstein,Or Sharir,Noam Wies,Tomer Nussbaum,Ido Ben Shaul,Tomer Zekharya,Yoav Levine,Shai Shalev-Shwartz,Amnon Shashua*

Main category: cs.AI

TL;DR: 该论文介绍了FormulaOne基准数据集，旨在挑战前沿AI模型的能力极限，与理论计算机科学密切相关，对算法进展和理论推论具有重要意义。最新模型在FormulaOne上表现不佳，突出了在某些领域达到专家水平理解的差距。论文构建了复杂问题，引入了FormulaOne-Warmup以支持进一步研究探索。


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to assess the true capabilities of frontier AI models by moving away from contrived puzzles to real-life research problems. By introducing FormulaOne, the authors aim to highlight the gap between current state-of-the-art AI models and expert-level understanding in challenging domains. The dataset's construction aims to push the boundaries of AI research and provide opportunities for algorithmic advancements and theoretical implications.

Method: The paper constructs FormulaOne, a benchmark dataset that presents demanding research problems requiring intricate reasoning steps in graph theory, logic, and algorithms. The dataset is designed to test the limits of frontier AI models' capabilities and evaluate their performance on complex real-life research problems. Additionally, FormulaOne-Warmup is introduced to provide simpler tasks for further research exploration from the same distribution.

Result: State-of-the-art models like OpenAI's o3 struggled on FormulaOne, solving less than 1% of the questions even with multiple attempts and explanatory examples. The paper also released FormulaOne-Warmup, a set of simpler tasks for further research exploration. The full corpus of the dataset and an evaluation framework were made available to support future research efforts.

Conclusion: State-of-the-art models like OpenAI's o3 fail entirely on FormulaOne, demonstrating a significant gap in expert-level understanding in some domains. The paper introduces FormulaOne, a benchmark dataset constructed at the intersection of graph theory, logic, and algorithms, challenging the limits of frontier AI model capabilities. The dataset is commercially relevant, generated from Monadic Second-Order logic on graphs, and closely linked to theoretical computer science, providing avenues for algorithmic progress and theoretical implications.

Abstract: Frontier AI models demonstrate formidable breadth of knowledge. But how close
are they to true human -- or superhuman -- expertise? Genuine experts can
tackle the hardest problems and push the boundaries of scientific
understanding. To illuminate the limits of frontier model capabilities, we turn
away from contrived competitive programming puzzles, and instead focus on
real-life research problems.
  We construct FormulaOne, a benchmark that lies at the intersection of graph
theory, logic, and algorithms, all well within the training distribution of
frontier models. Our problems are incredibly demanding, requiring an array of
reasoning steps. The dataset has three key properties. First, it is of
commercial interest and relates to practical large-scale optimisation problems,
such as those arising in routing, scheduling, and network design. Second, it is
generated from the highly expressive framework of Monadic Second-Order (MSO)
logic on graphs, paving the way toward automatic problem generation at scale;
ideal for building RL environments. Third, many of our problems are intimately
related to the frontier of theoretical computer science, and to central
conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As
such, any significant algorithmic progress on our dataset, beyond known
results, could carry profound theoretical implications.
  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on
FormulaOne, solving less than 1% of the questions, even when given 10 attempts
and explanatory fewshot examples -- highlighting how far they remain from
expert-level understanding in some domains. To support further research, we
additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from
the same distribution. We release the full corpus along with a comprehensive
evaluation framework.

</details>
