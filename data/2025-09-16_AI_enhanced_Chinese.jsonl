{"id": "2509.10541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10541", "abs": "https://arxiv.org/abs/2509.10541", "authors": ["V. Benes", "M. Svitek", "A. Michalikova", "M. Melicherik"], "title": "Situation Model of the Transport, Transport Emissions and Meteorological Conditions", "comment": null, "summary": "Air pollution in cities and the possibilities of reducing this pollution\nrepresents one of the most important factors that today's society has to deal\nwith. This paper focuses on a systemic approach to traffic emissions with their\nrelation to meteorological conditions, analyzing the effect of weather on the\nquantity and dispersion of traffic emissions in a city. Using fuzzy inference\nsystems (FIS) the model for prediction of changes in emissions depending on\nvarious conditions is developed. The proposed model is based on traffic,\nmeteorology and emission data measured in Prague, Czech Republic. The main\nobjective of the work is to provide insight into how urban planners and\npolicymakers can plan and manage urban transport more effectively with\nenvironmental protection in mind.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ee5\u5e03\u62c9\u683c\uff08\u6377\u514b\u5171\u548c\u56fd\uff09\u7684\u4ea4\u901a\u3001\u6c14\u8c61\u548c\u6392\u653e\u6570\u636e\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u5f00\u53d1\u4e86\u6a21\u578b\uff0c\u65e8\u5728\u9884\u6d4b\u548c\u7ba1\u7406\u57ce\u5e02\u4ea4\u901a\u7684\u6392\u653e\u53d8\u5316\uff0c\u4ee5\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u73af\u5883\u4fdd\u62a4\u65b9\u6848\u3002", "motivation": "\u76ee\u524d\u793e\u4f1a\u9762\u4e34\u7684\u57ce\u5e02\u7a7a\u6c14\u6c61\u67d3\u548c\u51cf\u5c11\u6c61\u67d3\u7684\u53ef\u80fd\u6027\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002\u8be5\u8bba\u6587\u81f4\u529b\u4e8e\u63a2\u8ba8\u4ea4\u901a\u6392\u653e\u4e0e\u6c14\u8c61\u6761\u4ef6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u89e3\u6790\u6c14\u5019\u5bf9\u57ce\u5e02\u4ea4\u901a\u6392\u653e\u6570\u91cf\u548c\u6269\u6563\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\uff08FIS\uff09\uff0c\u57fa\u4e8e\u5e03\u62c9\u683c\uff08\u6377\u514b\u5171\u548c\u56fd\uff09\u7684\u4ea4\u901a\u3001\u6c14\u8c61\u548c\u6392\u653e\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u9884\u6d4b\u6392\u653e\u53d8\u5316\u7684\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eFIS\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u9884\u6d4b\u4e0d\u540c\u6761\u4ef6\u4e0b\u6392\u653e\u53d8\u5316\uff0c\u5e76\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "\u8be5\u8bba\u6587\u7740\u91cd\u4e8e\u901a\u8fc7\u7cfb\u7edf\u6027\u65b9\u6cd5\u7814\u7a76\u4ea4\u901a\u6392\u653e\u4e0e\u6c14\u8c61\u6761\u4ef6\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u57ce\u5e02\u4ea4\u901a\u6392\u653e\u7684\u53d8\u5316\u3002\u7814\u7a76\u65e8\u5728\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u548c\u51b3\u7b56\u8005\u63d0\u4f9b\u5173\u4e8e\u5982\u4f55\u66f4\u6709\u6548\u5730\u89c4\u5212\u548c\u7ba1\u7406\u57ce\u5e02\u4ea4\u901a\u4ee5\u5b9e\u73b0\u73af\u5883\u4fdd\u62a4\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.10660", "categories": ["cs.AI", "cs.MA", "q-bio.CB"], "pdf": "https://arxiv.org/pdf/2509.10660", "abs": "https://arxiv.org/abs/2509.10660", "authors": ["Nam H. Le", "Patrick Erickson", "Yanbo Zhang", "Michael Levin", "Josh Bongard"], "title": "ZapGPT: Free-form Language Prompting for Simulated Cellular Control", "comment": null, "summary": "Human language is one of the most expressive tools for conveying intent, yet\nmost artificial or biological systems lack mechanisms to interpret or respond\nmeaningfully to it. Bridging this gap could enable more natural forms of\ncontrol over complex, decentralized systems. In AI and artificial life, recent\nwork explores how language can specify high-level goals, but most systems still\ndepend on engineered rewards, task-specific supervision, or rigid command sets,\nlimiting generalization to novel instructions. Similar constraints apply in\nsynthetic biology and bioengineering, where the locus of control is often\ngenomic rather than environmental perturbation.\n  A key open question is whether artificial or biological collectives can be\nguided by free-form natural language alone, without task-specific tuning or\ncarefully designed evaluation metrics. We provide one possible answer here by\nshowing, for the first time, that simple agents' collective behavior can be\nguided by free-form language prompts: one AI model transforms an imperative\nprompt into an intervention that is applied to simulated cells; a second AI\nmodel scores how well the prompt describes the resulting cellular dynamics; and\nthe former AI model is evolved to improve the scores generated by the latter.\n  Unlike previous work, our method does not require engineered fitness\nfunctions or domain-specific prompt design. We show that the evolved system\ngeneralizes to unseen prompts without retraining. By treating natural language\nas a control layer, the system suggests a future in which spoken or written\nprompts could direct computational, robotic, or biological systems to desired\nbehaviors. This work provides a concrete step toward this vision of AI-biology\npartnerships, in which language replaces mathematical objective functions,\nfixed rules, and domain-specific programming.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u7528\u8bed\u8a00\u6307\u5b9a\u9ad8\u5c42\u6b21\u76ee\u6807\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5f15\u5bfc\u7b80\u5355\u4ee3\u7406\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u6f14\u5316\u7b97\u6cd5\u6539\u8fdb\u8fd9\u79cd\u5f15\u5bfc\uff0c\u4ece\u800c\u4e3aAI\u4e0e\u751f\u7269\u5408\u4f5c\u7684\u613f\u666f\u8fc8\u51fa\u4e86\u5b9e\u8d28\u6027\u6b65\u9aa4\u3002\u7cfb\u7edf\u4e0d\u9700\u8981\u5de5\u7a0b\u5316\u9002\u5e94\u6027\u51fd\u6570\u6216\u9886\u57df\u7279\u5b9a\u63d0\u793a\u8bbe\u8ba1\uff0c\u800c\u80fd\u6cdb\u5316\u5230\u65b0\u63d0\u793a\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u81ea\u7136\u8bed\u8a00\u662f\u5426\u53ef\u4ee5\u5355\u72ec\u5f15\u5bfc\u4eba\u5de5\u6216\u751f\u7269\u96c6\u4f53\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u6574\u6216\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u53ca\u514d\u53bb\u5de5\u7a0b\u5316\u9002\u5e94\u6027\u51fd\u6570\u6216\u7279\u5b9a\u9886\u57df\u63d0\u793a\u8bbe\u8ba1\u7684\u9700\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u547d\u4ee4\u5f0f\u63d0\u793a\u8f6c\u5316\u4e3a\u5e72\u9884\u63aa\u65bd\u5e76\u8bc4\u5206\u6765\u6307\u5bfc\u6a21\u62df\u7ec6\u80de\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u6f14\u5316\u7b97\u6cd5\u6539\u8fdb\u8bc4\u5206\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u63d0\u793a\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\uff0c\u7b80\u5355\u4ee3\u7406\u7684\u96c6\u4f53\u884c\u4e3a\u53ef\u4ee5\u7531\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u63d0\u793a\u6765\u5f15\u5bfc\uff0c\u8fdb\u5316\u7b97\u6cd5\u7684\u673a\u5236\u53ef\u4ee5\u6301\u7eed\u6539\u8fdb\u8fd9\u79cd\u5f15\u5bfc\u80fd\u529b\uff0c\u800c\u4e14\u7cfb\u7edf\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u63d0\u793a\u3002", "conclusion": "\u4eba\u7c7b\u8bed\u8a00\u662f\u4e00\u79cd\u975e\u5e38\u8868\u8fbe\u610f\u56fe\u7684\u5de5\u5177\uff0c\u6700\u4eba\u5de5\u6216\u751f\u7269\u7cfb\u7edf\u7f3a\u4e4f\u89e3\u91ca\u6216\u6709\u610f\u4e49\u5730\u56de\u5e94\u5b83\u7684\u673a\u5236\u3002\u7814\u7a76\u5982\u4f55\u7528\u8bed\u8a00\u6307\u5b9a\u9ad8\u5c42\u6b21\u76ee\u6807\uff0c\u901a\u8fc7\u8bed\u8a00\u63d0\u793a\u6307\u5bfc\u7b80\u5355\u4ee3\u7406\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5e76\u7ee7\u7eed\u5b9e\u9a8c\u5ba4\u8fdb\u884c\u8fdb\u5316\u4ee5\u6539\u8fdb\u8fd9\u79cd\u5f15\u5bfc\u6027\uff0c\u4e3a\u672a\u6765AI\u4e0e\u751f\u7269\u5408\u4f5c\u7684\u613f\u666f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u8d28\u6027\u6b65\u9aa4\u3002"}}
{"id": "2509.10704", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10704", "abs": "https://arxiv.org/abs/2509.10704", "authors": ["Xingchen Wan", "Han Zhou", "Ruoxi Sun", "Hootan Nakhost", "Ke Jiang", "Rajarishi Sinha", "Sercan \u00d6. Ar\u0131k"], "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration", "comment": "15 pages, 7 figures, 2 tables (22 pages, 9 figures and 3 tables\n  including references and appendices)", "summary": "Text-to-image (T2I) models, while offering immense creative potential, are\nhighly reliant on human intervention, posing significant usability challenges\nthat often necessitate manual, iterative prompt engineering over often\nunderspecified prompts. This paper introduces Maestro, a novel self-evolving\nimage generation system that enables T2I models to autonomously self-improve\ngenerated images through iterative evolution of prompts, using only an initial\nprompt. Maestro incorporates two key innovations: 1) self-critique, where\nspecialized multimodal LLM (MLLM) agents act as 'critics' to identify\nweaknesses in generated images, correct for under-specification, and provide\ninterpretable edit signals, which are then integrated by a 'verifier' agent\nwhile preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge\nfor head-to-head comparisons between iteratively generated images, eschewing\nproblematic images, and evolving creative prompt candidates that align with\nuser intents. Extensive experiments on complex T2I tasks using black-box models\ndemonstrate that Maestro significantly improves image quality over initial\nprompts and state-of-the-art automated methods, with effectiveness scaling with\nmore advanced MLLM components. This work presents a robust, interpretable, and\neffective pathway towards self-improving T2I generation.", "AI": {"tldr": "Maestro is a self-evolving image generation system for Text-to-Image models that autonomously improves generated images through self-critique and self-evolution. It outperforms existing methods, especially with advanced MLLM components, offering a robust pathway for self-improving T2I generation.", "motivation": "Current Text-to-Image models rely heavily on human intervention and manual prompt engineering, posing usability challenges. Maestro aims to address these challenges by enabling T2I models to self-improve without the need for extensive human intervention or iterative prompts.", "method": "Introducing Maestro, a self-evolving image generation system that enables Text-to-Image models to autonomously improve generated images through iterative evolution of prompts. It incorporates self-critique using specialized multimodal LLM agents as 'critics' to identify weaknesses in images and correct under-specification, along with self-evolution by utilizing MLLM-as-a-judge for comparisons between generated images and evolving creative prompt candidates aligned with user intents.", "result": "Extensive experiments on complex T2I tasks show that Maestro improves image quality significantly and outperforms state-of-the-art methods. The effectiveness of Maestro increases with the incorporation of advanced MLLM components.", "conclusion": "Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods in Text-to-Image generation tasks. The effectiveness scales with more advanced MLLM components, presenting a robust and interpretable pathway towards self-improving T2I generation."}}
{"id": "2509.10707", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10707", "abs": "https://arxiv.org/abs/2509.10707", "authors": ["Sajjad Abdoli", "Rudi Cilibrasi", "Rima Al-Shikh"], "title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions", "comment": null, "summary": "As AI systems increasingly evaluate other AI outputs, understanding their\nassessment behavior becomes crucial for preventing cascading biases. This study\nanalyzes vision-language descriptions generated by NVIDIA's Describe Anything\nModel and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to\nuncover distinct \"evaluation personalities\" the underlying assessment\nstrategies and biases each model demonstrates. GPT-4o-mini exhibits systematic\nconsistency with minimal variance, GPT-4o excels at error detection, while\nGPT-5 shows extreme conservatism with high variability. Controlled experiments\nusing Gemini 2.5 Pro as an independent question generator validate that these\npersonalities are inherent model properties rather than artifacts. Cross-family\nanalysis through semantic similarity of generated questions reveals significant\ndivergence: GPT models cluster together with high similarity while Gemini\nexhibits markedly different evaluation strategies. All GPT models demonstrate a\nconsistent 2:1 bias favoring negative assessment over positive confirmation,\nthough this pattern appears family-specific rather than universal across AI\narchitectures. These findings suggest that evaluation competence does not scale\nwith general capability and that robust AI assessment requires diverse\narchitectural perspectives.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86NVIDIA\u7684Describe Anything Model\u751f\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u63cf\u8ff0\uff0c\u7531\u4e09\u4e2aGPT\u53d8\u79cd\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5404\u6a21\u578b\u7684\u8bc4\u4f30\u7b56\u7565\u548c\u504f\u89c1\u3002\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bc4\u4f30\u7279\u6027\u662f\u56fa\u6709\u6a21\u578b\u5c5e\u6027\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\u4e0d\u540cGPT\u6a21\u578b\u5728\u8bc4\u4f30\u7279\u6027\u548c\u8d1f\u9762\u8bc4\u4f30\u504f\u89c1\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u80fd\u529b\u4e0e\u901a\u7528\u80fd\u529b\u4e0d\u540c\u6b65\u63d0\u5347\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u8bc4\u4f30\u5176\u4ed6AI\u7684\u8f93\u51fa\uff0c\u4e86\u89e3\u5b83\u4eec\u7684\u8bc4\u4f30\u884c\u4e3a\u5bf9\u4e8e\u9632\u6b62\u7ea7\u8054\u504f\u89c1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u7531NVIDIA\u7684Describe Anything Model\u751f\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u7531\u4e09\u4e2aGPT\u53d8\u79cd\uff08GPT-4o\u3001GPT-4o-mini\u3001GPT-5\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u6bcf\u4e2a\u6a21\u578b\u5c55\u793a\u7684\u8bc4\u4f30\u7b56\u7565\u548c\u504f\u89c1\u3002\u4f7f\u7528Gemini 2.5 Pro\u4f5c\u4e3a\u72ec\u7acb\u95ee\u9898\u751f\u6210\u5668\u8fdb\u884c\u4e86\u63a7\u5236\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u8bc4\u4f30\u7279\u6027\u662f\u56fa\u6709\u6a21\u578b\u5c5e\u6027\u3002\u901a\u8fc7\u751f\u6210\u7684\u95ee\u9898\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u8fdb\u884c\u8de8\u7fa4\u5206\u6790\uff0c\u53d1\u73b0GPT\u6a21\u578b\u5728\u9ad8\u76f8\u4f3c\u6027\u4e0b\u805a\u96c6\uff0c\u800cGemini\u5c55\u793a\u51fa\u660e\u663e\u4e0d\u540c\u7684\u8bc4\u4f30\u7b56\u7565\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86GPT-4o-mini\u3001GPT-4o\u548cGPT-5\u4e4b\u95f4\u7684\u8bc4\u4f30\u7279\u6027\u5dee\u5f02\uff0c\u4ee5\u53caGPT\u6a21\u578b\u5728\u8d1f\u9762\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7684\u504f\u89c1\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u80fd\u529b\u4e0e\u4e00\u822c\u80fd\u529b\u4e0d\u540c\u6b65\u63d0\u5347\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0GPT-4o-mini\u8868\u73b0\u51fa\u7cfb\u7edf\u4e00\u81f4\u6027\uff0cGPT-4o\u64c5\u957f\u9519\u8bef\u68c0\u6d4b\uff0cGPT-5\u8868\u73b0\u51fa\u6781\u7aef\u4fdd\u5b88\u4e3b\u4e49\u3002\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u8bc4\u4f30\u7279\u6027\u662f\u6a21\u578b\u56fa\u6709\u5c5e\u6027\u800c\u975e\u5076\u7136\u73b0\u8c61\u3002GPT\u6a21\u578b\u5728\u8d1f\u9762\u8bc4\u4f30\u65b9\u9762\u5b58\u57282:1\u7684\u504f\u89c1\uff0c\u800c\u8fd9\u4e00\u6a21\u5f0f\u5728AI\u67b6\u6784\u4e4b\u95f4\u5e76\u975e\u666e\u904d\u5b58\u5728\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bc4\u4f30\u80fd\u529b\u4e0d\u968f\u4e00\u822c\u80fd\u529b\u800c\u63d0\u5347\uff0c\u5f3a\u5927\u7684AI\u8bc4\u4f30\u9700\u8981\u591a\u5143\u7684\u67b6\u6784\u89c2\u70b9\u3002"}}
{"id": "2509.10762", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10762", "abs": "https://arxiv.org/abs/2509.10762", "authors": ["Arlen Kumar", "Leanid Palkhouski"], "title": "AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework", "comment": null, "summary": "AI answer engines increasingly mediate access to domain knowledge by\ngenerating responses and citing web sources. We introduce GEO-16, a 16 pillar\nauditing framework that converts on page quality signals into banded pillar\nscores and a normalized GEO score G that ranges from 0 to 1. Using 70 product\nintent prompts, we collected 1,702 citations across three engines (Brave\nSummary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In\nour corpus, the engines differed in the GEO quality of the pages they cited,\nand pillars related to Metadata and Freshness, Semantic HTML, and Structured\nData showed the strongest associations with citation. Logistic models with\ndomain clustered standard errors indicate that overall page quality is a strong\npredictor of citation, and simple operating points (for example, G at least\n0.70 combined with at least 12 pillar hits) align with substantially higher\ncitation rates in our data. We report per engine contrasts, vertical effects,\nthreshold analysis, and diagnostics, then translate findings into a practical\nplaybook for publishers. The study is observational and focuses on English\nlanguage B2B SaaS pages; we discuss limitations, threats to validity, and\nreproducibility considerations.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u7b54\u6848\u5f15\u64ce\u5728\u5f15\u7528\u7f51\u9875\u65f6\u5b58\u5728\u8d28\u91cf\u5dee\u5f02\uff0c\u5176\u4e2d\u5143\u6570\u636e\u548c\u65b0\u9c9c\u5ea6\u3001\u8bed\u4e49HTML\u548c\u7ed3\u6784\u5316\u6570\u636e\u7b49\u652f\u67f1\u4e0e\u5f15\u7528\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5173\u8054\u3002\u603b\u4f53\u9875\u9762\u8d28\u91cf\u662f\u5f15\u7528\u7684\u91cd\u8981\u9884\u6d4b\u56e0\u7d20\u3002\u7279\u5b9a\u64cd\u4f5c\u70b9\uff0c\u5982\u81f3\u5c1112\u4e2a\u652f\u67f1\u547d\u4e2d\u7684GEO\u5206\u6570\u81f3\u5c11\u4e3a0.70\uff0c\u5c06\u5bfc\u81f4\u66f4\u9ad8\u7684\u5f15\u7528\u7387\u3002\u7814\u7a76\u4e3a\u51fa\u7248\u5546\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5f15\u64ce\u5dee\u5f02\u3001\u5782\u76f4\u6548\u5e94\u3001\u9608\u503c\u548c\u8bca\u65ad\u7684\u89c1\u89e3\u3002", "motivation": "To understand how AI answer engines cite web sources based on page quality signals and to provide practical recommendations for publishers. Focuses on English language B2B SaaS pages.", "method": "Introduced GEO-16 auditing framework to convert on-page quality signals into pillar scores and a normalized GEO score. Collected 1,702 citations across three search engines and audited 1,100 unique URLs. Used logistic models with domain clustered standard errors to analyze the relationship between page quality and citation rates.", "result": "Identified differences in page quality citation by AI answer engines. Found strong associations between page quality pillars and citation rates. Highlighted specific operating points for higher citation rates. Provided insights and a playbook for publishers based on the study findings.", "conclusion": "AI answer engines differ in the quality of pages they cite, with Metadata and Freshness, Semantic HTML, and Structured Data pillars showing strong associations with citation. Overall page quality is a significant predictor of citation. Specific operating points, such as a GEO score of at least 0.70 combined with at least 12 pillar hits, lead to higher citation rates. The study provides insights for publishers based on engine differences, vertical effects, thresholds, and diagnostics."}}
{"id": "2509.10769", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10769", "abs": "https://arxiv.org/abs/2509.10769", "authors": ["Tara Bogavelli", "Roshnee Sharma", "Hari Subramani"], "title": "AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise", "comment": null, "summary": "While individual components of agentic architectures have been studied in\nisolation, there remains limited empirical understanding of how different\ndesign dimensions interact within complex multi-agent systems. This study aims\nto address these gaps by providing a comprehensive enterprise-specific\nbenchmark evaluating 18 distinct agentic configurations across state-of-the-art\nlarge language models. We examine four critical agentic system dimensions:\norchestration strategy, agent prompt implementation (ReAct versus function\ncalling), memory architecture, and thinking tool integration. Our benchmark\nreveals significant model-specific architectural preferences that challenge the\nprevalent one-size-fits-all paradigm in agentic AI systems. It also reveals\nsignificant weaknesses in overall agentic performance on enterprise tasks with\nthe highest scoring models achieving a maximum of only 35.3\\% success on the\nmore complex task and 70.8\\% on the simpler task. We hope these findings inform\nthe design of future agentic systems by enabling more empirically backed\ndecisions regarding architectural components and model selection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bc4\u4f3018\u79cd\u4e0d\u540cagent\u914d\u7f6e\u7684\u7efc\u5408\u4f01\u4e1a\u7279\u5b9a\u57fa\u51c6\uff0c\u7814\u7a76\u4e86\u56db\u4e2a\u5173\u952eagent\u7cfb\u7edf\u7ef4\u5ea6\uff0c\u63ed\u793a\u4e86\u5728\u4f01\u4e1a\u4efb\u52a1\u4e2dagent\u7cfb\u7edf\u67b6\u6784\u504f\u597d\u7684\u663e\u8457\u5dee\u5f02\uff0c\u4ee5\u53ca\u6574\u4f53agent\u6027\u80fd\u7684\u5de8\u5927\u5f31\u70b9\u3002", "motivation": "\u5c3d\u7ba1\u5bf9agent\u67b6\u6784\u7684\u4e2a\u522b\u7ec4\u4ef6\u8fdb\u884c\u4e86\u5b64\u7acb\u7814\u7a76\uff0c\u4f46\u5728\u590d\u6742\u7684\u591aAgent\u7cfb\u7edf\u4e2d\uff0c\u4e0d\u540c\u8bbe\u8ba1\u7ef4\u5ea6\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u7684\u5b9e\u8bc1\u7406\u89e3\u4ecd\u6709\u9650\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u8bc4\u4f3018\u79cd\u4e0d\u540cagent\u914d\u7f6e\u7684\u7efc\u5408\u4f01\u4e1a\u7279\u5b9a\u57fa\u51c6\uff0c\u7814\u7a76\u4e86agent\u7cfb\u7edf\u4e2d\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u7f16\u6392\u7b56\u7565\u3001agent\u63d0\u793a\u5b9e\u73b0\uff08ReAct\u4e0e\u51fd\u6570\u8c03\u7528\uff09\u3001\u5b58\u50a8\u67b6\u6784\u548c\u601d\u7ef4\u5de5\u5177\u96c6\u6210\u3002", "result": "\u5728\u7814\u7a76\u4e2d\u53d1\u73b0\u4e86\u663e\u8457\u7684\u6a21\u578b\u7279\u5b9a\u67b6\u6784\u504f\u597d\uff0c\u6311\u6218\u4e86agent AI\u7cfb\u7edf\u4e2d\u6d41\u884c\u7684\u4e00\u5200\u5207\u8303\u5f0f\u3002\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u5728\u4f01\u4e1a\u4efb\u52a1\u4e2d\u6574\u4f53agent\u6027\u80fd\u7684\u663e\u8457\u5f31\u70b9\uff0c\u6700\u9ad8\u5f97\u5206\u6a21\u578b\u5728\u66f4\u590d\u6742\u4efb\u52a1\u4e0a\u53ea\u80fd\u8fbe\u523035.3%\u7684\u6210\u529f\u7387\uff0c\u5728\u66f4\u7b80\u5355\u4efb\u52a1\u4e0a\u4e3a70.8%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5bf9\u4f01\u4e1a\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0d\u540cagent\u7cfb\u7edf\u7ef4\u5ea6\u7684\u67b6\u6784\u504f\u597d\uff0c\u6311\u6218\u4e86\u666e\u904d\u7684\u4e00\u5200\u5207\u8303\u5f0f\u3002\u540c\u65f6\uff0c\u5728\u4f01\u4e1a\u4efb\u52a1\u4e2d\uff0c\u6574\u4f53agent\u6027\u80fd\u5b58\u5728\u663e\u8457\u7684\u5f31\u70b9\uff0c\u6700\u9ad8\u5f97\u5206\u6a21\u578b\u5728\u66f4\u590d\u6742\u4efb\u52a1\u4e0a\u53ea\u80fd\u8fbe\u523035.3%\u7684\u6210\u529f\u7387\uff0c\u5728\u66f4\u7b80\u5355\u4efb\u52a1\u4e0a\u4e3a70.8%\u3002\u5e0c\u671b\u8fd9\u4e9b\u53d1\u73b0\u80fd\u591f\u4e3a\u672a\u6765agent\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u501f\u9274\uff0c\u4f7f\u5f97\u5173\u4e8e\u67b6\u6784\u7ec4\u4ef6\u548c\u6a21\u578b\u9009\u62e9\u7684\u51b3\u7b56\u66f4\u5177\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2509.10818", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.10818", "abs": "https://arxiv.org/abs/2509.10818", "authors": ["Boris Kovalerchuk", "Brent D. Fegley"], "title": "LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering", "comment": "25 pages,4 figures, 2 tables", "summary": "Difficult decision-making problems abound in various disciplines and domains.\nThe proliferation of generative techniques, especially large language models\n(LLMs), has excited interest in using them for decision support. However, LLMs\ncannot yet resolve missingness in their training data, leading to\nhallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by\nincorporating external information retrieval, reducing hallucinations and\nimproving accuracy. Yet, RAG and related methods are only partial solutions, as\nthey may lack access to all necessary sources or key missing information. Even\neveryday issues often challenge LLMs' abilities. Submitting longer prompts with\ncontext and examples is one approach to address knowledge gaps, but designing\neffective prompts is non-trivial and may not capture complex mental models of\ndomain experts. For tasks with missing critical information, LLMs are\ninsufficient, as are many existing systems poorly represented in available\ndocuments. This paper explores how LLMs can make decision-making more\nefficient, using a running example of evaluating whether to respond to a call\nfor proposals. We propose a technology based on optimized human-machine\ndialogue and monotone Boolean and k-valued functions to discover a\ncomputationally tractable personal expert mental model (EMM) of\ndecision-making. Our EMM algorithm for LLM prompt engineering has four steps:\n(1) factor identification, (2) hierarchical structuring of factors, (3)\ngenerating a generalized expert mental model specification, and (4) generating\na detailed generalized expert mental model from that specification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u7684\u4eba\u673a\u5bf9\u8bdd\u548c\u5e03\u5c14\u51fd\u6570\u6280\u672f\uff0c\u53d1\u73b0\u8ba1\u7b97\u4e0a\u53ef\u5904\u7406\u7684\u4e2a\u4eba\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86EMM\u7b97\u6cd5\u7684\u56db\u4e2a\u6b65\u9aa4\u3002", "motivation": "\u4f5c\u8005\u63a2\u8ba8\u4e86LLM\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u5c40\u9650\u6027\u548c\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u4e4b\u5904\uff0c\u6307\u51fa\u73b0\u6709\u7cfb\u7edf\u5728\u5904\u7406\u7f3a\u5931\u5173\u952e\u4fe1\u606f\u65b9\u9762\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdEMM\u7b97\u6cd5\uff0c\u5305\u62ec\u56db\u4e2a\u6b65\u9aa4\uff1a(1) \u56e0\u5b50\u8bc6\u522b\uff0c(2) \u56e0\u5b50\u7684\u5c42\u6b21\u7ed3\u6784\u5316\uff0c(3) \u751f\u6210\u5e7f\u4e49\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\u89c4\u8303\uff0c(4) \u4ece\u89c4\u8303\u751f\u6210\u8be6\u7ec6\u7684\u5e7f\u4e49\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\u3002", "result": "\u5f97\u51fa\u7ed3\u8bba\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u4eba\u673a\u5bf9\u8bdd\u548c\u5355\u8c03\u5e03\u5c14\u548ck\u503c\u51fd\u6570\u6280\u672f\uff0c\u53ef\u4ee5\u53d1\u73b0\u53ef\u8ba1\u7b97\u7684\u4e2a\u4eba\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\uff08EMM\uff09\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u7684\u4eba\u673a\u5bf9\u8bdd\u548c\u5355\u8c03\u5e03\u5c14\u548ck\u503c\u51fd\u6570\uff0c\u53d1\u73b0\u53ef\u8ba1\u7b97\u7684\u4e2a\u4eba\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\uff08EMM\uff09\u6765\u63d0\u9ad8LLM\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f18\u5316\u7684\u4eba\u673a\u5bf9\u8bdd\u548c\u5e03\u5c14\u51fd\u6570\u7684\u6280\u672f\uff0c\u4ee5\u53d1\u73b0\u8ba1\u7b97\u4e0a\u53ef\u5904\u7406\u7684\u4e2a\u4eba\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\u3002"}}
{"id": "2509.10837", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10837", "abs": "https://arxiv.org/abs/2509.10837", "authors": ["Yuyin Lu", "Hegang Chen", "Yanghui Rao"], "title": "From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering", "comment": null, "summary": "Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),\ntypically formalized as reasoning with Existential First-Order predicate logic\nwith one free variable (EFO$_1$), faces a fundamental trade-off between logical\nsoundness and computational efficiency. This work establishes the\nGrounding-Skolemization dichotomy for systematically analyzing CQA methods\nthrough the lens of formal logic. While Grounding-based methods inherently\nsuffer from combinatorial explosion, most Skolemization-based methods neglect\nto explicitly model Skolem functions and compromise logical consistency. To\naddress these limitations, we propose the Logic-constrained Vector Symbolic\nArchitecture (LVSA), a neuro-symbolic framework that unifies a differentiable\nSkolemization module and a neural negator, as well as a logical\nconstraint-driven optimization protocol to harmonize geometric and logical\nrequirements. Theoretically, LVSA guarantees universality for all EFO$_1$\nqueries. Empirically, it outperforms state-of-the-art Skolemization-based\nmethods and reduces inference costs by orders of magnitude compared to\nGrounding-based baselines.", "AI": {"tldr": "LVSA is a neuro-symbolic framework proposed to address limitations in Complex Query Answering methods over incomplete Knowledge Graphs. It outperforms existing methods, reduces inference costs significantly, and guarantees universality for all EFO$_1$ queries.", "motivation": "The motivation behind this work is to overcome the trade-off between logical soundness and computational efficiency in Complex Query Answering over incomplete Knowledge Graphs. Existing methods either face combinatorial explosion or compromise logical consistency, leading to limitations in CQA. LVSA aims to harmonize logical and geometric requirements to improve the performance of CQA methods.", "method": "The paper proposes the Logic-constrained Vector Symbolic Architecture (LVSA) framework, which includes a differentiable Skolemization module and a neural negator. It utilizes a logical constraint-driven optimization protocol to balance geometric and logical requirements.", "result": "Theoretically, LVSA guarantees universality for all EFO$_1$ queries. Empirically, it outperforms state-of-the-art Skolemization-based methods and reduces inference costs significantly compared to Grounding-based baselines.", "conclusion": "LVSA is proposed as a neuro-symbolic framework that unifies Skolemization and a neural negator to address the limitations in Complex Query Answering methods over incomplete Knowledge Graphs. It outperforms existing Skolemization-based methods and reduces inference costs significantly compared to Grounding-based approaches."}}
{"id": "2509.10875", "categories": ["cs.AI", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2509.10875", "abs": "https://arxiv.org/abs/2509.10875", "authors": ["Jesse Gardner", "Vladimir A. Baulin"], "title": "Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?", "comment": null, "summary": "The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)\nresearch, guiding development from foundational theories to contemporary\napplications like Large Language Model (LLM)-based systems. This paper\ncritically re-evaluates the necessity and optimality of this agent-centric\nparadigm. We argue that its persistent conceptual ambiguities and inherent\nanthropocentric biases may represent a limiting framework. We distinguish\nbetween agentic systems (AI inspired by agency, often semi-autonomous, e.g.,\nLLM-based agents), agential systems (fully autonomous, self-producing systems,\ncurrently only biological), and non-agentic systems (tools without the\nimpression of agency). Our analysis, based on a systematic review of relevant\nliterature, deconstructs the agent paradigm across various AI frameworks,\nhighlighting challenges in defining and measuring properties like autonomy and\ngoal-directedness. We argue that the 'agentic' framing of many AI systems,\nwhile heuristically useful, can be misleading and may obscure the underlying\ncomputational mechanisms, particularly in Large Language Models (LLMs). As an\nalternative, we propose a shift in focus towards frameworks grounded in\nsystem-level dynamics, world modeling, and material intelligence. We conclude\nthat investigating non-agentic and systemic frameworks, inspired by complex\nsystems, biology, and unconventional computing, is essential for advancing\ntowards robust, scalable, and potentially non-anthropomorphic forms of general\nintelligence. This requires not only new architectures but also a fundamental\nreconsideration of our understanding of intelligence itself, moving beyond the\nagent metaphor.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u8bc4\u4f30\u4e86\u4ee3\u7406\u4eba\u8303\u5f0f\u7684\u5fc5\u8981\u6027\u548c\u6700\u4f18\u6027\uff0c\u63d0\u51fa\u4e86\u5bf9\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u9886\u57df\u89c2\u5ff5\u7684\u8f6c\u53d8\u3002\u4f5c\u8005\u5f3a\u8c03\u4e86\u7814\u7a76\u975e\u4ee3\u7406\u548c\u7cfb\u7edf\u5206\u6790\u6846\u67b6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63a8\u52a8\u666e\u904d\u667a\u80fd\u7684\u53d1\u5c55\u3002", "motivation": "\u672c\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e2d\u4ee3\u7406\u4eba\u6982\u5ff5\u5bf9\u8be5\u9886\u57df\u53d1\u5c55\u7684\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u5176\u4e2d\u5b58\u5728\u7684\u6f5c\u5728\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u9886\u57df\u671d\u7740\u66f4\u52a0\u5065\u58ee\u548c\u53ef\u6301\u7eed\u7684\u65b9\u5411\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u6027\u5ba1\u67e5\u76f8\u5173\u6587\u732e\uff0c\u5bf9\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u4e2d\u7684\u4ee3\u7406\u8303\u5f0f\u8fdb\u884c\u89e3\u6784\uff0c\u5f3a\u8c03\u4e86\u5728\u5b9a\u4e49\u548c\u8861\u91cf\u81ea\u4e3b\u6027\u548c\u76ee\u6807\u5bfc\u5411\u6027\u7b49\u5c5e\u6027\u65b9\u9762\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u63d0\u51fa\u4e86\u7814\u7a76\u975e\u4ee3\u7406\u548c\u7cfb\u7edf\u5206\u6790\u6846\u67b6\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5728\u7cfb\u7edf\u6027\u6587\u732e\u5ba1\u67e5\u7684\u57fa\u7840\u4e0a\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u66ff\u4ee3\u4ee3\u7406\u4eba\u8303\u5f0f\u7684\u89c2\u70b9\uff0c\u5e76\u547c\u5401\u6df1\u5165\u7814\u7a76\u975e\u4ee3\u7406\u548c\u7cfb\u7edf\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u6279\u5224\u6027\u5730\u91cd\u65b0\u8bc4\u4f30\u4e86'\u4ee3\u7406\u4eba'\u8303\u5f0f\u7684\u5fc5\u8981\u6027\u548c\u6700\u4f18\u6027\uff0c\u4e3b\u5f20\u5411\u7cfb\u7edf\u7ea7\u52a8\u6001\u3001\u4e16\u754c\u5efa\u6a21\u548c\u7269\u8d28\u667a\u80fd\u6839\u57fa\u7684\u6846\u67b6\u8f6c\u53d8\u3002\u4f5c\u8005\u8ba4\u4e3a'\u4ee3\u7406'\u8303\u5f0f\u53ef\u80fd\u4f1a\u8bef\u5bfc\u5e76\u63a9\u76d6\u57fa\u7840\u8ba1\u7b97\u673a\u5236\uff0c\u540c\u65f6\u63d0\u51fa\u63a2\u7a76\u975e\u4ee3\u7406\u548c\u7cfb\u7edf\u8303\u5f0f\u5bf9\u4e8e\u63a8\u52a8\u5411\u5065\u58ee\u3001\u53ef\u6269\u5c55\u4e14\u6f5c\u5728\u975e\u4eba\u7c7b\u5f62\u5f0f\u7684\u666e\u904d\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.10931", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10931", "abs": "https://arxiv.org/abs/2509.10931", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "title": "Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding", "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their potential misuse for harmful purposes remains a\nsignificant concern. To strengthen defenses against such vulnerabilities, it is\nessential to investigate universal jailbreak attacks that exploit intrinsic\nweaknesses in the architecture and learning paradigms of LLMs. In response, we\npropose \\textbf{H}armful \\textbf{P}rompt \\textbf{La}undering (HaPLa), a novel\nand broadly applicable jailbreaking technique that requires only black-box\naccess to target models. HaPLa incorporates two primary strategies: 1)\n\\textit{abductive framing}, which instructs LLMs to infer plausible\nintermediate steps toward harmful activities, rather than directly responding\nto explicit harmful queries; and 2) \\textit{symbolic encoding}, a lightweight\nand flexible approach designed to obfuscate harmful content, given that current\nLLMs remain sensitive primarily to explicit harmful keywords. Experimental\nresults show that HaPLa achieves over 95% attack success rate on GPT-series\nmodels and 70% across all targets. Further analysis with diverse symbolic\nencoding rules also reveals a fundamental challenge: it remains difficult to\nsafely tune LLMs without significantly diminishing their helpfulness in\nresponding to benign queries.", "AI": {"tldr": "Large Language Models (LLMs) are powerful but susceptible to misuse. The paper introduces HaPLa, a jailbreaking technique that deceives LLMs with abductive framing and symbolic encoding to achieve high attack success rates. However, balancing security measures with LLM's functionality remains a challenge.", "motivation": "The motivation behind this paper is the concern over the potential misuse of large language models for harmful purposes. The authors aim to strengthen defenses against vulnerabilities in LLMs by investigating universal jailbreak attacks. They highlight the need to address intrinsic weaknesses in LLM architectures and learning paradigms.", "method": "In the paper, the authors propose HaPLa, a harmful prompt laundering technique that exploits weaknesses in LLMs. This technique involves abductive framing and symbolic encoding strategies to deceive LLMs into inferring harmful activities and obfuscate harmful content. Experimental results demonstrate the effectiveness of HaPLa.", "result": "Experimental results show that HaPLa achieves high attack success rates on various LLM models, indicating its effectiveness in exploiting vulnerabilities. Additionally, the analysis of symbolic encoding rules reveals the challenge of balancing security measures with maintaining the helpfulness of LLMs.", "conclusion": "HaPLa is a novel and broadly applicable jailbreaking technique that achieves over 95% attack success rate on GPT-series models and 70% across all targets. However, there is a fundamental challenge in tuning LLMs without reducing their helpfulness in responding to benign queries."}}
{"id": "2509.10932", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10932", "abs": "https://arxiv.org/abs/2509.10932", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "title": "Public Data Assisted Differentially Private In-Context Learning", "comment": "EMNLP 2025 Findings", "summary": "In-context learning (ICL) in Large Language Models (LLMs) has shown\nremarkable performance across various tasks without requiring fine-tuning.\nHowever, recent studies have highlighted the risk of private data leakage\nthrough the prompt in ICL, especially when LLMs are exposed to malicious\nattacks. While differential privacy (DP) provides strong privacy guarantees, it\noften significantly reduces the utility of in-context learning (ICL). To\naddress this challenge, we incorporate task-related public data into the ICL\nframework while maintaining the DP guarantee. Based on this approach, we\npropose a private in-context learning algorithm that effectively balances\nprivacy protection and model utility. Through experiments, we demonstrate that\nour approach significantly improves the utility of private ICL with the\nassistance of public data. Additionally, we show that our method is robust\nagainst membership inference attacks, demonstrating empirical privacy\nprotection.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u79c1\u4ebaICL\u7b97\u6cd5\uff0c\u5c06\u4efb\u52a1\u76f8\u5173\u516c\u5f00\u6570\u636e\u878d\u5165ICL\u6846\u67b6\u4e2d\u5e76\u4fdd\u6301\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79c1\u4ebaICL\u7684\u6548\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u6700\u8fd1\u7684\u7814\u7a76\u5f3a\u8c03\u4e86ICL\u4e2d\u63d0\u793a\u4fe1\u606f\u53ef\u80fd\u5bfc\u81f4\u79c1\u4eba\u6570\u636e\u6cc4\u9732\u7684\u98ce\u9669\uff0c\u5c24\u5176\u5f53LLMs\u5bb9\u6613\u53d7\u5230\u6076\u610f\u653b\u51fb\u65f6\u3002\u5dee\u5206\u9690\u79c1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u4f46\u5f80\u5f80\u4f1a\u663e\u8457\u964d\u4f4eICL\u7684\u5b9e\u7528\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u89e3\u51b3\u5982\u4f55\u5728ICL\u4e2d\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u7684\u6311\u6218\u3002", "method": "\u5c06\u4efb\u52a1\u76f8\u5173\u7684\u516c\u5f00\u6570\u636e\u7eb3\u5165ICL\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79c1\u4ebaICL\u7b97\u6cd5\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e00\u7b97\u6cd5\u53ef\u4ee5\u6709\u6548\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u79c1\u4ebaICL\u7684\u6548\u7528\uff0c\u5e76\u4e14\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\u5177\u6709\u7a33\u5065\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4efb\u52a1\u76f8\u5173\u516c\u5f00\u6570\u636e\u4e0eICL\u6846\u67b6\u7ed3\u5408\u7684\u79c1\u4ebaICL\u7b97\u6cd5\uff0c\u6709\u6548\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u79c1\u4ebaICL\u7684\u6548\u7528\u3002\u6b64\u5916\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u7a33\u5065\u6027\uff0c\u5177\u6709\u5b9e\u8bc1\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2509.10972", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10972", "abs": "https://arxiv.org/abs/2509.10972", "authors": ["Ron Sun"], "title": "Enhancing Computational Cognitive Architectures with LLMs: A Case Study", "comment": null, "summary": "Computational cognitive architectures are broadly scoped models of the human\nmind that combine different psychological functionalities (as well as often\ndifferent computational methods for these different functionalities) into one\nunified framework. They structure them in a psychologically plausible and\nvalidated way. However, such models thus far have only limited computational\ncapabilities, mostly limited by the computational tools and techniques that\nwere adopted. More recently, LLMs have proved to be more capable\ncomputationally than any other tools. Thus, in order to deal with both\nreal-world complexity and psychological realism at the same time, incorporating\nLLMs into cognitive architectures naturally becomes an important task. In the\npresent article, a synergistic combination of the Clarion cognitive\narchitecture and LLMs is discussed as a case study. The implicit-explicit\ndichotomy that is fundamental to Clarion is leveraged for a seamless\nintegration of Clarion and LLMs. As a result, computational power of LLMs is\ncombined with psychological nicety of Clarion.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u5c06Clarion\u8ba4\u77e5\u67b6\u6784\u4e0eLLMs\u7ed3\u5408\u7684\u60c5\u51b5\u7814\u7a76\uff0c\u901a\u8fc7\u5229\u7528Clarion\u7684\u5185\u9690-\u663e\u6027\u4e8c\u5206\u6cd5\uff0c\u5b9e\u73b0\u4e86Clarion\u548cLLMs\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5c06LLMs\u7684\u8ba1\u7b97\u80fd\u529b\u4e0eClarion\u7684\u5fc3\u7406\u7ec6\u81f4\u7ed3\u5408\u5728\u4e00\u8d77\uff0c\u4e3a\u8ba4\u77e5\u79d1\u5b66\u9886\u57df\u5e26\u6765\u65b0\u7684\u53d1\u5c55\u548c\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u5c06LLMs\u6574\u5408\u5230\u8ba4\u77e5\u67b6\u6784\u4e2d\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u548c\u5fc3\u7406\u73b0\u5b9e\u4e3b\u4e49\uff0c\u901a\u8fc7\u7ed3\u5408LLMs\u7684\u8ba1\u7b97\u80fd\u529b\u548cClarion\u7684\u5fc3\u7406\u7ec6\u81f4\u6765\u53d6\u5f97\u66f4\u597d\u7684\u7814\u7a76\u6210\u679c\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u5c06Clarion\u8ba4\u77e5\u67b6\u6784\u4e0eLLMs\u8fdb\u884c\u7ed3\u5408\u7684\u65b9\u6cd5\u4f5c\u4e3a\u7814\u7a76\u6848\u4f8b\uff0c\u901a\u8fc7\u5229\u7528Clarion\u7684\u5185\u9690-\u663e\u6027\u4e8c\u5206\u6cd5\u6765\u5b9e\u73b0Clarion\u548cLLMs\u7684\u6574\u5408\u3002", "result": "\u901a\u8fc7\u5c06Clarion\u8ba4\u77e5\u67b6\u6784\u4e0eLLMs\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86LLMs\u7684\u8ba1\u7b97\u80fd\u529b\u4e0eClarion\u7684\u5fc3\u7406\u7ec6\u81f4\u76f8\u878d\u5408\uff0c\u4e3a\u8ba4\u77e5\u79d1\u5b66\u9886\u57df\u5e26\u6765\u4e86\u65b0\u7684\u53d1\u5c55\u548c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u5728\u672c\u6587\u4e2d\uff0c\u4f5c\u8005\u8ba8\u8bba\u4e86\u5c06Clarion\u8ba4\u77e5\u67b6\u6784\u4e0eLLMs\u7ed3\u5408\u7684\u60c5\u51b5\u7814\u7a76\uff0c\u5e76\u5229\u7528Clarion\u7684\u5185\u9690-\u663e\u6027\u4e8c\u5206\u6cd5\uff0c\u5b9e\u73b0\u4e86Clarion\u548cLLMs\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5c06LLMs\u7684\u8ba1\u7b97\u80fd\u529b\u4e0eClarion\u7684\u5fc3\u7406\u7ec6\u81f4\u7ed3\u5408\u5728\u4e00\u8d77\u3002"}}
{"id": "2509.11026", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11026", "abs": "https://arxiv.org/abs/2509.11026", "authors": ["Ziang Li", "Manasi Ganti", "Zixian Ma", "Helena Vasconcelos", "Qijia He", "Ranjay Krishna"], "title": "Rethinking Human Preference Evaluation of LLM Rationales", "comment": "Published in the XLLM-Reason-Plan Workshop on the Application of LLM\n  Explainability to Reasoning and Planning at COLM 2025", "summary": "Large language models (LLMs) often generate natural language rationales --\nfree-form explanations that help improve performance on complex reasoning tasks\nand enhance interpretability for human users. However, evaluating these\nrationales remains challenging. While recent work has relied on binary\npreference judgments from humans or LLM judges, such evaluations are often\nopaque and coarse-grained, offering limited insight into what makes one\nrationale better than another. In this work, we rethink preference evaluation\nfor LLM-generated rationales by asking: (1) What attributes define good\nrationales? (2) Can human preferences be explained by these attributes? (3) Can\nattribute-based evaluation overcome the limitations of binary comparisons? We\nidentify a set of key rationale attributes from prior literature and assess\nthem using automatic metrics, LLM judgments, and human annotations. We then\nanalyze two standard human preference datasets MT Bench and Chatbot Arena using\nSHAP to identify which attributes best explain human preference outcomes.\nFinally, we re-evaluate model-generated rationales using attribute-specific ELO\nscores, revealing more nuanced model comparisons and insights. Our findings\nsuggest that fine-grained attribute evaluations can better characterize\nrationale quality and guide future research toward more interpretable and\nreliable evaluation practices.", "AI": {"tldr": "\u7814\u7a76\u91cd\u65b0\u601d\u8003\u4e86LLM\u751f\u6210\u7684\u7406\u6027\u7684\u504f\u597d\u8bc4\u4f30\uff0c\u901a\u8fc7\u5c5e\u6027\u5b9a\u4e49\u597d\u7406\u6027\u3001\u89e3\u91ca\u4eba\u7c7b\u504f\u597d\u4e0e\u5c5e\u6027\u7684\u5173\u7cfb\uff0c\u4f7f\u7528ELO\u5206\u6570\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u7684\u7406\u6027\uff0c\u53d1\u73b0\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8bc4\u4ef7\u53ef\u4ee5\u66f4\u597d\u5730\u8868\u5f81\u7406\u6027\u8d28\u91cf\uff0c\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u91cd\u65b0\u8003\u8651LLM\u751f\u6210\u7684\u7406\u6027\u7684\u6e34\u671b\uff1b\u8bc6\u522b\u597d\u7406\u6027\u7684\u5c5e\u6027\uff1b\u89e3\u91ca\u4eba\u7c7b\u504f\u597d\u4e0e\u8fd9\u4e9b\u5c5e\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\uff1b\u514b\u670d\u4e8c\u5143\u6bd4\u8f83\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5ea6\u91cf\u3001LLM\u8bc4\u5224\u548c\u4eba\u7c7b\u6ce8\u91ca\uff0c\u8bc6\u522b\u5173\u952e\u7406\u6027\u5c5e\u6027\u5e76\u8bc4\u4f30\u5b83\u4eec\u3002\u4f7f\u7528SHAP\u5206\u6790MT Bench\u548cChatbot Arena\u4e24\u4e2a\u6807\u51c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\uff0c\u4ee5\u786e\u5b9a\u54ea\u4e9b\u5c5e\u6027\u6700\u80fd\u89e3\u91ca\u4eba\u7c7b\u504f\u597d\u7ed3\u679c\u3002\u6700\u540e\uff0c\u4f7f\u7528\u7279\u5b9a\u5c5e\u6027\u7684ELO\u5206\u6570\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u7684\u7406\u6027\uff0c\u63ed\u793a\u66f4\u4e3a\u7ec6\u81f4\u7684\u6a21\u578b\u6bd4\u8f83\u548c\u89c1\u89e3\u3002", "result": "\u53d1\u73b0\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8bc4\u4ef7\u53ef\u4ee5\u66f4\u597d\u5730\u8868\u5f81\u7406\u6027\u8d28\u91cf\uff0c\u6307\u5bfc\u672a\u6765\u7814\u7a76\u671d\u7740\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u7684\u8bc4\u4f30\u5b9e\u8df5\u53d1\u5c55\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8bc4\u4ef7\u53ef\u4ee5\u66f4\u597d\u5730\u8868\u5f81\u7406\u6027\u8d28\u91cf\uff0c\u5f15\u5bfc\u672a\u6765\u7814\u7a76\u671d\u7740\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u7684\u8bc4\u4f30\u5b9e\u8df5\u53d1\u5c55\u3002"}}
{"id": "2509.11035", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11035", "abs": "https://arxiv.org/abs/2509.11035", "authors": ["Yu Cui", "Hang Fu", "Haibin Zhang", "Licheng Wang", "Cong Zuo"], "title": "Free-MAD: Consensus-Free Multi-Agent Debate", "comment": null, "summary": "Multi-agent debate (MAD) is an emerging approach to improving the reasoning\ncapabilities of large language models (LLMs). Existing MAD methods rely on\nmultiple rounds of interaction among agents to reach consensus, and the final\noutput is selected by majority voting in the last round. However, this\nconsensus-based design faces several limitations. First, multiple rounds of\ncommunication increases token overhead and limits scalability. Second, due to\nthe inherent conformity of LLMs, agents that initially produce correct\nresponses may be influenced by incorrect ones during the debate process,\ncausing error propagation. Third, majority voting introduces randomness and\nunfairness in the decision-making phase, and can degrade the reasoning\nperformance.\n  To address these issues, we propose \\textsc{Free-MAD}, a novel MAD framework\nthat eliminates the need for consensus among agents. \\textsc{Free-MAD}\nintroduces a novel score-based decision mechanism that evaluates the entire\ndebate trajectory rather than relying on the last round only. This mechanism\ntracks how each agent's reasoning evolves, enabling more accurate and fair\noutcomes. In addition, \\textsc{Free-MAD} reconstructs the debate phase by\nintroducing anti-conformity, a mechanism that enables agents to mitigate\nexcessive influence from the majority. Experiments on eight benchmark datasets\ndemonstrate that \\textsc{Free-MAD} significantly improves reasoning performance\nwhile requiring only a single-round debate and thus reducing token costs. We\nalso show that compared to existing MAD approaches, \\textsc{Free-MAD} exhibits\nimproved robustness in real-world attack scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u6846\u67b6\u201cFree-MAD\u201d\uff0c\u6d88\u9664\u4e86\u5171\u8bc6\u9700\u6c42\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u5f97\u5206\u7684\u51b3\u7b56\u673a\u5236\uff0c\u5e76\u91cd\u6784\u4e86\u8fa9\u8bba\u9636\u6bb5\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u201cFree-MAD\u201d\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u4ee3\u5e01\u6210\u672c\uff0c\u540c\u65f6\u5728\u73b0\u5b9e\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u52a0\u9c81\u68d2\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u65b9\u6cd5\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\uff0c\u5305\u62ec\u591a\u8f6e\u901a\u4fe1\u589e\u52a0\u4ee3\u5e01\u6210\u672c\u548c\u9650\u5236\u53ef\u6269\u5c55\u6027\u3001\u7531\u4e8eLLMs\u7684\u5185\u5728\u4e00\u81f4\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u667a\u80fd\u4f53\u5728\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u53d7\u5230\u9519\u8bef\u7b54\u6848\u7684\u5f71\u54cd\u7b49\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201cFree-MAD\u201d\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u201cFree-MAD\u201d\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u5f97\u5206\u7684\u51b3\u7b56\u673a\u5236\uff0c\u8ddf\u8e2a\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u63a8\u7406\u6f14\u53d8\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u516c\u5e73\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u201cFree-MAD\u201d\u8fd8\u901a\u8fc7\u5f15\u5165\u6297\u4e00\u81f4\u6027\u673a\u5236\u91cd\u6784\u4e86\u8fa9\u8bba\u9636\u6bb5\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u51cf\u5c11\u6765\u81ea\u591a\u6570\u7684\u8fc7\u5ea6\u5f71\u54cd\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u201cFree-MAD\u201d\u5728\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4ee3\u5e01\u6210\u672c\u3002\u4e0e\u73b0\u6709\u7684MAD\u65b9\u6cd5\u76f8\u6bd4\uff0c\u201cFree-MAD\u201d\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u6846\u67b6\u201cFree-MAD\u201d\uff0c\u8be5\u6846\u67b6\u6d88\u9664\u4e86\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5171\u8bc6\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u8bc4\u4f30\u6574\u4e2a\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u63a8\u7406\u53d1\u5c55\u7684\u5f97\u5206\u6765\u8fdb\u884c\u51b3\u7b56\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u201cFree-MAD\u201d\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u4ee3\u5e01\u6210\u672c\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.11067", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11067", "abs": "https://arxiv.org/abs/2509.11067", "authors": ["Liangxuan Guo", "Bin Zhu", "Qingqian Tao", "Kangning Liu", "Xun Zhao", "Xianzhe Qin", "Jin Gao", "Guangfu Hao"], "title": "Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration", "comment": null, "summary": "Autonomous agents for desktop automation struggle with complex multi-step\ntasks due to poor coordination and inadequate quality control. We introduce\n\\textsc{Agentic Lybic}, a novel multi-agent system where the entire\narchitecture operates as a finite-state machine (FSM). This core innovation\nenables dynamic orchestration. Our system comprises four components: a\nController, a Manager, three Workers (Technician for code-based operations,\nOperator for GUI interactions, and Analyst for decision support), and an\nEvaluator. The critical mechanism is the FSM-based routing between these\ncomponents, which provides flexibility and generalization by dynamically\nselecting the optimal execution strategy for each subtask. This principled\norchestration, combined with robust quality gating, enables adaptive replanning\nand error recovery. Evaluated officially on the OSWorld benchmark,\n\\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\\% success rate in 50\nsteps, substantially outperforming existing methods. Results demonstrate that\nprincipled multi-agent orchestration with continuous quality control provides\nsuperior reliability for generalized desktop automation in complex computing\nenvironments.", "AI": {"tldr": "Agentic Lybic\u662f\u4e00\u4e2a\u57fa\u4e8eFSM\u7684\u591aAgent\u7cfb\u7edf\uff0c\u901a\u8fc7\u5408\u7406\u7f16\u6392\u548c\u8d28\u91cf\u63a7\u5236\u5b9e\u73b0\u684c\u9762\u81ea\u52a8\u5316\u4efb\u52a1\u7684\u9ad8\u6210\u529f\u7387\u3002\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u4e8657.07%\u7684\u6210\u529f\u7387\u3002\u8be5\u7cfb\u7edf\u5177\u6709\u7075\u6d3b\u6027\u3001\u6cdb\u5316\u6027\u5e76\u652f\u6301\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u548c\u9519\u8bef\u6062\u590d\u3002", "motivation": "\u684c\u9762\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u591a\u6b65\u9aa4\u5bfc\u81f4\u4e86\u534f\u8c03\u56f0\u96be\u548c\u8d28\u91cf\u63a7\u5236\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5f15\u5165\u4e86Agentic Lybic\u7cfb\u7edf\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002\u7cfb\u7edf\u91c7\u7528FSM\u67b6\u6784\u5b9e\u73b0\u52a8\u6001\u7f16\u6392\uff0c\u65e8\u5728\u63d0\u9ad8\u8d28\u91cf\u63a7\u5236\u6c34\u5e73\u3002", "method": "\u4ecb\u7ecd\u4e86Agentic Lybic\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eFSM\u7684\u65b0\u578b\u591aAgent\u7cfb\u7edf\uff0c\u53ef\u4ee5\u52a8\u6001\u7f16\u6392\u4efb\u52a1\u3002\u7cfb\u7edf\u5305\u62ec\u63a7\u5236\u5668\u3001\u7ba1\u7406\u5668\u3001\u4e09\u4e2a\u5de5\u4f5c\u8005\uff08\u6280\u672f\u5458\u3001\u64cd\u4f5c\u5458\u548c\u5206\u6790\u5458\uff09\u4ee5\u53ca\u8bc4\u4f30\u5668\u3002\u5173\u952e\u673a\u5236\u662f\u57fa\u4e8eFSM\u7684\u7ec4\u4ef6\u4e4b\u95f4\u8def\u7531\uff0c\u5b9e\u73b0\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u9009\u62e9\u6700\u4f73\u6267\u884c\u7b56\u7565\u3002\u7cfb\u7edf\u91c7\u7528\u8d28\u91cf\u95e8\u63a7\u673a\u5236\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u548c\u9519\u8bef\u6062\u590d\u3002\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentic Lybic\u572850\u4e2a\u6b65\u9aa4\u4e2d\u53d6\u5f97\u4e8657.07%\u7684\u6210\u529f\u7387\u3002", "result": "Agentic Lybic\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8657.07%\u7684\u6210\u529f\u7387\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u539f\u5219\u7684\u591aAgent\u7f16\u6392\u7ed3\u5408\u6301\u7eed\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u5728\u590d\u6742\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u684c\u9762\u81ea\u52a8\u5316\u5177\u6709\u5353\u8d8a\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u5728\u590d\u6742\u7684\u684c\u9762\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\uff0c\u91c7\u7528\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u7684Agentic Lybic\u591aAgent\u7cfb\u7edf\u53d6\u5f97\u4e86\u6210\u529f\u3002\u7cfb\u7edf\u5177\u6709\u56db\u4e2a\u7ec4\u4ef6\uff1a\u63a7\u5236\u5668\u3001\u7ba1\u7406\u5668\u3001\u4e09\u4e2a\u5de5\u4f5c\u8005\uff08\u6280\u672f\u5458\u7528\u4e8e\u57fa\u4e8e\u4ee3\u7801\u7684\u64cd\u4f5c\u3001\u64cd\u4f5c\u5458\u7528\u4e8eGUI\u4ea4\u4e92\u548c\u5206\u6790\u5458\u7528\u4e8e\u51b3\u7b56\u652f\u6301\uff09\u3001\u4ee5\u53ca\u8bc4\u4f30\u5668\u3002\u7cfb\u7edf\u7684\u5173\u952e\u673a\u5236\u662f\u8fd9\u4e9b\u7ec4\u4ef6\u4e4b\u95f4\u57fa\u4e8eFSM\u7684\u8def\u7531\uff0c\u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u52a8\u6001\u9009\u62e9\u6700\u4f73\u6267\u884c\u7b56\u7565\u3002\u8fd9\u79cd\u5408\u7406\u7684\u7f16\u6392\u7ed3\u5408\u4e86\u5f3a\u5927\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u548c\u9519\u8bef\u6062\u590d\u3002\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentic Lybic\u572850\u4e2a\u6b65\u9aa4\u4e2d\u53d6\u5f97\u4e8657.07%\u7684\u6210\u529f\u7387\uff0c\u8fdc\u8fdc\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u539f\u5219\u7684\u591aAgent\u7f16\u6392\u7ed3\u5408\u6301\u7eed\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u5728\u590d\u6742\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u684c\u9762\u81ea\u52a8\u5316\u5177\u6709\u5353\u8d8a\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.11068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11068", "abs": "https://arxiv.org/abs/2509.11068", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability", "comment": null, "summary": "The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,\nmulti-agent systems. This introduces a fundamental challenge in establishing\ncomputational trust, specifically how one agent can verify that another's\noutput was genuinely produced by a claimed LLM, and not falsified or generated\nby a cheaper or inferior model. To address this challenge, this paper proposes\na verification framework that achieves tractable asymmetric effort, where the\ncost to verify a computation is substantially lower than the cost to perform\nit. Our approach is built upon the principle of deterministic replicability, a\nproperty inherent to autoregressive models that strictly necessitates a\ncomputationally homogeneous environment where all agents operate on identical\nhardware and software stacks. Within this defined context, our framework\nenables multiple validators to probabilistically audit small, random segments\nof an LLM's output and it distributes the verification workload effectively.\nThe simulations demonstrated that targeted verification can be over 12 times\nfaster than full regeneration, with tunable parameters to adjust the detection\nprobability. By establishing a tractable mechanism for auditable LLM systems,\nour work offers a foundational layer for responsible AI and serves as a\ncornerstone for future research into the more complex, heterogeneous\nmulti-agent systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u53ef\u590d\u5236\u6027\u539f\u5219\uff0c\u5b9e\u73b0\u4e86\u53ef\u9a8c\u8bc1\u6027\u3002\u76ee\u6807\u9a8c\u8bc1\u6bd4\u5b8c\u5168\u518d\u751f\u5feb12\u500d\u4ee5\u4e0a\uff0c\u5177\u6709\u53ef\u8c03\u53c2\u6570\u4ee5\u8c03\u6574\u68c0\u6d4b\u6982\u7387\u3002\u8be5\u7814\u7a76\u4e3a\u8d1f\u8d23\u4efb\u7684AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e5f\u4e3a\u672a\u6765\u7814\u7a76\u66f4\u590d\u6742\u7684\u591aAgent\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u77f3\u3002", "motivation": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u5411\u52a8\u6001\u3001\u591aAgent\u7cfb\u7edf\u6f14\u53d8\u7684\u80cc\u666f\u4e0b\uff0c\u786e\u4fdd\u8ba1\u7b97\u7684\u53ef\u4fe1\u4efb\u6027\u6210\u4e3a\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u786e\u4fdd\u4e00\u4e2aAgent\u53ef\u4ee5\u9a8c\u8bc1\u53e6\u4e00\u4e2a\u4ee3\u7406\u8f93\u51fa\u7684\u8ba1\u7b97\u662f\u7531\u58f0\u79f0\u7684LLM\u771f\u5b9e\u751f\u6210\u7684\uff0c\u800c\u4e0d\u662f\u7531\u6210\u672c\u8f83\u4f4e\u6216\u6027\u80fd\u8f83\u5dee\u7684\u6a21\u578b\u751f\u6210\u7684\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u65b9\u6cd5\u5efa\u7acb\u5728\u786e\u5b9a\u6027\u53ef\u590d\u5236\u6027\u539f\u5219\u4e4b\u4e0a\uff0c\u901a\u8fc7\u591a\u4e2a\u9a8c\u8bc1\u8005\u901a\u8fc7\u968f\u673a\u5ba1\u8ba1\u5c0f\u6bb5LLM\u8f93\u51fa\u6765\u6709\u6548\u5206\u914d\u9a8c\u8bc1\u5de5\u4f5c\u91cf\u3002\u5728\u8ba1\u7b97\u4ee3\u4ef7\u8f83\u4f4e\u4e8e\u6267\u884c\u4ee3\u4ef7\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u52aa\u529b\u4e0d\u5bf9\u79f0\u6027\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u76ee\u6807\u9a8c\u8bc1\u6bd4\u5b8c\u5168\u518d\u751f\u5feb12\u500d\u4ee5\u4e0a\uff0c\u5177\u6709\u53ef\u8c03\u53c2\u6570\u4ee5\u8c03\u6574\u68c0\u6d4b\u6982\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9a8c\u8bc1\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u53ef\u9a8c\u8bc1\u6027\uff0c\u5e76\u4e14\u6f14\u793a\u4e86\u76ee\u6807\u9a8c\u8bc1\u6bd4\u5b8c\u5168\u518d\u751f\u5feb12\u500d\u4ee5\u4e0a\u7684\u6548\u679c\u3002\u901a\u8fc7\u5efa\u7acb\u53ef\u5ba1\u6838\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\uff0c\u4e3a\u8d1f\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u66f4\u590d\u6742\u3001\u5f02\u6784\u7684\u591aAgent\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u77f3\u3002"}}
{"id": "2509.11078", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11078", "abs": "https://arxiv.org/abs/2509.11078", "authors": ["Yunghwei Lai", "Weizhi Ma", "Yang Liu"], "title": "Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation", "comment": null, "summary": "Synthetic data generation using large language models (LLMs) has emerged as a\npromising solution across various domains, particularly in medical field, to\nmitigate data collection challenges. However, existing studies mainly utilize\nLLMs to rewrite and complete existing medical records, where the limitations in\ndata privacy, accuracy, and diversity sill exist, and additionally lack the\nability to interact like real patients. To address these issues, we propose a\nrealistic patient generation framework, Patient-Zero, which requires no real\nmedical records. Patient-Zero first introduces a medically-aligned multi-step\ngeneration architecture, which builds comprehensive patient records through\nhierarchical medical knowledge injection without real medical records. Then, to\noptimize the virtual patient's interaction abilities with humans, Patient-Zero\ndesigns a dynamic updating mechanism to improve the consistency and\nconversational performance. Our framework enables the generation of\ncontextually diverse patient records while maintaining strict medical\ncoherence, supported by adaptive dialogue strategies and real-time clinical\nplausibility verification. Experimental results demonstrate that our model\nachieves good performance in accuracy, diversity, and consistency. After\ntraining with our generated virtual patients, existing models show significant\nimprovements on the MedQA dataset.", "AI": {"tldr": "\u6b64\u8bba\u6587\u63d0\u51fa\u4e86Patient-Zero\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u60a3\u8005\u8bb0\u5f55\uff0c\u4ee5\u89e3\u51b3\u5229\u7528LLMs\u751f\u6210\u5408\u6210\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u4e2d\u5b58\u5728\u7684\u95ee\u9898\u3002\u901a\u8fc7\u591a\u6b65\u751f\u6210\u67b6\u6784\u548c\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u865a\u62df\u60a3\u8005\u7684\u4e00\u81f4\u6027\u548c\u5bf9\u8bdd\u8868\u73b0\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u8bba\u6587\u9488\u5bf9\u73b0\u6709\u5229\u7528LLMs\u751f\u6210\u5408\u6210\u6570\u636e\u5b58\u5728\u7684\u9690\u79c1\u3001\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u7b49\u95ee\u9898\uff0c\u4ee5\u53ca\u865a\u62df\u60a3\u8005\u4e92\u52a8\u80fd\u529b\u4e0d\u8db3\u7684\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u6784\u5efa\u903c\u771f\u60a3\u8005\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u89e3\u51b3\u533b\u5b66\u9886\u57df\u6570\u636e\u6536\u96c6\u7684\u6311\u6218\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u8be5\u8bba\u6587\u91c7\u7528\u4e86\u5177\u6709\u533b\u5b66\u77e5\u8bc6\u6ce8\u5165\u7684\u591a\u6b65\u751f\u6210\u67b6\u6784\u548c\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff0c\u8bbe\u8ba1\u4e86Patient-Zero\u6846\u67b6\u6765\u751f\u6210\u903c\u771f\u60a3\u8005\uff0c\u5e76\u901a\u8fc7\u5b9e\u65f6\u4e34\u5e8a\u53ef\u4fe1\u6027\u9a8c\u8bc1\u548c\u81ea\u9002\u5e94\u5bf9\u8bdd\u7b56\u7565\u6765\u63d0\u9ad8\u865a\u62df\u60a3\u8005\u7684\u4e00\u81f4\u6027\u548c\u5bf9\u8bdd\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u751f\u6210\u7684\u865a\u62df\u60a3\u8005\u8bb0\u5f55\u5177\u6709\u4e25\u683c\u7684\u533b\u5b66\u8fde\u8d2f\u6027\uff0c\u4e14\u7ecf\u8fc7\u8bad\u7ec3\u540e\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPatient-Zero\u7684\u903c\u771f\u60a3\u8005\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u4e2d\u5b58\u5728\u7684\u9690\u79c1\u3001\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u7b49\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u533b\u5b66\u5bf9\u9f50\u7684\u591a\u6b65\u751f\u6210\u67b6\u6784\u548c\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u5177\u6709\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u7684\u60a3\u8005\u8bb0\u5f55\uff0c\u5e76\u63d0\u9ad8\u4e86\u865a\u62df\u60a3\u8005\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u901a\u8fc7MedQA\u6570\u636e\u96c6\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11079", "abs": "https://arxiv.org/abs/2509.11079", "authors": ["Jinwei Su", "Yinghui Xia", "Qizhen Lan", "Xinyuan Song", "Yang Jingsong", "Lewei He", "Tianyu Shi"], "title": "Difficulty-Aware Agent Orchestration in LLM-Powered Workflows", "comment": null, "summary": "Large Language Model (LLM)-based agentic systems have shown strong\ncapabilities across various tasks. However, existing multi-agent frameworks\noften rely on static or task-level workflows, which either over-process simple\nqueries or underperform on complex ones, while also neglecting the\nefficiency-performance trade-offs across heterogeneous LLMs. To address these\nlimitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a\ndynamic framework that adapts workflow depth, operator selection, and LLM\nassignment based on the difficulty of each input query. DAAO comprises three\ninterdependent modules: a variational autoencoder (VAE) for difficulty\nestimation, a modular operator allocator, and a cost- and performance-aware LLM\nrouter. By leveraging heterogeneous LLMs and dynamically tailoring workflows,\nDAAO enables fine-grained, query-specific reasoning strategies. DAAO\noutperforms prior multi-agent systems in both accuracy and inference efficiency\nacross six benchmarks. We will release our code and implementation details upon\npublication.", "AI": {"tldr": "Large Language Model (LLM)-based agentic systems face challenges with existing multi-agent frameworks that often struggle with workflow adaptations based on query difficulty. To overcome these limitations, the paper introduces Difficulty-Aware Agentic Orchestration (DAAO), a dynamic framework that tailors workflows based on query difficulty. DAAO outperforms existing systems in accuracy and efficiency across multiple benchmarks.", "motivation": "Existing multi-agent frameworks often struggle with static or task-level workflows, either over-processing simple queries or underperforming on complex ones. They also neglect efficiency-performance trade-offs across heterogeneous LLMs. The motivation behind this paper is to address these limitations and introduce a dynamic framework that can adapt to the difficulty of input queries for improved performance and efficiency.", "method": "The paper proposes a dynamic framework called Difficulty-Aware Agentic Orchestration (DAAO) that adapts workflow depth, operator selection, and LLM assignment based on the difficulty of each input query. DAAO includes a variational autoencoder (VAE) for difficulty estimation, a modular operator allocator, and a cost- and performance-aware LLM router. It leverages heterogeneous LLMs and tailors workflows dynamically for fine-grained, query-specific reasoning strategies.", "result": "The proposed Difficulty-Aware Agentic Orchestration (DAAO) framework performs better in accuracy and inference efficiency compared to previous multi-agent systems across six benchmarks.", "conclusion": "Difficulty-Aware Agentic Orchestration (DAAO) outperforms prior multi-agent systems in both accuracy and inference efficiency across six benchmarks. The paper will release the code and implementation details upon publication."}}
{"id": "2509.11131", "categories": ["cs.AI", "cs.MA", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2509.11131", "abs": "https://arxiv.org/abs/2509.11131", "authors": ["Benedikt Hartl", "Michael Levin", "L\u00e9o Pio-Lopez"], "title": "Neural cellular automata: applications to biology and beyond classical AI", "comment": null, "summary": "Neural Cellular Automata (NCA) represent a powerful framework for modeling\nbiological self-organization, extending classical rule-based systems with\ntrainable, differentiable (or evolvable) update rules that capture the adaptive\nself-regulatory dynamics of living matter. By embedding Artificial Neural\nNetworks (ANNs) as local decision-making centers and interaction rules between\nlocalized agents, NCA can simulate processes across molecular, cellular,\ntissue, and system-level scales, offering a multiscale competency architecture\nperspective on evolution, development, regeneration, aging, morphogenesis, and\nrobotic control. These models not only reproduce biologically inspired target\npatterns but also generalize to novel conditions, demonstrating robustness to\nperturbations and the capacity for open-ended adaptation and reasoning. Given\ntheir immense success in recent developments, we here review current literature\nof NCAs that are relevant primarily for biological or bioengineering\napplications. Moreover, we emphasize that beyond biology, NCAs display robust\nand generalizing goal-directed dynamics without centralized control, e.g., in\ncontrolling or regenerating composite robotic morphologies or even on\ncutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same\nprinciples of iterative state-refinement is reminiscent to modern generative\nArtificial Intelligence (AI), such as probabilistic diffusion models. Their\ngoverning self-regulatory behavior is constraint to fully localized\ninteractions, yet their collective behavior scales into coordinated\nsystem-level outcomes. We thus argue that NCAs constitute a unifying\ncomputationally lean paradigm that not only bridges fundamental insights from\nmultiscale biology with modern generative AI, but have the potential to design\ntruly bio-inspired collective intelligence capable of hierarchical reasoning\nand control.", "AI": {"tldr": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u7ed3\u5408\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u548c\u533a\u57df\u5316\u4ee3\u7406\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u89c4\u5219\uff0c\u80fd\u591f\u6a21\u62df\u751f\u7269\u81ea\u7ec4\u7ec7\u8fc7\u7a0b\uff0c\u5177\u6709\u591a\u5c3a\u5ea6\u80fd\u529b\u67b6\u6784\u89c6\u89d2\uff0c\u5c55\u73b0\u4e86\u9c81\u68d2\u6027\u548c\u5f00\u653e\u5f0f\u9002\u5e94\u80fd\u529b\u3002\u6b64\u6a21\u578b\u5728\u751f\u7269\u5b66\u548c\u751f\u7269\u5de5\u7a0b\u5e94\u7528\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u5728\u63a7\u5236\u3001\u91cd\u5851\u673a\u5668\u4eba\u5f62\u6001\u548c\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u8fde\u63a5\u591a\u5c3a\u5ea6\u751f\u7269\u5b66\u548c\u73b0\u4ee3\u751f\u6210\u4eba\u5de5\u667a\u80fd\u7684\u6f5c\u529b\u3002", "motivation": "\u9488\u5bf9\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u5728\u751f\u7269\u6216\u751f\u7269\u5de5\u7a0b\u5e94\u7528\u4e2d\u53d6\u5f97\u7684\u5de8\u5927\u6210\u529f\uff0c\u672c\u6587\u5bf9\u5176\u76f8\u5173\u6587\u732e\u8fdb\u884c\u4e86\u8bc4\u8ff0\u3002\u540c\u65f6\u5f3a\u8c03\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u5728\u63a7\u5236\u3001\u91cd\u5851\u590d\u5408\u673a\u5668\u4eba\u5f62\u6001\u7b49\u76ee\u6807\u5bfc\u5411\u52a8\u6001\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u5728\u8bf8\u5982 ARC-AGI-1 \u7b49\u5c16\u7aef\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u4f18\u79c0\u8868\u73b0\u3002", "method": "\u5c06\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u5230\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u4e2d\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u3001\u53ef\u5fae\u5206\uff08\u6216\u53ef\u8fdb\u5316\uff09\u7684\u66f4\u65b0\u89c4\u5219\u6355\u6349\u751f\u7269\u9002\u5e94\u6027\u81ea\u8c03\u8282\u52a8\u6001\u3002\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u6a21\u62df\u8fc7\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u9c81\u68d2\u6027\u548c\u5f00\u653e\u5f0f\u9002\u5e94\u80fd\u529b\u3002", "result": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u5c55\u793a\u4e86\u591a\u5c3a\u5ea6\u6a21\u62df\u751f\u7269\u5b66\u548c\u73b0\u4ee3\u751f\u6210\u4eba\u5de5\u667a\u80fd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u5177\u6709\u8bbe\u8ba1\u5177\u6709\u5c42\u6b21\u63a8\u7406\u548c\u63a7\u5236\u80fd\u529b\u7684\u771f\u6b63\u751f\u7269\u542f\u53d1\u5f0f\u96c6\u4f53\u667a\u80fd\u7684\u6f5c\u529b\u3002", "conclusion": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u6a21\u62df\u751f\u7269\u81ea\u7ec4\u7ec7\u8fc7\u7a0b\uff0c\u5177\u6709\u81ea\u9002\u5e94\u81ea\u8c03\u8282\u52a8\u6001\u3002\u5b83\u7ed3\u5408\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u672c\u5730\u51b3\u7b56\u4e2d\u5fc3\u548c\u533a\u57df\u5316\u4ee3\u7406\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u89c4\u5219\uff0c\u80fd\u591f\u5728\u5206\u5b50\u3001\u7ec6\u80de\u3001\u7ec4\u7ec7\u548c\u7cfb\u7edf\u7ea7\u522b\u6a21\u62df\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u4e86\u591a\u5c3a\u5ea6\u7684\u80fd\u529b\u67b6\u6784\u89c6\u89d2\uff0c\u5bf9\u8fdb\u5316\u3001\u53d1\u80b2\u3001\u518d\u751f\u3001\u8870\u8001\u3001\u5f62\u6001\u751f\u6210\u548c\u673a\u5668\u4eba\u63a7\u5236\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.11135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11135", "abs": "https://arxiv.org/abs/2509.11135", "authors": ["Jing Xiao", "Chang You", "Zhiyu Chen"], "title": "AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment", "comment": null, "summary": "Knowledge Tracing (KT) serves as a fundamental component of Intelligent\nTutoring Systems (ITS), enabling these systems to monitor and understand\nlearners' progress by modeling their knowledge state. However, many existing KT\nmodels primarily focus on fitting the sequences of learners' interactions, and\noften overlook the knowledge state itself. This limitation leads to reduced\ninterpretability and insufficient instructional support from the ITS. To\naddress this challenge, we propose AlignKT, which employs a frontend-to-backend\narchitecture to explicitly model a stable knowledge state. In this approach,\nthe preliminary knowledge state is aligned with an additional criterion.\nSpecifically, we define an ideal knowledge state based on pedagogical theories\nas the alignment criterion, providing a foundation for interpretability. We\nutilize five encoders to implement this set-up, and incorporate a contrastive\nlearning module to enhance the robustness of the alignment process. Through\nextensive experiments, AlignKT demonstrates superior performance, outperforming\nseven KT baselines on three real-world datasets. It achieves state-of-the-art\nresults on two of these datasets and exhibits competitive performance on the\nthird. The code of this work is available at\nhttps://github.com/SCNU203/AlignKT.", "AI": {"tldr": "AlignKT\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u524d\u7aef\u5230\u540e\u7aef\u67b6\u6784\u6765\u663e\u5f0f\u5efa\u6a21\u7a33\u5b9a\u77e5\u8bc6\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u5b9a\u4e49\u7406\u60f3\u77e5\u8bc6\u72b6\u6001\u4f5c\u4e3a\u5bf9\u9f50\u6807\u51c6\uff0c\u5229\u7528\u4e94\u4e2a\u7f16\u7801\u5668\u5b9e\u73b0\u6b64\u8bbe\u7f6e\uff0c\u5e76\u878d\u5165\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u589e\u5f3a\u5bf9\u9f50\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0cAlignKT\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u80dc\u8fc7\u4e03\u4e2a\u57fa\u51c6\u6a21\u578b\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u7b2c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u5f80\u5f80\u96c6\u4e2d\u4e8e\u9002\u5e94\u5b66\u4e60\u8005\u4e92\u52a8\u7684\u5e8f\u5217\uff0c\u800c\u5ffd\u7565\u77e5\u8bc6\u72b6\u6001\u672c\u8eab\uff0c\u5bfc\u81f4\u89e3\u91ca\u6027\u964d\u4f4e\u548cITS\u7684\u6307\u5bfc\u652f\u6301\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e86AlignKT\uff0c\u901a\u8fc7\u5bf9\u9f50\u77e5\u8bc6\u72b6\u6001\u548c\u9644\u52a0\u6807\u51c6\u6765\u589e\u52a0\u89e3\u91ca\u6027\u3002", "method": "AlignKT\u91c7\u7528\u524d\u7aef\u5230\u540e\u7aef\u67b6\u6784\uff0c\u660e\u786e\u5efa\u6a21\u7a33\u5b9a\u7684\u77e5\u8bc6\u72b6\u6001\u3002\u5b9a\u4e49\u7406\u60f3\u77e5\u8bc6\u72b6\u6001\u4f5c\u4e3a\u5bf9\u9f50\u6807\u51c6\uff0c\u5229\u7528\u4e94\u4e2a\u7f16\u7801\u5668\u5b9e\u73b0\u6b64\u8bbe\u7f6e\uff0c\u5e76\u878d\u5165\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u589e\u5f3a\u5bf9\u9f50\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0cAlignKT\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u5e76\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u7b2c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4e5f\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002", "conclusion": "AlignKT\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u524d\u7aef\u5230\u540e\u7aef\u67b6\u6784\u6765\u663e\u5f0f\u5efa\u6a21\u7a33\u5b9a\u77e5\u8bc6\u72b6\u6001\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u521d\u6b65\u7684\u77e5\u8bc6\u72b6\u6001\u4e0e\u9644\u52a0\u6807\u51c6\u5bf9\u9f50\uff0c\u5b9a\u4e49\u7406\u60f3\u77e5\u8bc6\u72b6\u6001\u4f5c\u4e3a\u5bf9\u9f50\u6807\u51c6\uff0c\u4ece\u800c\u589e\u52a0\u53ef\u89e3\u91ca\u6027\u3002\u91c7\u7528\u4e94\u4e2a\u7f16\u7801\u5668\u5b9e\u73b0\u6b64\u8bbe\u7f6e\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u4ee5\u589e\u5f3a\u5bf9\u9f50\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cAlignKT\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u80dc\u8fc7\u4e03\u4e2a\u77e5\u8bc6\u8ffd\u8e2a\u57fa\u51c6\u6a21\u578b\u3002\u5728\u5176\u4e2d\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u7b2c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.11151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11151", "abs": "https://arxiv.org/abs/2509.11151", "authors": ["Jianxin Li", "Liang Qu", "Taotao Cai", "Zhixue Zhao", "Nur Al Hasan Haldar", "Aneesh Krishna", "Xiangjie Kong", "Flavio Romero Macau", "Tanmoy Chakraborty", "Aniket Deroy", "Binshan Lin", "Karen Blackmore", "Nasimul Noman", "Jingxian Cheng", "Ningning Cui", "Jianliang Xu"], "title": "AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions", "comment": null, "summary": "Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the\ncapability to generate different forms of content, including text, images,\nvideos, and other modalities, which can achieve a quality similar to content\ncreated by humans. As a result, AIGC is now widely applied across various\ndomains such as digital marketing, education, and public health, and has shown\npromising results by enhancing content creation efficiency and improving\ninformation delivery. However, there are few studies that explore the latest\nprogress and emerging challenges of AIGC across different domains. To bridge\nthis gap, this paper brings together 16 scholars from multiple disciplines to\nprovide a cross-domain perspective on the trends and challenges of AIGC.\nSpecifically, the contributions of this paper are threefold: (1) It first\nprovides a broader overview of AIGC, spanning the training techniques of\nGenerative AI, detection methods, and both the spread and use of AI-generated\ncontent across digital platforms. (2) It then introduces the societal impacts\nof AIGC across diverse domains, along with a review of existing methods\nemployed in these contexts. (3) Finally, it discusses the key technical\nchallenges and presents research propositions to guide future work. Through\nthese contributions, this vision paper seeks to offer readers a cross-domain\nperspective on AIGC, providing insights into its current research trends,\nongoing challenges, and future directions.", "AI": {"tldr": "\u672c\u6587\u753116\u4f4d\u5b66\u8005\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u5728\u4e0d\u540c\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6311\u6218\uff0c\u5305\u62ec\u8bad\u7ec3\u6280\u672f\u3001\u793e\u4f1a\u5f71\u54cd\u548c\u6280\u672f\u6311\u6218\uff0c\u63d0\u51fa\u7814\u7a76\u547d\u9898\u4ee5\u5f15\u5bfc\u672a\u6765\u5de5\u4f5c\u3002", "motivation": "\u9274\u4e8e\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\u5728\u6570\u5b57\u8425\u9500\u3001\u6559\u80b2\u548c\u516c\u5171\u536b\u751f\u7b49\u9886\u57df\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u4e14\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u4e0d\u540c\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6311\u6218\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u4e3aAIGC\u7684\u53d1\u5c55\u63d0\u4f9b\u8de8\u9886\u57df\u7684\u89c6\u89d2\u3002", "method": "\u672c\u6587\u91c7\u7528\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u65b9\u5f0f\uff0c\u753116\u4f4d\u5b66\u8005\u5171\u540c\u64b0\u5199\uff0c\u5206\u6790\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6311\u6218\u3002\u5185\u5bb9\u6db5\u76d6AIGC\u7684\u8bad\u7ec3\u6280\u672f\u3001\u68c0\u6d4b\u65b9\u6cd5\u3001\u793e\u4f1a\u5f71\u54cd\u548c\u6280\u672f\u6311\u6218\uff0c\u63d0\u51fa\u7814\u7a76\u547d\u9898\u4ee5\u5f15\u5bfc\u672a\u6765\u5de5\u4f5c\u3002", "result": "16\u4f4d\u5b66\u8005\u5408\u4f5c\u5b8c\u6210\u4e86\u672c\u6587\uff0c\u63d0\u51fa\u4e86\u5bf9AIGC\u7684\u8de8\u9886\u57df\u89c6\u89d2\uff0c\u63a2\u8ba8\u4e86AIGC\u7684\u8bad\u7ec3\u6280\u672f\u3001\u793e\u4f1a\u5f71\u54cd\u548c\u6280\u672f\u6311\u6218\u3002\u901a\u8fc7\u672c\u6587\uff0c\u8bfb\u8005\u53ef\u4ee5\u4e86\u89e3AIGC\u7684\u7814\u7a76\u8d8b\u52bf\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u901a\u8fc716\u4f4d\u5b66\u8005\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u5168\u9762\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u5728\u4e0d\u540c\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6311\u6218\u3002\u8ba8\u8bba\u4e86AIGC\u7684\u8bad\u7ec3\u6280\u672f\u3001\u68c0\u6d4b\u65b9\u6cd5\u3001\u793e\u4f1a\u5f71\u54cd\u4ee5\u53ca\u6280\u672f\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u65e8\u5728\u4e3a\u8bfb\u8005\u63d0\u4f9bAIGC\u7684\u8de8\u9886\u57df\u89c6\u89d2\uff0c\u6df1\u5165\u4e86\u89e3\u5f53\u524d\u7814\u7a76\u8d8b\u52bf\u3001\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2509.11253", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11253", "abs": "https://arxiv.org/abs/2509.11253", "authors": ["Xiao Liang", "Bangxin Li", "Zixuan Chen", "Hanyue Zheng", "Zhi Ma", "Di Wang", "Cong Tian", "Quan Wang"], "title": "VideoAgent: Personalized Synthesis of Scientific Videos", "comment": null, "summary": "Automating the generation of scientific videos is a crucial yet challenging\ntask for effective knowledge dissemination. However, existing works on document\nautomation primarily focus on static media such as posters and slides, lacking\nmechanisms for personalized dynamic orchestration and multimodal content\nsynchronization. To address these challenges, we introduce VideoAgent, a novel\nmulti-agent framework that synthesizes personalized scientific videos through a\nconversational interface. VideoAgent parses a source paper into a fine-grained\nasset library and, guided by user requirements, orchestrates a narrative flow\nthat synthesizes both static slides and dynamic animations to explain complex\nconcepts. To enable rigorous evaluation, we also propose SciVidEval, the first\ncomprehensive suite for this task, which combines automated metrics for\nmultimodal content quality and synchronization with a Video-Quiz-based human\nevaluation to measure knowledge transfer. Extensive experiments demonstrate\nthat our method significantly outperforms existing commercial scientific video\ngeneration services and approaches human-level quality in scientific\ncommunication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86VideoAgent\u591aAgent\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u4e2a\u6027\u5316\u79d1\u5b66\u89c6\u9891\uff0c\u5e76\u63d0\u51fa\u4e86SciVidEval\u8bc4\u4f30\u5957\u4ef6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u79d1\u5b66\u4ea4\u6d41\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u6548\u679c\u3002", "motivation": "\u81ea\u52a8\u5316\u751f\u6210\u79d1\u5b66\u89c6\u9891\u5bf9\u4e8e\u6709\u6548\u4f20\u64ad\u77e5\u8bc6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u73b0\u6709\u7684\u6587\u6863\u81ea\u52a8\u5316\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u9759\u6001\u5a92\u4f53\u4e0a\uff0c\u7f3a\u4e4f\u4e2a\u6027\u5316\u52a8\u6001\u7f16\u6392\u548c\u591a\u6a21\u6001\u5185\u5bb9\u540c\u6b65\u7684\u673a\u5236\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f5c\u8005\u5f15\u5165\u4e86VideoAgent\u6846\u67b6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86VideoAgent\u591aAgent\u6846\u67b6\uff0c\u5c06\u6e90\u8bba\u6587\u89e3\u6790\u4e3a\u7ec6\u7c92\u5ea6\u8d44\u4ea7\u5e93\uff0c\u5e76\u6839\u636e\u7528\u6237\u9700\u6c42\u7f16\u6392\u53d9\u4e8b\u6d41\u7a0b\uff0c\u7efc\u5408\u9759\u6001\u5e7b\u706f\u7247\u548c\u52a8\u6001\u52a8\u753b\u6765\u89e3\u91ca\u590d\u6742\u6982\u5ff5\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51faSciVidEval\u8bc4\u4f30\u5957\u4ef6\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u6307\u6807\u548c\u57fa\u4e8e\u89c6\u9891\u6d4b\u9a8c\u7684\u4eba\u7c7b\u8bc4\u4f30\uff0c\u7528\u4e8e\u8861\u91cf\u77e5\u8bc6\u4f20\u9012\u6548\u679c\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u5546\u4e1a\u79d1\u5b66\u89c6\u9891\u751f\u6210\u670d\u52a1\uff0c\u5e76\u5728\u79d1\u5b66\u4ea4\u6d41\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aVideoAgent\u7684\u65b0\u578b\u591aAgent\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bdd\u754c\u9762\u5408\u6210\u4e2a\u6027\u5316\u79d1\u5b66\u89c6\u9891\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u79d1\u5b66\u4ea4\u6d41\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u5546\u4e1a\u79d1\u5b66\u89c6\u9891\u751f\u6210\u670d\u52a1\u66f4\u4f18\u79c0\u7684\u6027\u80fd\uff0c\u5e76\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u8d28\u91cf\u3002"}}
{"id": "2509.11311", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.11311", "abs": "https://arxiv.org/abs/2509.11311", "authors": ["Bingchen Wang", "Zi-Yu Khoo", "Bryan Kian Hsiang Low"], "title": "Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble", "comment": "Preprint of work originally submitted to AAAI 2026. Under revision\n  for resubmission to a machine learning venue", "summary": "Large language models (LLMs) have demonstrated promise in emulating\nhuman-like responses across a wide range of tasks. In this paper, we propose a\nnovel alignment framework that treats LLMs as agent proxies for human survey\nrespondents, affording a cost-effective and steerable solution to two pressing\nchallenges in the social sciences: the rising cost of survey deployment and the\ngrowing demographic imbalance in survey response data. Drawing inspiration from\nthe theory of revealed preference, we formulate alignment as a two-stage\nproblem: constructing diverse agent personas called endowments that simulate\nplausible respondent profiles, and selecting a representative subset to\napproximate a ground-truth population based on observed data. To implement the\nparadigm, we introduce P2P, a system that steers LLM agents toward\nrepresentative behavioral patterns using structured prompt engineering,\nentropy-based sampling, and regression-based selection. Unlike\npersonalization-heavy approaches, our alignment approach is\ndemographic-agnostic and relies only on aggregate survey results, offering\nbetter generalizability and parsimony. Beyond improving data efficiency in\nsocial science research, our framework offers a testbed for studying the\noperationalization of pluralistic alignment. We demonstrate the efficacy of our\napproach on real-world opinion survey datasets, showing that our aligned agent\npopulations can reproduce aggregate response patterns with high fidelity and\nexhibit substantial response diversity, even without demographic conditioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06LLMs\u89c6\u4e3a\u4eba\u7c7b\u8c03\u67e5\u53d7\u8bbf\u8005\u7684\u4ee3\u7406\uff0c\u901a\u8fc7P2P\u7cfb\u7edf\u5b9e\u73b0\u5bf9LLM\u4ee3\u7406\u7684\u5bf9\u9f50\uff0c\u4ee5\u6a21\u62df\u89c2\u5bdf\u6570\u636e\u57fa\u7840\u4e0a\u7684\u771f\u5b9e\u4eba\u53e3\u3002\u8be5\u65b9\u6cd5\u4e0d\u8003\u8651\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u805a\u5408\u8c03\u67e5\u7ed3\u679c\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6548\u679c\u5e76\u63d0\u4f9b\u4e86\u7814\u7a76\u5e73\u53f0\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u6539\u5584\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u6570\u636e\u6548\u7387\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u591f\u5904\u7406\u52d8\u8bef\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7814\u7a76\u591a\u5143\u5bf9\u9f50\u64cd\u4f5c\u5316\u7684\u5e73\u53f0\u3002", "method": "\u672c\u6587\u5c06\u5bf9\u9f50\u89c6\u4e3a\u4e00\u4e2a\u4e24\u9636\u6bb5\u95ee\u9898\uff1a\u6784\u5efa\u591a\u6837\u5316\u7684\u4ee3\u7406\u4eba\u89d2\u8272\uff08\u79f0\u4e3aendowments\uff09\uff0c\u6a21\u62df\u53ef\u4fe1\u7684\u53d7\u8bbf\u8005\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u9009\u62e9\u4ee3\u8868\u6027\u5b50\u96c6\u4ee5\u8fd1\u4f3c\u57fa\u4e8e\u89c2\u5bdf\u6570\u636e\u7684\u5730\u9762\u771f\u5b9e\u4eba\u53e3\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u8303\u5f0f\uff0c\u5f15\u5165\u4e86P2P\u7cfb\u7edf\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u3001\u57fa\u4e8e\u71b5\u7684\u62bd\u6837\u548c\u57fa\u4e8e\u56de\u5f52\u7684\u9009\u62e9\uff0c\u5f15\u5bfcLLM\u4ee3\u7406\u6a21\u62df\u5177\u6709\u4ee3\u8868\u6027\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u7ecf\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u89c2\u70b9\u8c03\u67e5\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u51fa\u4e86\u826f\u597d\u7684\u6548\u679c\uff0c\u5bf9\u9f50\u4ee3\u7406\u4eba\u53e3\u53ef\u4ee5\u9ad8\u5ea6\u590d\u5236\u805a\u5408\u54cd\u5e94\u6a21\u5f0f\uff0c\u5e76\u8868\u73b0\u51fa\u663e\u8457\u7684\u54cd\u5e94\u591a\u6837\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c6\u4e3a\u4eba\u7c7b\u8c03\u67e5\u53d7\u8bbf\u8005\u7684\u4ee3\u7406\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u4e2d\u4e24\u4e2a\u7d27\u8feb\u6311\u6218\u63d0\u4f9b\u4e86\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u53ef\u63a7\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u5f15\u5165 P2P \u7cfb\u7edf\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u3001\u57fa\u4e8e\u71b5\u7684\u62bd\u6837\u548c\u57fa\u4e8e\u56de\u5f52\u7684\u9009\u62e9\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u5bf9LLM\u4ee3\u7406\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ee5\u6a21\u62df\u5730\u89c2\u5bdf\u6570\u636e\u57fa\u7840\u4e0a\u7684\u771f\u5b9e\u4eba\u53e3\u3002\u4e0e\u4e2a\u6027\u5316\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u5bf9\u9f50\u65b9\u6cd5\u4e0d\u8003\u8651\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u805a\u5408\u8c03\u67e5\u7ed3\u679c\uff0c\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u548c\u7b80\u6d01\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4e3a\u7814\u7a76\u591a\u5143\u5bf9\u9f50\u7684\u64cd\u4f5c\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u5e73\u53f0\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7684\u89c2\u70b9\u8c03\u67e5\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u9f50\u4ee3\u7406\u4eba\u53e3\u80fd\u591f\u4ee5\u9ad8\u5ea6\u5fe0\u5b9e\u5730\u590d\u5236\u805a\u5408\u54cd\u5e94\u6a21\u5f0f\uff0c\u5e76\u5448\u73b0\u51fa\u663e\u8457\u7684\u54cd\u5e94\u591a\u6837\u6027\uff0c\u5373\u4f7f\u6ca1\u6709\u4eba\u53e3\u7edf\u8ba1\u6761\u4ef6\u3002"}}
{"id": "2509.11330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11330", "abs": "https://arxiv.org/abs/2509.11330", "authors": ["Sudeshna Jana", "Manjira Sinha", "Tirthankar Dasgupta"], "title": "Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts", "comment": "11 pages, 6 figures, 4 tables", "summary": "The widespread use of plastics and their persistence in the environment have\nled to the accumulation of micro- and nano-plastics across air, water, and\nsoil, posing serious health risks including respiratory, gastrointestinal, and\nneurological disorders. We propose a novel framework that leverages large\nlanguage models to extract relational metapaths, multi-hop semantic chains\nlinking pollutant sources to health impacts, from scientific abstracts. Our\nsystem identifies and connects entities across diverse contexts to construct\nstructured relational metapaths, which are aggregated into a Toxicity\nTrajectory Graph that traces pollutant propagation through exposure routes and\nbiological systems. Moreover, to ensure consistency and reliability, we\nincorporate a dynamic evidence reconciliation module that resolves semantic\nconflicts arising from evolving or contradictory research findings. Our\napproach demonstrates strong performance in extracting reliable, high-utility\nrelational knowledge from noisy scientific text and offers a scalable solution\nfor mining complex cause-effect structures in domain-specific corpora.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u7cfb\u77e5\u8bc6\u7684\u65b0\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u6c61\u67d3\u7269\u7684\u4f20\u64ad\u56fe\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u8bed\u4e49\u51b2\u7a81\u7684\u8bc1\u636e\u8c03\u548c\u6a21\u5757\u3002\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u9760\u5730\u4ece\u6742\u4e71\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u7cfb\u77e5\u8bc6\uff0c\u5e76\u4e3a\u6316\u6398\u9886\u57df\u7279\u5b9a\u8bed\u6599\u5e93\u4e2d\u590d\u6742\u56e0\u679c\u7ed3\u6784\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5851\u6599\u7684\u5e7f\u6cdb\u4f7f\u7528\u548c\u5728\u73af\u5883\u4e2d\u7684\u6301\u4e45\u5b58\u5728\u5bfc\u81f4\u4e86\u5fae\u89c2\u548c\u7eb3\u7c73\u5851\u6599\u5728\u7a7a\u6c14\u3001\u6c34\u548c\u571f\u58e4\u4e2d\u7684\u79ef\u7d2f\uff0c\u5bf9\u5065\u5eb7\u9020\u6210\u4e25\u91cd\u98ce\u9669\u3002\u4e3a\u4e86\u7406\u89e3\u6c61\u67d3\u6e90\u4e0e\u5065\u5eb7\u5f71\u54cd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u7cfb\u77e5\u8bc6\u7684\u65b0\u65b9\u6cd5\u3002\u7531\u4e8e\u79d1\u5b66\u6587\u672c\u53ef\u80fd\u5b58\u5728\u566a\u97f3\u548c\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u9760\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u6587\u672c\u5e76\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u79d1\u5b66\u6458\u8981\u4e2d\u7684\u5173\u7cfb\u5143\u8def\u5f84\uff0c\u6784\u5efa\u6c61\u67d3\u7269\u4f20\u64ad\u7684\u7ed3\u6784\u5316\u56fe\uff1b\u5f15\u5165\u52a8\u6001\u8bc1\u636e\u8c03\u548c\u6a21\u5757\u89e3\u51b3\u8bed\u4e49\u51b2\u7a81\uff1b\u9a8c\u8bc1\u65b9\u6cd5\u5728\u4ece\u6742\u4e71\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u53ef\u9760\u5173\u7cfb\u77e5\u8bc6\u7684\u6548\u679c\uff1b\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u7528\u4e8e\u6316\u6398\u590d\u6742\u56e0\u679c\u7ed3\u6784\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ece\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u7cfb\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u5f3a\u5927\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u7528\u4e8e\u6316\u6398\u590d\u6742\u56e0\u679c\u7ed3\u6784\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u53d6\u79d1\u5b66\u6458\u8981\u4e2d\u6c61\u67d3\u6e90\u4e0e\u5065\u5eb7\u5f71\u54cd\u4e4b\u95f4\u5173\u7cfb\u7684\u65b0\u6846\u67b6\u3002\u4ed6\u4eec\u6784\u5efa\u4e86\u7ed3\u6784\u5316\u7684\u5173\u7cfb\u5143\u8def\u5f84\uff0c\u8fdb\u800c\u5efa\u7acb\u4e86\u6bd2\u6027\u8f68\u8ff9\u56fe\uff0c\u8ffd\u8e2a\u6c61\u67d3\u7269\u901a\u8fc7\u66b4\u9732\u9014\u5f84\u548c\u751f\u7269\u7cfb\u7edf\u7684\u4f20\u64ad\u3002\u8bba\u6587\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u52a8\u6001\u8bc1\u636e\u8c03\u548c\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u7531\u4e0d\u65ad\u53d1\u5c55\u6216\u77db\u76fe\u7684\u7814\u7a76\u7ed3\u679c\u4ea7\u751f\u7684\u8bed\u4e49\u51b2\u7a81\u3002\u7814\u7a76\u8868\u660e\uff0c\u4ed6\u4eec\u7684\u65b9\u6cd5\u5728\u4ece\u6742\u4e71\u7684\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u53ef\u9760\u3001\u9ad8\u6548\u7684\u5173\u7cfb\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e3a\u6316\u6398\u9886\u57df\u7279\u5b9a\u8bed\u6599\u5e93\u4e2d\u590d\u6742\u56e0\u679c\u7ed3\u6784\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11336", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11336", "abs": "https://arxiv.org/abs/2509.11336", "authors": ["William Farlessyost", "Sebastian Oberst", "Shweta Singh"], "title": "The power of dynamic causality in observer-based design for soft sensor applications", "comment": null, "summary": "This paper introduces a novel framework for optimizing observer-based soft\nsensors through dynamic causality analysis. Traditional approaches to sensor\nselection often rely on linearized observability indices or statistical\ncorrelations that fail to capture the temporal evolution of complex systems. We\naddress this gap by leveraging liquid-time constant (LTC) networks,\ncontinuous-time neural architectures with input-dependent time constants, to\nsystematically identify and prune sensor inputs with minimal causal influence\non state estimation. Our methodology implements an iterative workflow: training\nan LTC observer on candidate inputs, quantifying each input's causal impact\nthrough controlled perturbation analysis, removing inputs with negligible\neffect, and retraining until performance degradation occurs. We demonstrate\nthis approach on three mechanistic testbeds representing distinct physical\ndomains: a harmonically forced spring-mass-damper system, a nonlinear\ncontinuous stirred-tank reactor, and a predator-prey model following the\nstructure of the Lotka-Volterra model, but with seasonal forcing and added\ncomplexity. Results show that our causality-guided pruning consistently\nidentifies minimal sensor sets that align with underlying physics while\nimproving prediction accuracy. The framework automatically distinguishes\nessential physical measurements from noise and determines when derived\ninteraction terms provide complementary versus redundant information. Beyond\ncomputational efficiency, this approach enhances interpretability by grounding\nsensor selection decisions in dynamic causal relationships rather than static\ncorrelations, offering significant benefits for soft sensing applications\nacross process engineering, ecological monitoring, and agricultural domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u56e0\u679c\u5206\u6790\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u57fa\u4e8e\u89c2\u5bdf\u8005\u7684\u8f6f\u4f20\u611f\u5668\u3002\u901a\u8fc7\u6db2\u6001\u65f6\u95f4\u5e38\u6570\uff08LTC\uff09\u7f51\u7edc\uff0c\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u4fee\u526a\u5bf9\u72b6\u6001\u4f30\u8ba1\u5177\u6709\u6700\u5c0f\u56e0\u679c\u5f71\u54cd\u7684\u4f20\u611f\u5668\u8f93\u5165\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u79cd\u56e0\u679c\u5f15\u5bfc\u4fee\u526a\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u7406\u9886\u57df\u7684\u673a\u68b0\u6d4b\u8bd5\u57fa\u7840\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u4f20\u611f\u5668\u9009\u62e9\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7ebf\u6027\u5316\u7684\u53ef\u89c2\u6d4b\u6027\u6307\u6570\u6216\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u672a\u80fd\u6355\u6349\u590d\u6742\u7cfb\u7edf\u7684\u65f6\u95f4\u6f14\u53d8\u3002\u6211\u4eec\u5229\u7528\u6db2\u6001\u65f6\u95f4\u5e38\u6570\uff08LTC\uff09\u7f51\u7edc\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u8fd9\u4e9b\u7f51\u7edc\u662f\u5e26\u6709\u8f93\u5165\u76f8\u5173\u65f6\u95f4\u5e38\u6570\u7684\u8fde\u7eed\u65f6\u95f4\u795e\u7ecf\u7ed3\u6784\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc6\u522b\u548c\u4fee\u526a\u5bf9\u72b6\u6001\u4f30\u8ba1\u5177\u6709\u6700\u5c0f\u56e0\u679c\u5f71\u54cd\u7684\u4f20\u611f\u5668\u8f93\u5165\u3002", "method": "\u8be5\u8bba\u6587\u91c7\u7528\u4e86\u4e00\u4e2a\u8fed\u4ee3\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a\u5728\u5019\u9009\u8f93\u5165\u4e0a\u8bad\u7ec3LTC\u89c2\u5bdf\u5458\uff0c\u901a\u8fc7\u53d7\u63a7\u6270\u52a8\u5206\u6790\u91cf\u5316\u6bcf\u4e2a\u8f93\u5165\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u53bb\u9664\u6548\u679c\u5fae\u4e0d\u8db3\u9053\u7684\u8f93\u5165\uff0c\u7136\u540e\u91cd\u65b0\u8bad\u7ec3\u76f4\u5230\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u56e0\u679c\u5f15\u5bfc\u4fee\u526a\u65b9\u6cd5\u5728\u4e09\u4e2a\u4ee3\u8868\u4e0d\u540c\u7269\u7406\u9886\u57df\u7684\u673a\u68b0\u6d4b\u8bd5\u57fa\u7840\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u56e0\u679c\u5206\u6790\u4f18\u5316\u57fa\u4e8e\u89c2\u5bdf\u8005\u7684\u8f6f\u4f20\u611f\u5668\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6db2\u6001\u65f6\u95f4\u5e38\u6570\uff08LTC\uff09\u7f51\u7edc\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u4fee\u526a\u5bf9\u72b6\u6001\u4f30\u8ba1\u5177\u6709\u6700\u5c0f\u56e0\u679c\u5f71\u54cd\u7684\u4f20\u611f\u5668\u8f93\u5165\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u56e0\u679c\u5f15\u5bfc\u4fee\u526a\u4e00\u81f4\u5730\u8bc6\u522b\u51fa\u7b26\u5408\u57fa\u7840\u7269\u7406\u7684\u6700\u5c0f\u4f20\u611f\u5668\u7ec4\u5408\uff0c\u5e76\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2509.11361", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11361", "abs": "https://arxiv.org/abs/2509.11361", "authors": ["Yichen Han", "Bojun Liu", "Zhengpeng zhou", "Guanyu Liu", "Zeng Zhang", "Yang Yang", "Wenli Wang", "Isaac N Shi", "Yunyan", "Lewei He", "Tianyu Shi"], "title": "MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization", "comment": null, "summary": "Prompt engineering is crucial for leveraging large language models (LLMs),\nbut existing methods often rely on a single optimization trajectory, limiting\nadaptability and efficiency while suffering from narrow perspectives, gradient\nconflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt\nGradient Descent), a framework integrating multi-agent collaboration with\ngradient-based optimization. MAPGD features specialized agents for task\nclarity, example selection, format design, and stylistic refinement; semantic\ngradient coordination to resolve conflicts; bandit-based candidate selection\nfor efficient exploration-exploitation; and theoretical convergence guarantees.\nExperiments on classification, generation, and reasoning tasks show MAPGD\noutperforms single-agent and random baselines in accuracy and efficiency.\nAblations confirm the benefits of gradient fusion, agent specialization, and\nconflict resolution, providing a unified, gradient-inspired multi-agent\napproach to robust and interpretable prompt optimization.", "AI": {"tldr": "MAPGD framework integrates multi-agent collaboration with gradient-based optimization for prompt engineering, outperforming single-agent and random baselines in accuracy and efficiency. Experimental results validate the effectiveness of MAPGD in various tasks and confirm the benefits of specialized agents, conflict resolution, and gradient fusion.", "motivation": "Existing prompt engineering methods are limited by a single optimization trajectory, leading to adaptability and efficiency issues, narrow perspectives, gradient conflicts, and high computational costs.", "method": "Proposed a framework integrating multi-agent collaboration with gradient-based optimization, featuring specialized agents for different tasks, semantic gradient coordination, bandit-based candidate selection, and theoretical convergence guarantees.", "result": "Experiments on classification, generation, and reasoning tasks demonstrate the superiority of MAPGD over baseline methods in accuracy and efficiency. Ablation studies confirm the benefits of gradient fusion, agent specialization, and conflict resolution in prompt optimization.", "conclusion": "MAPGD (Multi-Agent Prompt Gradient Descent) framework outperforms single-agent and random baselines in accuracy and efficiency for prompt optimization tasks."}}
{"id": "2509.11431", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11431", "abs": "https://arxiv.org/abs/2509.11431", "authors": ["Aadil Gani Ganie"], "title": "Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has significantly advanced\nsolutions across various domains, from political science to software\ndevelopment. However, these models are constrained by their training data,\nwhich is static and limited to information available up to a specific date.\nAdditionally, their generalized nature often necessitates fine-tuning --\nwhether for classification or instructional purposes -- to effectively perform\nspecific downstream tasks. AI agents, leveraging LLMs as their core, mitigate\nsome of these limitations by accessing external tools and real-time data,\nenabling applications such as live weather reporting and data analysis. In\nindustrial settings, AI agents are transforming operations by enhancing\ndecision-making, predictive maintenance, and process optimization. For example,\nin manufacturing, AI agents enable near-autonomous systems that boost\nproductivity and support real-time decision-making. Despite these advancements,\nAI agents remain vulnerable to security threats, including prompt injection\nattacks, which pose significant risks to their integrity and reliability. To\naddress these challenges, this paper proposes a framework for integrating\nRole-Based Access Control (RBAC) into AI agents, providing a robust security\nguardrail. This framework aims to support the effective and scalable deployment\nof AI agents, with a focus on on-premises implementations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5c40\u9650\u6027\u4ee5\u53caAI\u4ee3\u7406\u5728\u5de5\u4e1a\u9886\u57df\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u6574\u5408RBAC\u5230AI\u4ee3\u7406\u4e2d\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u7684\u6846\u67b6\u3002", "motivation": "\u8bba\u6587\u6307\u51fa\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u4e2a\u9886\u57df\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u53d7\u9650\u4e8e\u9759\u6001\u548c\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u3002\u4f5c\u8005\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8bbf\u95ee\u5916\u90e8\u5de5\u5177\u548c\u5b9e\u65f6\u6570\u636e\u4ee5\u53ca\u5f15\u5165\u89d2\u8272\u8bbf\u95ee\u63a7\u5236\u6765\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u9762\u4e34\u7684\u5b89\u5168\u6311\u6218\u3002", "method": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u8bbf\u95ee\u5916\u90e8\u5de5\u5177\u548c\u5b9e\u65f6\u6570\u636e\u6765\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9650\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86AI\u4ee3\u7406\u5728\u5de5\u4e1a\u9886\u57df\u4e2d\u8f6c\u5316\u4e1a\u52a1\u8fd0\u8425\u7684\u65b9\u5f0f\u3002", "result": "\u901a\u8fc7\u6574\u5408\u89d2\u8272\u8bbf\u95ee\u63a7\u5236\uff08RBAC\uff09\u5230\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u4e2d\uff0c\u53ef\u4ee5\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u652f\u6301\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u6709\u6548\u548c\u53ef\u6269\u5c55\u90e8\u7f72\u3002\u8be5\u6846\u67b6\u4e13\u6ce8\u4e8e\u672c\u5730\u90e8\u7f72\u7684\u5b9e\u65bd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5c06\u57fa\u4e8e\u89d2\u8272\u7684\u8bbf\u95ee\u63a7\u5236\uff08RBAC\uff09\u96c6\u6210\u5230\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u4e2d\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u4f9b\u5f3a\u5927\u7684\u5b89\u5168\u4fdd\u969c\u3002\u8be5\u6846\u67b6\u65e8\u5728\u652f\u6301\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u91cd\u70b9\u5173\u6ce8\u672c\u5730\u90e8\u7f72\u7684\u5b9e\u65bd\u3002"}}
{"id": "2509.11459", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11459", "abs": "https://arxiv.org/abs/2509.11459", "authors": ["Chen Jiang", "Kofi Osei", "Sai Deepthi Yeddula", "Dongji Feng", "Wei-Shinn Ku"], "title": "Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction", "comment": "13 pages", "summary": "Accurate precipitation forecasting is indispensable in agriculture, disaster\nmanagement, and sustainable strategies. However, predicting rainfall has been\nchallenging due to the complexity of climate systems and the heterogeneous\nnature of multi-source observational data, including radar, satellite imagery,\nand surface-level measurements. The multi-source data vary in spatial and\ntemporal resolution, and they carry domain-specific features, making it\nchallenging for effective integration in conventional deep learning models.\nPrevious research has explored various machine learning techniques for weather\nprediction; however, most struggle with the integration of data with\nheterogeneous modalities. To address these limitations, we propose an Adaptive\nMixture of Experts (MoE) model tailored for precipitation rate prediction. Each\nexpert within the model specializes in a specific modality or spatio-temporal\npattern. We also incorporated a dynamic router that learns to assign inputs to\nthe most relevant experts. Our results show that this modular design enhances\npredictive accuracy and interpretability. In addition to the modeling\nframework, we introduced an interactive web-based visualization tool that\nenables users to intuitively explore historical weather patterns over time and\nspace. The tool was designed to support decision-making for stakeholders in\nclimate-sensitive sectors. We evaluated our approach using a curated multimodal\nclimate dataset capturing real-world conditions during Hurricane Ian in 2022.\nThe benchmark results show that the Adaptive MoE significantly outperformed all\nthe baselines.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u9488\u5bf9\u964d\u6c34\u7387\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u7279\u5b9a\u4e13\u5bb6\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u52a8\u6001\u8def\u7531\u5668\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5f15\u5165\u4e86\u57fa\u4e8eWeb\u7684\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u63a2\u7d22\u5386\u53f2\u5929\u6c14\u6a21\u5f0f\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6a21\u578b\u57282022\u5e74\u4f0a\u6069\u98d3\u98ce\u6c14\u5019\u6761\u4ef6\u4e0b\u660e\u663e\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u63a2\u7d22\u4e86\u5404\u79cd\u673a\u5668\u5b66\u4e60\u6280\u672f\u7528\u4e8e\u5929\u6c14\u9884\u6d4b\uff0c\u4f46\u5927\u591a\u6570\u5728\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u7531\u4e8e\u591a\u6e90\u6570\u636e\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0a\u53d8\u5316\u5de8\u5927\uff0c\u5e76\u4e14\u643a\u5e26\u9886\u57df\u7279\u5b9a\u7279\u5f81\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u964d\u6c34\u7387\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9\u964d\u6c34\u7387\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u578b\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u7279\u5b9a\u6a21\u6001\u6216\u65f6\u7a7a\u6a21\u5f0f\u3002\u6a21\u578b\u4e2d\u8fd8\u52a0\u5165\u4e86\u52a8\u6001\u8def\u7531\u5668\uff0c\u5b66\u4e60\u5c06\u8f93\u5165\u5206\u914d\u7ed9\u6700\u76f8\u5173\u7684\u4e13\u5bb6\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eWeb\u7684\u4e92\u52a8\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u63a2\u7d22\u5386\u53f2\u5929\u6c14\u6a21\u5f0f\u3002\u4f7f\u75282022\u5e74\u4f0a\u6069\u98d3\u98ce\u671f\u95f4\u7684\u6c14\u5019\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u8bbe\u8ba1\u589e\u52a0\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u6548\u679c\u3002\u901a\u8fc7\u5bf9\u771f\u5b9e\u6c14\u5019\u6570\u636e\u96c6\u7684\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u81ea\u9002\u5e94MoE\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u964d\u6c34\u7387\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u6a21\u5757\u5316\u8bbe\u8ba1\u589e\u5f3a\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5f15\u5165\u4e86\u52a8\u6001\u8def\u7531\u5668\uff0c\u5b66\u4e60\u5c06\u8f93\u5165\u5206\u914d\u7ed9\u6700\u76f8\u5173\u7684\u4e13\u5bb6\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u51fa\u4e86\u57fa\u4e8eWeb\u7684\u4e92\u52a8\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u76f4\u89c2\u5730\u63a2\u7d22\u5386\u53f2\u5929\u6c14\u6a21\u5f0f\u3002\u901a\u8fc7\u4f7f\u75282022\u5e74\u4f0a\u6069\u98d3\u98ce\u671f\u95f4\u6355\u83b7\u7684\u591a\u6a21\u6001\u6c14\u5019\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u8fd9\u4e00\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u81ea\u9002\u5e94MoE\u660e\u663e\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\u6a21\u578b\u3002"}}
{"id": "2509.11480", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11480", "abs": "https://arxiv.org/abs/2509.11480", "authors": ["Amir Taherin", "Juyi Lin", "Arash Akbari", "Arman Akbari", "Pu Zhao", "Weiwei Chen", "David Kaeli", "Yanzhi Wang"], "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs", "comment": "To appear in the Asilomar Conference on Signals, Systems, and\n  Computers 2025", "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist\npolicies for robotic control, yet their performance scaling across model\narchitectures and hardware platforms, as well as their associated power\nbudgets, remain poorly understood. This work presents an evaluation of five\nrepresentative VLA models -- spanning state-of-the-art baselines and two newly\nproposed architectures -- targeting edge and datacenter GPU platforms. Using\nthe LIBERO benchmark, we measure accuracy alongside system-level metrics,\nincluding latency, throughput, and peak memory usage, under varying edge power\nconstraints and high-performance datacenter GPU configurations. Our results\nidentify distinct scaling trends: (1) architectural choices, such as action\ntokenization and model backbone size, strongly influence throughput and memory\nfootprint; (2) power-constrained edge devices exhibit non-linear performance\ndegradation, with some configurations matching or exceeding older datacenter\nGPUs; and (3) high-throughput variants can be achieved without significant\naccuracy loss. These findings provide actionable insights when selecting and\noptimizing VLAs across a range of deployment constraints. Our work challenges\ncurrent assumptions about the superiority of datacenter hardware for robotic\ninference.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cdVision-Language-Action\uff08VLA\uff09\u6a21\u578b\u5728\u8fb9\u7f18\u548c\u6570\u636e\u4e2d\u5fc3GPU\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u5f71\u54cd\u541e\u5410\u91cf\u548c\u5185\u5b58\u5360\u7528\uff0c\u8fb9\u7f18\u8bbe\u5907\u6027\u80fd\u5728\u529f\u8017\u9650\u5236\u4e0b\u6709\u975e\u7ebf\u6027\u964d\u4f4e\uff0c\u9ad8\u541e\u5410\u91cf\u53d8\u4f53\u53ef\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u6709\u5173\u6570\u636e\u4e2d\u5fc3\u786c\u4ef6\u5728\u673a\u5668\u4eba\u63a8\u65ad\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u5047\u8bbe\u3002", "motivation": "\u5c3d\u7ba1Vision-Language-Action\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6027\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u6269\u5c55\u6027\u4ee5\u53ca\u76f8\u5173\u7684\u529f\u8017\u9884\u7b97\u4ecd\u7136\u4e0d\u662f\u5f88\u6e05\u695a\u3002", "method": "\u4f7f\u7528LIBERO\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9488\u5bf9\u4e0d\u540c\u8fb9\u7f18\u529f\u8017\u9650\u5236\u548c\u9ad8\u6027\u80fd\u6570\u636e\u4e2d\u5fc3GPU\u914d\u7f6e\uff0c\u8861\u91cf\u51c6\u786e\u6027\u4ee5\u53ca\u7cfb\u7edf\u7ea7\u6307\u6807\uff0c\u5305\u62ec\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u7684\u67b6\u6784\u9009\u62e9\u5bf9\u541e\u5410\u91cf\u548c\u5185\u5b58\u5360\u7528\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8fb9\u7f18\u8bbe\u5907\u5728\u529f\u8017\u9650\u5236\u4e0b\u8868\u73b0\u51fa\u975e\u7ebf\u6027\u6027\u80fd\u4e0b\u964d\uff0c\u9ad8\u541e\u5410\u91cf\u53d8\u4f53\u53ef\u4ee5\u5728\u4e0d\u663e\u8457\u51c6\u786e\u6027\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cd\u4ee3\u8868\u6027\u7684Vision-Language-Action\uff08VLA\uff09\u6a21\u578b\uff0c\u5728\u8fb9\u7f18\u548c\u6570\u636e\u4e2d\u5fc3GPU\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u6027\u80fd\u6d4b\u8bd5\uff0c\u53d1\u73b0\u4e0d\u540c\u7684\u67b6\u6784\u9009\u62e9\u5bf9\u541e\u5410\u91cf\u548c\u5185\u5b58\u5360\u7528\u6709\u5f3a\u70c8\u5f71\u54cd\uff0c\u8fb9\u7f18\u8bbe\u5907\u5728\u529f\u8017\u9650\u5236\u4e0b\u8868\u73b0\u51fa\u975e\u7ebf\u6027\u6027\u80fd\u4e0b\u964d\uff0c\u4e00\u4e9b\u914d\u7f6e\u4e0e\u751a\u81f3\u8d85\u8fc7\u65e7\u6570\u636e\u4e2d\u5fc3GPU\uff0c\u5728\u4e0d\u4e22\u5931\u91cd\u8981\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u53d8\u4f53\u3002\u8fd9\u4e9b\u53d1\u73b0\u5728\u9009\u62e9\u548c\u4f18\u5316VLAs\u65f6\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u6311\u6218\u4e86\u6709\u5173\u6570\u636e\u4e2d\u5fc3\u786c\u4ef6\u4f18\u8d8a\u6027\u7684\u5f53\u524d\u5047\u8bbe\u3002"}}
{"id": "2509.11507", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11507", "abs": "https://arxiv.org/abs/2509.11507", "authors": ["Jared Zhu", "Junde Wu"], "title": "MedicalOS: An LLM Agent based Operating System for Digital Healthcare", "comment": null, "summary": "Decades' advances in digital health technologies, such as electronic health\nrecords, have largely streamlined routine clinical processes. Yet, most these\nsystems are still hard to learn and use: Clinicians often face the burden of\nmanaging multiple tools, repeating manual actions for each patient, navigating\ncomplicated UI trees to locate functions, and spending significant time on\nadministration instead of caring for patients. The recent rise of large\nlanguage model (LLM) based agents demonstrates exceptional capability in coding\nand computer operation, revealing the potential for humans to interact with\noperating systems and software not by direct manipulation, but by instructing\nagents through natural language. This shift highlights the need for an\nabstraction layer, an agent-computer interface, that translates human language\ninto machine-executable commands. In digital healthcare, however, requires a\nmore domain-specific abstractions that strictly follow trusted clinical\nguidelines and procedural standards to ensure safety, transparency, and\ncompliance. To address this need, we present \\textbf{MedicalOS}, a unified\nagent-based operational system designed as such a domain-specific abstract\nlayer for healthcare. It translates human instructions into pre-defined digital\nhealthcare commands, such as patient inquiry, history retrieval, exam\nmanagement, report generation, referrals, treatment planning, that we wrapped\nas off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,\nLinux). We empirically validate MedicalOS on 214 patient cases across 22\nspecialties, demonstrating high diagnostic accuracy and confidence, clinically\nsound examination requests, and consistent generation of structured reports and\nmedication recommendations. These results highlight MedicalOS as a trustworthy\nand scalable foundation for advancing workflow automation in clinical practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MedicalOS\u4f5c\u4e3a\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u9886\u57df\u7279\u5b9a\u62bd\u8c61\u5c42\uff0c\u5c06\u4eba\u7c7b\u6307\u4ee4\u8f6c\u5316\u4e3a\u6570\u5b57\u533b\u7597\u6307\u4ee4\u3002\u7ecf\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793aMedicalOS\u5177\u6709\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5408\u7406\u6027\uff0c\u5728\u63a8\u52a8\u4e34\u5e8a\u5b9e\u8df5\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u65b9\u9762\u5177\u6709\u53ef\u4fe1\u8d56\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5f53\u524d\u6570\u5b57\u533b\u7597\u9886\u57df\u4e2d\u5b58\u5728\u7684\u4f7f\u7528\u56f0\u96be\u548c\u7e41\u7410\u64cd\u4f5c\uff0c\u4ee5\u53ca\u5bf9\u9886\u57df\u7279\u5b9a\u62bd\u8c61\u5c42\u7684\u9700\u6c42\u3002\u4f5c\u8005\u8ba4\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u4eba\u7c7b\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u548c\u8f6f\u4ef6\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86MedicalOS\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1MedicalOS\u8fd9\u4e00\u7edf\u4e00\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9886\u57df\u7279\u5b9a\u7684\u62bd\u8c61\u5c42\u3002\u8be5\u7cfb\u7edf\u5c06\u4eba\u7c7b\u6307\u4ee4\u7ffb\u8bd1\u4e3a\u9884\u5b9a\u4e49\u7684\u6570\u5b57\u533b\u7597\u6307\u4ee4\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u8bed\u8a00\uff08\u5982Python\u3001API\u3001MCP\u3001Linux\uff09\u5c01\u88c5\u4e86\u8fd9\u4e9b\u6307\u4ee4\u4f5c\u4e3a\u73b0\u6210\u7684\u5de5\u5177\u3002\u4f5c\u8005\u572822\u4e2a\u4e13\u4e1a\u9886\u57df\u7684214\u4e2a\u75c5\u4f8b\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u4f5c\u8005\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660eMedicalOS\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5177\u6709\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e00\u81f4\u7684\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\u548c\u836f\u7269\u63a8\u8350\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u4fe1\u8d56\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MedicalOS\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u62bd\u8c61\u5c42\uff0c\u7528\u4e8e\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u5c06\u4eba\u7c7b\u6307\u4ee4\u8f6c\u5316\u4e3a\u9884\u5b9a\u4e49\u7684\u6570\u5b57\u533b\u7597\u6307\u4ee4\u3002\u7ecf\u9a8c\u9a8c\u8bc1\u663e\u793aMedicalOS\u5728214\u4e2a\u75c5\u4f8b\u4e2d\u5c55\u793a\u4e86\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4fe1\u5fc3\uff0c\u4e34\u5e8a\u5408\u7406\u7684\u68c0\u67e5\u8bf7\u6c42\u4ee5\u53ca\u7ed3\u6784\u5316\u62a5\u544a\u548c\u836f\u7269\u63a8\u8350\u7684\u4e00\u81f4\u751f\u6210\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86MedicalOS\u4f5c\u4e3a\u63a8\u52a8\u4e34\u5e8a\u5b9e\u8df5\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u7684\u53ef\u4fe1\u53ef\u6269\u5c55\u57fa\u7840\u3002"}}
{"id": "2509.11547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11547", "abs": "https://arxiv.org/abs/2509.11547", "authors": ["Shanmuka Sadhu", "Arca Baran", "Preeti Pandey", "Ayush Kumar"], "title": "Task Decoding based on Eye Movements using Synthetic Data Augmentation", "comment": null, "summary": "Machine learning has been extensively used in various applications related to\neye-tracking research. Understanding eye movement is one of the most\nsignificant subsets of eye-tracking research that reveals the scanning pattern\nof an individual. Researchers have thoroughly analyzed eye movement data to\nunderstand various eye-tracking applications, such as attention mechanisms,\nnavigational behavior, task understanding, etc. The outcome of traditional\nmachine learning algorithms used for decoding tasks based on eye movement data\nhas received a mixed reaction to Yarbus' claim that it is possible to decode\nthe observer's task from their eye movements. In this paper, to support the\nhypothesis by Yarbus, we are decoding tasks categories while generating\nsynthetic data samples using well-known Synthetic Data Generators CTGAN and its\nvariations such as CopulaGAN and Gretel AI Synthetic Data generators on\navailable data from an in-person user study. Our results show that augmenting\nmore eye movement data combined with additional synthetically generated\nimproves classification accuracy even with traditional machine learning\nalgorithms. We see a significant improvement in task decoding accuracy from\n28.1% using Random Forest to 82% using Inception Time when five times more data\nis added in addition to the 320 real eye movement dataset sample. Our proposed\nframework outperforms all the available studies on this dataset because of the\nuse of additional synthetic datasets. We validated our claim with various\nalgorithms and combinations of real and synthetic data to show how decoding\naccuracy increases with the increase in the augmentation of generated data to\nreal data.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u5668CTGAN\u53ca\u5176\u53d8\u4f53CopulaGAN\u548cGretel AI\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff0c\u5728\u773c\u52a8\u6570\u636e\u4e2d\u8bc6\u522b\u4efb\u52a1\u7c7b\u522b\u5e76\u63d0\u9ad8\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002\u901a\u8fc7\u589e\u52a0\u773c\u52a8\u6570\u636e\u548c\u5408\u6210\u6570\u636e\uff0c\u4ece28.1%\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u523082%\u7684\u51c6\u786e\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5728\u589e\u52a0\u6570\u636e\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u7801\u51c6\u786e\u6027\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u5e76\u9a8c\u8bc1\u4e86\u63d0\u51fa\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "Yarbus\u63d0\u51fa\u4e86\u53ef\u4ee5\u4ece\u89c2\u5bdf\u8005\u7684\u773c\u52a8\u4e2d\u89e3\u7801\u4efb\u52a1\u7684\u5047\u8bbe\uff0c\u4f46\u5bf9\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u773c\u52a8\u6570\u636e\u89e3\u7801\u4efb\u52a1\u4e2d\u7684\u53cd\u5e94\u5b58\u5728\u5206\u6b67\u3002\u672c\u7814\u7a76\u65e8\u5728\u652f\u6301Yarbus\u7684\u5047\u8bbe\uff0c\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u589e\u52a0\u773c\u52a8\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u4efb\u52a1\u7c7b\u522b\u7684\u8bc6\u522b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u5668CTGAN\u53ca\u5176\u53d8\u4f53CopulaGAN\u548cGretel AI\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u5728\u773c\u52a8\u6570\u636e\u4e2d\u8bc6\u522b\u4efb\u52a1\u7c7b\u522b\u3002\u901a\u8fc7\u589e\u52a0\u66f4\u591a\u773c\u52a8\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u6539\u5584\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u4e2d\u7ed3\u5408320\u4e2a\u771f\u5b9e\u773c\u52a8\u6570\u636e\u6837\u672c\u548c\u66f4\u591a\u5408\u6210\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u63d0\u51fa\u6846\u67b6\u7684\u6027\u80fd\u4f18\u8d8a\u6027\u3002", "result": "\u901a\u8fc7\u5bf9\u773c\u52a8\u6570\u636e\u8fdb\u884c\u5408\u6210\u6570\u636e\u589e\u5f3a\u4ee5\u53ca\u589e\u52a0\u771f\u5b9e\u773c\u52a8\u6570\u636e\u6837\u672c\uff0c\u5b9e\u73b0\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u4efb\u52a1\u7c7b\u522b\u89e3\u7801\u4e2d\u7684\u663e\u8457\u6539\u8fdb\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u589e\u52a0\u4e94\u500d\u6570\u636e\u5e76\u7ed3\u5408320\u4e2a\u771f\u5b9e\u773c\u52a8\u6570\u636e\u6837\u672c\u65f6\uff0c\u4ece\u4f7f\u7528\u968f\u673a\u68ee\u6797\u768428.1%\u63d0\u9ad8\u5230\u4f7f\u7528Inception Time\u768482%\u3002\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u589e\u52a0\u5408\u6210\u6570\u636e\u5bf9\u89e3\u7801\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u5668CTGAN\u53ca\u5176\u53d8\u4f53CopulaGAN\u548cGretel AI\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff0c\u5728\u773c\u52a8\u6570\u636e\u4e2d\u8bc6\u522b\u4efb\u52a1\u7c7b\u522b\u3002\u7ed3\u679c\u663e\u793a\u901a\u8fc7\u589e\u52a0\u66f4\u591a\u773c\u52a8\u6570\u636e\u548c\u5408\u6210\u6570\u636e\uff0c\u5373\u4f7f\u4f7f\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e5f\u80fd\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u5f53\u589e\u52a0\u4e94\u500d\u6570\u636e\u5e76\u7ed3\u5408320\u4e2a\u771f\u5b9e\u773c\u52a8\u6570\u636e\u6837\u672c\u65f6\uff0c\u4ece\u4f7f\u7528\u968f\u673a\u68ee\u6797\u768428.1%\u63d0\u9ad8\u5230\u4f7f\u7528Inception Time\u768482%\u65f6\uff0c\u4efb\u52a1\u7684\u89e3\u7801\u51c6\u786e\u6027\u663e\u8457\u63d0\u9ad8\u3002\u63d0\u51fa\u7684\u6846\u67b6\u4f18\u4e8e\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u6240\u6709\u73b0\u6709\u7814\u7a76\uff0c\u8fd9\u5f52\u529f\u4e8e\u4f7f\u7528\u989d\u5916\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u901a\u8fc7\u9a8c\u8bc1\u4e0d\u540c\u7b97\u6cd5\u548c\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u7684\u7ec4\u5408\uff0c\u5c55\u793a\u4e86\u968f\u7740\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u589e\u52a0\uff0c\u89e3\u7801\u51c6\u786e\u6027\u5982\u4f55\u589e\u52a0\u3002"}}
{"id": "2509.11572", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11572", "abs": "https://arxiv.org/abs/2509.11572", "authors": ["Tuan Bui", "An Nguyen", "Phat Thai", "Minh Hua", "Ngan Pham L. N.", "Ngan Pham T. B.", "Dung Le", "Long Nguyen", "Thanh-Tung Tran", "Thang Bui", "Tho Quan"], "title": "Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain", "comment": "Published at the 2nd ACM Workshop in AI-powered Question & Answering\n  Systems (AIQAM '25), co-located with ACM Multimedia 2025", "summary": "Reasoning is essential for closed-domain QA systems in which procedural\ncorrectness and policy compliance are critical. While large language models\n(LLMs) have shown strong performance on many reasoning tasks, recent work\nreveals that their reasoning traces are often unfaithful - serving more as\nplausible justifications than as causally grounded derivations. Efforts to\ncombine LLMs with symbolic engines (e.g., Prover9, Z3) have improved\nreliability but remain limited to static forms of logic, struggling with\ndynamic, state-based reasoning such as multi-step progressions and conditional\ntransitions.\n  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a\nneuro-symbolic framework that integrates LLMs with model checking to support\nproperty verification. MCFR translates natural language into formal\nspecifications and verifies them over transition models. To support evaluation,\nwe introduce EduMC-QA, a benchmark dataset grounded in real academic\nprocedures. Our results show that MCFR improves reasoning faithfulness and\ninterpretability, offering a viable path toward verifiable QA in high-stakes\nclosed-domain applications. In addition to evaluating MCFR, we compare its\nperformance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to\ncontextualize its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMCFR\u6846\u67b6\uff0c\u5c06LLMs\u4e0e\u6a21\u578b\u68c0\u9a8c\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u7684\u5fe0\u5b9e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7EduMC-QA\u57fa\u51c6\u6570\u636e\u96c6\u8bc4\u4f30\u4e86MCFR\uff0c\u5e76\u4e0eChatGPT\uff0cDeepSeek\u548cClaude\u7b49\u73b0\u6709LLMs\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMCFR\u5728\u5c01\u95ed\u9886\u57df\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9a8c\u8bc1\u95ee\u7b54\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "motivation": "\u5c01\u95ed\u9886\u57df\u7684\u95ee\u7b54\u7cfb\u7edf\u9700\u8981\u6709\u6548\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u5b58\u5728\u63a8\u7406\u75d5\u8ff9\u4e0d\u5fe0\u5b9e\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u7b26\u53f7\u5f15\u64ce\u4e0eLLMs\u7684\u7ed3\u5408\u4ecd\u53d7\u9650\u4e8e\u9759\u6001\u5f62\u5f0f\u903b\u8f91\uff0c\u4e0d\u80fd\u5f88\u597d\u5730\u5904\u7406\u52a8\u6001\u3001\u57fa\u4e8e\u72b6\u6001\u7684\u63a8\u7406\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6MCFR\u6765\u6539\u5584\u63a8\u7406\u7684\u5fe0\u5b9e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u5c01\u95ed\u9886\u57df\u5e94\u7528\u4e2d\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684QA\u3002", "method": "\u63d0\u51faMCFR\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u5f62\u5f0f\u89c4\u8303\u5e76\u5728\u8f6c\u6362\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u5408\u4e86LLMs\u548c\u6a21\u578b\u68c0\u9a8c\u6280\u672f\u3002\u901a\u8fc7\u4ecb\u7ecd\u57fa\u51c6\u6570\u636e\u96c6EduMC-QA\u8fdb\u884c\u8bc4\u4f30\u3002\u4e0e\u5176\u4ed6LLMs\u8fdb\u884c\u6bd4\u8f83\u4ee5\u8bc4\u4f30MCFR\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMCFR\u5728\u63a8\u7406\u5fe0\u5b9e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u4e3a\u5c01\u95ed\u9886\u57df\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9a8c\u8bc1\u95ee\u7b54\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u8def\u5f84\u3002\u4e0e\u5176\u4ed6\u73b0\u6709\u7684LLMs\u76f8\u6bd4\uff0cMCFR\u8868\u73b0\u51fa\u8f83\u597d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86MCFR\uff08Model Checking for Formal Reasoning\uff09\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06LLMs\u4e0e\u6a21\u578b\u68c0\u9a8c\u76f8\u7ed3\u5408\u6765\u652f\u6301\u5c5e\u6027\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u63a8\u7406\u7684\u5fe0\u5b9e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u771f\u5b9e\u5b66\u672f\u7a0b\u5e8f\u7684\u57fa\u51c6\u6570\u636e\u96c6EduMC-QA\uff0c\u5c55\u793a\u4e86MCFR\u5728\u9ad8\u98ce\u9669\u5c01\u95ed\u9886\u57df\u5e94\u7528\u4e2d\u9a8c\u8bc1\u95ee\u7b54\uff08QA\uff09\u7684\u53ef\u884c\u6027\u8def\u5f84\u3002\u4e0eChatGPT\uff0cDeepSeek\u548cClaude\u7b49\u6700\u5148\u8fdb\u7684LLMs\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u4ee5\u8868\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.11575", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11575", "abs": "https://arxiv.org/abs/2509.11575", "authors": ["Ching Chang", "Yidan Shi", "Defu Cao", "Wei Yang", "Jeehyun Hwang", "Haixin Wang", "Jiacheng Pang", "Wei Wang", "Yan Liu", "Wen-Chih Peng", "Tien-Fu Chen"], "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models", "comment": "This paper is currently under review", "summary": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\uff0c\u5c06\u5176\u89c6\u4e3a\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5bf9\u6587\u732e\u5206\u7c7b\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u7684\u63a8\u7406\u62d3\u6251\u7ed3\u6784\u53ca\u5176\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u3002\u63d0\u4f9b\u4e86\u652f\u6301\u7814\u7a76\u548c\u90e8\u7f72\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8d44\u6e90\u3002\u6307\u51fa\u4e86\u63a8\u7406\u7ed3\u6784\u5fc5\u987b\u5728\u5bb9\u91cf\u3001\u81ea\u6211\u7ea0\u6b63\u3001\u8ba1\u7b97\u6210\u672c\u548c\u53ef\u91cd\u590d\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u672a\u6765\u7684\u8fdb\u5c55\u6709\u8d56\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u5c06\u63a8\u7406\u8d28\u91cf\u4e0e\u6548\u7528\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u53ca\u5728\u5c01\u95ed\u73af\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5728\u6210\u672c\u548c\u98ce\u9669\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u672c\u6587\u5c06\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u89c6\u4e3a\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5bf9\u6587\u732e\u8fdb\u884c\u6574\u7406\u548c\u5206\u7c7b\uff0c\u65e8\u5728\u7406\u6e05\u4e0d\u540c\u63a8\u7406\u62d3\u6251\u7ed3\u6784\u53ca\u5176\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u7a81\u51fa\u4e86\u8bc4\u4f30\u5b9e\u8df5\u548c\u6307\u5bfc\u5bf9\u62d3\u6251\u7ed3\u6784\u4e0e\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u89c2\u5bdf\u5de5\u4ef6\u57fa\u7840\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u8bbe\u8ba1\u9884\u7b97\u7b49\u65b9\u9762\u7684\u5339\u914d\u3002", "method": "\u8fd9\u9879\u8c03\u67e5\u901a\u8fc7\u63a8\u7406\u62d3\u6251\u5c06\u65f6\u95f4\u5e8f\u5217\u6587\u732e\u8fdb\u884c\u5206\u7c7b\uff0c\u6db5\u76d6\u4e86\u4e09\u79cd\u63a8\u7406\u5bb6\u65cf\uff1a\u4e00\u6b65\u76f4\u63a5\u63a8\u7406\uff0c\u5177\u6709\u663e\u5f0f\u4e2d\u95f4\u6b65\u9aa4\u7684\u7ebf\u6027\u94fe\u63a8\u7406\u4ee5\u53ca\u63a2\u7d22\u3001\u4fee\u8ba2\u548c\u805a\u5408\u7684\u5206\u652f\u7ed3\u6784\u63a8\u7406\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u5404\u79cd\u65b9\u6cd5\u548c\u7cfb\u7edf\u7684\u7efc\u8ff0\uff0c\u5c55\u793a\u4e86\u6bcf\u79cd\u62d3\u6251\u7ed3\u6784\u7684\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u4ee5\u53ca\u652f\u6301\u7814\u7a76\u548c\u90e8\u7f72\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8d44\u6e90\u3002", "result": "\u7814\u7a76\u56de\u987e\u4e86\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u7684\u4f18\u52bf\u548c\u9650\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u652f\u6301\u7814\u7a76\u548c\u90e8\u7f72\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8d44\u6e90\u3002\u5f3a\u8c03\u4e86\u63a8\u7406\u7ed3\u6784\u5fc5\u987b\u5e73\u8861\u5bb9\u91cf\u3001\u81ea\u6211\u7ea0\u6b63\u3001\u8ba1\u7b97\u6210\u672c\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7684\u8fdb\u5c55\u5c06\u53d6\u51b3\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u5c06\u63a8\u7406\u8d28\u91cf\u4e0e\u6548\u7528\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u53ca\u5728\u5c01\u95ed\u73af\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5728\u6210\u672c\u548c\u98ce\u9669\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u5c06\u65f6\u95f4\u89c6\u4e3a\u4e00\u4e2a\u7b2c\u4e00\u7c7b\u8f74\uff0c\u5e76\u76f4\u63a5\u5c06\u4e2d\u95f4\u8bc1\u636e\u7eb3\u5165\u7b54\u6848\u3002\u8fd9\u9879\u8c03\u67e5\u901a\u8fc7\u63a8\u7406\u62d3\u6251\u5c06\u6587\u732e\u8fdb\u884c\u5206\u7c7b\uff0c\u5206\u4e3a\u4e09\u4e2a\u5bb6\u65cf\uff1a\u4e00\u6b65\u76f4\u63a5\u63a8\u7406\uff0c\u5177\u6709\u663e\u5f0f\u4e2d\u95f4\u6b65\u9aa4\u7684\u7ebf\u6027\u94fe\u63a8\u7406\uff0c\u4ee5\u53ca\u63a2\u7d22\u3001\u4fee\u8ba2\u548c\u805a\u5408\u7684\u5206\u652f\u7ed3\u6784\u63a8\u7406\u3002\u8fd9\u4e2a\u62d3\u6251\u4e0e\u9886\u57df\u7684\u4e3b\u8981\u76ee\u6807\u4ea4\u53c9\uff0c\u5305\u62ec\u4f20\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3001\u89e3\u91ca\u548c\u7406\u89e3\u3001\u56e0\u679c\u63a8\u65ad\u548c\u51b3\u7b56\u5236\u5b9a\uff0c\u4ee5\u53ca\u65f6\u95f4\u5e8f\u5217\u751f\u6210\uff0c\u540c\u65f6\u4e00\u4e2a\u7d27\u51d1\u7684\u6807\u8bb0\u96c6\u8de8\u8d8a\u8fd9\u4e9b\u8f74\uff0c\u5e76\u6355\u6349\u4e86\u5206\u89e3\u548c\u9a8c\u8bc1\u3001\u96c6\u6210\u3001\u5de5\u5177\u4f7f\u7528\u3001\u77e5\u8bc6\u83b7\u53d6\u3001\u591a\u6a21\u6001\u3001\u667a\u80fd\u5faa\u73af\u548cLLM\u5bf9\u51c6\u673a\u5236\u3002\u56de\u987e\u4e86\u8de8\u9886\u57df\u7684\u65b9\u6cd5\u548c\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u6bcf\u79cd\u62d3\u6251\u7ed3\u6784\u7684\u4f18\u52bf\u548c\u52a3\u52bf\u6240\u5728\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8d44\u6e90\uff0c\u652f\u6301\u7814\u7a76\u548c\u90e8\u7f72\u3002\u7a81\u51fa\u4e86\u4fdd\u6301\u8bc1\u636e\u53ef\u89c1\u548c\u4e0e\u65f6\u95f4\u5bf9\u9f50\u7684\u8bc4\u4f30\u5b9e\u8df5\uff0c\u5e76\u63d0\u70bc\u4e86\u5173\u4e8e\u5c06\u62d3\u6251\u5339\u914d\u5230\u4e0d\u786e\u5b9a\u6027\u3001\u4ee5\u53ef\u89c2\u5bdf\u7684\u5de5\u4ef6\u4f5c\u4e3a\u57fa\u7840\u3001\u8ba1\u5212\u8f6c\u53d8\u548c\u6d41\u52a8\u3001\u5c06\u6210\u672c\u548c\u5ef6\u8fdf\u89c6\u4e3a\u8bbe\u8ba1\u9884\u7b97\u7684\u6307\u5bfc\u3002\u6211\u4eec\u5f3a\u8c03\u63a8\u7406\u7ed3\u6784\u5fc5\u987b\u5728\u5bb9\u91cf\u3001\u81ea\u6211\u7ea0\u6b63\u4e0e\u8ba1\u7b97\u6210\u672c\u548c\u53ef\u91cd\u590d\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u672a\u6765\u7684\u8fdb\u5c55\u53ef\u80fd\u53d6\u51b3\u4e8e\u5c06\u63a8\u7406\u8d28\u91cf\u4e0e\u6548\u7528\u8054\u7cfb\u8d77\u6765\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u5728\u8003\u8651\u8f6c\u53d8\u611f\u77e5\u3001\u6d41\u52a8\u548c\u957f\u671f\u89c6\u89d2\u7684\u5c01\u95ed\u73af\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5728\u6210\u672c\u548c\u98ce\u9669\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u4e9b\u65b9\u5411\u6807\u5fd7\u7740\u4ece\u72ed\u9698\u7684\u51c6\u786e\u6027\u5411\u89c4\u6a21\u53ef\u9760\u6027\u7684\u8f6c\u53d8\uff0c\u4f7f\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u591f\u5206\u6790\uff0c\u8fd8\u80fd\u7406\u89e3\u3001\u89e3\u91ca\u548c\u884c\u52a8\u5728\u5145\u6ee1\u53ef\u8ffd\u8e2a\u8bc1\u636e\u548c\u53ef\u4fe1\u6210\u679c\u7684\u52a8\u6001\u4e16\u754c\u4e2d\u3002"}}
{"id": "2509.11595", "categories": ["cs.AI", "cs.CE", "cs.CR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11595", "abs": "https://arxiv.org/abs/2509.11595", "authors": ["Sabin Huda", "Ernest Foo", "Zahra Jadidi", "MA Hakim Newton", "Abdul Sattar"], "title": "AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions", "comment": null, "summary": "Anti-money laundering (AML) research is constrained by the lack of publicly\nshareable, regulation-aligned transaction datasets. We present AMLNet, a\nknowledge-based multi-agent framework with two coordinated units: a\nregulation-aware transaction generator and an ensemble detection pipeline. The\ngenerator produces 1,090,173 synthetic transactions (approximately 0.16\\%\nlaundering-positive) spanning core laundering phases (placement, layering,\nintegration) and advanced typologies (e.g., structuring, adaptive threshold\nbehavior). Regulatory alignment reaches 75\\% based on AUSTRAC rule coverage\n(Section 4.2), while a composite technical fidelity score of 0.75 summarizes\ntemporal, structural, and behavioral realism components (Section 4.4). The\ndetection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the\ninternal test partitions of AMLNet and adapts to the external SynthAML dataset,\nindicating architectural generalizability across different synthetic generation\nparadigms. We provide multi-dimensional evaluation (regulatory, temporal,\nnetwork, behavioral) and release the dataset (Version 1.0,\nhttps://doi.org/10.5281/zenodo.16736515), to advance reproducible and\nregulation-conscious AML experimentation.", "AI": {"tldr": "AMLNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u89c4\u5219\u751f\u6210\u5668\u548c\u68c0\u6d4b\u7ba1\u9053\u3002\u751f\u6210\u7ea61,090,173\u7b14\u5408\u6210\u4ea4\u6613\uff0c\u68c0\u6d4b\u96c6\u5408\u8fbe\u5230F1 0.90\u3002\u6570\u636e\u96c6\u5177\u6709\u9ad8\u76d1\u7ba1\u4e00\u81f4\u6027\u548c\u6280\u672f\u4fdd\u771f\u5ea6\u8bc4\u5206\uff0c\u68c0\u6d4b\u96c6\u5408\u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u9002\u5e94\u6027\u597d\u3002\u63d0\u4f9b\u591a\u7ef4\u5ea6\u8bc4\u4f30\u548c\u6570\u636e\u96c6\u53d1\u5e03\uff0c\u63a8\u52a8AML\u5b9e\u9a8c\u7684\u53ef\u91cd\u73b0\u6027\u548c\u89c4\u5219\u9075\u4ece\u6027\u3002", "motivation": "AML\u7814\u7a76\u53d7\u5230\u7f3a\u4e4f\u53ef\u516c\u5f00\u5171\u4eab\u3001\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u7684\u4ea4\u6613\u6570\u636e\u96c6\u7684\u9650\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51faAMLNet\u6846\u67b6\uff0c\u4ee5\u751f\u6210\u5408\u6210\u4ea4\u6613\u6570\u636e\u96c6\uff0c\u4ece\u800c\u63a8\u52a8\u53ef\u91cd\u73b0\u4e14\u7b26\u5408\u89c4\u5219\u7684AML\u5b9e\u9a8c\u3002", "method": "AMLNet\u5305\u542b\u4e24\u4e2a\u534f\u8c03\u7684\u5355\u5143\uff1a\u4e00\u4e2a\u89c4\u5219\u611f\u77e5\u7684\u4ea4\u6613\u751f\u6210\u5668\u548c\u4e00\u4e2a\u96c6\u6210\u68c0\u6d4b\u7ba1\u9053\u3002\u751f\u6210\u5668\u4ea7\u751f\u5408\u6210\u4ea4\u6613\uff0c\u5305\u62ec\u6d17\u94b1\u6838\u5fc3\u9636\u6bb5\u548c\u5148\u8fdb\u7c7b\u578b\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u76d1\u7ba1\u4e00\u81f4\u6027\uff08\u57fa\u4e8eAUSTRAC\u89c4\u5219\u8986\u76d6\u7387\uff09\uff0c\u5e76\u62e5\u6709\u7efc\u5408\u6280\u672f\u4fdd\u771f\u5ea6\u8bc4\u5206\u3002\u68c0\u6d4b\u96c6\u5408\u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u9002\u5e94\u5916\u90e8\u6570\u636e\u96c6\uff0c\u663e\u793a\u51fa\u8de8\u4e0d\u540c\u5408\u6210\u751f\u6210\u8303\u4f8b\u7684\u67b6\u6784\u6cdb\u5316\u6027\u3002\u4f5c\u8005\u63d0\u4f9b\u4e86\u591a\u7ef4\u5ea6\u7684\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u73b0\u548c\u9075\u5b88\u89c4\u5219\u7684AML\u5b9e\u9a8c\u3002", "result": "AMLNet\u751f\u6210\u4e86\u5305\u62ec\u6d17\u94b1\u6838\u5fc3\u9636\u6bb5\u548c\u5148\u8fdb\u7c7b\u578b\u5728\u5185\u7684\u5408\u6210\u4ea4\u6613\u6570\u636e\u96c6\uff0c\u5177\u6709\u76f8\u5bf9\u9ad8\u7684\u76d1\u7ba1\u4e00\u81f4\u6027\u548c\u6280\u672f\u4fdd\u771f\u5ea6\u8bc4\u5206\u3002\u68c0\u6d4b\u96c6\u5408\u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u9002\u5e94\u6027\u826f\u597d\uff0c\u663e\u793a\u51fa\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdbAML\u5b9e\u9a8c\u7684\u53ef\u91cd\u73b0\u6027\u548c\u89c4\u5219\u9075\u4ece\u6027\u3002", "conclusion": "AMLNet \u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u5177\u6709\u610f\u8bc6\u7684\u89c4\u5219\u751f\u6210\u5668\u548c\u4e00\u4e2a\u96c6\u6210\u68c0\u6d4b\u7ba1\u9053\u3002\u751f\u6210\u5668\u4ea7\u751f\u4e86\u5305\u62ec\u6838\u5fc3\u6d17\u94b1\u9636\u6bb5\u548c\u5148\u8fdb\u7c7b\u578b\u5728\u5185\u7684\u7ea61,090,173\u7b14\u5408\u6210\u4ea4\u6613\uff0c\u8986\u76d6AUSTRAC\u89c4\u5219\u7ea675%\uff0c\u6280\u672f\u4fdd\u771f\u5ea6\u5f97\u5206\u4e3a0.75\u3002\u68c0\u6d4b\u96c6\u5408\u5728AMLNet\u7684\u5185\u90e8\u6d4b\u8bd5\u5206\u533a\u4e0a\u8fbe\u5230\u4e86 F1 0.90\uff08\u7cbe\u5ea60.84\uff0c\u53ec\u56de\u73870.97\uff09\uff0c\u5e76\u9002\u5e94\u5916\u90e8SynthAML\u6570\u636e\u96c6\uff0c\u8868\u660e\u5176\u5728\u4e0d\u540c\u5408\u6210\u751f\u6210\u8303\u4f8b\u4e2d\u7684\u67b6\u6784\u6cdb\u5316\u80fd\u529b\u3002\u63d0\u4f9b\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff08\u76d1\u7ba1\u3001\u65f6\u95f4\u3001\u7f51\u7edc\u3001\u884c\u4e3a\uff09\u5e76\u53d1\u5e03\u6570\u636e\u96c6\uff08Version 1.0\uff0c https://doi.org/10.5281/zenodo.16736515\uff09\uff0c\u4ee5\u63a8\u52a8\u53ef\u91cd\u73b0\u548c\u9075\u5b88\u89c4\u5219\u7684 AML \u5b9e\u9a8c\u3002"}}
{"id": "2509.11645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11645", "abs": "https://arxiv.org/abs/2509.11645", "authors": ["Zhaolong Wu", "Pu Luo", "Jason Pui Yin Cheung", "Teng Zhang"], "title": "Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework", "comment": "Accepted by MICCAI 2025 MLLMCP Workshop", "summary": "This study presents the first comprehensive evaluation of Multimodal Large\nLanguage Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)\nself-management. We constructed a database of approximately 3,000\nanteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a\n`Divide and Conquer' framework consisting of a visual question-answering task,\na domain knowledge assessment task, and a patient education counseling\nassessment task. Our investigation revealed limitations of MLLMs' ability in\ninterpreting complex spinal radiographs and comprehending AIS care knowledge.\nTo address these, we pioneered enhancing MLLMs with spinal keypoint prompting\nand compiled an AIS knowledge base for retrieval augmented generation (RAG),\nrespectively. Results showed varying effectiveness of visual prompting across\ndifferent architectures, while RAG substantially improved models' performances\non the knowledge assessment task. Our findings indicate current MLLMs are far\nfrom capable in realizing personalized assistant in AIS care. The greatest\nchallenge lies in their abilities to obtain accurate detections of spinal\ndeformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u9752\u5c11\u5e74\u7279\u53d1\u6027\u810a\u67f1\u4fa7\u51f8\uff08AIS\uff09\u81ea\u6211\u7ba1\u7406\u65b9\u9762\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5f53\u524dMLLMs\u5728\u89e3\u91ca\u590d\u6742\u810a\u67f1X\u5c04\u7ebf\u56fe\u50cf\u548c\u7406\u89e3AIS\u62a4\u7406\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u80fd\u529b\u9650\u5236\u3002\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u810a\u67f1\u5173\u952e\u70b9\u63d0\u793a\u548c\u5efa\u7acbAIS\u77e5\u8bc6\u5e93\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u5e76\u8868\u660eRAG\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5f53\u524dMLLMs\u5728\u5b9e\u73b0\u4e2a\u6027\u5316AIS\u62a4\u7406\u52a9\u624b\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u4e3b\u8981\u5728\u4e8e\u51c6\u786e\u68c0\u6d4b\u810a\u67f1\u7578\u5f62\u4f4d\u7f6e\u548c\u65b9\u5411\u65b9\u9762\u7684\u51c6\u786e\u6027\u6709\u9650\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5c11\u5e74\u7279\u53d1\u6027\u810a\u67f1\u4fa7\u51f8\uff08AIS\uff09\u81ea\u6211\u7ba1\u7406\u65b9\u9762\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3\u5f53\u524dMLLMs\u5728\u7406\u89e3\u590d\u6742\u810a\u67f1X\u5c04\u7ebf\u56fe\u50cf\u548cAIS\u62a4\u7406\u77e5\u8bc6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u7ea63,000\u5f20\u524d\u540e\u4f4dX\u5c04\u7ebf\u56fe\u50cf\u6570\u636e\u5e93\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cdMLLMs\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u3001\u9886\u57df\u77e5\u8bc6\u8bc4\u4f30\u4efb\u52a1\u548c\u60a3\u8005\u6559\u80b2\u54a8\u8be2\u8bc4\u4f30\u4efb\u52a1\u6784\u5efa\u4e86\u201c\u5206\u800c\u6cbb\u4e4b\u201d\u7684\u6846\u67b6\u3002\u7814\u7a76\u8fd8\u4ecb\u7ecd\u4e86\u5728MLLMs\u4e2d\u5f15\u5165\u810a\u67f1\u5173\u952e\u70b9\u63d0\u793a\u548c\u5efa\u7acbAIS\u77e5\u8bc6\u5e93\u7684\u521b\u65b0\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u89e3\u91ca\u590d\u6742\u810a\u67f1X\u5c04\u7ebf\u56fe\u50cf\u548c\u7406\u89e3AIS\u62a4\u7406\u77e5\u8bc6\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\uff0c\u540c\u65f6\u4e5f\u5c55\u793a\u4e86\u52a0\u5165\u810a\u67f1\u5173\u952e\u70b9\u63d0\u793a\u548c\u5f15\u5165AIS\u77e5\u8bc6\u5e93\u7684\u6539\u8fdb\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u89c6\u89c9\u63d0\u793a\u5728\u4e0d\u540c\u67b6\u6784\u4e0b\u7684\u6548\u679c\u4e0d\u540c\uff0c\u800cRAG\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u5c11\u5e74\u7279\u53d1\u6027\u810a\u67f1\u4fa7\u51f8\uff08AIS\uff09\u8fdb\u884c\u81ea\u6211\u7ba1\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u89e3\u91ca\u590d\u6742\u810a\u67f1X\u5c04\u7ebf\u56fe\u50cf\u548c\u7406\u89e3AIS\u62a4\u7406\u77e5\u8bc6\u65b9\u9762\u3002\u8be5\u7814\u7a76\u901a\u8fc7\u5728MLLMs\u4e2d\u5f15\u5165\u810a\u67f1\u5173\u952e\u70b9\u63d0\u793a\u548c\u5efa\u7acbAIS\u77e5\u8bc6\u5e93\u6765\u6539\u5584\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8868\u660eRAG\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u7136\u800c\uff0c\u5f53\u524dMLLMs\u5728\u5b9e\u73b0\u4e2a\u6027\u5316AIS\u62a4\u7406\u52a9\u624b\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6311\u6218\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u51c6\u786e\u68c0\u6d4b\u810a\u67f1\u7578\u5f62\u4f4d\u7f6e\u548c\u65b9\u5411\u65b9\u9762\u7684\u51c6\u786e\u6027\u6709\u9650\u3002"}}
{"id": "2509.11719", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11719", "abs": "https://arxiv.org/abs/2509.11719", "authors": ["Bingqing Wei", "Lianmin Chen", "Zhongyu Xia", "Yongtao Wang"], "title": "HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction", "comment": null, "summary": "Multi-agent trajectory prediction in autonomous driving requires a\ncomprehensive understanding of complex social dynamics. Existing methods,\nhowever, often struggle to capture the full richness of these dynamics,\nparticularly the co-existence of multi-scale interactions and the diverse\nbehaviors of heterogeneous agents. To address these challenges, this paper\nintroduces HeLoFusion, an efficient and scalable encoder for modeling\nheterogeneous and multi-scale agent interactions. Instead of relying on global\ncontext, HeLoFusion constructs local, multi-scale graphs centered on each\nagent, allowing it to effectively model both direct pairwise dependencies and\ncomplex group-wise interactions (\\textit{e.g.}, platooning vehicles or\npedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of\nagent heterogeneity through an aggregation-decomposition message-passing scheme\nand type-specific feature networks, enabling it to learn nuanced,\ntype-dependent interaction patterns. This locality-focused approach enables a\nprincipled representation of multi-level social context, yielding powerful and\nexpressive agent embeddings. On the challenging Waymo Open Motion Dataset,\nHeLoFusion achieves state-of-the-art performance, setting new benchmarks for\nkey metrics including Soft mAP and minADE. Our work demonstrates that a\nlocality-grounded architecture, which explicitly models multi-scale and\nheterogeneous interactions, is a highly effective strategy for advancing motion\nforecasting.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 HeLoFusion\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u7684\u7f16\u7801\u5668\u3002HeLoFusion\u901a\u8fc7\u672c\u5730\u3001\u591a\u5c3a\u5ea6\u56fe\u5efa\u6a21\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6709\u6548\u6355\u6349\u4e0d\u540c\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002\u5728 Waymo Open Motion \u6570\u636e\u96c6\u4e0a\uff0cHeLoFusion\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5c40\u90e8\u6027\u67b6\u6784\u5728\u63d0\u9ad8\u8fd0\u52a8\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u5230\u591a\u5c3a\u5ea6\u76f8\u4e92\u4f5c\u7528\u548c\u5f02\u6784\u667a\u80fd\u4f53\u591a\u6837\u884c\u4e3a\u7684\u4e30\u5bcc\u52a8\u6001\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u7f16\u7801\u5668 HeLoFusion\uff0c\u65e8\u5728\u6709\u6548\u5730\u5efa\u6a21\u591a\u5c3a\u5ea6\u548c\u5f02\u8d28\u667a\u80fd\u4f53\u4ea4\u4e92\u3002\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u8be5\u65b9\u6cd5\u5728 Waymo Open Motion \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5c40\u90e8\u6027\u67b6\u6784\u662f\u63a8\u8fdb\u8fd0\u52a8\u9884\u6d4b\u7684\u6709\u6548\u7b56\u7565\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HeLoFusion \u7684\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5efa\u6a21\u5f02\u6784\u548c\u591a\u5c3a\u5ea6\u667a\u80fd\u4f53\u4ea4\u4e92\u3002HeLoFusion \u6784\u5efa\u672c\u5730\u3001\u591a\u5c3a\u5ea6\u56fe\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u76f4\u63a5\u6210\u5bf9\u4f9d\u8d56\u5173\u7cfb\u548c\u590d\u6742\u7684\u7fa4\u4f53\u76f8\u4e92\u4f5c\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u5f02\u8d28\u6027\uff0cHeLoFusion\u91c7\u7528\u4e86\u805a\u5408-\u5206\u89e3\u6d88\u606f\u4f20\u9012\u65b9\u6848\u548c\u7279\u5b9a\u7c7b\u578b\u7684\u7279\u5f81\u7f51\u7edc\u3002\u4f5c\u8005\u8fd8\u901a\u8fc7\u5728 Waymo Open Motion Dataset \u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86 HeLoFusion \u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5e76\u521b\u9020\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002", "result": "HeLoFusion \u5728 Waymo Open Motion \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5173\u952e\u6307\u6807 Soft mAP \u548c minADE \u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f73\u8868\u73b0\u3002\u8fd9\u8868\u660e HeLoFusion \u662f\u4e00\u79cd\u5177\u6709\u5f3a\u5927\u8868\u8fbe\u80fd\u529b\u7684\u667a\u80fd\u4f53\u5d4c\u5165\u65b9\u6cd5\uff0c\u5e76\u5728\u8fd0\u52a8\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u51fa\u6210\u679c\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86 HeLoFusion\uff0c\u4e00\u79cd\u7528\u4e8e\u5efa\u6a21\u5f02\u6784\u548c\u591a\u5c3a\u5ea6\u667a\u80fd\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u7f16\u7801\u5668\u3002HeLoFusion \u901a\u8fc7\u6784\u5efa\u4ee5\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e3a\u4e2d\u5fc3\u7684\u672c\u5730\u591a\u5c3a\u5ea6\u56fe\uff0c\u6709\u6548\u5730\u5efa\u6a21\u4e86\u76f4\u63a5\u6210\u5bf9\u4f9d\u8d56\u5173\u7cfb\u548c\u590d\u6742\u7684\u7fa4\u4f53\u76f8\u4e92\u4f5c\u7528\u3002\u6b64\u5916\uff0cHeLoFusion\u901a\u8fc7\u805a\u5408-\u5206\u89e3\u6d88\u606f\u4f20\u9012\u65b9\u6848\u548c\u7279\u5b9a\u7c7b\u578b\u7684\u7279\u5f81\u7f51\u7edc\u6765\u5904\u7406\u667a\u80fd\u4f53\u5f02\u8d28\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5b66\u4e60\u5fae\u5999\u7684\u3001\u4e0e\u7c7b\u578b\u76f8\u5173\u7684\u76f8\u4e92\u4f5c\u7528\u6a21\u5f0f\u3002\u8fd9\u79cd\u4ee5\u5c40\u90e8\u4e3a\u91cd\u70b9\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u591a\u5c42\u793e\u4ea4\u80cc\u666f\u7684\u6b63\u5f53\u8868\u793a\uff0c\u4ea7\u751f\u4e86\u5f3a\u5927\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u667a\u80fd\u4f53\u5d4c\u5165\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684 Waymo Open Motion \u6570\u636e\u96c6\u4e0a\uff0cHeLoFusion \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u5173\u952e\u6307\u6807\u5305\u62ec Soft mAP \u548c minADE \u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u4e00\u79cd\u660e\u786e\u5efa\u6a21\u591a\u5c3a\u5ea6\u548c\u5f02\u8d28\u76f8\u4e92\u4f5c\u7528\u7684\u5c40\u90e8\u6027\u67b6\u6784\u662f\u63a8\u8fdb\u8fd0\u52a8\u9884\u6d4b\u7684\u4e00\u79cd\u9ad8\u6548\u7b56\u7565\u3002"}}
{"id": "2509.11880", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11880", "abs": "https://arxiv.org/abs/2509.11880", "authors": ["Carlos Celemin", "Joseph Brennan", "Pierluigi Vito Amadori", "Tim Bradley"], "title": "Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning", "comment": null, "summary": "This paper introduces a novel application of Supervised Contrastive Learning\n(SupCon) to Imitation Learning (IL), with a focus on learning more effective\nstate representations for agents in video game environments. The goal is to\nobtain latent representations of the observations that capture better the\naction-relevant factors, thereby modeling better the cause-effect relationship\nfrom the observations that are mapped to the actions performed by the\ndemonstrator, for example, the player jumps whenever an obstacle appears ahead.\nWe propose an approach to integrate the SupCon loss with continuous output\nspaces, enabling SupCon to operate without constraints regarding the type of\nactions of the environment. Experiments on the 3D games Astro Bot and Returnal,\nand multiple 2D Atari games show improved representation quality, faster\nlearning convergence, and better generalization compared to baseline models\ntrained only with supervised action prediction loss functions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5c06SupCon\u5e94\u7528\u4e8eImitation Learning\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u89c6\u9891\u6e38\u620f\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u72b6\u6001\u8868\u793a\u6765\u6539\u8fdb\u4ee3\u7406\u7684\u5b66\u4e60\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u4ec5\u4f7f\u7528\u76d1\u7763\u52a8\u4f5c\u9884\u6d4b\u635f\u5931\u51fd\u6570\u7684\u57fa\u51c6\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u8868\u793a\u8d28\u91cf\u3001\u66f4\u5feb\u7684\u5b66\u4e60\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7ed3\u5408SupCon\u548cImitation Learning\u6765\u63d0\u9ad8\u4ee3\u7406\u5728\u89c6\u9891\u6e38\u620f\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u6548\u679c\u3002\u76ee\u7684\u662f\u83b7\u5f97\u66f4\u597d\u7684\u6f5c\u5728\u8868\u793a\uff0c\u66f4\u597d\u5730\u6a21\u62df\u89c2\u5bdf\u7ed3\u679c\u5230\u52a8\u4f5c\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4ece\u800c\u6539\u8fdb\u4ee3\u7406\u7684\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06SupCon\u635f\u5931\u4e0e\u8fde\u7eed\u8f93\u51fa\u7a7a\u95f4\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4f7fSupCon\u80fd\u591f\u5728\u4e0d\u53d7\u73af\u5883\u52a8\u4f5c\u7c7b\u578b\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u64cd\u4f5c\u3002\u901a\u8fc7\u5c06SupCon\u5e94\u7528\u4e8eImitation Learning\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u5730\u6355\u6349\u89c2\u5bdf\u7ed3\u679c\u7684\u6f5c\u5728\u8868\u793a\uff0c\u66f4\u597d\u5730\u5efa\u6a21\u4ece\u89c2\u5bdf\u7ed3\u679c\u5230\u64cd\u4f5c\u6267\u884c\u8005\u52a8\u4f5c\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u5728Astro Bot\u548cReturnal\u7b493D\u6e38\u620f\u4ee5\u53ca\u591a\u4e2a2D Atari\u6e38\u620f\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u8868\u793a\u8d28\u91cf\u3001\u66f4\u5feb\u7684\u5b66\u4e60\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06Supervised Contrastive Learning\uff08SupCon\uff09\u5e94\u7528\u4e8eImitation Learning\uff08IL\uff09\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u89c6\u9891\u6e38\u620f\u73af\u5883\u4e2d\u7684\u4ee3\u7406\u5b66\u4e60\u66f4\u6709\u6548\u7684\u72b6\u6001\u8868\u793a\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u76d1\u7763\u52a8\u4f5c\u9884\u6d4b\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u7684\u57fa\u51c6\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u57283D\u6e38\u620fAstro Bot\u548cReturnal\u4ee5\u53ca\u591a\u4e2a2D Atari\u6e38\u620f\u4e0a\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u8868\u793a\u8d28\u91cf\u3001\u66f4\u5feb\u7684\u5b66\u4e60\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.11914", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11914", "abs": "https://arxiv.org/abs/2509.11914", "authors": ["Yiqun Yao", "Naitong Yu", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Wenjia Ma", "Xuying Meng", "Jing Li", "Aixin Sun", "Yequan Wang"], "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models", "comment": null, "summary": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research.", "AI": {"tldr": "EgoMem\u662f\u7b2c\u4e00\u4e2a\u4e3a\u5168\u53cc\u5de5\u6a21\u578b\u8bbe\u8ba1\u7684\u7ec8\u8eab\u8bb0\u5fc6\u4ee3\u7406\u7a0b\u5e8f\uff0c\u80fd\u591f\u4ece\u5b9e\u65f6\u5168\u6a21\u6001\u6d41\u4e2d\u76f4\u63a5\u8bc6\u522b\u591a\u4e2a\u7528\u6237\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u54cd\u5e94\u3002\u5b83\u7684\u64cd\u4f5c\u5305\u62ec\u4e09\u4e2a\u5f02\u6b65\u8fc7\u7a0b\uff1a\u68c0\u7d22\u8fc7\u7a0b\u3001\u5168\u6a21\u6001\u5bf9\u8bdd\u8fc7\u7a0b\u548c\u8bb0\u5fc6\u7ba1\u7406\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4e0eRoboEgo\u5168\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba\u96c6\u6210\u540e\uff0c\u5728\u5b9e\u65f6\u4e2a\u6027\u5316\u5bf9\u8bdd\u4e2d\u4f53\u73b0\u51fa\u8272\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u4ecb\u7ecdEgoMem\uff0c\u4e3a\u5168\u53cc\u5de5\u6a21\u578b\u8bbe\u8ba1\u4e86\u7b2c\u4e00\u4e2a\u7ec8\u8eab\u8bb0\u5fc6\u4ee3\u7406\u7a0b\u5e8f\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5168\u6a21\u6001\u6d41\u5904\u7406\u3002\u8be5\u7cfb\u7edf\u7684\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u5b8c\u5168\u4f9d\u8d56\u539f\u59cb\u97f3\u9891\u89c6\u89c9\u6d41\uff0c\u5e76\u80fd\u591f\u5b9e\u73b0\u4e2a\u6027\u5316\u8bc6\u522b\u548c\u54cd\u5e94\uff0c\u9002\u7528\u4e8e\u7ec8\u8eab\u3001\u5b9e\u65f6\u548c\u5177\u4f53\u573a\u666f\u3002", "method": "EgoMem\u64cd\u4f5c\u5305\u62ec\u4e09\u4e2a\u5f02\u6b65\u8fc7\u7a0b\uff1a\u68c0\u7d22\u8fc7\u7a0b\u3001\u5168\u6a21\u6001\u5bf9\u8bdd\u8fc7\u7a0b\u548c\u8bb0\u5fc6\u7ba1\u7406\u8fc7\u7a0b\u3002\u5b83\u901a\u8fc7\u9762\u90e8\u548c\u58f0\u97f3\u52a8\u6001\u8bc6\u522b\u7528\u6237\uff0c\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u5e76\u66f4\u65b0\u957f\u671f\u8bb0\u5fc6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u68c0\u7d22\u548c\u8bb0\u5fc6\u7ba1\u7406\u6a21\u5757\u5728\u6d4b\u8bd5\u96c6\u4e0a\u51c6\u786e\u7387\u8d85\u8fc795%\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEgoMem\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4e0eRoboEgo\u5168\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba\u96c6\u6210\u540e\uff0c\u5728\u5b9e\u65f6\u4e2a\u6027\u5316\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "EgoMem\u662f\u7b2c\u4e00\u4e2a\u4e3a\u5168\u53cc\u5de5\u6a21\u578b\u5b9a\u5236\u7684\u7ec8\u8eab\u8bb0\u5fc6\u4ee3\u7406\u7a0b\u5e8f\uff0c\u80fd\u591f\u4ece\u5b9e\u65f6\u5168\u6a21\u6001\u6d41\u4e2d\u76f4\u63a5\u8bc6\u522b\u591a\u4e2a\u7528\u6237\uff0c\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u54cd\u5e94\u3002\u8be5\u7cfb\u7edf\u64cd\u4f5c\u5305\u62ec\u4e09\u4e2a\u5f02\u6b65\u8fc7\u7a0b\uff1a\uff08i\uff09\u68c0\u7d22\u8fc7\u7a0b\u52a8\u6001\u8bc6\u522b\u7528\u6237\uff0c\u901a\u8fc7\u9762\u90e8\u548c\u58f0\u97f3\u83b7\u53d6\u76f8\u5173\u4e0a\u4e0b\u6587\uff1b\uff08ii\uff09\u5168\u6a21\u6001\u5bf9\u8bdd\u8fc7\u7a0b\u6839\u636e\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u4e2a\u6027\u5316\u97f3\u9891\u54cd\u5e94\uff1b\u4ee5\u53ca\uff08iii\uff09\u8bb0\u5fc6\u7ba1\u7406\u8fc7\u7a0b\u4ece\u5168\u6a21\u6001\u6d41\u4e2d\u81ea\u52a8\u68c0\u6d4b\u5bf9\u8bdd\u8fb9\u754c\uff0c\u5e76\u63d0\u53d6\u5fc5\u8981\u4fe1\u606f\u66f4\u65b0\u957f\u671f\u8bb0\u5fc6\u3002\u4e0e\u73b0\u6709\u7684LLM\u8bb0\u5fc6\u4ee3\u7406\u4e0d\u540c\uff0cEgoMem\u5b8c\u5168\u4f9d\u8d56\u539f\u59cb\u7684\u97f3\u9891\u89c6\u89c9\u6d41\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ec8\u8eab\u3001\u5b9e\u65f6\u548c\u5177\u4f53\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEgoMem\u7684\u68c0\u7d22\u548c\u8bb0\u5fc6\u7ba1\u7406\u6a21\u5757\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8695%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002\u4e0e\u7ecf\u8fc7\u5fae\u8c03\u7684RoboEgo\u5168\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba\u96c6\u6210\u65f6\uff0c\u7cfb\u7edf\u5728\u5b9e\u65f6\u4e2a\u6027\u5316\u5bf9\u8bdd\u4e2d\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u5f97\u5206\u9ad8\u8fbe87%\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2509.11922", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11922", "abs": "https://arxiv.org/abs/2509.11922", "authors": ["Xilei Dai", "Ruotian Chen", "Songze Guan", "Wen-Tai Li", "Chau Yuen"], "title": "BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning", "comment": null, "summary": "Reinforcement learning (RL) has proven effective for AI-based building energy\nmanagement. However, there is a lack of flexible framework to implement RL\nacross various control problems in building energy management. To address this\ngap, we propose BuildingGym, an open-source tool designed as a\nresearch-friendly and flexible framework for training RL control strategies for\ncommon challenges in building energy management. BuildingGym integrates\nEnergyPlus as its core simulator, making it suitable for both system-level and\nroom-level control. Additionally, BuildingGym is able to accept external\nsignals as control inputs instead of taking the building as a stand-alone\nentity. This feature makes BuildingGym applicable for more flexible\nenvironments, e.g. smart grid and EVs community. The tool provides several\nbuilt-in RL algorithms for control strategy training, simplifying the process\nfor building managers to obtain optimal control strategies. Users can achieve\nthis by following a few straightforward steps to configure BuildingGym for\noptimization control for common problems in the building energy management\nfield. Moreover, AI specialists can easily implement and test state-of-the-art\ncontrol algorithms within the platform. BuildingGym bridges the gap between\nbuilding managers and AI specialists by allowing for the easy configuration and\nreplacement of RL algorithms, simulators, and control environments or problems.\nWith BuildingGym, we efficiently set up training tasks for cooling load\nmanagement, targeting both constant and dynamic cooling load management. The\nbuilt-in algorithms demonstrated strong performance across both tasks,\nhighlighting the effectiveness of BuildingGym in optimizing cooling strategies.", "AI": {"tldr": "BuildingGym is an open-source tool integrating EnergyPlus as the core simulator and offering built-in RL algorithms for training control strategies in building energy management. It simplifies configuration for optimization control, targeting cooling load management tasks. The tool effectively bridges the gap between building managers and AI specialists, providing a user-friendly platform for control algorithm implementation and testing.", "motivation": "The motivation behind this paper is the lack of a flexible framework for implementing reinforcement learning across various control problems in building energy management. BuildingGym aims to bridge the gap between building managers and AI specialists by providing an easy-to-use tool with built-in RL algorithms, allowing for efficient training tasks for cooling load management.", "method": "The paper proposes BuildingGym, an open-source tool that serves as a research-friendly and flexible framework for training RL control strategies in building energy management. It integrates EnergyPlus as the core simulator and provides built-in RL algorithms for control strategy training. BuildingGym can accept external signals as control inputs, making it suitable for smart grid and EVs community environments. The tool simplifies configuration for optimization control in common building energy management problems and allows for easy implementation and testing of control algorithms.", "result": "BuildingGym demonstrated strong performance in optimizing cooling strategies for both constant and dynamic cooling load management tasks. It effectively addresses the need for a flexible RL framework in building energy management and provides a user-friendly interface for building managers and AI specialists to implement and test control algorithms.", "conclusion": "BuildingGym is an open-source tool designed to address the lack of a flexible framework for implementing reinforcement learning in building energy management. It integrates EnergyPlus as its core simulator, offers built-in RL algorithms for control strategy training, and is suitable for various control problems in building energy management. The tool simplifies the process for building managers to obtain optimal control strategies and allows AI specialists to implement state-of-the-art control algorithms easily."}}
{"id": "2509.11940", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11940", "abs": "https://arxiv.org/abs/2509.11940", "authors": ["Marcel van Gerven"], "title": "Neuromorphic Intelligence", "comment": "18 pages, 3 figures", "summary": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which depend on massive computational and\nenergy resources, neuromorphic systems exploit brain-inspired principles of\ncomputation to achieve orders of magnitude greater energy efficiency. By\ndrawing on insights from artificial intelligence, neuroscience, physics,\nchemistry, and materials science, neuromorphic computing promises to deliver\nintelligent systems that are sustainable, transparent, and widely accessible. A\ncentral challenge, however, is to identify a unifying theoretical framework\ncapable of bridging these diverse disciplines. We argue that dynamical systems\ntheory provides such a foundation. Rooted in differential calculus, it offers a\nprincipled language for modeling inference, learning, and control in both\nnatural and artificial substrates. Within this framework, noise can be\nharnessed as a resource for learning, while differential genetic programming\nenables the discovery of dynamical systems that implement adaptive behaviors.\nEmbracing this perspective paves the way toward emergent neuromorphic\nintelligence, where intelligent behavior arises from the dynamics of physical\nsubstrates, advancing both the science and sustainability of AI.", "AI": {"tldr": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u65e8\u5728\u590d\u5236\u4eba\u8111\u7684\u9ad8\u6548\u3001\u7075\u6d3b\u548c\u9002\u5e94\u6027\u3002\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u4f5c\u4e3a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u65b0\u578b\u7684\u795e\u7ecf\u5f62\u6001\u667a\u80fd\u3002\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u53ef\u4ee5\u5229\u7528\u566a\u97f3\u4f5c\u4e3a\u5b66\u4e60\u8d44\u6e90\uff0c\u901a\u8fc7\u9057\u4f20\u7f16\u7a0b\u53d1\u73b0\u5b9e\u73b0\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u52a8\u529b\u7cfb\u7edf\u3002\u8fd9\u79cd\u89c2\u70b9\u4fc3\u8fdb\u4e86\u65b0\u578b\u667a\u80fd\u884c\u4e3a\u7684\u6d8c\u73b0\uff0c\u63d0\u5347\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u79d1\u5b66\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u65e8\u5728\u590d\u5236\u4eba\u8111\u7684\u9ad8\u6548\u3001\u7075\u6d3b\u548c\u9002\u5e94\u6027\uff0c\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7684\u6570\u5b57\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u80fd\u6548\u3002\u7136\u800c\uff0c\u9762\u4e34\u7684\u6311\u6218\u662f\u5982\u4f55\u627e\u5230\u4e00\u4e2a\u80fd\u591f\u8fde\u63a5\u4eba\u5de5\u667a\u80fd\u3001\u795e\u7ecf\u79d1\u5b66\u3001\u7269\u7406\u5b66\u3001\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u63a8\u7406\u3001\u5b66\u4e60\u548c\u63a7\u5236\uff0c\u4ee5\u5b9e\u73b0\u65b0\u578b\u7684\u795e\u7ecf\u5f62\u6001\u667a\u80fd\u3002", "result": "\u901a\u8fc7\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\uff0c\u8bba\u6587\u8ba4\u4e3a\u566a\u97f3\u53ef\u4ee5\u4f5c\u4e3a\u5b66\u4e60\u7684\u8d44\u6e90\uff0c\u9057\u4f20\u7f16\u7a0b\u53ef\u4ee5\u53d1\u73b0\u5b9e\u73b0\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u52a8\u529b\u7cfb\u7edf\u3002\u8fd9\u79cd\u89c2\u70b9\u4e3a\u65b0\u578b\u795e\u7ecf\u5f62\u6001\u667a\u80fd\u94fa\u5e73\u9053\u8def\uff0c\u4fc3\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u79d1\u5b66\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "conclusion": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u901a\u8fc7\u5229\u7528\u5927\u8111\u542f\u53d1\u7684\u8ba1\u7b97\u539f\u7406\uff0c\u5b9e\u73b0\u80fd\u6548\u66f4\u9ad8\u7684\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2509.11941", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.11941", "abs": "https://arxiv.org/abs/2509.11941", "authors": ["Ilia Kopanichuk", "Petr Anokhin", "Vladimir Shaposhnikov", "Vladimir Makharev", "Ekaterina Tsapieva", "Iaroslav Bespalov", "Dmitry V. Dylov", "Ivan Oseledets"], "title": "How to Evaluate Medical AI", "comment": "10 pages, 7 fugures", "summary": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86RPAD\u548cRRAD\u4f5c\u4e3a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u5bb6\u610f\u89c1\u5bf9\u6bd4AI\u8f93\u51fa\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u73b0\u5b9e\u7684\u8bca\u65ad\u8d28\u91cf\u5ea6\u91cf\u3002\u7814\u7a76\u53d1\u73b0\u9876\u5c16\u6a21\u578b\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4e13\u5bb6\u5171\u8bc6\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u4e13\u5bb6\u5224\u65ad\u7684\u53d8\u5f02\u6027\uff0c\u652f\u6301\u5728\u533b\u5b66AI\u4e2d\u91c7\u7528\u76f8\u5bf9\u6307\u6807\u3002", "motivation": "\u533b\u5b66\u8bca\u65ad\u4e2dAI\u7684\u6574\u5408\u9700\u8981\u53ef\u9760\u548c\u4e00\u81f4\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u3001\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u4e13\u5bb6\u5224\u65ad\u7684\u56fa\u6709\u53d8\u5f02\u6027\u3002\u4f20\u7edf\u6307\u6807\u5982\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u5e38\u5e38\u65e0\u6cd5\u89e3\u91ca\u4e13\u5bb6\u5224\u65ad\u7684\u56fa\u6709\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u5bf9AI\u6027\u80fd\u7684\u8bc4\u4f30\u4e0d\u4e00\u81f4\u3002\u4e13\u5bb6\u95f4\u8bc4\u7ea7\u4e00\u81f4\u6027\u7edf\u8ba1\u91cf\u5982Cohen's Kappa\u66f4\u4e3a\u53ef\u9760\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faRPAD\u548cRRAD\u4f5c\u4e3a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u5c06AI\u8f93\u51fa\u4e0e\u591a\u4e2a\u4e13\u5bb6\u610f\u89c1\u8fdb\u884c\u6bd4\u8f83\uff0c\u89c4\u8303\u6027\u80fd\u4ee5\u5bf9\u6297\u4e13\u5bb6\u95f4\u5206\u6b67\uff0c\u5e76\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u73b0\u5b9e\u7684\u8bca\u65ad\u8d28\u91cf\u5ea6\u91cf\u3002\u7814\u7a76\u4f7f\u7528360\u4e2a\u533b\u5b66\u5bf9\u8bdd\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4e00\u7ec4\u533b\u751f\u7684\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u4e0e\u591a\u4e2a\u4e13\u5bb6\u610f\u89c1\u6bd4\u8f83AI\u8f93\u51fa\uff0cRPAD\u548cRRAD\u7b49\u76f8\u5bf9\u6027\u80fd\u6307\u6807\u76f8\u5bf9\u4e8e\u4e13\u5bb6\u95f4\u5206\u6b67\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u4e14\u73b0\u5b9e\u7684\u8bca\u65ad\u8d28\u91cf\u5ea6\u91cf\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u9876\u5c16\u6a21\u578b\u5728\u533b\u5b66\u8bca\u65ad\u65b9\u9762\u7684\u4e00\u81f4\u6027\u8fbe\u5230\u6216\u8d85\u8fc7\u4e13\u5bb6\u5171\u8bc6\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u7814\u7a76\u7ed3\u679c\u8fd8\u8868\u660e\u4e13\u5bb6\u5224\u65ad\u5b58\u5728\u663e\u8457\u7684\u53d8\u5f02\u6027\uff0c\u652f\u6301\u5728\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u9886\u57df\u91c7\u7528\u76f8\u5bf9\u6307\u6807\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u76f8\u5bf9\u7b97\u6cd5\u8bca\u65ad\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff08RPAD\u548cRRAD\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u6bd4\u8f83AI\u8f93\u51fa\u4e0e\u591a\u4e2a\u4e13\u5bb6\u610f\u89c1\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u73b0\u5b9e\u7684\u8bca\u65ad\u8d28\u91cf\u8861\u91cf\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9876\u5c16\u6a21\u578b\u5982DeepSeek-V3\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u80fd\u8fbe\u5230\u4e0e\u6216\u8d85\u8fc7\u4e13\u5bb6\u5171\u8bc6\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e13\u5bb6\u5224\u65ad\u5b58\u5728\u663e\u8457\u7684\u53d8\u5f02\u6027\uff0c\u5f3a\u8c03\u4e86\u7edd\u5bf9\u5ea6\u91cf\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u652f\u6301\u5728\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u4e2d\u91c7\u7528\u76f8\u5bf9\u6307\u6807\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.11943", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11943", "abs": "https://arxiv.org/abs/2509.11943", "authors": ["Antonin Sulc", "Thorsten Hellert"], "title": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics", "comment": "10 pages, 1 figure, Scaling Environments for Agents (SEA) Workshop at\n  NeuralIPS", "summary": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5229\u7528\u6a21\u6001\u903b\u8f91\u8fdb\u884c\u63a8\u7406\uff0c\u6210\u529f\u5728\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\u8bca\u65ad\u4e86\u590d\u6742\u6545\u969c\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u548c\u6a21\u6001\u903b\u8f91\u7684\u7ed3\u5408\u5e26\u6765\u7684\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u6269\u5c55\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u8303\u5f0f\u5bfc\u81f4\u51fa\u8272\u7684\u65b0\u9896\u80fd\u529b\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\uff0c\u5728\u667a\u80fd\u4f53\u63a8\u7406\u7ed3\u6784\u3001\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6269\u5c55\u662f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e2d\u5173\u952e\u4e14\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u7684\u7ef4\u5ea6\u3002\u4f5c\u8005\u7684\u52a8\u673a\u4e3b\u8981\u5728\u4e8e\u63a2\u7d22AI\u7814\u7a76\u4e2d\u7ed3\u6784\u3001\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6f5c\u5728\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u4e2a\u4f53\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u72b6\u6001\u5f62\u5f0f\u5316\u8868\u793a\u4e3aKripke\u6a21\u578b\uff0c\u5229\u7528\u6a21\u6001\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u8a00\u8fdb\u884c\u63a8\u7406\u3002\u5229\u7528\u4e0d\u53d8\u7684\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6765\u5236\u5b9a\u903b\u8f91\u7ea6\u675f\u7528\u4e8e\u5047\u8bbe\u751f\u6210\uff0c\u4ee5\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u5f97\u51fa\u4e0d\u53ef\u884c\u7ed3\u8bba\u3002\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u7684\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\uff0c\u6210\u529f\u8bca\u65ad\u590d\u6742\u3001\u8fde\u9501\u6545\u969c\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u7684\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u7684\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\u6210\u529f\u8bca\u65ad\u4e86\u590d\u6742\u7684\u3001\u8fde\u9501\u6545\u969c\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u76f4\u89c9\u548c\u6a21\u6001\u903b\u8f91\u7684\u9a8c\u8bc1\u7ed3\u5408\uff0c\u4e3a\u66f4\u5065\u58ee\u3001\u53ef\u9760\u548c\u53ef\u9a8c\u8bc1\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5176\u4e2d\u4e2a\u4f53\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u72b6\u6001\u88ab\u5f62\u5f0f\u5316\u8868\u793a\u4e3a\u514b\u91cc\u666e\u514b\u6a21\u578b\u3002\u901a\u8fc7\u4f7f\u7528\u6a21\u6001\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u8a00\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u63a8\u7406\u5173\u4e8e\u201c\u53ef\u80fd\u6027\u201d\u548c\u201c\u5fc5\u7136\u6027\u201d\u7684\u5df2\u77e5\u6982\u5ff5\u3002\u5728\u6a21\u578b\u4e2d\u4f7f\u7528\u4e86\u4e0d\u53d8\u7684\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6765\u63a8\u65ad\u4fe1\u606f\uff0c\u5c06\u4fe1\u606f\u7f16\u7801\u4e3a\u9002\u7528\u4e8e\u6b63\u786e\u8bca\u65ad\u7684\u903b\u8f91\u7ea6\u675f\u3002\u6587\u4e2d\u5c55\u793a\u4e86\u8fd9\u4e9b\u7ea6\u675f\u5982\u4f55\u79ef\u6781\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u7684\u5047\u8bbe\u751f\u6210\uff0c\u6709\u6548\u5730\u963b\u6b62\u5b83\u4eec\u5f97\u51fa\u7269\u7406\u4e0a\u6216\u903b\u8f91\u4e0a\u4e0d\u53ef\u884c\u7684\u7ed3\u8bba\u3002\u901a\u8fc7\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u7684\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\uff0c\u6210\u529f\u8bca\u65ad\u4e86\u590d\u6742\u7684\u3001\u8fde\u9501\u6545\u969c\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u6a21\u578b\u5f3a\u5927\u7684\u8bed\u4e49\u76f4\u89c9\u548c\u6a21\u6001\u903b\u8f91\u7684\u4e25\u683c\u3001\u53ef\u9a8c\u8bc1\u7684\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u4e8b\u5b9e\u4e16\u754c\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u901a\u5411\u66f4\u5f3a\u5927\u3001\u53ef\u9760\u548c\u53ef\u9a8c\u8bc1\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.11944", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11944", "abs": "https://arxiv.org/abs/2509.11944", "authors": ["Susanta Mitra"], "title": "Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare", "comment": null, "summary": "Healthcare and medicine are multimodal disciplines that deal with multimodal\ndata for reasoning and diagnosing multiple diseases. Although some multimodal\nreasoning models have emerged for reasoning complex tasks in scientific\ndomains, their applications in the healthcare domain remain limited and fall\nshort in correct reasoning for diagnosis. To address the challenges of\nmultimodal medical reasoning for correct diagnosis and assist the healthcare\nprofessionals, a novel temporal graph-based reasoning process modelled through\na directed graph has been proposed in the current work. It helps in\naccommodating dynamic changes in reasons through backtracking, refining the\nreasoning content, and creating new or deleting existing reasons to reach the\nbest recommendation or answer. Again, consideration of multimodal data at\ndifferent time points can enable tracking and analysis of patient health and\ndisease progression. Moreover, the proposed multi-agent temporal reasoning\nframework provides task distributions and a cross-validation mechanism to\nfurther enhance the accuracy of reasoning outputs. A few basic experiments and\nanalysis results justify the novelty and practical utility of the proposed\npreliminary approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u56fe\u7684\u63a8\u7406\u8fc7\u7a0b\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u533b\u7597\u63a8\u7406\uff0c\u4ee5\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u8fdb\u884c\u6b63\u786e\u8bca\u65ad\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u56de\u6eaf\u3001\u7cbe\u5316\u63a8\u7406\u5185\u5bb9\u548c\u521b\u5efa\u6216\u5220\u9664\u539f\u56e0\u6765\u8fbe\u5230\u6700\u4f73\u63a8\u8350\u6216\u7b54\u6848\uff0c\u5e76\u5229\u7528\u591aAgent\u65f6\u95f4\u63a8\u7406\u6846\u67b6\u63d0\u9ad8\u63a8\u7406\u8f93\u51fa\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u533b\u7597\u63a8\u7406\u4e2d\u6b63\u786e\u8bca\u65ad\u7684\u6311\u6218\uff0c\u534f\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u3002\u8003\u8651\u591a\u6a21\u6001\u6570\u636e\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u5e94\u7528\uff0c\u4ee5\u8ffd\u8e2a\u548c\u5206\u6790\u75c5\u4eba\u7684\u5065\u5eb7\u548c\u75be\u75c5\u8fdb\u5c55\u3002\u63d0\u9ad8\u63a8\u7406\u8f93\u51fa\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8e\u65f6\u95f4\u56fe\u7684\u63a8\u7406\u8fc7\u7a0b\u6a21\u578b\uff0c\u6709\u5411\u56fe\u5efa\u6a21\uff0c\u56de\u6eaf\u3001\u7cbe\u5316\u63a8\u7406\u5185\u5bb9\uff0c\u521b\u5efa\u6216\u5220\u9664\u539f\u56e0\u3002\u63d0\u51fa\u591aAgent\u65f6\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4efb\u52a1\u5206\u914d\u548c\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u901a\u8fc7\u4e00\u4e9b\u57fa\u7840\u5b9e\u9a8c\u548c\u5206\u6790\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u65b0\u9896\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u56fe\u7684\u63a8\u7406\u8fc7\u7a0b\u6a21\u578b\uff0c\u901a\u8fc7\u6709\u5411\u56fe\u5bf9\u591a\u6a21\u6001\u533b\u7597\u63a8\u7406\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u505a\u51fa\u6b63\u786e\u8bca\u65ad\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u56de\u6eaf\u3001\u7cbe\u5316\u63a8\u7406\u5185\u5bb9\u4ee5\u53ca\u521b\u5efa\u6216\u5220\u9664\u539f\u56e0\u6765\u8fbe\u5230\u6700\u4f73\u63a8\u8350\u6216\u7b54\u6848\u3002\u540c\u65f6\uff0c\u8003\u8651\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u591a\u6a21\u6001\u6570\u636e\u53ef\u4ee5\u8ffd\u8e2a\u548c\u5206\u6790\u60a3\u8005\u5065\u5eb7\u548c\u75be\u75c5\u8fdb\u5c55\u3002\u63d0\u51fa\u7684\u591aAgent\u65f6\u95f4\u63a8\u7406\u6846\u67b6\u63d0\u4f9b\u4efb\u52a1\u5206\u914d\u548c\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u63a8\u7406\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3002\u4e00\u4e9b\u57fa\u7840\u5b9e\u9a8c\u548c\u5206\u6790\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u521d\u6b65\u65b9\u6cd5\u7684\u65b0\u9896\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.11973", "categories": ["cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.11973", "abs": "https://arxiv.org/abs/2509.11973", "authors": ["Markus J. Buehler"], "title": "MusicSwarm: Biologically Inspired Intelligence for Music Composition", "comment": null, "summary": "We show that coherent, long-form musical composition can emerge from a\ndecentralized swarm of identical, frozen foundation models that coordinate via\nstigmergic, peer-to-peer signals, without any weight updates. We compare a\ncentralized multi-agent system with a global critic to a fully decentralized\nswarm in which bar-wise agents sense and deposit harmonic, rhythmic, and\nstructural cues, adapt short-term memory, and reach consensus. Across symbolic,\naudio, and graph-theoretic analyses, the swarm yields superior quality while\ndelivering greater diversity and structural variety and leads across creativity\nmetrics. The dynamics contract toward a stable configuration of complementary\nroles, and self-similarity networks reveal a small-world architecture with\nefficient long-range connectivity and specialized bridging motifs, clarifying\nhow local novelties consolidate into global musical form. By shifting\nspecialization from parameter updates to interaction rules, shared memory, and\ndynamic consensus, MusicSwarm provides a compute- and data-efficient route to\nlong-horizon creative structure that is immediately transferable beyond music\nto collaborative writing, design, and scientific discovery.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cMusicSwarm\u4f7f\u7528\u53bb\u4e2d\u5fc3\u5316\u6a21\u578b\u5b9e\u73b0\u97f3\u4e50\u5408\u6210\uff0c\u901a\u8fc7\u534f\u4f5c\u548c\u5171\u8bc6\u8fbe\u6210\u4f18\u8d28\u548c\u521b\u9020\u6027\u7684\u97f3\u4e50\u4f5c\u54c1\uff0c\u5c55\u793a\u4e86\u5728\u97f3\u4e50\u521b\u4f5c\u9886\u57df\u7684\u6f5c\u5728\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u65b0\u578b\u97f3\u4e50\u5408\u6210\u65b9\u6cd5\uff0c\u5c06\u4e13\u4e1a\u97f3\u4e50\u521b\u4f5c\u4ea4\u7531\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u6765\u5b8c\u6210\uff0c\u63d0\u9ad8\u97f3\u4e50\u521b\u4f5c\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u9ecf\u5408\u3001\u70b9\u5bf9\u70b9\u4fe1\u53f7\u534f\u8c03\u7684\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u611f\u77e5\u548c\u5b58\u50a8\u548c\u8c10\u3001\u8282\u594f\u548c\u7ed3\u6784\u7ebf\u7d22\uff0c\u81ea\u9002\u5e94\u77ed\u671f\u8bb0\u5fc6\u5e76\u8fbe\u6210\u5171\u8bc6\u3002\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5168\u9762\u53bb\u4e2d\u5fc3\u5316\u7fa4\u4f53\u7684\u4f18\u52a3\uff0c\u5e76\u8fdb\u884c\u4e86\u7b26\u53f7\u3001\u97f3\u9891\u548c\u56fe\u8bba\u5206\u6790\u3002", "result": "\u97f3\u4e50\u5408\u6210\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u53bb\u4e2d\u5fc3\u5316\u7fa4\u4f53\u7684\u5408\u4f5c\u6a21\u5f0f\u80fd\u591f\u63d0\u4f9b\u4f18\u8d28\u3001\u591a\u6837\u5316\u548c\u5177\u6709\u521b\u9020\u6027\u7684\u97f3\u4e50\u4f5c\u54c1\uff0c\u540c\u65f6\u5728\u5168\u5c40\u521b\u9020\u5ea6\u548c\u7ed3\u6784\u591a\u6837\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u57fa\u4e8e\u53bb\u4e2d\u5fc3\u5316\u6a21\u578b\u7684\u97f3\u4e50\u5408\u6210\u65b9\u6cd5\u53ef\u4ee5\u4ea7\u751f\u4f18\u8d28\u548c\u591a\u6837\u5316\u7684\u97f3\u4e50\u4f5c\u54c1\uff0c\u6bd4\u4f20\u7edf\u4e2d\u5fc3\u5316\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u521b\u9020\u529b\u548c\u7ed3\u6784\u591a\u6837\u6027\u3002"}}
{"id": "2509.12034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12034", "abs": "https://arxiv.org/abs/2509.12034", "authors": ["Emmanuel Adjei Domfeh", "Christopher L. Dancy"], "title": "Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review", "comment": "10 pages, 2 figures", "summary": "In high-stakes disaster scenarios, timely and informed decision-making is\ncritical yet often challenged by uncertainty, dynamic environments, and limited\nresources. This paper presents a systematic review of Human-AI collaboration\npatterns that support decision-making across all disaster management phases.\nDrawing from 51 peer-reviewed studies, we identify four major categories:\nHuman-AI Decision Support Systems, Task and Resource Coordination, Trust and\nTransparency, and Simulation and Training. Within these, we analyze\nsub-patterns such as cognitive-augmented intelligence, multi-agent\ncoordination, explainable AI, and virtual training environments. Our review\nhighlights how AI systems may enhance situational awareness, improves response\nefficiency, and support complex decision-making, while also surfacing critical\nlimitations in scalability, interpretability, and system interoperability. We\nconclude by outlining key challenges and future research directions,\nemphasizing the need for adaptive, trustworthy, and context-aware Human-AI\nsystems to improve disaster resilience and equitable recovery outcomes.", "AI": {"tldr": "\u901a\u8fc7\u5bf951\u7bc7\u540c\u884c\u8bc4\u8bae\u7684\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6027\u56de\u987e\uff0c\u63d0\u51fa\u4e86\u5728\u707e\u96be\u7ba1\u7406\u5404\u9636\u6bb5\u652f\u6301\u51b3\u7b56\u7684\u4eba\u5de5\u667a\u80fd\u534f\u4f5c\u6a21\u5f0f\u3002\u53d1\u73b0\u4e86\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\uff1a\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3001\u4efb\u52a1\u548c\u8d44\u6e90\u534f\u8c03\u3001\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u4ee5\u53ca\u6a21\u62df\u548c\u57f9\u8bad\u3002\u5206\u6790\u4e86\u8ba4\u77e5\u589e\u5f3a\u667a\u80fd\u3001\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3001\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u548c\u865a\u62df\u57f9\u8bad\u73af\u5883\u7b49\u5b50\u6a21\u5f0f\u3002\u603b\u7ed3\u8ba4\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53ef\u4ee5\u589e\u5f3a\u60c5\u5883\u611f\u77e5\u3001\u63d0\u9ad8\u54cd\u5e94\u6548\u7387\u5e76\u652f\u6301\u590d\u6742\u51b3\u7b56\uff0c\u4f46\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7cfb\u7edf\u4e92\u64cd\u4f5c\u6027\u65b9\u9762\u5b58\u5728\u5173\u952e\u9650\u5236\u3002\u5f3a\u8c03\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5173\u952e\u6311\u6218\uff0c\u5f3a\u8c03\u9700\u8981\u5177\u6709\u9002\u5e94\u6027\u3001\u53ef\u4fe1\u8d56\u6027\u548c\u4e0a\u4e0b\u6587\u610f\u8bc6\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4ee5\u6539\u5584\u707e\u96be\u97e7\u6027\u548c\u6062\u590d\u7ed3\u679c\u3002", "motivation": "In high-stakes disaster scenarios, timely and informed decision-making is critical but often challenged by uncertainty, dynamic environments, and limited resources. Addressing the need for effective decision support systems in such scenarios, this paper aims to explore Human-AI collaboration patterns to enhance decision-making across all disaster management phases.", "method": "Systematic review of Human-AI collaboration patterns in decision-making across disaster management phases, drawing from 51 peer-reviewed studies. Identified four major categories: Human-AI Decision Support Systems, Task and Resource Coordination, Trust and Transparency, and Simulation and Training. Analyzed sub-patterns such as cognitive-augmented intelligence, multi-agent coordination, explainable AI, and virtual training environments.", "result": "Identified key categories and sub-patterns in Human-AI collaboration for decision-making in disaster management phases. Highlighted the potential of AI systems to improve situational awareness, response efficiency, and decision-making complexity. Pointed out limitations in scalability, interpretability, and system interoperability. Emphasized the importance of adaptive, trustworthy, and context-aware Human-AI systems for enhancing disaster resilience and recovery outcomes.", "conclusion": "AI systems can enhance situational awareness, improve response efficiency, and support complex decision-making in disaster scenarios but face limitations in scalability, interpretability, and system interoperability. Key challenges and future research directions include the need for adaptive, trustworthy, and context-aware Human-AI systems to improve disaster resilience and recovery outcomes."}}
{"id": "2509.12060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12060", "abs": "https://arxiv.org/abs/2509.12060", "authors": ["Wei Cai", "Shujuan Liu", "Jian Zhao", "Ziyan Shi", "Yusheng Zhao", "Yuchen Yuan", "Tianle Zhang", "Chi Zhang", "Xuelong Li"], "title": "When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are susceptible to the implicit\nreasoning risk, wherein innocuous unimodal inputs synergistically assemble into\nrisky multimodal data that produce harmful outputs. We attribute this\nvulnerability to the difficulty of MLLMs maintaining safety alignment through\nlong-chain reasoning. To address this issue, we introduce\nSafe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring\ninterpretable reasoning paths tailored for such a cross-modal challenge. A\nnovel training framework, Safety-aware Reasoning Path Optimization (SRPO), is\nalso designed based on the SSUI dataset to align the MLLM's internal reasoning\nprocess with human safety values. Experimental results show that our\nSRPO-trained models achieve state-of-the-art results on key safety benchmarks,\nincluding the proposed Reasoning Path Benchmark (RSBench), significantly\noutperforming both open-source and top-tier commercial MLLMs.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86SSUI\u6570\u636e\u96c6\u548cSRPO\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8MLLMs\u7684\u5b89\u5168\u6027\u548c\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSRPO\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5b89\u5168\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "MLLMs\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u96be\u4ee5\u7ef4\u6301\u5b89\u5168\u5bf9\u9f50\u6027\uff0c\u4e3a\u6b64\u4ecb\u7ecd\u4e86SSUI\u6570\u636e\u96c6\u548cSRPO\u8bad\u7ec3\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86SSUI\u6570\u636e\u96c6\u548cSRPO\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\u6765\u63d0\u9ad8MLLMs\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5c06\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e0e\u4eba\u7c7b\u5b89\u5168\u4ef7\u503c\u5bf9\u9f50\u3002", "result": "SRPO\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5173\u952e\u5b89\u5168\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4e1a\u5185\u9886\u5148\u7684\u6210\u679c\uff0c\u5305\u62ec\u63d0\u51fa\u7684Reasoning Path Benchmark\uff08RSBench\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u548c\u9876\u5c16\u5546\u4e1aMLLMs\u3002", "conclusion": "\u4ecb\u7ecd\u4e86 Safe-Semantics-but-Unsafe-Interpretation\uff08SSUI\uff09\u6570\u636e\u96c6\u4ee5\u53ca Safety-aware Reasoning Path Optimization\uff08SRPO\uff09\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u9690\u542b\u63a8\u7406\u98ce\u9669\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u5b9e\u9a8c\u7ed3\u679c\u8fbe\u5230\u4e86\u4e1a\u5185\u9886\u5148\u6c34\u5e73\u3002"}}
{"id": "2509.12091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12091", "abs": "https://arxiv.org/abs/2509.12091", "authors": ["Hamied Nabizada", "Lasse Beers", "Alain Chahine", "Felix Gehlhoff", "Oliver Niggemann", "Alexander Fay"], "title": "Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants", "comment": "Presented at the KEPS-Workshop, ICAPS 2025", "summary": "Engineering models created in Model-Based Systems Engineering (MBSE)\nenvironments contain detailed information about system structure and behavior.\nHowever, they typically lack symbolic planning semantics such as preconditions,\neffects, and constraints related to resource availability and timing. This\nlimits their ability to evaluate whether a given system variant can fulfill\nspecific tasks and how efficiently it performs compared to alternatives.\n  To address this gap, this paper presents a model-driven method that enables\nthe specification and automated generation of symbolic planning artifacts\nwithin SysML-based engineering models. A dedicated SysML profile introduces\nreusable stereotypes for core planning constructs. These are integrated into\nexisting model structures and processed by an algorithm that generates a valid\ndomain file and a corresponding problem file in Planning Domain Definition\nLanguage (PDDL). In contrast to previous approaches that rely on manual\ntransformations or external capability models, the method supports native\nintegration and maintains consistency between engineering and planning\nartifacts.\n  The applicability of the method is demonstrated through a case study from\naircraft assembly. The example illustrates how existing engineering models are\nenriched with planning semantics and how the proposed workflow is applied to\ngenerate consistent planning artifacts from these models. The generated\nplanning artifacts enable the validation of system variants through AI\nplanning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728SysML\u57fa\u7840\u5de5\u7a0b\u6a21\u578b\u4e2d\u751f\u6210\u7b26\u53f7\u89c4\u5212\u5de5\u4ef6\u3002\u5176\u4e2d\uff0c\u5f15\u5165\u4e86\u4e13\u7528SysML\u914d\u7f6e\u6587\u4ef6\uff0c\u652f\u6301\u81ea\u52a8\u751f\u6210\u7b26\u53f7\u89c4\u5212\u5de5\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u5de5\u7a0b\u6a21\u578b\u4e0e\u89c4\u5212\u5de5\u4ef6\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u98de\u673a\u88c5\u914d\u6848\u4f8b\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5de5\u7a0b\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u7b26\u53f7\u89c4\u5212\u8bed\u4e49\uff0c\u4f7f\u5f97\u8bc4\u4f30\u7ed9\u5b9a\u7cfb\u7edf\u53d8\u4f53\u662f\u5426\u80fd\u591f\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u4ee5\u53ca\u5176\u6027\u80fd\u6548\u7387\u76f8\u5bf9\u4e8e\u5176\u4ed6\u9009\u62e9\u7684\u80fd\u529b\u53d7\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u586b\u8865\u6b64\u95f4\u9699\uff0c\u5f15\u5165\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f7f\u5de5\u7a0b\u6a21\u578b\u80fd\u591f\u5177\u5907\u89c4\u5212\u8bed\u4e49\uff0c\u5e76\u80fd\u751f\u6210\u7b26\u53f7\u89c4\u5212\u5de5\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u7cfb\u7edf\u53d8\u4f53\u7684\u9a8c\u8bc1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e13\u7528\u7684SysML\u914d\u7f6e\u6587\u4ef6\u5f15\u5165\u6838\u5fc3\u89c4\u5212\u6784\u9020\u7684\u53ef\u91cd\u7528\u6784\u9020\u3002\u8fd9\u4e9b\u6784\u9020\u88ab\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u7ed3\u6784\u4e2d\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u7b97\u6cd5\u5904\u7406\uff0c\u751f\u6210\u7b26\u5408\u89c4\u5212\u57df\u5b9a\u4e49\u8bed\u8a00\uff08PDDL\uff09\u4e2d\u6709\u6548\u7684\u57df\u6587\u4ef6\u548c\u76f8\u5e94\u95ee\u9898\u6587\u4ef6\u3002\u4e0e\u5148\u524d\u4f9d\u8d56\u4e8e\u624b\u52a8\u8f6c\u6362\u6216\u5916\u90e8\u80fd\u529b\u6a21\u578b\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u672c\u5730\u96c6\u6210\uff0c\u4fdd\u6301\u5de5\u7a0b\u548c\u89c4\u5212\u5de5\u4ef6\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u6a21\u578b\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5de5\u7a0b\u6a21\u578b\u4e2d\u5d4c\u5165\u89c4\u5212\u6784\u9020\uff0c\u6709\u6548\u751f\u6210\u4e86\u89c4\u5212\u5de5\u4ef6\uff0c\u5e76\u5448\u73b0\u4e86\u5728\u98de\u673a\u88c5\u914d\u6848\u4f8b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002\u901a\u8fc7\u6240\u751f\u6210\u7684\u89c4\u5212\u5de5\u4ef6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u7cfb\u7edf\u53d8\u4f53\u7684\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728SysML\u57fa\u7840\u5de5\u7a0b\u6a21\u578b\u4e2d\u5f15\u5165\u53ef\u91cd\u7528\u7684\u89c4\u5212\u6784\u9020\uff0c\u5b9e\u73b0\u4e86\u7b26\u53f7\u89c4\u5212\u5de5\u4ef6\u7684\u89c4\u8303\u5316\u751f\u6210\u3002\u8fd9\u79cd\u65b9\u6cd5\u652f\u6301\u672c\u5730\u96c6\u6210\uff0c\u4fdd\u6301\u4e86\u5de5\u7a0b\u548c\u89c4\u5212\u5de5\u4ef6\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u98de\u673a\u88c5\u914d\u6848\u4f8b\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u9002\u7528\u6027\uff0c\u5e76\u8bf4\u660e\u4e86\u5982\u4f55\u4ece\u5de5\u7a0b\u6a21\u578b\u4e2d\u751f\u6210\u4e00\u81f4\u7684\u89c4\u5212\u5de5\u4ef6\uff0c\u4ee5\u5b9e\u73b0\u901a\u8fc7AI\u89c4\u5212\u5bf9\u7cfb\u7edf\u53d8\u4f53\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2509.12104", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12104", "abs": "https://arxiv.org/abs/2509.12104", "authors": ["Zongyue Xue", "Siyuan Zheng", "Shaochun Wang", "Yiran Hu", "Shenran Wang", "Yuxin Yao", "Haitao Li", "Qingyao Ai", "Yiqun Liu", "Yun Liu", "Weixing Shen"], "title": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference", "comment": "This paper has been accepted at CIKM 2025 (Demo Track)", "summary": "The integration of Large Language Models (LLMs) into legal practice raises\npressing concerns about judicial fairness, particularly due to the nature of\ntheir \"black-box\" processes. This study introduces JustEva, a comprehensive,\nopen-source evaluation toolkit designed to measure LLM fairness in legal tasks.\nJustEva features several advantages: (1) a structured label system covering 65\nextra-legal factors; (2) three core fairness metrics - inconsistency, bias, and\nimbalanced inaccuracy; (3) robust statistical inference methods; and (4)\ninformative visualizations. The toolkit supports two types of experiments,\nenabling a complete evaluation workflow: (1) generating structured outputs from\nLLMs using a provided dataset, and (2) conducting statistical analysis and\ninference on LLMs' outputs through regression and other statistical methods.\nEmpirical application of JustEva reveals significant fairness deficiencies in\ncurrent LLMs, highlighting the lack of fair and trustworthy LLM legal tools.\nJustEva offers a convenient tool and methodological foundation for evaluating\nand improving algorithmic fairness in the legal domain.", "AI": {"tldr": "JustEva\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u516c\u5e73\u6027\u7684\u5de5\u5177\u5305\uff0c\u5177\u6709\u7ed3\u6784\u5316\u6807\u7b7e\u7cfb\u7edf\u3001\u6838\u5fc3\u516c\u5e73\u6027\u6307\u6807\u3001\u7edf\u8ba1\u63a8\u65ad\u65b9\u6cd5\u548c\u53ef\u89c6\u5316\u3002\u5b9e\u8bc1\u5e94\u7528\u663e\u793a\u5f53\u524dLLM\u5b58\u5728\u663e\u8457\u7684\u516c\u5e73\u6027\u7f3a\u9677\uff0c\u7f3a\u4e4f\u516c\u5e73\u548c\u53ef\u4fe1\u8d56\u7684LLM\u6cd5\u5f8b\u5de5\u5177\u3002", "motivation": "LLM\u6a21\u578b\u6574\u5408\u5230\u6cd5\u5f8b\u5b9e\u8df5\u4e2d\u5f15\u53d1\u4e86\u5bf9\u53f8\u6cd5\u516c\u5e73\u7684\u7d27\u8feb\u5173\u6ce8\uff0c\u5c24\u5176\u662f\u7531\u4e8e\u5b83\u4eec\u7684\u201c\u9ed1\u5323\u5b50\u201d\u8fc7\u7a0b\u7684\u6027\u8d28\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63d0\u4f9b\u4e00\u79cd\u8bc4\u4f30\u516c\u5e73\u6027\u7684\u5de5\u5177\u65b9\u6cd5\uff0c\u63ed\u793a\u5f53\u524dLLM\u5728\u6cd5\u5f8b\u9886\u57df\u4e2d\u5b58\u5728\u7684\u516c\u5e73\u6027\u7f3a\u9677\uff0c\u5e76\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u7b97\u6cd5\u516c\u5e73\u6027\u63d0\u4f9b\u4fbf\u5229\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u8bba\u57fa\u7840\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86JustEva\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6d4b\u91cfLLM\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u516c\u5e73\u6027\u7684\u8bc4\u4f30\u5de5\u5177\u5305\u3002JustEva\u5177\u6709\u4ee5\u4e0b\u4f18\u52bf\uff1a\uff081\uff09\u6db5\u76d665\u4e2a\u989d\u5916\u6cd5\u5f8b\u56e0\u7d20\u7684\u7ed3\u6784\u5316\u6807\u7b7e\u7cfb\u7edf\uff1b\uff082\uff09\u4e09\u4e2a\u6838\u5fc3\u516c\u5e73\u6027\u6307\u6807 - \u4e0d\u4e00\u81f4\u6027\u3001\u504f\u89c1\u548c\u4e0d\u5e73\u8861\u7684\u4e0d\u51c6\u786e\u6027\uff1b\uff083\uff09\u5f3a\u5927\u7684\u7edf\u8ba1\u63a8\u65ad\u65b9\u6cd5\uff1b\uff084\uff09\u4fe1\u606f\u4e30\u5bcc\u7684\u53ef\u89c6\u5316\u3002\u5de5\u5177\u5305\u652f\u6301\u4e24\u79cd\u7c7b\u578b\u7684\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u5b8c\u6574\u7684\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\uff1a\uff081\uff09\u4f7f\u7528\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u4eceLLM\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff1b\uff082\uff09\u901a\u8fc7\u56de\u5f52\u548c\u5176\u4ed6\u7edf\u8ba1\u65b9\u6cd5\u5bf9LLM\u7684\u8f93\u51fa\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u548c\u63a8\u65ad\u3002", "result": "\u901a\u8fc7JustEva\u7684\u5b9e\u8bc1\u5e94\u7528\u63ed\u793a\u4e86\u5f53\u524dLLM\u5b58\u5728\u663e\u8457\u7684\u516c\u5e73\u6027\u7f3a\u9677\uff0c\u7a81\u51fa\u4e86\u7f3a\u4e4f\u516c\u5e73\u548c\u53ef\u4fe1\u8d56\u7684LLM\u6cd5\u5f8b\u5de5\u5177\u3002", "conclusion": "JustEva\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u3001\u5f00\u6e90\u7684\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u8861\u91cfLLM\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u5f53\u524dLLM\u5b58\u5728\u663e\u8457\u7684\u516c\u5e73\u6027\u7f3a\u9677\uff0c\u7a81\u51fa\u4e86\u7f3a\u4e4f\u516c\u5e73\u548c\u53ef\u4fe1\u8d56\u7684LLM\u6cd5\u5f8b\u5de5\u5177\u3002"}}
{"id": "2509.12179", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.12179", "abs": "https://arxiv.org/abs/2509.12179", "authors": ["Yubo Li", "Weiyi Song"], "title": "Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation", "comment": null, "summary": "Current AI alignment through RLHF follows a single directional paradigm that\nAI conforms to human preferences while treating human cognition as fixed. We\npropose a shift to co-alignment through Bidirectional Cognitive Alignment\n(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,\nrepresentation mapping, and KL-budget constraints for controlled co-evolution.\nIn collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,\nwith 230% better mutual adaptation and 332% better protocol convergence.\nEmergent protocols outperformed handcrafted ones by 84%, while bidirectional\nadaptation unexpectedly improved safety (+23% out-of-distribution robustness).\nThe 46% synergy improvement demonstrates optimal collaboration exists at the\nintersection, not union, of human and AI capabilities, validating the shift\nfrom single-directional to co-alignment paradigms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u53cc\u5411\u8ba4\u77e5\u5bf9\u9f50\uff08BiCA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5728\u534f\u4f5c\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5305\u62ec\u6210\u529f\u7387\u63d0\u9ad8\u3001\u53cc\u5411\u9002\u5e94\u6027\u548c\u534f\u8bae\u6536\u655b\u6027\u663e\u8457\u6539\u5584\uff0c\u9a8c\u8bc1\u4e86\u4ece\u5355\u5411\u5230\u53cc\u5411\u5bf9\u9f50\u8303\u5f0f\u7684\u8f6c\u53d8\u3002", "motivation": "\u76ee\u524d\u7684\u4eba\u5de5\u667a\u80fd\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u662f\u5355\u5411\u7684\uff0c\u8ba4\u4e3a\u4eba\u5de5\u667a\u80fd\u5e94\u7b26\u5408\u4eba\u7c7b\u504f\u597d\uff0c\u800c\u4eba\u7c7b\u8ba4\u77e5\u56fa\u5b9a\u4e0d\u53d8\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5bf9\u9f50\u65b9\u5411\u8f6c\u53d8\u4e3a\u53cc\u5411\u5bf9\u9f50\uff0c\u4f7f\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u80fd\u591f\u76f8\u4e92\u9002\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u5411\u8ba4\u77e5\u5bf9\u9f50\uff08BiCA\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u7684\u534f\u8bae\u3001\u8868\u793a\u6620\u5c04\u548cKL-\u9884\u7b97\u7ea6\u675f\u8fdb\u884c\u53d7\u63a7\u7684\u5171\u540c\u8fdb\u5316\u3002\u901a\u8fc7\u534f\u4f5c\u5bfc\u822a\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u534f\u4f5c\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cBiCA\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5305\u62ec\u6210\u529f\u7387\u63d0\u9ad8\u3001\u53cc\u5411\u9002\u5e94\u6027\u548c\u534f\u8bae\u6536\u655b\u6027\u663e\u8457\u6539\u5584\uff0c\u65b0\u5174\u534f\u8bae\u8868\u73b0\u4f18\u4e8e\u624b\u5de5\u5236\u5b9a\u7684\u534f\u8bae\uff0c\u5e76\u4e14\u53cc\u5411\u9002\u5e94\u8fd8\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6700\u4f73\u534f\u4f5c\u5b58\u5728\u4e8e\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u7684\u4ea4\u96c6\u70b9\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u8ba4\u77e5\u5bf9\u9f50\uff08BiCA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u9002\u5e94\u5b9e\u73b0\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u7684\u534f\u540c\u5bf9\u9f50\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBiCA\u5728\u534f\u4f5c\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8685.5%\u7684\u6210\u529f\u7387\uff0c\u6bd4\u57fa\u51c670.3%\u9ad8\u51fa230%\uff0c\u5728\u534f\u8bae\u6536\u655b\u65b9\u9762\u63d0\u9ad8332%\u3002\u65b0\u5174\u534f\u8bae\u7684\u6027\u80fd\u6bd4\u624b\u5de5\u5236\u5b9a\u7684\u63d0\u9ad8\u4e8684%\uff0c\u800c\u53cc\u5411\u9002\u5e94\u610f\u5916\u5730\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff08+23%\u7684\u8d85\u51fa\u5206\u5e03\u9c81\u68d2\u6027\uff09\u300246%\u7684\u534f\u540c\u4f5c\u7528\u6539\u8fdb\u8868\u660e\uff0c\u5728\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u4ea4\u96c6\u7684\u6700\u4f73\u4f4d\u7f6e\u5b58\u5728\u6700\u4f73\u7684\u534f\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u4ece\u5355\u5411\u5230\u53cc\u5411\u5bf9\u9f50\u8303\u5f0f\u7684\u8f6c\u53d8\u3002"}}
{"id": "2509.12194", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12194", "abs": "https://arxiv.org/abs/2509.12194", "authors": ["Thomas A. Buckley", "Riccardo Conci", "Peter G. Brodeur", "Jason Gusdorf", "Sourik Beltr\u00e1n", "Bita Behrouzi", "Byron Crowe", "Jacob Dockterman", "Muzzammil Muhammad", "Sarah Ohnigian", "Andrew Sanchez", "James A. Diao", "Aashna P. Shah", "Daniel Restrepo", "Eric S. Rosenberg", "Andrew S. Lea", "Marinka Zitnik", "Scott H. Podolsky", "Zahir Kanjee", "Raja-Elie E. Abdulnour", "Jacob M. Koshy", "Adam Rodman", "Arjun K. Manrai"], "title": "Advancing Medical Artificial Intelligence Using a Century of Cases", "comment": null, "summary": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u533b\u5e08\u6807\u6ce8\u548c\u81ea\u52a8\u5316\u5904\u7406\u521b\u5efa\u4e86CPC-Bench\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u5f00\u53d1\u4e86\u4eba\u5de5\u667a\u80fd\u8ba8\u8bba\u8005Dr. CaBot\uff0c\u6a21\u62df\u4e13\u5bb6\u533b\u5b66\u8868\u8fbe\u3002LLMs\u5728\u590d\u6742\u57fa\u4e8e\u6587\u672c\u7684\u4e0d\u540c\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u56fe\u50cf\u89e3\u91ca\u548c\u6587\u732e\u68c0\u7d22\u65b9\u9762\u8f83\u5f31\u3002CPC-Bench\u548cCaBot\u6709\u52a9\u4e8e\u4fc3\u8fdb\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u8fdb\u5c55\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u65b0\u82f1\u683c\u5170\u533b\u5b66\u6742\u5fd7\u7684\u4e34\u5e8a\u75c5\u7406\u4f1a\u8bae\uff08CPCs\uff09\u5df2\u7ecf\u6d4b\u8bd5\u4e86\u4e13\u4e1a\u533b\u751f\u548c\u6700\u8fd1\u4eba\u5de5\u667a\u80fd\u7684\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7684\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u4fa7\u91cd\u4e8e\u6700\u7ec8\u8bca\u65ad\uff0c\u800c\u6ca1\u6709\u6d89\u53ca\u4e13\u5bb6\u8ba8\u8bba\u8005\u6240\u9700\u7684\u591a\u65b9\u9762\u63a8\u7406\u548c\u8868\u8fbe\u6280\u80fd\u3002", "method": "\u5229\u75287102\u4e2aCPC\uff081923-2025\u5e74\uff09\u548c1021\u4e2a\u56fe\u50cf\u6311\u6218\uff082006-2025\u5e74\uff09\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u533b\u5e08\u6807\u6ce8\u548c\u81ea\u52a8\u5316\u5904\u7406\uff0c\u521b\u5efa\u4e86CPC-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ecf\u533b\u5e08\u9a8c\u8bc1\u7684\u57fa\u51c6\uff0c\u8de8\u8d8a\u4e8610\u4e2a\u57fa\u4e8e\u6587\u672c\u548c\u591a\u6a21\u5f0f\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e3b\u8981\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u7136\u540e\uff0c\u5f00\u53d1\u4e86\u201cDr. CaBot\u201d\uff0c\u8fd9\u662f\u4e00\u79cd\u4eba\u5de5\u667a\u80fd\u8ba8\u8bba\u8005\uff0c\u65e8\u5728\u4ec5\u4f7f\u7528\u75c5\u4f8b\u4ecb\u7ecd\u4ea7\u751f\u4e66\u9762\u548c\u57fa\u4e8e\u5e7b\u706f\u7247\u7684\u89c6\u9891\u6f14\u793a\uff0c\u6a21\u62df\u8fd9\u4e9b\u6848\u4f8b\u4e2d\u4eba\u7c7b\u4e13\u5bb6\u7684\u4f5c\u7528\u3002", "result": "\u5728\u6311\u6218\u4e86377\u4e2a\u5f53\u4ee3CPCs\u7684\u60c5\u51b5\u4e0b\uff0co3\uff08OpenAI\uff09\u572860%\u7684\u60c5\u51b5\u4e0b\u5c06\u6700\u7ec8\u8bca\u65ad\u6392\u540d\u7b2c\u4e00\uff0c\u5e76\u572884%\u7684\u60c5\u51b5\u4e0b\u8fdb\u5165\u524d\u5341\u540d\uff0c\u8d85\u8fc7\u4e8620\u540d\u533b\u5e08\u7684\u57fa\u7ebf\uff1b\u4e0b\u4e00\u6b21\u6d4b\u8bd5\u7684\u9009\u62e9\u51c6\u786e\u7387\u8fbe\u5230\u4e8698%\u3002\u4e8b\u4ef6\u7ea7\u533b\u5e08\u6ce8\u91ca\u91cf\u5316\u4e86\u5355\u4f4d\u4fe1\u606f\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u8bca\u65ad\u51c6\u786e\u6027\u3002CaBot\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u751f\u6210\u7684\u6587\u672c\u76f2\u76ee\u6bd4\u8f83\u65f6\uff0c\u533b\u751f\u572862\u6b21\u8bd5\u9a8c\u4e2d\u670946\u6b21\u5bf9\u57fa\u7840\u7684\u5dee\u5f02\u6765\u6e90\u8fdb\u884c\u4e86\u9519\u8bef\u5206\u7c7b\uff0c\u5e76\u8de8\u8d28\u91cf\u7ef4\u5ea6\u66f4\u9752\u7750\u4e8eCaBot\u3002", "conclusion": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u57fa\u4e8e\u6587\u672c\u7684\u4e0d\u540c\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u903c\u771f\u5730\u6a21\u62df\u4e86\u4e13\u5bb6\u533b\u5b66\u8868\u8fbe\uff0c\u4f46\u56fe\u50cf\u89e3\u91ca\u548c\u6587\u732e\u68c0\u7d22\u4ecd\u7136\u8f83\u5f31\u3002CPC-Bench\u548cCaBot\u53ef\u80fd\u4fc3\u8fdb\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u900f\u660e\u548c\u6301\u7eed\u8fdb\u6b65\u8ddf\u8e2a\u3002"}}
