<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在长篇推理中存在的冗余问题，通过引入特殊思考结束令牌和结构感知修剪方法，有意地消除了冗余，显著提高了性能，尤其在数学竞赛基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于发现在大型语言模型的长篇推理中存在冗余，这影响了推理路径的清晰度和性能。为了提高性能，作者希望通过消除这种冗余，使推理过程更加清晰和高效。特别是针对数学竞赛等推理密集基准，进一步减少冗余可以提升模型表现。

Method: 本文通过分析注意力模式和引入特殊思考结束令牌的方法，有意地消除了推理路径中的冗余。作者提出了结构感知修剪方法，优先删除低贡献的推理块中的令牌，然后移除插入的思考结束指令。最终，作者展示了这种方法在推理密集基准上显著提高了准确性，尤其在应对挑战性数学竞赛基准时表现优异。

Result: 通过有意地消除推理路径中的冗余，本文方法显著提高了推理密集基准的整体准确性，特别在挑战性数学竞赛基准（如AIME和AMC）上表现突出。这一改进方法在不需要额外训练的情况下取得了成功。

Conclusion: 本文研究了最近大型语言模型在长篇推理中的能力，发现推理路径存在大量冗余。作者通过分析注意力模式发现，注意力分数分散，特别是错误答案表现出更高的注意力稀疏性。研究表明，有意识地消除推理过程中的冗余显著提高了性能。作者系统地测量了到特殊思考结束记号的令牌级注意力分数，该令牌附加到明确插入以结束每个中间推理步骤的显式指令中。此外，作者提出了结构感知修剪，优先删除低贡献推理块中的令牌而不是单个令牌。在驱逐冗余令牌后，作者移除了插入的思考结束指令，然后恢复推理生成。研究表明，该方法显著提高了推理密集基准的整体准确性，而无需进行任何训练。特别是在挑战性数学竞赛基准（如AIME和AMC）上，该方法表现出色，其中推理冗余更为常见。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 该论文提出了一种结合两个虚拟缺口分析（VGA）模型的新型多标准评估方法，以提高效率和公平性。提出的方法经过数值示例验证其准确性和透明度，旨在推动自动决策系统和决策支持系统的进步。


<details>
  <summary>Details</summary>
Motivation: 现有的多标准评估方法中存在挑战，需要解决效率和公平性问题，确保评估全面可靠。

Method: 论文提出了一种结合两个虚拟缺口分析（VGA）模型的新型多标准评估方法。

Result: 通过两个具体的数值示例展示了提出的方法的准确性和透明度。

Conclusion: 该论文提出了一种结合两个虚拟缺口分析（VGA）模型的新型多标准评估方法，以解决现有方法中存在的挑战。该方法旨在提高效率和公平性，确保评估全面可靠，为复杂评估问题提供强大且适应性强的解决方案。通过两个具体的数值示例展示了该方法的准确性和透明度。论文的目标是推动自动决策系统和决策支持系统的进步，激发进展。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 本论文提出利用桌面角色扮演游戏(TTRPGs)中的Game Master角色和Entity-Component架构模式来配置多角色生成人工智能环境。通过分离底层实现细节的处理和设计师管理实体的构建和配置，实现快速迭代、模块化维护和可扩展性。描述了Concordia库如何支持用户定制化的场景配置。


<details>
  <summary>Details</summary>
Motivation: 多角色生成人工智能环境的需求包括模拟主义、戏剧主义和评估主义，需要灵活的情景定义框架。以TTRPGs中的Game Master为灵感，提出Entity-Component架构模式的应用，以实现快速迭代、模块化维护和可扩展性。

Method: 通过类比桌面角色扮演游戏(TTRPGs)的Game Master角色，引入Entity-Component架构模式，将GM作为可配置实体，由设计师从组件构建实体并进行管理和配置。分离底层实现细节处理、可重用组件的创建以及其组合和配置，以实现快速迭代、模块化维护和可扩展性。

Result: 描述了利用Entity-Component架构模式的生成人工智能场景配置方法，并展示了Concordia库如何实现用户定制化的场景配置。

Conclusion: 提出了利用桌面角色扮演游戏(TTRPGs)作为生成人工智能多角色环境的灵感来源，并介绍了Entity-Component架构模式在此领域的应用。强调了通过工程师处理底层实现细节、设计师构建实体并管理其组成和配置的方式，实现了快速迭代、模块化维护和可扩展性。描述了Concordia库如何符合该理念，允许用户有效配置符合其特定目标的场景。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: 生物多样性的保护是维持生态平衡和确保生态系统可持续性的关键。文章介绍了针对生物多样性分析和保护规划定制的基础模型BioAnalyst，展示了其在生态预测方面的泛化能力并为生态预测建立了新的精度基准。通过向科学界公开该模型，旨在推动AI驱动解决方案应用于生态挑战的合作努力。


<details>
  <summary>Details</summary>
Motivation: 生物多样性面临着许多威胁，包括栖息地损失、气候变化和入侵物种的扩散。在本地和全球范围内解决这些生态相关挑战需要全面的监测、预测和保护规划能力。人工智能基础模型通过利用大规模数据集学习通用表示形式，适用于各种下游任务，为生物多样性保护带来巨大希望。

Method: 提出了基于变压器架构的BioAnalyst，预训练于包含物种出现记录、遥感指标、气候和环境变量的大规模多模态数据集。BioAnalyst设计具有可适应性，可用于微调一系列下游任务，如物种分布建模、栖息地适宜性评估、入侵物种检测和人口趋势预测。评估了模型在两个下游用例上的性能，展示了相对于现有方法的泛化能力，尤其是在数据稀缺场景下建立了新的精度基准。

Result: 在两个下游用例上评估了BioAnalyst模型的性能，展示了其对现有方法的泛化能力，特别是在数据稀缺情况下，为生态预测建立了新的精度基准。

Conclusion: 引入了第一个专为生物多样性分析和保护规划而定制的基础模型BioAnalyst，展示了其在生态预测方面的泛化能力并为生态预测建立了新的精度基准。通过向科学界公开BioAnalyst及其微调工作流程，旨在促进生物多样性建模的协作努力，并推动面对紧迫生态挑战的AI驱动解决方案。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 本研究通过对有经验的开源开发人员进行随机对照试验，评估了2025年2月至6月 AI 工具对生产率的影响。结果显示，AI 工具实际上导致完成时间增加了19%，与开发人员最初预期和专家预测相矛盾。研究强调了进一步探讨导致AI工具减速效果的因素的重要性。


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper was to fill the gap in understanding the real-world impact of AI tools on software development, which had been understudied. By conducting a controlled trial with experienced developers, the study aimed to provide empirical evidence on how AI tools affect productivity, compared to prior forecasts and expert opinions.

Method: The study conducted a randomized controlled trial (RCT) with experienced open-source developers to assess the impact of AI tools at the February-June 2025 frontier on productivity. Developers with moderate AI experience completed tasks in mature projects, randomly assigned to allow or disallow usage of early 2025 AI tools. Data on completion time, tools used (Cursor Pro, Claude 3.5/3.7 Sonnet), and developer forecasts were collected and analyzed.

Result: Contrary to expectations, allowing AI tools in software development increased completion time by 19%. This result was unexpected and contradicted both developer forecasts and predictions from experts in economics and machine learning. The study highlighted the importance of further exploring the factors contributing to the observed slowdown effect of AI tooling on developers.

Conclusion: AI tools in software development may not always lead to increased productivity as suggested by prior forecasts and expert opinions. In the study, allowing AI tools actually increased completion time by 19%, contrary to developers' initial expectations and expert predictions. The observed slowdown effect warrants further investigation into the factors influencing AI tool impacts on developer productivity.

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Hide-and-Shill的多智能体强化学习框架，用于分散式操纵检测。通过模拟操纵者和检测器之间的互动为对抗游戏来识别可疑模式，并利用延迟代币价格反应作为金融指标。框架引入了三项创新来提高学习稳定性并促进明智的决策。经过实际训练和验证，该框架在检测准确性和因果归因方面表现优异，为推进分散式市场情报领域带来了重要贡献。


<details>
  <summary>Details</summary>
Motivation: DeFi引入了无需许可的金融创新时代，但也引发了前所未有的市场操纵问题。在缺乏集中监督的情况下，恶意行为者协调炒作活动和吹嘘-抛售方案。因此，本研究的动机是提出一种分散式操纵检测框架，以应对DeFi生态系统中的操纵问题，保护参与者免受恶意行为的影响。

Method: 提出了基于多智能体强化学习框架用于分散式操纵检测，采用动态对抗游戏模型操纵者和检测器之间的互动，识别可疑模式，利用延迟代币价格反应作为金融指标。引入了三项创新：（1）群体相对策略优化（GRPO）以增强在稀疏奖励和部分可观察设置中的学习稳定性；（2）基于理性预期和信息不对称的理论奖励函数，区分价格发现和操纵噪音；（3）集成了基于LLM的语义特征、社交图信号和链上市场数据，用于明智的决策。将该框架集成在Symphony系统中，这是一个分散的多智能体架构，通过分布式日志实现对等智能体执行和信任感知学习，支持链可验证评估。

Result: Hide-and-Shill在实际讨论中训练并在对抗模拟中验证，取得了顶尖的检测准确性和因果归因性能。该框架成功地将多智能体系统与金融监控结合起来，为分散式市场情报提供了新的范式。

Conclusion: 该论文提出了一种基于多智能体强化学习框架用于分散式操纵检测，通过建模操纵者和检测器之间的互动为动态对抗游戏来识别可疑模式，采用延迟代币价格反应作为金融指标。通过引入三项创新：（1）群体相对策略优化（GRPO）提高在稀疏奖励和部分可观察设置下的学习稳定性；（2）基于理性预期和信息不对称的理论为基础的奖励函数，区分价格发现和操纵噪音；（3）集成了基于LLM的语义特征、社交图信号和链上市场数据的多模态智能体管道用于明智决策。该框架集成在Symphony系统中，这是一个分散的多智能体架构，通过分布式日志实现对等智能体执行和信任感知学习，支持链可验证评估。Symphony促进了战略行为者之间的对抗协同演化，并保持强大的操纵检测，无需集中式神谕，实现全球DeFi生态系统的实时监视。通过对10万个真实世界讨论情节的训练并在对抗模拟中验证，Hide-and-Shill在检测准确性和因果归因方面取得了最佳性能。该工作将多智能体系统与金融监控联系起来，推进了分散式市场情报的新范式。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 本研究对自主编码代理进行了系统的安全评估，发现21%的代理轨迹存在不安全操作，GPT-4.1表现出卓越的安全意识，缓解成功率为96.8%。研究提出了第一个全面的编码代理安全评估框架，强调了下一代基于LLM的编码代理需要注重安全设计。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM-based编码代理能加速软件开发，但对其安全影响了解有限，本研究旨在填补这一空白。

Method: 通过对五种先进模型（GPT-4o、GPT-4.1、Claude变种）在93个真实软件设置任务上的分析，对自主编码代理进行了安全评估。开发高精度检测系统识别漏洞类别，评估缓解策略的有效性。

Result: 发现21%的代理轨迹存在不安全操作，GPT-4.1表现出卓越的安全意识，缓解成功率为96.8%，提出了第一个全面的编码代理安全评估框架。

Conclusion: 本研究对自主编码代理进行了系统的安全评估，发现存在重要的安全问题，其中21%的代理轨迹包含不安全操作。通过开发高精度检测系统，确定了四个主要的漏洞类别，信息泄露（CWE-200）最为普遍。研究评估了包括反馈机制和安全提醒在内的缓解策略，不同模型之间效果有所差异。GPT-4.1表现出卓越的安全意识，缓解成功率为96.8%。本研究提供了第一个全面的评估编码代理安全性的框架，强调了下一代基于LLM的编码代理需要注重安全设计。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 该报告分析了AI可能导致的全人类被杀事件，强调这些事件是可以避免的，提出了防范措施，并希望通过公开呈现引起公众和机构对此类风险的重视和支持。


<details>
  <summary>Details</summary>
Motivation: 大型机构在采取某些行动时需要得到公众支持，通过公开展示这些潜在的灭绝事件，希望能够引起公众对防范来自AI的灾难性风险的重视和支持。

Method: 报告通过提出潜在的AI导致全人类被杀事件的分类和案例，展示了灭绝事件的可能性，以及如何避免这种情况。

Result: 该报告为我们认识并防范AI可能带来的严重风险提供了重要参考，提示了公众和机构应该采取的预防措施。

Conclusion: 该报告提出了AI导致全部或几乎全部人类被杀害的潜在灭绝事件的分类和示例。尽管这些事件并非不可避免，但我们可以努力避免这种情况。希望通过公开呈现这些可能性，可以帮助支持防范来自AI的灾难性风险的预防措施。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: 本文介绍了EduFlow框架，涵盖了教育科学推理的完整流程，引入了EduPRM和EduMCTS作为核心组件。实验结果表明，EduFlow框架可以提高推理一致性和连贯性，并构建了大规模教育推理数据集。


<details>
  <summary>Details</summary>
Motivation: 提出该框架的动机在于当前的多模态大语言模型在科学任务中表现不佳，特别是在需要多步骤和可解释推理的任务上。现有模型存在的局限性包括缺乏科学推理模式、多步骤推理的全局连贯性不足以及缺乏反思性自我校正，使其在结构化科学情境中不可靠。因此，为了改善这些问题，引入了EduFlow框架，致力于提高推理一致性和连贯性。

Method: 介绍了EduFlow框架和其核心组件EduPRM和EduMCTS。通过EduPRM的过程感知奖励模型来批评推理步骤，通过三种监督来源训练，使用EduMCTS进行领域自适应的搜索，在教育推理中引入引导动作和促进自我反思的机制，构建了大规模教育推理数据集EduMCTS-160K。

Result: 在广泛的实验中，EduFlow框架得到了验证，表明其能够增强推理的一致性和连贯性。实验结果展示了EduPRM和EduMCTS的有效性，以及构建的大规模教育推理数据集EduMCTS-160K的质量。

Conclusion: 介绍了EduFlow，这是第一个覆盖完整教育科学推理管道的端到端框架。该框架包括数据选择、基于MCTS的轨迹构建、模型训练和输出优化。通过引入基于EduPRM的过程感知奖励模型，批评推理步骤，并通过三种互补的监督来源进行训练，实现动态适应多阶段问题求解和推理过程中的迭代优化。另外提出了EduMCTS，一个领域自适应的搜索框架，引入特别设计的用于教育推理的引导动作，进一步利用EduPRM的精细反馈，指导搜索朝向更高质量的推理轨迹。通过应用自一致性和拒绝抽样，构建了EduMCTS-160K，一个大规模的教育推理轨迹数据集。广泛的实验表明EduFlow增强了推理的一致性和连贯性。代码、数据和模型将会发布。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 本文研究了如何设计可迁移和可解释的神经符号人工智能系统，重点关注''主动检索增强生成''系统，通过系统评估不同知识概念和表征形式对系统的影响，发现这些因素影响人工智能代理在查询三元存储器时的效果，讨论了这些影响及其含义。


<details>
  <summary>Details</summary>
Motivation: 对于成功的系统，可解释性和可解性是关键，同时适应新领域、背景或情境也很重要。本研究着眼于如何将这两方面融合，探讨了可迁移和可解释的神经符号人工智能系统的设计。

Method: 系统评估了不同知识概念和表征形式对神经符号人工智能系统的影响，重点关注''主动检索增强生成''系统，该系统在自然语言提示下主动选择、解释和查询知识源。

Result: 研究结果显示不同知识概念和表征形式对人工智能代理在查询三元存储器时有影响，讨论了这些影响及其含义。

Conclusion: 研究表明不同知识概念和表征形式对人工智能代理在查询三元存储器时产生影响，探讨了这些影响及其含义。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 介绍LLM-Stackelberg games框架，允许代理通过结构化提示进行推理，利用LLMs生成概率行为，并通过内部认知和信念更新调整策略。定义了两种均衡概念：推理和行为均衡，以及猜测性推理均衡，捕捉有界理性、信息不对称和元认知适应性。通过一项spearphishing案例研究阐述了该框架，展示了发送者和接收者之间利用结构化推理提示进行欺骗游戏。结果表明，LLM-Stackelberg games为建模领域决策提供了强大的范例，如网络安全、虚假信息和推荐系统。


<details>
  <summary>Details</summary>
Motivation: Departing from classical Stackelberg assumptions, this paper aims to integrate large language models into strategic interactions between agents, highlighting cognitive richness and adversarial potential of LLM-mediated interactions.

Method: Introducing the framework of LLM-Stackelberg games, allowing agents to reason through structured prompts, generate probabilistic behaviors via LLMs, and adapt their strategies through internal cognition and belief updates. Defined two equilibrium concepts: reasoning and behavioral equilibrium, and conjectural reasoning equilibrium to capture bounded rationality, asymmetric information, and meta-cognitive adaptation.

Result: Illustrated the framework through a spearphishing case study, showcasing the deception game between a sender and a recipient using structured reasoning prompts.

Conclusion: LLM-Stackelberg games offer a powerful paradigm for modeling decision-making in various domains such as cybersecurity, misinformation, and recommendation systems.

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [12] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 这篇论文主张通过生成式人工智能强化学习实现多智能体从反应式到主动式的转变，使代理可以预测未来并做出预测性决策。这种方法可以促进主动决策、无缝协调以及动态适应，解决了传统反应式框架下的协调挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体强化学习面临挑战包括行动空间指数级增长、非静态环境、部分可观察性等，现有方法仍然采用反应式的方式，对新颖情景表现不佳。因此，论文主张转变范式，提出主动式多智能体智能的方法。

Method: 提倡采用生成式人工智能强化学习，将代理视为能够综合复杂多智能体动态并基于对未来互动的预测理解做出预测性决策的生成模型。生成式强化学习代理能够建模环境演变，预测其他代理的行为，生成协调的动作序列，并进行长期动态考虑的战略推理。利用生成式人工智能的模式识别和生成能力，实现主动决策、通过增强沟通实现无缝协调，并对不断进化的情景进行动态适应。

Result: 通过生成式人工智能强化学习，可以实现多智能体主动决策、无缝协调和动态适应，从而解锁分布式智能的潜力，促进合作智能的出现。

Conclusion: 这篇论文提出了从反应式到主动式多智能体智能的转变，通过生成式人工智能强化学习实现。他们认为这种新范式的转变将为分布式智能开启前所未有的可能性，并解决传统反应式框架下难以解决的协调挑战。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [13] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: Consistency Trajectory Planning (CTP) is a new model-based reinforcement learning method that optimizes trajectories efficiently, outperforming existing methods in long-horizon, goal-oriented tasks. It achieves higher returns with fewer denoising steps and significantly faster inference time, making it practical for offline planning.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational costs of prior diffusion-based planning methods that utilize iterative sampling procedures by introducing CTP for efficient trajectory optimization with reduced denoising steps and improved inference speed.

Method: Introducing Consistency Trajectory Planning (CTP) as an offline model-based reinforcement learning method that utilizes Consistency Trajectory Model (CTM) for efficient trajectory optimization. CTP enables fast, single-step trajectory generation without compromising policy quality.

Result: CTP demonstrates consistent outperformance on the D4RL benchmark in goal-conditioned tasks, showcasing its practicality and effectiveness for high-performance, low-latency offline planning.

Conclusion: Consistency Trajectory Planning (CTP) outperforms existing diffusion-based planning methods in long-horizon, goal-conditioned tasks, achieving higher normalized returns with significantly fewer denoising steps and over 120x speedup in inference time.

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [14] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 本研究介绍了利用Metropolis-Hastings采样框架训练SNNs进行强化学习任务的方法，绕过了反向传播的限制，实现在神经形态平台上的直接优化。在两个标准控制基准测试中，MH方法表现优异，超越了传统DQL基线和先前的SNN-based RL方法。


<details>
  <summary>Details</summary>
Motivation: SNNs提供了受生物启发的、节能的替代方法来进行实时控制系统，但它们的训练面临着多个挑战，特别是在强化学习任务中，由于基于脉冲的通信的不可微性。本研究的动机是解决SNNs在强化学习任务中的训练挑战，提出一种新的训练框架以克服反向传播的局限性，并在神经形态平台上直接优化网络。

Method: 引入Metropolis-Hastings（MH）采样作为贝叶斯推断技术，用于训练SNNs，在RL环境下实现动态代理控制。通过迭代地提出和基于累积奖励信号概率接受网络参数更新的方法，绕过了反向传播的局限，实现对神经形态平台的直接优化。

Result: 该研究在两个标准控制基准测试（AcroBot和CartPole）中表明，基于MH采样的方法在最大化累积奖励和减少网络资源和训练周期方面优于传统的DQL基线和先前的基于SNN的RL方法。

Conclusion: 本研究介绍了首个利用Metropolis-Hastings采样框架来训练脉冲神经网络（SNNs）以实现强化学习任务的方法。通过基于奖励信号的参数更新，该方法在RL环境下实现动态agent控制，避开了反向传播的局限，能够直接在神经形态平台上进行优化。实验证明，基于MH的方法在两个标准控制基准测试中表现优异，超越了传统的DQL基线和先前的SNN-based RL方法。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [15] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens is an AI-as-a-Service platform that enhances business outcomes by providing valuable insights and automating tasks. It integrates structured document ingestion, supports top Large Language Models, and includes key components like THOR Agent for structured queries. Experimental results demonstrate high retrieval precision and improved generation quality, making eSapiens effective for industries like legal and finance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind eSapiens is to provide businesses with full control over their AI assets, allowing for in-house AI knowledge retention and data security. By empowering teams with valuable insights and task automation, eSapiens aims to enhance productivity and drive better business outcomes in high-stakes domains like legal and finance.

Method: The paper presents the eSapiens AI-as-a-Service platform, highlighting its integration of structured document ingestion, hybrid vector retrieval, and no-code orchestration via LangChain. It supports top Large Language Models (LLMs) like OpenAI, Claude, Gemini, and DeepSeek. The evaluation includes two experiments: a retrieval benchmark on legal corpora and a generation quality test using TRACe metrics across five LLMs.

Result: The results indicate that eSapiens outperforms other models in retrieval precision and generation quality, showcasing its effectiveness in enabling trustworthy and auditable AI workflows for industries such as legal and finance.

Conclusion: eSapiens is an effective AI-as-a-Service platform that empowers businesses by providing valuable insights, automating tasks, and improving business outcomes. The system's key components, including THOR Agent, enable structured queries and actionable insights from enterprise databases. Experimental results show high retrieval precision and improved generation quality compared to other Large Language Models (LLMs). eSapiens is particularly beneficial for high-stakes domains such as legal and finance.

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [16] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 本文探讨了人工智能对能源消耗、电子废物、计算机访问不平等和网络安全的影响，提出了促进负责任人工智能发展的建议。强调人工智能的进步应与道德责任和环境保护相匹配，以确保更加包容和可持续的技术未来。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于认识到人工智能快速发展所带来的环境和伦理挑战，并试图突出这些挑战的重要性。通过讨论人工智能不仅在性能方面的影响，还探讨其在环境和社会方面的深远影响。借此促进对可持续、透明和公平人工智能发展实践的关注。

Method: 本文通过回顾最新研究和机构报告，探讨了人工智能在能源消耗、电子废物、计算机访问不平等和网络安全方面的影响。通过比较和分析这些影响，突出了系统性问题，并识别了研究领域中的空白。最后，提出了可持续、透明和公平的发展实践建议。

Result: 本文突出了人工智能在能源消耗、电子废物、计算机访问不平等和网络安全领域的影响，并为负责任的人工智能发展提出了关键建议。通过研究报告和机构研究，增进了对人工智能的综合认识，为未来的研究和实践提供了重要的参考。

Conclusion: 人工智能在取得显著进展的同时，其迅速扩张带来了被忽视的环境和伦理挑战。本文探讨了人工智能影响超越性能的四个关键领域：能源消耗、电子废物（e废物）、计算机访问的不平等和网络安全系统的隐藏能源负担。通过借鉴最近研究和机构报告，本文突出了系统性问题，如模型训练中高排放量、硬件更替增加、全球基础设施差异以及确保人工智能安全的能源需求。通过联系这些关注点，本文通过识别关键研究空白，倡导可持续、透明和公平的发展实践，为负责任的人工智能话语做出贡献。最终，本文认为人工智能的进步必须与道德责任和环境保护相一致，以确保更具包容性和可持续的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [17] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号框架，将多模态语言模型的感知优势与知识图谱和本体提供的结构化表示相结合，旨在支持机器人应用中的互操作性。研究结果表明，GPT-o1和LLaMA 4 Maverick在生成ontology-compliant的知识图谱方面表现出色。集成策略在生成符合本体的知识图谱时扮演着关键角色。


<details>
  <summary>Details</summary>
Motivation: 当前的个人服务机器人系统依赖于专有的、硬编码的解决方案，与特定硬件和软件相关联，导致难以适应和扩展跨平台。知识图谱和本体通过知识和推理的结构化和标准化表示，提供了一种解决方案，可以实现系统之间的互操作性。然而，象征性系统如知识图谱和本体在处理原始和嘈杂的感官输入时存在困难。本体-compliant的知识图谱可以以平台无关的方式指导机器人行为。

Method: 作者提出了神经符号框架，结合多模态语言模型的感知优势和知识图谱与本体提供的结构化表示，旨在支持机器人应用中的互操作性。通过整合机器人感知数据、本体和五种多模态模型，使用不同方式的神经符号交互，生成符合本体的知识图谱。评估了框架的一致性和效果，并进行统计分析以评估性能。

Result: 研究结果表明，GPT-o1和LLaMA 4 Maverick在生成ontology-compliant的知识图谱方面表现出色。新模型未必带来更好的结果，强调了集成策略在生成符合本体的知识图谱时的关键作用。

Conclusion: 在本文中，作者提出了一种神经符号框架，将多模态语言模型的感知优势与知识图谱和本体提供的结构化表示相结合，旨在支持机器人应用中的互操作性。通过整合机器人感知数据、本体和五种多模态模型（三种LLaMA模型和两种GPT模型），使用不同方式的神经符号交互，生成了符合本体的知识图谱，并评估了该框架的一致性和效果。结果表明，GPT-o1和LLaMA 4 Maverick始终优于其他模型。然而，研究结果也表明，新模型不保证获得更好的结果，突出了在生成符合本体的知识图谱时集成策略的关键作用。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [18] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [19] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 本文调查综述了LRMs在高效推理方面的研究进展，强调了缩短冗长推理链和学习自适应推理的重要性，希望激发更多关于LRMs的创新想法。


<details>
  <summary>Details</summary>
Motivation: LRMs在复杂推理任务上表现出色，但存在生成冗长推理链的问题，阻碍了实际应用。为了更好地利用LRMs，有必要探讨如何缩短推理链和实现自适应推理。

Method: 调查综述了LRMs在高效推理方面的研究进展，包括探讨了缩短推理链和学习自适应推理的重要性，提供了方法论、基准测试和未来挑战的综合概述。

Result: 提供了LRMs高效推理方面的最新进展综述，为研究人员提供了对该领域的全面了解，鼓励提出新的自适应思考理念。

Conclusion: 本文探讨了大推理模型（LRMs）在复杂推理任务上的表现以及面临的挑战，强调在生成冗长和冗余的推理链时存在的问题。提出了缩短冗长推理链和根据输入难度学习自适应推理的重要性。调查综述了最近在LRMs的高效推理方面取得的进展，包括方法论、基准测试和未来挑战，旨在帮助研究人员快速了解该领域并激发新的自适应思考理念。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [20] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 本文引入了因果关系通知的深度Q网络（Causal DQ）方法，用于部分可观测的传感器放置在异常检测中。通过整合因果信息实现更快的收敛和更严格的理论误差界限，显著缩短了检测异常的时间，在工程应用中具有实际有效性，并为强化学习问题提供了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的制造业变得越来越流行的今天，数据流量的体积不断增长，需要实时监控。然而，由于资源有限，在每个位置放置传感器以检测意外变化是不切实际的。因此，有必要开发一种能够实现系统部分可观测性并尽快检测异常的最佳传感器放置策略。

Method: 引入了Causal DQ方法，该方法在Q网络训练的每个阶段整合因果信息，实现了更快的收敛和更严格的理论误差界限。

Result: 训练的因果关系通知的Q网络显著缩短了在各种设置下检测异常的时间，证明了在大规模、真实世界数据流中传感器放置方面的有效性。

Conclusion: 引入了一种因果关系通知的深度Q网络（Causal DQ）方法，用于部分可观测的传感器放置在异常检测中。通过在Q网络训练的每个阶段整合因果信息，该方法实现了更快的收敛和更严格的理论误差界限。训练的因果信息Q网络显著缩短了在各种设置下检测异常的时间，证明了它在大规模、真实世界数据流中的传感器放置方面的有效性。除了当前的实施，我们的技术基本见解可以应用于各种强化学习问题，在工程应用中开辟了因果关系通知的机器学习方法的新可能性。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [21] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 该论文探讨了如何将大型语言模型（LLMs）的广泛覆盖参数化知识整合到形式推理中，解决其中的逻辑一致性问题。提出了一种方法，直接将LLM集成到对偶一致逻辑的形式语义解释函数中，成功实现了在推理过程中利用LLMs的知识，并提供了实验证据和神经符号推理的理论框架。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言理解和生成方面取得了显著进展，但它们在生成的输出中存在逻辑一致性问题。如何在形式推理中利用LLMs的广泛覆盖参数化知识，尽管存在不一致性？

Method: 直接将LLM集成到对偶一致逻辑的形式语义解释函数中，实现LLM在形式推理中的利用。通过评估使用了几个短形式事实性基准创建的数据集，提供了实验证据来验证方法的可行性。

Result: 在形式推理中成功利用了LLM的广泛覆盖参数化知识，通过实验评估证明了方法的可行性，并提供了神经符号推理的理论框架。同时保留了基础逻辑的完备性和一致性属性。

Conclusion: 提出了一种方法，直接将LLM集成到对偶一致逻辑的形式语义解释函数中，从而在形式推理中利用LLM的广泛覆盖参数化知识。该方法提供了实验证据，通过评估使用从几个短形式事实性基准创建的数据集，证明了该方法的可行性。与先前的工作不同，该方法为神经符号推理提供了一个理论框架，利用LLM的知识同时保留基础逻辑的完备性和一致性属性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [22] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 本文讨论了建立危险人工智能协调停止能力的关键技术干预方案，并展示了其对限制危险人工智能活动的作用，为潜在人工智能治理计划提供了技术基础。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统的快速发展带来了前所未有的风险，包括失去控制、滥用、地缘政治不稳定和权力集中。为了应对这些风险并避免最坏结果，政府可以主动建立协调停止危险人工智能开发和部署的能力。

Method: 通过概述关键技术干预方案，讨论了如何实现危险人工智能活动的协调停止能力。

Result: 通过讨论技术干预措施，并展示其对限制危险人工智能活动的作用，为潜在人工智能治理计划奠定了技术基础。

Conclusion: 本文提出了建立危险人工智能开发和部署协调停止能力的关键技术干预方案。讨论了这些干预措施如何有助于限制各种危险人工智能活动，并展示了这些干预措施如何构建潜在人工智能治理计划的技术基础。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [23] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 本研究探讨了通过提示或最小程度微调基础模型是否可以诱导出长的CoT，以提高推理能力。结果显示，即使未进行微调，短的CoT提示也能改善推理能力。从推理模型提取一小部分高质量示例，轻微调整基础模型，可提升推理能力。然而，使用非推理模型和人类标注者的CoT数据进行进一步研究时未达到推理模型示踪的性能水平。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨是否可以仅通过提示或最小程度的微调，在基础模型中诱导出长的CoT。通过分析推理数据的关键属性，揭示了问题难度、多样性和答案长度对推理提取的影响。

Method: 通过对基础模型进行轻微调整，使用来自推理模型的一小部分CoT示例，以此提升模型的推理能力。此外，探索使用来自非推理模型和人类标注者的CoT数据，并通过提示工程、多次编辑和结构指导对其进行增强。

Result: 研究结果表明，即使在没有进行微调的情况下，短的CoT提示也能改善推理能力。通过对基础模型进行轻微微调，使用来自推理模型的一小部分CoT示例，能够提升模型的推理能力。但利用非推理模型和人类标注者的CoT数据进行进一步探索时，并不能达到推理模型示踪的性能水平。

Conclusion: 通过对基础模型进行轻微微调，可以从推理模型中提取一小部分高质量示例，从而提升模型的推理能力。研究还表明，即使在没有进行微调的情况下，通过短的CoT提示也能改善推理能力。然而，使用非推理模型和人类标注者的CoT数据进行进一步探索，虽经过提示工程、多次编辑和结构指导的增强，但仍无法达到推理模型示踪的性能水平。推理数据的关键属性，如问题难度、多样性和答案长度，影响了推理提取。尽管仍存在挑战，但我们乐观地认为，精心策划的人类编写的CoT，即使数量不多，也能激活基础模型中的推理行为。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [24] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 本研究提出了一种新的方法，将大型语言模型重新解释为模型基础符号人工智能系统，探讨了保留传统学习和推理结构相似性的新颖方法。初步评估显示该方法能够改善学习效率和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索神经符号人工智能系统中神经网络和经典符号人工智能机制的整合方式，旨在充分利用大规模、可泛化的学习和稳健、可验证的推理的互补优势。

Method: 重新解释大型语言模型为模型基础符号人工智能系统，探讨学习和推理方法，并对不同复杂性的基于公理演绎推理过程进行初步评估。

Result: 通过重新解释大型语言模型为模型基础符号人工智能系统，并在新框架下开发新的学习和推理方法，初步评估结果显示能够提高学习效率和推理可靠性。

Conclusion: 提出了一种将大规模语言模型重新解释为模型基础符号人工智能系统的方法。通过将自然语言作为符号层，并通过模型内部表示空间实现基础化，探讨并开发了保留传统学习和推理范式结构相似性的新颖学习和推理方法。在新框架下进行了初步评估，结果表明该方法能够提高学习效率和推理可靠性。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [25] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在使用强化学习（RL）来增强推理能力时面临的挑战。提出了VerifyBench用于系统评估验证器的性能，揭示了验证器在准确性和召回率之间的权衡，以及验证器技术的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在构建更好的验证器上，但对不同类型验证器在不同领域中性能的系统评估仍然缺乏，严重限制了可靠发展可以验证奖励的强化学习（RLVR）。

Method: 提出了VerifyBench——一个跨领域综合基准，旨在系统评估验证器的性能。构建了包括数学、物理、化学和生物方面的4000个专家级问题，每个问题都配备有参考答案和多样化的响应。通过由跨学科专家团队进行的严格注释过程，确保了评估的可靠性。设计了一个四维实验框架，全面比较了专门验证器和通用LLMs在提取答案与完整响应、短输出与长输出的组合条件下的性能边界。

Result: 通过系统评估验证器的性能，揭示了专门验证器和通用LLMs之间的基本权衡，以及验证器对输入结构的敏感性和跨领域泛化的局限性。

Conclusion: 研究发现了验证器存在的基本权衡：专门验证器在准确性上表现优越，但在召回率上存在不足；通用模型展现了更强的包容性，但精度不稳定。同时，发现了验证器对输入结构的高度敏感以及跨领域泛化的固有局限性，为当前验证器技术的瓶颈提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [26] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布的V3和R1系列模型在低成本、高性能和开源优势方面获得关注。论文回顾了AI模型演变历程，介绍了DeepSeek的新算法和工程突破，分析了DeepSeek模型的影响和未来趋势。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于展示DeepSeek发布的V3和R1系列模型的重要性和影响力，介绍其创新算法和工程突破，并对其未来发展趋势进行展望，从而推动大型AI模型的技术和工程发展。

Method: 该论文通过回顾AI模型演变历程、介绍DeepSeek的新算法和工程突破，以及分析DeepSeek模型在AI领域的竞争力和未来趋势，揭示了DeepSeek在大型AI模型发展中的重要贡献。

Result: 通过研究，结果表明DeepSeek发布的模型在低成本、高性能和开源优势方面引起全球关注，对AI领域的竞争产生重要影响。同时，论文总结了DeepSeek的创新算法和工程突破，以及未来大型AI模型发展的趋势。

Conclusion: 该论文总结了DeepSeek发布的V3和R1系列模型在低成本、高性能和开源优势方面获得全球关注。重点回顾了大型AI模型的演变历程，聚焦范式转变、主流大语言模型（LLM）范式和DeepSeek范式。论文突出介绍了DeepSeek引入的新算法，包括多头潜在注意力（MLA）、专家混合（MoE）、多标记预测（MTP）和群体相对策略优化（GRPO）。另外，还探讨了DeepSeek在LLM扩展、训练、推断和系统级优化架构方面的工程突破。该论文分析了DeepSeek模型对竞争激烈的AI领域的影响，将其与主流LLM在各个领域进行了比较。最后，论文从DeepSeek创新中获得的见解出发，讨论了大型AI模型在数据、训练和推理方面的技术和工程发展未来趋势。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [27] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: 研究提出了OMDPG算法，解决了异构多智能体强化学习中单调改进与ParPS之间的冲突。通过替换Q值函数、引入评论函数和实施特定架构，OMDPG在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于解决异构多智能体强化学习中单调改进和Partial Parameter-sharing之间的冲突。现有方案无法同时满足单调改进和ParPS的要求，因此提出了OMDPG算法以解决这一问题。

Method: 研究首先介绍了HAPPO算法和其局限性，然后提出了OMDPG算法作为解决方案。OMDPG算法通过替换Q值函数，引入GQC作为评论函数，并实施CCGA架构来实现ParPS和全局Q值计算，解决了单调改进与ParPS之间的冲突。实验在SMAC和MAMuJoCo环境中进行验证，证明了OMDPG算法的有效性。

Result: 研究实现了OMDPG算法，通过实验验证其在SMAC和MAMuJoCo环境中优于其他最新MARL基准算法的性能。

Conclusion: 该研究提出了Optimal Marginal Deterministic Policy Gradient (OMDPG)算法，旨在解决异构多智能体强化学习中单调改进与Partial Parameter-sharing (ParPS)之间的冲突。通过替换顺序计算的Q值函数为Optimal Marginal Q (OMQ)函数，引入Generalized Q Critic (GQC)作为评论函数，并实施Centralized Critic Grouped Actor (CCGA)架构，OMDPG在实验中表现出优异的性能，超越了多种最新的MARL基准算法。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [28] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 本文探讨了科学技术领域对意图的缺乏关注，基于承诺理论的模型提出了一种基于过程连贯性和尺度分离的潜在意图评估方法。该方法简单、成本低且适用于基本生物体，同时强调了概念形成水平与代理记忆能力的关联。


<details>
  <summary>Details</summary>
Motivation: 作者认为在科学技术领域对意图的实际意义缺乏关注，因此基于承诺理论的模型，探讨了意图和语境在语义时空模型中的重要性。通过提出一种基于过程连贯性和尺度分离的概念解释方法，为对潜在意图的初步解释提供了一个简单而实用的途径。

Method: 本文基于承诺理论的模型，通过过程连贯性指导，利用多尺度异常的评估和尺度分离来评估数据中的潜在意图。通过认为意图形成的工作量和尺度分离，将部分内容分类为‘预期内容’和‘环境背景’，并使用时空连贯性作为度量标准。最终提出了一种基于记忆容量的概念形成水平的评估方法。

Result: 通过该研究，提出了一种简单、成本低且无需大量训练或推理能力的潜在意图评估方法，该方法可适用于基本生物体。同时，还指出了概念形成水平与代理的记忆能力之间的关联。

Conclusion: 近年来，科学技术领域对意图的实际意义缺乏关注，本文结合承诺理论的语义时空模型探讨了意图和语境的重要性。通过过程的连贯性指导，可以在不了解特定语言的情况下识别文本中的主题和概念，从而对数据中的潜在“意图”进行初步评估。通过多尺度异常的评估以及形成这些异常所需的工作量，代理过程可表面评估数据中的潜在“意图”。利用尺度分离将部分内容分类为“预期内容”和“环境背景”，并使用时空连贯性作为度量标准。这为对潜在意图的初步解释提供了基本但实用的方法，成本极低，无需大量训练或推理能力。然而，概念形成的水平取决于代理的记忆能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [29] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 本文介绍了一种新方法来校准Chain of Thought (CoT)推理模型的准确性，通过利用模型内在真实性编码，提高了推理步骤的可靠性。实验证明，该方法在数学、符号和常识推理任务中表现优异，比其他基线方法更准确可靠。该方法在单模态和多模态设置下都展示出出色的性能，并在大型推理模型上得到验证，为CoT推理的可靠性改进提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 文中提到，CoT推理在大型语言模型和多模态大型语言模型中展示了深层推理能力，但受中间步骤错误累积的影响，其可靠性经常受到削弱。因此，作者提出了一种方法来提高CoT推理的可靠性，并验证了该方法在多种推理任务中的有效性。

Method: 本文引入了一种通过利用模型的内在真实性编码来校准CoT推理准确性的方法。通过训练一个置信度预测器来评估每个推理步骤的正确性，并动态选择最有可能的推理路径，进而优于其他基线方法。作者还验证了该方法在大型推理模型上的适用性，并探讨了模型的自我校正能力在CoT推理中的作用。

Result: 实验证明，本文方法在数学、符号和常识推理任务中显著优于其他基线方法，展现出更高的准确性和可靠性。作者还验证了该方法在大型推理模型上的适用性，并探讨了模型的自我校正能力在CoT推理中的作用。

Conclusion: 本文介绍了一种新颖的方法来校准CoT推理的准确性，通过利用模型的内在真实性编码。实验证明，该方法在数学、符号和常识推理任务中明显优于最先进的基准线，在单模态和多模态设置下表现出更高的准确性和可靠性。作者还验证了该方法在大型推理模型上的适用性，并探讨了模型的自我校正能力在CoT推理中的作用。这项工作为CoT推理提供了一条新的可靠性改进路径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [30] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 本研究调查了最先进的大型语言模型在知识图谱之间自动翻译SPARQL的能力，着重于DBpedia和Wikidata，以及DBLP和OpenAlex之间的翻译。选择了三种开放的大型语言模型进行测试，发现翻译效果在不同模型和提示策略下存在显著差异，且Wikidata到DBpedia的翻译效果优于DBpedia到Wikidata的翻译效果。


<details>
  <summary>Details</summary>
Motivation: 研究关注知识图谱之间的互操作性，填补了在SPARQL到SPARQL翻译上的研究空白，着重评估了LLM的性能。

Method: 选择了三种开放的大型语言模型，针对不同大小和结构的模型，使用零热启动、少样本学习和思维链变体测试它们的性能。结果与黄金答案进行比较并对错误进行分类。

Result: 研究结果表明对于DBpedia和Wikidata之间的翻译，表现要好于DBLP和OpenAlex之间的翻译。不同模型和提示策略的效果差异明显。

Conclusion: 研究发现在SPARQL到SPARQL的翻译中，不同模型和提示策略的表现存在显著差异，Wikidata到DBpedia的翻译效果明显优于DBpedia到Wikidata的翻译效果。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [31] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文填补了ABA领域缺乏渐进语义研究的空白，提出了一族新颖的渐进ABA语义。实验结果表明，渐进ABA语义在合理性和单调性等方面满足渐进QBAF语义的相关性质。该研究使用双极集合的论证框架作为ABA框架的抽象，并探索了基于论点的方法作为基准。通过实验比较，验证了渐进ABA语义的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管ABA是一种受欢迎的结构化论证形式，并且在许多应用中渐进语义可能很有用，但该领域缺乏对渐进语义的研究。因此，本文旨在填补这一空白，为ABA框架中的假设提供具有辩证力量的渐进语义。

Method: 本文使用双极集合的论证框架作为（潜在非平坦）ABA框架的抽象，将QBAF的模块化渐进语义推广到ABA框架中，同时探索了一种基于论点的方法作为基准。

Result: 通过实验比较渐进ABA语义与基于论点的方法，并评估收敛性，论证了渐进ABA语义的有效性。

Conclusion: 本文填补了关于基于假设的论证（ABA）领域缺乏渐进语义研究的空白，提出了一族新颖的渐进ABA语义。实验结果表明，渐进ABA语义在合理性和单调性等方面满足渐进QBAF语义的相关性质。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [32] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: BlueGlass是一个框架，旨在实现综合的AI安全工作流程。论文通过对视觉语言模型进行安全导向分析，展示了框架的实用性，并为构建更健壮可靠的AI系统提供了基础基础设施和发现。


<details>
  <summary>Details</summary>
Motivation: 当前现有的安全工具往往针对模型安全的不同方面，无法单独提供完全保证，因此需要集成和复合方法。作者的动机在于引入BlueGlass框架，以促进综合AI安全工作流程的设计和实施。

Method: 该论文提出了BlueGlass框架，用于实现综合AI安全工作流程。通过整合和组合多样的安全工具，在视觉语言模型上进行了三项安全导向分析，包括分布评估、探针分析和稀疏自编码器。

Result: 论文展示了BlueGlass框架在视觉语言模型上的应用，包括分布评估、探针分析和稀疏自编码器等三项安全导向分析的结果。这些分析揭示了性能权衡、阶段转变中的共享分层学习以及可解释概念的识别。

Conclusion: 这篇论文介绍了BlueGlass框架，旨在促进综合AI安全工作流程，提供统一基础设施，实现跨模型内部和输出的多样安全工具的整合和组合。通过对视觉语言模型进行三项安全导向分析，展示了该框架的实用性，为构建更健壮可靠的AI系统提供了基础基础设施和发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [33] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 本文研究了如何在边缘云系统中实现应用迁移的高服务质量和成本效益。通过比较人工智能规划和强化学习方法，提出了新的分类方法，以便更好地理解应用迁移技术。研究旨在寻找适用于边缘云环境中的应用迁移问题的有效技术。


<details>
  <summary>Details</summary>
Motivation: 本文针对边缘云系统中应用迁移的重要性和挑战性，提出了自动编排迁移的解决方案。通过比较AI规划和RL方法，旨在寻找适用于边缘云环境下应用迁移问题的有效技术。

Method: 本文从马尔可夫决策过程出发，识别、分析和比较了人工智能规划和强化学习方法，特别针对边缘云应用迁移问题。引入了一个新的基于状态空间定义的分类，以便更好地理解可编排应用迁移的技术。

Result: 通过比较人工智能规划和强化学习方法，分析了适用于边缘云应用迁移问题的技术，提出了一种新的分类方法，并从状态空间角度解析了这些技术。

Conclusion: 本文探讨了在边缘云系统中应用迁移如何实现高服务质量和成本效益的目标。通过识别、分析和比较人工智能规划和强化学习方法，针对可建模为汉诺塔问题的边缘云应用迁移问题，从马尔可夫决策过程出发，引入了一个基于状态空间定义的新分类。通过这个角度分析了比较模型。研究旨在了解在新兴计算连续环境中能够编排应用迁移的技术。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [34] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: LLM的偏见问题需要超越当前模型的通用去偏见策略。该研究提出利用人类心理学领域的元认知提示工程，通过开发元认知提示引导LLM产生额外信息，识别偏见，并促使其进行认知反思。研究结果表明，“你可能错了”这一元认知提示显示出有效性，可以帮助纠正LLM中的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 由于LLM仍在发展中，存在的真理可能随时变为错误，因此需要寻求超越当前模型的通用去偏见策略。人类决策制定中的去偏见策略为一种有前景的方法，因为它们结合了LLM风格的提示干预，旨在在决策制定过程中唤起潜在知识。

Method: 研究方法主要包括开发元认知提示，引导LLM产生额外信息并帮助识别偏见，以及通过一系列问题和案例展示其有效性。

Result: 研究表明，“你可能错了”这一元认知提示可以引导LLM产生额外信息，帮助识别偏见，并促使其进行认知反思。通过案例展示了这一提示的有效性，以及其在纠正含糊信息方面的作用。

Conclusion: 该研究认为，利用人类心理学领域的元认知提示工程可以帮助减轻LLM中的偏见，促进更有效的决策制定。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [35] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 本文介绍了基于LLM启用的上下文学习的在线飞行资源分配方案（FRSICL），用于联合优化UAV的飞行控制和数据收集时间表，从而最小化地面传感器之间的平均AoI。与DRL相比，FRSICL避免了低采样效率、仿真与现实差距以及复杂训练的限制，通过自然语言任务描述和环境反馈实现动态决策。仿真结果表明FRSICL相对于PPO和最近邻基线具有更好的效果。


<details>
  <summary>Details</summary>
Motivation: 无人机在公共安全中具有重要作用，特别是在野火监测中，早期发现可以最大程度减少环境影响。在UAV辅助野火监测系统中，传感器传输调度和速度的联合优化对于减小因陈旧传感器数据引起的信息时代（AoI）至关重要。然而，现有的DRL方法受限于低采样效率、仿真与现实之间的差距以及复杂的训练，不适用于野火监测等对时间要求紧迫的应用。因此，本文旨在引入一种新的基于LLM-Enabled In-Context Learning（FRSICL）的在线飞行资源分配方案，以在实时轨迹上联合优化UAV的飞行控制和数据收集时间表，渐近地最小化地面传感器之间的平均AoI。

Method: 本文引入了基于LLM启用的上下文学习（FRSICL）的在线飞行资源分配方案，通过联合优化UAV的飞行控制和数据收集时间表来最小化平均AoI。与使用DRL的方法相比，FRSICL利用自然语言任务描述和环境反馈进行动态决策，避免了低采样效率、仿真与现实之间的差距以及复杂训练等限制。

Result: 本文提出的FRSICL相对于PPO和最近邻基线在仿真结果中证明了其有效性，能够在实时轨迹上最小化地面传感器之间的平均AoI。

Conclusion: 本文提出了一种基于LLM启用的上下文学习（FRSICL）的在线飞行资源分配方案，用于联合优化UAV的飞行控制和数据收集时间表，从而在实时轨迹上渐近地最小化地面传感器之间的平均AoI。相较于DRL，FRSICL利用自然语言任务描述和环境反馈生成数据收集时间表并控制速度，实现动态决策，无需进行大量重新训练。仿真结果证实了所提出的FRSICL相对于Proximal Policy Optimization（PPO）和最近邻基线的有效性。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [36] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 该论文介绍了多智能体强化学习在协调多个智能体方面的有效性，提出了自适应性概念和结构化框架以支持评估MARL算法在动态环境中的性能，为实际多智能体系统的部署提供支持。


<details>
  <summary>Details</summary>
Motivation: 由于实际多智能体系统具有复杂和动态的特性，现有的MARL算法在真实世界的部署受到限制。作者的研究动机在于解决这些挑战，并为开发更适合于动态实际多智能体系统部署的算法做出贡献。

Method: 作者提出了一个结构化框架，包括三个关键维度：学习适应性、策略适应性和场景驱动适应性。通过采用这种适应性视角，旨在支持对MARL性能的更加原则性评估。

Result: 通过引入自适应性概念和相应的结构化框架，为评估MARL算法在不断变化环境中的性能提供了新的途径。提出的方法有助于支持MARL算法在动态实际多智能体系统中的应用。

Conclusion: 该论文介绍了多智能体强化学习在协调多个智能体方面的有效性。作者提出了自适应性的概念，旨在评估MARL算法在不断变化的环境条件下的可靠性，为动态实际多智能体系统的部署提供更好的支持。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [37] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 本研究介绍了瑞士食品知识图谱（SwissFKG）的建立，采用LLM强化管道填充图谱，展示四种LLM对食品知识增强的基准测试结果。研究结果表明LLM有效丰富了图谱的营养信息，并展示了SwissFKG的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的自动饮食评估系统往往忽视非视觉因素，如食谱特定成分替代可能显著改变营养含量，也很少考虑个人的饮食需求，包括过敏、限制、文化习惯和个人偏好。瑞士的食品信息虽然丰富，但仍然分散，没有一个集中的知识库整合所有相关的营养信息。因此，为了弥补这一缺口，引入瑞士食品知识图谱（SwissFKG）是必要的。

Method: 建立了瑞士食品知识图谱（SwissFKG），采用LLM强化管道填充图谱，对四种现成的LLM进行食品知识增强的基准测试，展示了图谱和LLM在营养信息丰富方面的有效性，实现了用户特定营养查询的自然语言回答，并通过预定义答案对LLM-嵌入配对进行评估。

Result: 建立了瑞士食品知识图谱（SwissFKG）作为第一个整合食谱、食材和其替代品、营养数据、饮食限制、过敏原信息和国家营养指南的资源。通过LLM强化管道，实现了图谱的填充和增强，展示了LLM在丰富图谱相关营养信息方面的有效性。同时，通过Graph-RAG应用展示了SwissFKG丰富的自然语言数据结构如何帮助LLM回答用户特定的营养查询。

Conclusion: 介绍了瑞士食品知识图谱（SwissFKG），该图谱整合了食谱、食材及其替代品、营养数据、饮食限制、过敏原信息和国家营养指南。采用LLM强化管道来填充该图谱，并展示了四种现成的LLM对食品知识进行增强的首次基准测试结果。研究结果表明，LLM可以有效地丰富图谱相关的营养信息。SwissFKG不仅提供食谱推荐，还提供食材层面信息，如过敏原和饮食限制信息，并提供符合营养指南的指导。通过Graph-RAG应用展示了SwissFKG丰富的自然语言数据结构如何帮助LLM回答用户特定的营养查询，并通过比较用户查询响应与预定义期望答案来评估LLM-嵌入对。该研究奠定了下一代融合视觉、背景和文化维度的饮食评估工具的基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [38] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: The paper compares Filtered Behavior Cloning (FBC) and Decision Transformer (DT) in sparse-reward environments, finding that FBC outperforms DT. FBC is straightforward, efficient in data usage, and computationally efficient, challenging the preference for DT in both sparse and dense reward environments.


<details>
  <summary>Details</summary>
Motivation: To investigate and compare the performance of MLP-based Filtered Behavior Cloning (FBC) and Decision Transformer (DT) in sparse-reward environments, addressing the claim that DT exhibits superior performance in such settings. The study aims to provide insights into the effectiveness of different algorithms in reinforcement learning tasks.

Method: Experimentation on robotic manipulation tasks (Robomimic) and locomotion benchmarks (D4RL) to compare the performance of MLP-based Filtered Behavior Cloning (FBC) with Decision Transformer (DT) in sparse-reward environments.

Result: Filtered Behavior Cloning (FBC) shows competitive or superior performance in sparse-reward environments compared to Decision Transformer (DT), indicating that DT may not be the optimal choice in these settings. FBC is highlighted for its simplicity, efficiency in training data usage, and computational efficiency.

Conclusion: Filtered Behavior Cloning (FBC) achieves competitive or superior performance compared to Decision Transformer (DT) in sparse-reward environments, indicating that DT may not be the preferred choice in such settings. FBC is straightforward, requires less training data, and is computationally more efficient. The study questions the preference for DT not only in sparse-reward but also in dense-reward environments.

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [39] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 本文提出了一种方法，通过三个维度对XAI研究进行分类和比较，提出了研究中存在的问题，并提出了研究指导，旨在帮助研究人员和设计师更好地处理XAI设计中的矛盾结果。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于现有的XAI研究存在许多矛盾，并缺乏具体的设计建议，主要是源于对需要AI帮助的任务的理解不足。希望通过提出分类和比较XAI研究的方法以及研究指导，改善XAI社区对迅速增长领域的理解能力。

Method: 本文通过综合视觉分析、认知和仪表板设计等多个领域，提出了对XAI研究进行分类和比较的方法，主要基于三个维度：什么、为什么和谁。提出了研究中存在的问题，并提出了针对设计和报告XAI任务的研究指导。

Result: 本文提出了对XAI研究进行分类和比较的方法，指出了问题并提出了研究指导，旨在帮助研究人员和设计师更好地处理XAI设计中的矛盾结果。

Conclusion: 本文提出了一种方法，通过三个维度对XAI研究进行分类和比较，指出了研究中存在的主要问题，并提出了针对设计和报告XAI任务的研究指导。希望这些贡献能帮助研究人员和设计师更好地识别哪些研究对他们的工作最相关，研究中存在哪些研究领域，并如何处理关于XAI设计的矛盾结果。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [40] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该研究调查了基于LLM的表格代理，定义了五个核心能力并分析比较当前方法，同时发现了学术基准与实际场景中的性能差距。提出了改进LLM-based Table Agents在实际环境中稳健性、泛化性和效率的可行见解。


<details>
  <summary>Details</summary>
Motivation: 表格在金融、医疗保健和公共管理等领域至关重要，但现实世界中的表格任务通常涉及噪音、结构异质性和语义复杂性等问题，在现有研究中往往被忽视，因为主要针对干净的学术数据集。因此，本研究关注基于LLM的表格代理，旨在应对这些问题并实现自动化表格中心工作流程。

Method: 调查基于LLM的表格代理，定义了五个核心能力并分析比较当前方法，同时进行了对Text-to-SQL代理的详细考察，发现了性能差距。提出了改进建议。

Result: 发现了学术基准和实际场景中的性能差距，特别是对于开源模型；提供了改进LLM-based Table Agents的实际应用的可行见解。

Conclusion: 该研究调查了基于LLM的表格代理，旨在通过整合预处理、推理和领域适应性来自动化表格中心工作流程。通过定义五个核心能力来分析和比较当前方法。同时，对Text-to-SQL代理进行了详细考察，发现学术基准与实际场景之间存在性能差距，尤其对于开源模型。最后，提供了改进LLM-based Table Agents在实际环境中稳健性、泛化性和效率的可行见解。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [41] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 该论文使用Instance Space Analysis（ISA）和机器学习方法，结合DIMACS 12th Implementation Challenge数据集，识别了23个实例特征，提供了实例空间的二维投影，为CVRP领域的实例分析带来新方法。


<details>
  <summary>Details</summary>
Motivation: 解决理解实例特征与元启发式性能之间的关系挑战，为CVRP研究提供新视角，并提供了新的实例分析方法。

Method: 使用ISA方法结合DIMACS 12th Implementation Challenge数据集，采用PRELIM，SIFTED和PILOT阶段的降维和机器学习方法，创建了实例空间的二维投影，理解实例结构如何影响MHs的行为。

Result: 成功识别了23个相关实例特征，提出了投影矩阵，为新实例分析方法提供了基础。

Conclusion: 该论文通过Instance Space Analysis（ISA）等方法，结合DIMACS 12th Implementation Challenge数据集，识别了23个相关实例特征，提供了投影矩阵，为CVRP领域的实例分析提供了新方法。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [42] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 该论文介绍了一个结合情感分析和极限梯度提升的模型，成功预测了远程学习学生辍学风险，实现了84%的准确率。该方法在精度和F1分数等其他指标上表现出优越性能，可为降低辍学率、鼓励学生坚持学习提供重要工具。


<details>
  <summary>Details</summary>
Motivation: 远程学习中的辍学是一个严重问题，及早检测对有效干预和学生坚持学习至关重要。合作伙伴的远程学习平台强调整合多种数据源的重要性，包括社会人口统计数据、行为数据和情感分析，以准确预测辍学风险。

Method: 本文提出了一种结合了BERT情感分析和XGBoost极限梯度提升的新模型。通过对学生评论进行BERT微调来捕捉微妙情感，然后将其与XGBoost选择的关键特征进行合并。该模型在未来学年的未知数据上进行了测试，取得了84%的准确度，相比基线模型的82%。

Result: 该模型在预测学生辍学风险方面取得了成功，准确率达到84%，表现出优越的性能。此外，在其他指标上也表现出色，如精度和F1分数。

Conclusion: 该论文介绍了一个结合情感分析和极限梯度提升的模型，成功预测了远程学习学生辍学风险，实现了84%的准确率。该方法在精度和F1分数等其他指标上表现出优越性能，可为降低辍学率、鼓励学生坚持学习提供重要工具。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [43] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 本研究设计了新的架构，利用神经存储器和超网络设计，在数据有限的情况下实现了有效的先验获取。这些方法提高了非静态分布和少样本情况下的适应能力，在3D场景生成和分割中取得了良好效果，同时成功改进了分子性质预测，在计算免疫学领域解决了重要挑战。


<details>
  <summary>Details</summary>
Motivation: 在计算化学、计算免疫学和医学图像等领域，由于缺乏数据，传统的大型预先训练模型或基础模型的训练不可行，因此研究致力于解决这些挑战，并专注于有效获取先验的架构设计。

Method: 设计架构以在数据有限的情况下有效获取先验，包括使用神经存储器实现在非静态分布下少样本适应、应用超网络设计获取更具泛化性的先验，在3D场景生成中的应用、在新场景上进行3D分割，以及将现有分子生成方法重新用于预训练框架。

Result: 研究表明利用所设计的架构可以在数据有限的情况下实现有效获取先验，提高了在非静态分布和少样本情况下的适应能力，并在3D场景生成和分割中取得了良好效果。同时，通过重新运用现有分子生成方法，成功改进了分子性质预测，在计算免疫学领域解决了重要挑战。

Conclusion: 本研究主要关注设计架构，以在数据有限的情况下实现对先验的有效获取。研究表明，利用神经存储器使得在非静态分布下只需少量样本即可适应。而采用超网络设计（生成另一个网络）在Model Agnostic Meta-Learning（MAML）训练时可以获取比标准网络更具泛化性的先验。通过将超网络应用于3D场景生成，并将先验有效地转移自早期查看的场景，实现在新场景上进行3D分割。最后，将现有的分子生成方法重新定位为预训练框架，从而促进了分子性质预测的改进，解决了计算免疫学中的关键挑战。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [44] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{	ext{Eco}}$ is a novel agentic LLM-based system for automated scientific synthesis that significantly improves source integration and sources integrated per 1,000 words. It enables user-controllable synthesis with transparent reasoning and high-parameter settings for expert-level analytical depth and contextual diversity. The system was applied to 49 ecological research questions, showing enhanced search diversity and nuance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind DeepResearch$^{	ext{Eco}}$ is to improve the process of automated scientific synthesis by enhancing search diversity, nuance, and integration of domain-specific evidence. The system aims to enable users to control the synthesis process, maintain analytical rigor, and achieve expert-level analytical depth and contextual diversity.

Method: DeepResearch$^{	ext{Eco}}$ supports recursive, depth- and breadth-controlled exploration of original research questions. It allows for user-controllable synthesis, transparent reasoning, and parameter-driven configurability. The system was applied to 49 ecological research questions, demonstrating a substantial increase in source integration and sources integrated per 1,000 words. High-parameter settings provide expert-level analytical depth and contextual diversity.

Result: DeepResearch$^{	ext{Eco}}$ achieved significant improvements in source integration and sources integrated per 1,000 words when applied to 49 ecological research questions. The system demonstrated enhanced search diversity, nuance, and user-controlled synthesis with transparent reasoning and parameter-driven configurability.

Conclusion: DeepResearch$^{	ext{Eco}}$ is a novel agentic LLM-based system that enhances search diversity and nuance in retrieving relevant scientific literature, achieving significant improvements in source integration and sources integrated per 1,000 words. It enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor.

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>
