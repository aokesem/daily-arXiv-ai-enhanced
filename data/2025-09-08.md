<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management](https://arxiv.org/abs/2509.04505)
*Somtochukwu Azie,Yiping Meng*

Main category: cs.AI

TL;DR: 本研究评估了LLMs在CPM中的道德可行性和可靠性，发现它们在结构化领域表现良好但在处理情境差别和提供透明推理方面存在缺陷。利益相关者主张对AI的自主使用进行监督。研究强调LLMs目前适合作为决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估LLMs应用于CPM中道德敏感、高风险决策背景的道德可行性和可靠性，以填补该领域的研究空白。研究结论将为建筑领域的决策者和利益相关者提供关于LLMs在伦理推理方面的实证数据和建议。

Method: 采用混合方法研究设计，实施定量性能测试和定性分析。使用新型道德决策支持评估清单（EDSAC）评估LLMs在十二个现实世界道德场景下的表现。对12位行业专家进行半结构化访谈以捕捉专业认知。

Result: 研究发现LLMs在结构化领域表现良好，但在处理情境细微差别、确保问责制和提供透明推理方面存在缺陷。利益相关者强烈主张对AI的自主使用进行监督。研究引入了EDSAC框架作为一种可复制的方法论，并强调LLMs目前最适用于决策支持工具。

Conclusion: 人工智能（AI）在建筑项目管理（CPM）中的整合正在加速，大型语言模型（LLMs）作为可访问的决策支持工具崭露头角。研究旨在批判性评估将LLM应用于CPM中固有的道德敏感、高风险决策背景时的道德可行性和可靠性。研究采用了混合方法研究设计，涉及使用新型道德决策支持评估清单（EDSAC）对两种主要LLMs在十二个现实世界道德场景下的定量性能测试，并对12位行业专家进行半结构化访谈的定性分析，以捕捉专业认知。研究结果显示，虽然LLMs在结构化领域（如法律合规）中表现出色，但在处理情境细微差别、确保问责制和提供透明推理方面存在显著缺陷。利益相关者对于将AI自主用于伦理判断表示相当保留，强烈主张强大的人类监督。据我们所知，这是第一项在建筑领域内对LLMs的道德推理进行实证测试的研究之一。它将EDSAC框架作为可复制的方法论，并提供可操作的建议，强调目前LLMs最适用于决策支持工具而非自主道德代理的立场。

Abstract: The integration of Artificial Intelligence (AI) into construction project
management (CPM) is accelerating, with Large Language Models (LLMs) emerging as
accessible decision-support tools. This study aims to critically evaluate the
ethical viability and reliability of LLMs when applied to the ethically
sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods
research design was employed, involving the quantitative performance testing of
two leading LLMs against twelve real-world ethical scenarios using a novel
Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis
of semi-structured interviews with 12 industry experts to capture professional
perceptions. The findings reveal that while LLMs demonstrate adequate
performance in structured domains such as legal compliance, they exhibit
significant deficiencies in handling contextual nuance, ensuring
accountability, and providing transparent reasoning. Stakeholders expressed
considerable reservations regarding the autonomous use of AI for ethical
judgments, strongly advocating for robust human-in-the-loop oversight. To our
knowledge, this is one of the first studies to empirically test the ethical
reasoning of LLMs within the construction domain. It introduces the EDSAC
framework as a replicable methodology and provides actionable recommendations,
emphasising that LLMs are currently best positioned as decision-support aids
rather than autonomous ethical agents.

</details>


### [2] [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
*Wenxiao Wang,Priyatham Kattakinda,Soheil Feizi*

Main category: cs.AI

TL;DR: Maestro is a framework-agnostic optimizer for LLM agents that searches over graphs and configurations, outperforming existing prompt optimizers by improving sample efficiency and addressing structural failure modes. It consistently surpasses leading prompt optimizers on benchmarks and achieves significant improvements in applications like interviewer & RAG agents.


<details>
  <summary>Details</summary>
Motivation: Existing optimizers focus on tuning configurations while ignoring structural failure modes, leading to suboptimal performance of LLM agents. Maestro aims to maximize agent quality by considering both graph structures and node configurations.

Method: Introducing Maestro, a holistic optimizer for LLM agents that jointly searches over graphs and configurations, leveraging reflective textual feedback for prioritizing edits.

Result: Maestro consistently surpasses MIPROv2, GEPA, and GEPA+Merge prompt optimizers on IFBench and HotpotQA benchmarks, achieving an average improvement of 12%, 4.9%, and 4.86%, respectively. Even in prompt-only optimization, Maestro leads by 9.65%, 2.37%, and 2.41%. It requires fewer rollouts compared to GEPA and shows significant gains in interviewer & RAG agent applications.

Conclusion: Maestro outperforms leading prompt optimizers in optimizing LLM agents by simultaneously searching over graphs and configurations, improving sample efficiency and addressing structural failure modes.

Abstract: Building reliable LLM agents requires decisions at two levels: the graph
(which modules exist and how information flows) and the configuration of each
node (models, prompts, tools, control knobs). Most existing optimizers tune
configurations while holding the graph fixed, leaving structural failure modes
unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for
LLM agents that jointly searches over graphs and configurations to maximize
agent quality, subject to explicit rollout/token budgets. Beyond numeric
metrics, Maestro leverages reflective textual feedback from traces to
prioritize edits, improving sample efficiency and targeting specific failure
modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses
leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,
4.9%, and 4.86%, respectively; even when restricted to prompt-only
optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these
results with far fewer rollouts than GEPA. We further show large gains on two
applications (interviewer & RAG agents), highlighting that joint graph &
configuration search addresses structural failure modes that prompt tuning
alone cannot fix.

</details>


### [3] [Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization](https://arxiv.org/abs/2509.04646)
*Philippe J. Giabbanelli,Ameeta Agrawal*

Main category: cs.AI

TL;DR: 本文探讨了如何利用大型语言模型生成定制的健康模拟解释。通过逐步框架和混合方法设计，成功识别了健康利益相关者的需求，并通过优化LLMs的能力生成了定制输出，最终进一步改进了生成的摘要。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于现有的M&S方法在支持决策活动中具有潜在潜力，尤其在健康领域。然而，由于这些模型的复杂性，这种潜力可能无法完全实现，因为这使得这些模型对最有利益的利益相关者不可及。另外，当前的LLMs方法通常依赖于一揽子摘要，无法满足临床医生，政策制定者，患者，照料者和健康倡导者的多样化信息需求和风格偏好。因此，本文旨在填补这一领域的知识空白，以提供更好地满足不同利益相关者需求的解释。

Method: 本文采用了混合方法设计，通过逐步框架来识别健康利益相关者的需求，指导LLMs生成定制的解释。首先引出了健康利益相关者的解释需求和风格偏好，然后通过优化LLMs的能力，并使用可控属性调整来生成定制输出，最后通过全面的度量评估来改进生成的摘要。

Result: 通过提出的逐步框架和混合方法设计，成功识别了健康利益相关者的需求，并指导LLMs生成了定制的健康模拟解释。通过优化和评估，进一步改进了生成的摘要。

Conclusion: 本文提出了一个逐步框架，旨在识别利益相关者的需求并指导大型语言模型（LLMs）生成定制的健康模拟解释。通过混合方法设计，首先引出了不同健康利益相关者的解释需求和风格偏好，然后通过优化LLMs的能力来生成定制输出（例如通过可控属性调整），最后通过全面的度量评估来进一步改进摘要的定制生成。

Abstract: Modeling & Simulation (M&S) approaches such as agent-based models hold
significant potential to support decision-making activities in health, with
recent examples including the adoption of vaccines, and a vast literature on
healthy eating behaviors and physical activity behaviors. These models are
potentially usable by different stakeholder groups, as they support
policy-makers to estimate the consequences of potential interventions and they
can guide individuals in making healthy choices in complex environments.
However, this potential may not be fully realized because of the models'
complexity, which makes them inaccessible to the stakeholders who could benefit
the most. While Large Language Models (LLMs) can translate simulation outputs
and the design of models into text, current approaches typically rely on
one-size-fits-all summaries that fail to reflect the varied informational needs
and stylistic preferences of clinicians, policymakers, patients, caregivers,
and health advocates. This limitation stems from a fundamental gap: we lack a
systematic understanding of what these stakeholders need from explanations and
how to tailor them accordingly. To address this gap, we present a step-by-step
framework to identify stakeholder needs and guide LLMs in generating tailored
explanations of health simulations. Our procedure uses a mixed-methods design
by first eliciting the explanation needs and stylistic preferences of diverse
health stakeholders, then optimizing the ability of LLMs to generate tailored
outputs (e.g., via controllable attribute tuning), and then evaluating through
a comprehensive range of metrics to further improve the tailored generation of
summaries.

</details>


### [4] [An Approach to Grounding AI Model Evaluations in Human-derived Criteria](https://arxiv.org/abs/2509.04676)
*Sasha Mitts*

Main category: cs.AI

TL;DR: 本研究提出了一种新的方法，通过整合人类衍生的评估标准来增强现有基准测试的可解释性和适用性。研究结果表明参与者认为人工智能在解释能力和情感理解方面存在不足，但对其性能寄予高期望。该研究强调了用户中心的评估在人工智能发展中的重要性，并为研究人员和从业者提供了可操作的指导，为人工智能模型评估的未来进展奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 本研究针对人工智能在模型行为方面存在的可解释性和适用性问题，以物理世界建模案例为重点，旨在通过人类衍生的评估标准增强模型行为的可解释性和适用性。参与者认为人工智能在解释能力和移情能力方面存在不足，但对其性能寄予厚望。

Method: 本研究通过深入访谈和大规模调查，从感知测试和OpenEQA基准出发，识别出对人工智能和人类推理都至关重要的关键认知技能，如优先级制定、记忆、辨别和情境化等。将所得结论整合到基准设计中，提出了一个框架，用于开发更符合人类需求的定义和衡量进展的方式。

Result: 研究结果揭示了参与者认为人工智能在解释能力和情感理解方面存在不足，但对其性能寄予较高期望。通过融合研究发现的见解到基准设计中，为制定更符合人类认知过程的定义和衡量进展的框架提供了新思路。

Conclusion: 本研究通过整合人类衍生的评估标准，提出了一种新颖方法来增强现有基准测试的可解释性和适用性，强调了用户中心的评估在人工智能发展中的重要性，并为研究人员和从业者提供可操作的指导。该方法不仅增强了当前的基准测试实践，还为人工智能模型评估的未来进展奠定了基础。

Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional
benchmarks can fall short in attempting to capture the nuanced capabilities of
AI models. We focus on the case of physical world modeling and propose a novel
approach to augment existing benchmarks with human-derived evaluation criteria,
aiming to enhance the interpretability and applicability of model behaviors.
Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted
in-depth interviews and large-scale surveys to identify key cognitive skills,
such as Prioritization, Memorizing, Discerning, and Contextualizing, that are
critical for both AI and human reasoning. Our findings reveal that participants
perceive AI as lacking in interpretive and empathetic skills yet hold high
expectations for AI performance. By integrating insights from our findings into
benchmark design, we offer a framework for developing more human-aligned means
of defining and measuring progress. This work underscores the importance of
user-centered evaluation in AI development, providing actionable guidelines for
researchers and practitioners aiming to align AI capabilities with human
cognitive processes. Our approach both enhances current benchmarking practices
and sets the stage for future advancements in AI model evaluation.

</details>


### [5] [Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning](https://arxiv.org/abs/2509.04731)
*Brennen Hill*

Main category: cs.AI

TL;DR: 本文阐述了语言模型、Agent模型和世界模型的融合对人工智能的重要性，强调了具有分层世界模型的环境对培养智能Agent的关键性。通过结合符号和分层方法与多智能体强化学习，可以构建任务为基础的世界模型来指导Agent学习。作者提出了一种新的范式转变，利用大型语言模型动态生成分层脚手架，使得Agent模型能够以更高的样本效率获得复杂的战略行为。


<details>
  <summary>Details</summary>
Motivation: 论文指出了发展具有明确的、分层世界模型的环境对于开发能力强大的Agent的重要性。传统的强化学习训练在结构平坦的模拟器中往往由于探索空间过大和奖励稀疏而失败。作者认为下一个发展前沿在于创建具有显式、分层世界模型的环境，通过分解复杂目标为结构化子目标实现。

Method: 论文通过系统性审阅2024年的多智能体足球研究，识别出一种明显和决定性的趋势，即将符号和分层方法与多智能体强化学习相结合。这些方法隐式或显式构建了基于任务的世界模型以指导Agent学习。作者提出了一种新的范式转变，即利用大型语言模型来动态生成这种分层脚手架，有效利用语言来实时构建世界模型。

Result: 通过结合符号和分层方法与多智能体强化学习，可以构建任务为基础的世界模型来指导Agent学习。提出利用大型语言模型动态生成分层脚手架的新范式，使得Agent模型能够以更高的样本效率获得复杂的战略行为。

Conclusion: 这篇论文探讨了发展具有明确分层世界模型的环境对培养能力强大的智能Agent的重要性。通过将复杂目标分解为结构化、可管理的子目标，可以实现创建具有分层世界模型的环境。作者提出了一种新的范式转变，即利用大型语言模型动态生成这种层次化脚手架，从而有效利用语言来实时构建世界模型。通过创建具有明确的、可配置语言任务层的环境，可以填补低层反应行为和高层战略团队协作之间的鸿沟，为训练下一代智能Agent提供了一个强大且通用的框架。

Abstract: The convergence of Language models, Agent models, and World models represents
a critical frontier for artificial intelligence. While recent progress has
focused on scaling Language and Agent models, the development of sophisticated,
explicit World Models remains a key bottleneck, particularly for complex,
long-horizon multi-agent tasks. In domains such as robotic soccer, agents
trained via standard reinforcement learning in high-fidelity but
structurally-flat simulators often fail due to intractable exploration spaces
and sparse rewards. This position paper argues that the next frontier in
developing capable agents lies in creating environments that possess an
explicit, hierarchical World Model. We contend that this is best achieved
through hierarchical scaffolding, where complex goals are decomposed into
structured, manageable subgoals. Drawing evidence from a systematic review of
2024 research in multi-agent soccer, we identify a clear and decisive trend
towards integrating symbolic and hierarchical methods with multi-agent
reinforcement learning (MARL). These approaches implicitly or explicitly
construct a task-based world model to guide agent learning. We then propose a
paradigm shift: leveraging Large Language Models to dynamically generate this
hierarchical scaffold, effectively using language to structure the World Model
on the fly. This language-driven world model provides an intrinsic curriculum,
dense and meaningful learning signals, and a framework for compositional
learning, enabling Agent Models to acquire sophisticated, strategic behaviors
with far greater sample efficiency. By building environments with explicit,
language-configurable task layers, we can bridge the gap between low-level
reactive behaviors and high-level strategic team play, creating a powerful and
generalizable framework for training the next generation of intelligent agents.

</details>


### [6] [What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking](https://arxiv.org/abs/2509.04791)
*Yuan Sui,Yanming Zhang,Yi Liao,Yu Gu,Guohua Tang,Zhongqian Sun,Wei Yang,Bryan Hooi*

Main category: cs.AI

TL;DR: WiA-LLM integrates What-If Analysis (WIA) into Large Language Models (LLMs) to enable proactive thinking, anticipate future outcomes, and make informed decisions. Experimental validation in a multiplayer game environment shows a remarkable 74.2% accuracy in forecasting game-state changes, surpassing baselines by up to two times. This work is the first to formally explore and integrate what-if analysis capabilities within LLMs, paving the way for proactive reasoning and robust decision-making in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Large language models excel at reactive processing but lack the ability to explore hypothetical futures and anticipate consequences before acting, limiting their utility in dynamic and high-stakes scenarios. The motivation behind this work is to bridge this critical gap by introducing proactive thinking capabilities to LLMs through WiA-LLM, enabling them to make informed decisions in challenging and rapidly changing environments such as strategic planning, risk assessment, and real-time decision-making.

Method: The paper proposes WiA-LLM, a paradigm that integrates What-If Analysis (WIA) to empower LLMs with proactive thinking abilities. WiA-LLM leverages environmental feedback through reinforcement learning to simulate potential outcomes of actions and anticipate future states. The approach is validated in a multiplayer game environment that requires accurate multi-step consequence prediction, demonstrating remarkable forecasting accuracy of 74.2% in game-state changes, surpassing baselines by up to two times.

Result: Experimental results show that WiA-LLM achieves a significant 74.2% accuracy in forecasting game-state changes, particularly excelling in high-difficulty scenarios. The model's proactive reasoning capabilities through What-If Analysis demonstrate a fundamental advance in LLMs, offering a scalable framework for robust decision-making in dynamic environments.

Conclusion: WiA-LLM introduces proactive thinking capabilities to Large Language Models (LLMs) through What-If Analysis (WiA), enabling them to anticipate future outcomes and make informed decisions. Experimental results in a multiplayer game environment demonstrate significant improvements in forecasting game-state changes, especially in high-difficulty scenarios. This work represents the first formal exploration and integration of what-if analysis within LLMs, paving the way for proactive reasoning and robust decision-making in dynamic environments.

Abstract: Large language models (LLMs) excel at processing information reactively but
lack the ability to systemically explore hypothetical futures. They cannot ask,
"what if we take this action? how will it affect the final outcome" and
forecast its potential consequences before acting. This critical gap limits
their utility in dynamic, high-stakes scenarios like strategic planning, risk
assessment, and real-time decision making. To bridge this gap, we propose
WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.
Our approach integrates What-If Analysis (WIA), a systematic approach for
evaluating hypothetical scenarios by changing input variables. By leveraging
environmental feedback via reinforcement learning, WiA-LLM moves beyond
reactive thinking. It dynamically simulates the outcomes of each potential
action, enabling the model to anticipate future states rather than merely react
to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a
complex multiplayer game environment characterized by rapid state changes and
intricate interactions. The game's real-time state changes require precise
multi-step consequence prediction, making it an ideal testbed for our approach.
Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy
in forecasting game-state changes (up to two times gain over baselines). The
model shows particularly significant gains in high-difficulty scenarios where
accurate foresight is critical. To our knowledge, this is the first work to
formally explore and integrate what-if analysis capabilities within LLMs.
WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,
providing a scalable framework for robust decision-making in dynamic
environments with broad implications for strategic applications.

</details>


### [7] [TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models](https://arxiv.org/abs/2509.04809)
*Haechang Kim,Hao Chen,Can Li,Jong Min Lee*

Main category: cs.AI

TL;DR: 本研究引入了TalkToAgent，旨在提供RL策略的自然语言解释。通过五个专门的LLM代理，TalkToAgent能够交互性地解释RL策略，并成功将用户查询映射到XRL任务。验证结果表明，TalkToAgent在解释代理行为方面取得了成功，有效将其意义放置于问题领域内。


<details>
  <summary>Details</summary>
Motivation: XRL在提高RL代理透明度方面具有潜力，但存在复杂RL政策与领域专家之间的差距。现有XRL方法的可理解性有限且覆盖范围有限，用户不确定使用哪些工具。

Method: 引入了TalkToAgent，一个包含五个专门的LLM代理（协调员、解释器、编码器、评估器和调试器）的架构，利用自然语言交互性地为RL策略提供解释。

Result: 通过在四重罐过程控制问题上验证，结果显示TalkToAgent成功将用户查询映射到XRL任务，并最大限度减少了反事实生成过程中的失败。定性评估证实，TalkToAgent有效地解释了代理的行为并将其意义置于问题领域内。

Conclusion: TalkToAgent是一个多代理大型语言模型框架，旨在提供RL策略的互动自然语言解释。研究结果表明，TalkToAgent成功将用户查询映射到XRL任务，高准确度地执行了这一任务，并最大限度减少了反事实生成过程中的失败。定性评价证实，TalkToAgent有效地解释了代理的行为并将其意义置于问题领域内。

Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach
in improving the transparency of Reinforcement Learning (RL) agents. However,
there remains a gap between complex RL policies and domain experts, due to the
limited comprehensibility of XRL results and isolated coverage of current XRL
approaches that leave users uncertain about which tools to employ. To address
these challenges, we introduce TalkToAgent, a multi-agent Large Language Models
(LLM) framework that delivers interactive, natural language explanations for RL
policies. The architecture with five specialized LLM agents (Coordinator,
Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically
map user queries to relevant XRL tools and clarify an agent's actions in terms
of either key state variables, expected outcomes, or counterfactual
explanations. Moreover, our approach extends previous counterfactual
explanations by deriving alternative scenarios from qualitative behavioral
descriptions, or even new rule-based policies. We validated TalkToAgent on
quadruple-tank process control problem, a well-known nonlinear control
benchmark. Results demonstrated that TalkToAgent successfully mapped user
queries into XRL tasks with high accuracy, and coder-debugger interactions
minimized failures in counterfactual generation. Furthermore, qualitative
evaluation confirmed that TalkToAgent effectively interpreted agent's actions
and contextualized their meaning within the problem domain.

</details>


### [8] [Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory](https://arxiv.org/abs/2509.04847)
*Mukul Singh,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: 本文研究了语言模型在迭代囚徒困境中的行为动态，发现它们表现出强大的合作策略特性，对对手策略的变化具有快速适应能力，甚至超越人类的适应性。通过与经典策略对抗实验，发现语言模型在性能上可以达到甚至超过最佳经典策略的水平。结果为未来研究语言模型在更复杂社会环境中的角色提供了基础。


<details>
  <summary>Details</summary>
Motivation: 之前的研究忽略了语言模型在长期互动、人-模型合作和行为模式演变过程中的作用，本文针对此展开探究。研究语言模型在迭代囚徒困境中的行为动态可以揭示它们在合作与冲突中的表现，为未来研究其在更复杂、混合人工智能社会环境中的角色提供基础。

Method: 本文通过在迭代囚徒困境中与240个经典策略进行对抗，发现语言模型表现出与最佳经典策略相当甚至超过的性能。对语言模型进行行为分析，揭示出其表现出强大合作策略的关键特性，同时展示了对对手策略变化的快速适应能力。在“策略切换”实验中，语言模型能在几轮内探测和回应对手策略变化，与甚至超越人类的适应性能力。

Result: 通过在迭代囚徒困境中进行实验，发现语言模型展现出与经典策略相当甚至领先的性能。行为分析表明，语言模型表现出强大合作策略的特性，还展现出对对手策略变化的快速适应能力。在“策略切换”实验中，语言模型展现出探测和回应对手策略变化的能力，与人类的适应性相媲美甚至更胜一筹。

Conclusion: 本文调查了语言模型在迭代囚徒困境中的行为动态，发现语言模型表现出与强大合作策略相关的关键特性，同时对对手策略的变化表现出迅速的适应能力。语言模型在控制的“策略切换”实验中展现出探测和回应对手策略变化的能力，远超人类的适应性。研究结果为语言模型代理在长期合作行为方面提供了系统性特征，为未来探讨它们在更复杂的混合人工智能社会环境中的角色奠定了基础。

Abstract: Language models are increasingly deployed in interactive online environments,
from personal chat assistants to domain-specific agents, raising questions
about their cooperative and competitive behavior in multi-party settings. While
prior work has examined language model decision-making in isolated or
short-term game-theoretic contexts, these studies often neglect long-horizon
interactions, human-model collaboration, and the evolution of behavioral
patterns over time. In this paper, we investigate the dynamics of language
model behavior in the iterated prisoner's dilemma (IPD), a classical framework
for studying cooperation and conflict. We pit model-based agents against a
suite of 240 well-established classical strategies in an Axelrod-style
tournament and find that language models achieve performance on par with, and
in some cases exceeding, the best-known classical strategies. Behavioral
analysis reveals that language models exhibit key properties associated with
strong cooperative strategies - niceness, provocability, and generosity while
also demonstrating rapid adaptability to changes in opponent strategy mid-game.
In controlled "strategy switch" experiments, language models detect and respond
to shifts within only a few rounds, rivaling or surpassing human adaptability.
These results provide the first systematic characterization of long-term
cooperative behaviors in language model agents, offering a foundation for
future research into their role in more complex, mixed human-AI social
environments.

</details>


### [9] [Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales](https://arxiv.org/abs/2509.04871)
*Krittanon Kaewtawee,Wachiravit Modecrua,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: 该论文介绍了如何从电话记录语料库中克隆对话式语音AI代理的方法，讨论了系统构建过程和评估结果。研究发现AI代理在例行通话方面接近人类表现，但在一些关键领域表现不佳，提出了改进方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前语言和语音建模的进展使得构建能够实时理解和生成人类对话的自主语音助手成为可能。这些系统越来越多地被部署在客户服务和医疗保健等领域，能够自动化重复任务、降低运营成本，并提供全天候的持续支持。

Method: 本文介绍了从电话记录语料库中克隆对话式语音AI代理的一般方法。系统通过监听电话客户、使用合成语音回应并遵循从表现优越的人类代理学习而来的结构化指导。描述了域选择、知识提取和提示工程等构建代理的方法，并整合了自动语音识别、基于大型语言模型的对话管理器以及文本转语音合成成为流式推理系统。

Result: 通过在22项标准覆盖介绍、产品沟通、销售动力、异议处理和结束等方面对克隆代理进行评估，结果显示AI代理在通话的例行方面接近人类表现，但在说服力和异议处理方面表现不佳。通过分析这些不足之处并相应调整提示，不断改进AI代理的性能。

Conclusion: 该论文介绍了从通话记录语料库中克隆对话式语音AI代理的一般方法论。通过对电话记录进行监听、使用合成语音进行回应，并遵循从表现优越的人类代理学习而来的结构化指导。研究结果表明，在日常通话的方方面面，AI代理接近人类表现，但在说服力和异议处理方面表现不佳。通过分析这些不足之处并相应调整提示，论文得出设计经验教训，并提出未来研究方向包括大规模模拟和自动评估。

Abstract: Recent advances in language and speech modelling have made it possible to
build autonomous voice assistants that understand and generate human dialogue
in real time. These systems are increasingly being deployed in domains such as
customer service and healthcare care, where they can automate repetitive tasks,
reduce operational costs, and provide constant support around the clock. In
this paper, we present a general methodology for cloning a conversational voice
AI agent from a corpus of call recordings. Although the case study described in
this paper uses telesales data to illustrate the approach, the underlying
process generalizes to any domain where call transcripts are available. Our
system listens to customers over the telephone, responds with a synthetic
voice, and follows a structured playbook learned from top performing human
agents. We describe the domain selection, knowledge extraction, and prompt
engineering used to construct the agent, integrating automatic speech
recognition, a large language model based dialogue manager, and text to speech
synthesis into a streaming inference pipeline. The cloned agent is evaluated
against human agents on a rubric of 22 criteria covering introduction, product
communication, sales drive, objection handling, and closing. Blind tests show
that the AI agent approaches human performance in routine aspects of the call
while underperforming in persuasion and objection handling. We analyze these
shortcomings and refine the prompt accordingly. The paper concludes with design
lessons and avenues for future research, including large scale simulation and
automated evaluation.

</details>


### [10] [OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration](https://arxiv.org/abs/2509.04876)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Xiaofei Sun,Keze Wang*

Main category: cs.AI

TL;DR: OSC是一种知识感知自适应协作框架，通过引入CKM实现Agent之间的认知协同。实验证明OSC显著提高了任务性能和通信效率，将个体转变为深度协作认知团队，为LLM代理的互动行为提供新见解。


<details>
  <summary>Details</summary>
Motivation: 先前的工作虽然推进了Agent选择和结果聚合，但在专家Agent之间进行高效的语言交互仍然是一个关键瓶颈。OSC作为选择和聚合之间的关键中间层，填补了这一缺口，为实现深度协作引入了Collaborator Knowledge Models（CKM）。

Method: 介绍了OSC框架，引入Collaborator Knowledge Models（CKM），使每个Agent能够动态感知其协作者的认知状态。通过实时认知差距分析，Agent可以自适应性地调整通信行为，包括内容焦点、详细级别和表达风格，以增强深度协作。

Result: 实验证明OSC框架显著改善了任务性能和通信效率，将个体转变为深度协作认知团队。

Conclusion: OSC（Orchestrating Cognitive Synergy）是一种知识感知自适应协作框架，旨在增强具有大型语言模型的多Agent系统中的认知协同。实验证明OSC显著提高了任务性能和通信效率，使“并行工作的个体”转变为“深度协作认知团队”。该框架不仅优化了多Agent协作，还为LLM代理的互动行为提供了新的见解。

Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a
knowledge-aware adaptive collaboration framework designed to enhance cognitive
synergy in multi-agent systems with large language models. While prior work has
advanced agent selection and result aggregation, efficient linguistic
interactions for deep collaboration among expert agents remain a critical
bottleneck. OSC addresses this gap as a pivotal intermediate layer between
selection and aggregation, introducing Collaborator Knowledge Models (CKM) to
enable each agent to dynamically perceive its collaborators' cognitive states.
Through real-time cognitive gap analysis, agents adaptively adjust
communication behaviors, including content focus, detail level, and expression
style, using learned strategies. Experiments on complex reasoning and
problem-solving benchmarks demonstrate that OSC significantly improves task
performance and communication efficiency, transforming "parallel-working
individuals'' into a "deeply collaborative cognitive team.'' This framework not
only optimizes multi-agent collaboration but also offers new insights into LLM
agent interaction behaviors.

</details>


### [11] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: 提出了SparkUI-Parser，一个端到端框架，通过连续坐标建模和拒绝机制提高GUI感知模型的准确性和速度。在多个基准测试上表现优越，资源可在GitHub获取。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型（MLLMs）在GUI感知方面取得了巨大进展，但仍存在一些挑战，如低定位精度、较慢的推理速度和无法解析整个界面。因此，为了提高模型的准确性、速度和鲁棒性，需要解决这些问题。

Method: 通过基于预训练的Multimodal Large Language Model (MLLM)进行连续坐标建模，配备额外的令牌路由器和坐标解码器，有效地缓解了离散输出特性和MLLM逐令牌生成过程中的局限性。同时引入基于修改后的匈牙利匹配算法的拒绝机制，识别和拒绝不存在的元素，减少误报。提出了用于系统评估GUI模型结构感知能力的基准测试ScreenParse。

Result: 在多个基准测试上，SparkUI-Parser方法表现优异，优于当前最先进的方法。资源可在https://github.com/antgroup/SparkUI-Parser获取。

Conclusion: 提出了SparkUI-Parser，一个新颖的端到端框架，同时实现了更高的定位精度和整个界面的细粒度解析能力。引入了基于修改后的匈牙利匹配算法的拒绝机制，进一步提高了模型的鲁棒性。在多个基准测试上，我们的方法始终优于最先进的方法。

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


### [12] [Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts](https://arxiv.org/abs/2509.04926)
*Barbara Gendron,Gaël Guibon,Mathieu D'aquin*

Main category: cs.AI

TL;DR: 本研究提出了一种基于本体论的方法，通过定义定性特征实现对LLMs在对话代理中的可控性。研究利用语言描述符将定性概念转化为定量定义，并将其整合到本体中进行推理和一致性检查。将这一框架应用于控制对话中的熟练水平，通过描述逻辑和本体进行形式化，指导LLM进行受控文本生成。实验结果显示该方法提供了一致和可解释的熟练水平定义，提高了对话人工智能的透明度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决LLMs作为对话代理时的可控性挑战，特别是为了确保可预测和用户个性化的响应。希望提供一种方法来定义和管理对话中的定性概念，从而提高对话人工智能的透明度和一致性。

Method: 基于本体论的方法定义了定性特征，并通过语言描述符的定量定义将其整合到本体中，实现对LLMs在对话代理中的可控性。通过将定性概念形式化为描述逻辑，并加入本体，指导LLM的受控文本生成，提供一致和可解释的熟练水平定义。

Result: 实验结果表明，本研究提出的基于本体论的方法能够实现一致和可解释的熟练水平定义，为对话人工智能提供更多的透明度。

Conclusion: 该研究提出了一个基于本体论的方法，通过定义定性特征，实现了对大型语言模型（LLMs）在对话代理中的可控性。研究将定性概念通过一组语言描述符转化为定量定义，从而使其整合到一个本体论中进行推理和一致性检查。将这一框架应用于对话中的熟练水平控制任务，以CEFR语言熟练水平作为案例研究。研究结果表明，通过描述逻辑将这些定义形式化并纳入本体论，可以通过微调指导LLM的受控文本生成，提供一致和可解释的熟练水平定义，从而提高了对话人工智能的透明度。

Abstract: The controllability of Large Language Models (LLMs) when used as
conversational agents is a key challenge, particularly to ensure predictable
and user-personalized responses. This work proposes an ontology-based approach
to formally define conversational features that are typically qualitative in
nature. By leveraging a set of linguistic descriptors, we derive quantitative
definitions for qualitatively-defined concepts, enabling their integration into
an ontology for reasoning and consistency checking. We apply this framework to
the task of proficiency-level control in conversations, using CEFR language
proficiency levels as a case study. These definitions are then formalized in
description logic and incorporated into an ontology, which guides controlled
text generation of an LLM through fine-tuning. Experimental results demonstrate
that our approach provides consistent and explainable proficiency-level
definitions, improving transparency in conversational AI.

</details>


### [13] [Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents](https://arxiv.org/abs/2509.04979)
*Rajesh Tembarai Krishnamachari,Srividya Rajesh*

Main category: cs.AI

TL;DR: The paper discusses the transformation of the internet into a Web of Agents and the need for Agent Ranking based on proven performance. It proposes the DOVIS protocol and AgentRank-UC algorithm, showing their effectiveness through simulations and theoretical guarantees. The contributions highlight the potential for coordinated protocols and performance-aware ranking to facilitate a trustworthy Agentic Web.


<details>
  <summary>Details</summary>
Motivation: The motivation is to transform the internet into a Web of Agents where autonomous agents interact at scale, requiring a method for selecting agents based on proven performance rather than just declared capabilities. The lack of a global, transparent network for agent interactions necessitates the development of coordinated protocols for ranking.

Method: Proposes the DOVIS protocol and AgentRank-UC algorithm for enabling Agent Ranking based on usage and competence, conducts simulations, and provides theoretical guarantees on convergence, robustness, and Sybil resistance.

Result: The paper introduces the DOVIS protocol and AgentRank-UC algorithm, showcasing their effectiveness through simulation results and theoretical guarantees on convergence, robustness, and Sybil resistance. These contributions demonstrate the potential for coordinated protocols and performance-aware ranking to enable a scalable and trustworthy Agentic Web.

Conclusion: AI agents are poised to transform the internet into a Web of Agents, requiring Agent Ranking for selecting agents based on proven performance. The paper proposes the DOVIS protocol and AgentRank-UC algorithm to enable ranking based on usage and competence, with simulation results demonstrating viability for a trustworthy Agentic Web.

Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and
integrated with tools, data, and web search -- are poised to transform the
internet into a \emph{Web of Agents}: a machine-native ecosystem where
autonomous agents interact, collaborate, and execute tasks at scale. Realizing
this vision requires \emph{Agent Ranking} -- selecting agents not only by
declared capabilities but by proven, recent performance. Unlike Web~1.0's
PageRank, a global, transparent network of agent interactions does not exist;
usage signals are fragmented and private, making ranking infeasible without
coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol
(\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that
enables the collection of minimal, privacy-preserving aggregates of usage and
performance across the ecosystem. On this substrate, we implement
\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines
\emph{usage} (selection frequency) and \emph{competence} (outcome quality,
cost, safety, latency) into a unified ranking. We present simulation results
and theoretical guarantees on convergence, robustness, and Sybil resistance,
demonstrating the viability of coordinated protocols and performance-aware
ranking in enabling a scalable, trustworthy Agentic Web.

</details>


### [14] [Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework](https://arxiv.org/abs/2509.05007)
*Jie Chen,Jinhao Jiang,Yingqian Min,Zican Dong,Shijie Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 本研究提出了Sticker-TTS框架，通过协调三个大推理模型，利用历史尝试指导的迭代探索和完善解决方案，克服了测试时间缩放方法过多依赖冗余抽样而忽略历史经验利用的限制。Sticker-TTS在数学推理基准测试中表现出色，优于强基线和强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型在复杂推理任务上表现出色，但测试时间缩放方法主要依赖冗余抽样，忽略历史经验利用，限制了计算效率。本研究旨在克服这一限制，提出Sticker-TTS框架，通过引导历史尝试指导迭代探索和完善解决方案。

Method: 我们的框架核心是精炼关键条件（称为sticker），驱动关键信息的提取、完善和复用，通过结合模仿学习和自我改进的两阶段优化策略，实现渐进式完善，增强了框架的效率和性能。

Result: 通过对三个具有挑战性的数学推理基准测试（包括AIME-24、AIME-25和OlymMATH）的广泛评估，证明Sticker-TTS在可比推理预算下始终优于强基线，包括自一致性和先进的强化学习方法。

Conclusion: 我们提出了一种新颖的测试时间缩放框架Sticker-TTS，通过协调三个协同大推理模型，引导由历史尝试指导的迭代探索和完善解决方案，克服了当前测试时间缩放方法过多依赖冗余抽样而忽略历史经验利用的限制。实验证明，Sticker-TTS始终优于强基线，包括自一致性和先进的强化学习方法，在可比推理预算下。

Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex
reasoning tasks, with further gains achievable through increased computational
budgets at inference. However, current test-time scaling methods predominantly
rely on redundant sampling, ignoring the historical experience utilization,
thereby limiting computational efficiency. To overcome this limitation, we
propose Sticker-TTS, a novel test-time scaling framework that coordinates three
collaborative LRMs to iteratively explore and refine solutions guided by
historical attempts. At the core of our framework are distilled key
conditions-termed stickers-which drive the extraction, refinement, and reuse of
critical information across multiple rounds of reasoning. To further enhance
the efficiency and performance of our framework, we introduce a two-stage
optimization strategy that combines imitation learning with self-improvement,
enabling progressive refinement. Extensive evaluations on three challenging
mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,
demonstrate that Sticker-TTS consistently surpasses strong baselines, including
self-consistency and advanced reinforcement learning approaches, under
comparable inference budgets. These results highlight the effectiveness of
sticker-guided historical experience utilization. Our code and data are
available at https://github.com/RUCAIBox/Sticker-TTS.

</details>


### [15] [Finding your MUSE: Mining Unexpected Solutions Engine](https://arxiv.org/abs/2509.05072)
*Nir Sweed,Hanit Hakim,Ben Wolfson,Hila Lifshitz,Dafna Shahaf*

Main category: cs.AI

TL;DR: 本文介绍了一种构建Functional Concept Graphs（FCGs）的方法，以支持抽象、问题重构和类比启发。使用该方法生成了大规模、高质量的FCGs，并提出了MUSE算法来为给定问题生成创意启发。通过对50万专利进行计算得到了FCG，并将其发布供进一步研究使用。


<details>
  <summary>Details</summary>
Motivation: 创新者经常在现有解决方案或新兴思想上表现出认知定势，阻碍了对新颖替代方案的探索。

Method: 构建Functional Concept Graphs（FCGs），使用MUSE算法生成创意启发，计算500K专利以获得FCG

Result: 提出了一种能够克服先前工作局限的方法，成功构建了大规模且高质量的FCGs，并展示了利用FCGs生成创意启发的算法MUSE。

Conclusion: 介绍了一种构建Functional Concept Graphs（FCGs）的方法，支持抽象、问题重构和类比启发。通过该方法得到了大规模且高质量的FCGs，克服了先前工作的局限。提出了MUSE算法，利用FCGs为给定问题生成创意启发。通过对50万项专利计算得到FCG，并为进一步研究发布了这一方法。

Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent
ideas, hindering the exploration of novel alternatives. This paper introduces a
methodology for constructing Functional Concept Graphs (FCGs), interconnected
representations of functional elements that support abstraction, problem
reframing, and analogical inspiration. Our approach yields large-scale,
high-quality FCGs with explicit abstraction relations, overcoming limitations
of prior work. We further present MUSE, an algorithm leveraging FCGs to
generate creative inspirations for a given problem. We demonstrate our method
by computing an FCG on 500K patents, which we release for further research.

</details>


### [16] [ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback](https://arxiv.org/abs/2509.05091)
*Matteo Bortoletto,Yichao Zhou,Lance Ying,Tianmin Shu,Andreas Bulling*

Main category: cs.AI

TL;DR: The paper introduces ProToM, an AI system that promotes prosocial behavior in multi-agent systems by providing context-sensitive feedback. ProToM surpasses existing models in communication quality, task efficiency, and user preference.


<details>
  <summary>Details</summary>
Motivation: Humans face challenges in identifying when and how to assist and collaborate with others, especially when pursuing independent goals. To overcome this hindrance to cooperation, the paper aims to develop an AI system that promotes prosocial behavior by offering useful feedback.

Method: The AI system, ProToM, utilizes Bayesian inverse planning to infer agents' goals and maximizes expected utility to select feedback. It provides targeted and context-sensitive feedback to individual agents in multi-agent systems.

Result: Evaluation of ProToM against baselines in two multi-agent environments shows superior performance in promoting prosocial actions. It outperforms large language and reasoning models in communication effectiveness, task completion speed, and user preference.

Conclusion: ProToM, an AI system developed to promote prosocial behavior in multi-agent systems, outperforms state-of-the-art language and reasoning models in providing targeted and context-sensitive feedback. It achieves a higher success rate, shorter task completion times, and is preferred by human users.

Abstract: While humans are inherently social creatures, the challenge of identifying
when and how to assist and collaborate with others - particularly when pursuing
independent goals - can hinder cooperation. To address this challenge, we aim
to develop an AI system that provides useful feedback to promote prosocial
behaviour - actions that benefit others, even when not directly aligned with
one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator
that promotes prosocial actions in multi-agent systems by providing targeted,
context-sensitive feedback to individual agents. ProToM first infers agents'
goals using Bayesian inverse planning, then selects feedback to communicate by
maximising expected utility, conditioned on the inferred goal distribution. We
evaluate our approach against baselines in two multi-agent environments: Doors,
Keys, and Gems, as well as Overcooked. Our results suggest that
state-of-the-art large language and reasoning models fall short of
communicating feedback that is both contextually grounded and well-timed -
leading to higher communication overhead and task speedup. In contrast, ProToM
provides targeted and helpful feedback, achieving a higher success rate,
shorter task completion times, and is consistently preferred by human users.

</details>


### [17] [Evaluation and Comparison Semantics for ODRL](https://arxiv.org/abs/2509.05139)
*Jaime Osvaldo Salas,Paolo Pareti,Semih Yumuşak,Soulmaz Gheisari,Luis-Daniel Ibáñez,George Konstantinidis*

Main category: cs.AI

TL;DR: 本文在ODRL领域提出了一个简单直观的正式语义，基于查询回答，用于评估和比较计算策略。该正式语义与最新语言规范一致，定义了比较两种策略的问题，并可以检测等价、更严格或更宽松的策略。


<details>
  <summary>Details</summary>
Motivation: 尽管已经在语言功能的正式规范上取得了初步进展，但仍然缺乏ODRL的全面正式语义。受数据共享场景的激励，建立在评估语义的基础上，定义并研究了比较两种策略的问题。

Method: 提供了基于查询回答的简单直观的ODRL正式语义，用于评估和比较计算策略。

Result: 正式语义通过查询回答建立评估和比较计算策略，与最新语言规范保持一致。提出了比较两种策略的问题，可以检测等价、更严格或更宽松的策略。

Conclusion: 提供了一个简单直观的ODRL正式语义，基于查询回答，用于评估和比较计算策略。在最新的语言规范（2.2）下与前几次正式化的内容保持一致。定义和研究了比较两种策略的问题，检测等价、更严格或更宽松策略。

Abstract: We consider the problem of evaluating, and comparing computational policies
in the Open Digital Rights Language (ODRL), which has become the de facto
standard for governing the access and usage of digital resources. Although
preliminary progress has been made on the formal specification of the
language's features, a comprehensive formal semantics of ODRL is still missing.
In this paper, we provide a simple and intuitive formal semantics for ODRL that
is based on query answering. Our semantics refines previous formalisations, and
is aligned with the latest published specification of the language (2.2).
Building on our evaluation semantics, and motivated by data sharing scenarios,
we also define and study the problem of comparing two policies, detecting
equivalent, more restrictive or more permissive policies.

</details>


### [18] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: 最近的研究集中在发展模拟复杂真实场景的 3D 世界模型。LatticeWorld 是一个简单而有效的 3D 世界生成框架，通过接受文本描述和视觉指令，可以创建具有动态代理、多代理互动、高保真物理模拟和实时渲染的大规模 3D 交互世界。该框架提高了工业生产效率并保持高创意质量。


<details>
  <summary>Details</summary>
Motivation: 最近的研究聚焦于发展模拟复杂真实场景的 3D 世界模型。传统的手动建模虽然能够创建虚拟 3D 场景，但现代方法利用先进的机器学习算法进行 3D 世界生成，最近的进展集中在可以根据用户指令创建虚拟世界的生成方法。本研究探索了提出 LatticeWorld，一个简单而有效的 3D 世界生成框架，简化了 3D 环境的工业生产流程。

Method: LatticeWorld 使用轻量级 LLMs（LLaMA-2-7B）和行业级渲染引擎（例如 Unreal Engine 5）来生成动态环境。该框架接受文本描述和视觉指令作为多模态输入，创建具有动态代理、竞争性多代理互动、高保真物理模拟和实时渲染的大规模 3D 交互世界。

Result: LatticeWorld 在工业生产效率上实现了超过 90 倍的增加，同时保持高创意质量，相比传统手动生产方法。

Conclusion: LatticeWorld 提出了一个简单而有效的 3D 世界生成框架，优化了工业生产管道，实现了高生产效率和高创意质量。通过综合实验评估表明，LatticeWorld 在场景布局生成和视觉保真度方面达到了优越的准确性。

Abstract: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18

</details>
