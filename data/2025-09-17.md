<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: 该论文开发了V-Math，一个自主主体框架，旨在帮助越南高中学生准备国家高中数学考试。通过整合AI代理，V-Math支持学生自主练习，生成创新考试问题，减轻教师工作量，丰富教学资源，提供高质量的题库，以及与教学标准对齐的数学准备。初步评估显示V-Math能够产生高度准确的解答，提供连贯的解释，并增加练习材料的多样性，具有支持学生和教师的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为越南高中学生提供准备国家高中数学考试的自主学习工具，同时减轻教师的工作负担，提供创新的考试问题，并增加教学资源。

Method: 论文描述了系统架构，着重介绍了学习者的练习模式和教师导向的问题生成特性。论文通过初步评估表明，V-Math生成了与矩阵对齐的考试题目，解决准确率高，提供连贯的解释，增加了练习材料的多样性。

Result: V-Math通过生成高质量考试题目和解释，提供了支持学生自主学习和教师工作的潜力。

Conclusion: 该论文开发了一个自主主体框架 V-Math，旨在帮助越南高中学生准备国家高中数学考试。框架整合了三个专门的人工智能代理：一个基于规范矩阵的问题生成器，一个详细的逐步推理求解器/解释器，以及一个根据学生表现进行个性化调整的导师。V-Math不仅能够支持学生自主练习，还通过生成创新的，符合要求的考试问题和构建多样化、高质量的题库来支持教师，从而减少手动工作量并丰富教学资源。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [2] [DISPLIB: a library of train dispatching problems](https://arxiv.org/abs/2509.12254)
*Oddvar Kloster,Bjørnar Luteberget,Carlo Mannino,Giorgio Sartor*

Main category: cs.AI

TL;DR: 本文介绍了一个通用的问题定义和文件格式DISPLIB，目的在于为铁路运输中的列车重新路由和重新调度问题提供一个共享的平台。作者们搜集了多个实际用例的问题实例，并提供了参考求解器的实现。所有材料均可在线获取。


<details>
  <summary>Details</summary>
Motivation: 作者们受到MILP、SAT、TSP、VRP等领域成功社区的启发，希望解决铁路运输中列车重新路由和重新调度问题不同研究之间缺乏共享和评估数据的问题。

Method: 本文通过引入通用问题定义和文件格式DISPLIB来解决铁路运输中列车重新路由和重新调度的问题。作者们收集了来自多个实际用例的问题实例，并提出了一个参考求解器实现。

Result: 作者们介绍了DISPLIB的问题定义和文件格式，并提供了实际问题实例和一个参考求解器实现。这些材料可以通过https://displib.github.io在线获取。

Conclusion: 本文介绍了一个通用的问题定义和文件格式DISPLIB，旨在捕捉列车重新路由和重新调度的所有主要特征。作者们收集了来自多个实际用例的问题实例，并将它们公开提供。他们描述了问题定义、工业实例和一个参考求解器实现。这使得任何研究人员或开发人员都可以在没有工业连接的情况下处理列车调度问题，并使研究社区能够对求解器进行经验比较。

Abstract: Optimization-based decision support systems have a significant potential to
reduce delays, and thus improve efficiency on the railways, by automatically
re-routing and re-scheduling trains after delays have occurred. The operations
research community has dedicated a lot of effort to developing optimization
algorithms for this problem, but each study is typically tightly connected with
a specific industrial use case. Code and data are seldom shared publicly. This
fact hinders reproducibility, and has led to a proliferation of papers
describing algorithms for more or less compatible problem definitions, without
any real opportunity for readers to assess their relative performance. Inspired
by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a
common problem definition and file format, DISPLIB, which captures all the main
features of train re-routing and re-scheduling. We have gathered problem
instances from multiple real-world use cases and made them openly available. In
this paper, we describe the problem definition, the industrial instances, and a
reference solver implementation. This allows any researcher or developer to
work on the train dispatching problem without an industrial connection, and
enables the research community to perform empirical comparisons between
solvers. All materials are available online at https://displib.github.io.

</details>


### [3] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: Large multimodal models (LMMs) need to improve inductive physical reasoning for safety-critical applications; a new benchmark, InPhyRe, revealed LMMs' difficulties with reasoning when faced with scenarios violating physical laws and highlighted issues with language bias and neglect of visual input.


<details>
  <summary>Details</summary>
Motivation: Humans possess inductive physical reasoning skills to adapt their physical reasoning to unseen environments from a few visual examples, which is essential for LMMs in safety-critical applications. Existing benchmarks only evaluate parametric knowledge in LMMs, not inductive physical reasoning.

Method: Proposed InPhyRe, a visual question answering benchmark to measure inductive physical reasoning in LMMs by evaluating their ability to predict the outcome of collision events in synthetic collision videos.

Result: InPhyRe showed that LMMs struggle with applying parametric knowledge, have weak inductive physical reasoning when violating physical laws, and are impacted by language bias and overlook visual inputs, raising concerns about their reliability with visual data.

Conclusion: LMMs struggle to apply their limited parametric knowledge about universal physical laws to reasoning, inductive physical reasoning in LMMs is weak when demonstration samples violate universal physical laws, and inductive physical reasoning in LMMs suffers from language bias and largely ignores the visual inputs.

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [4] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 本文介绍了一种LLM辅助路径规划系统（LLMAP），采用LLM作为解析器理解自然语言，多目标优化方法自适应调整目标权重，最大化兴趣点质量和任务完成率，最小化路径距离，在多项约束条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前研究中存在两种不同的方法来进行路径规划：一种是使用LLM作为代理的直接路径规划，另一种是基于图搜索策略的路径规划。然而，前一种方法中的LLM在处理庞大的地图数据时存在困难，而后一种方法在理解自然语言偏好方面能力有限。另外，全球用户的高度异质性和不可预测的时空分布构成了更为关键的挑战。

Method: 本文采用LLM作为解析器来理解自然语言，识别任务，提取用户偏好以及识别任务依赖关系，同时采用多步图构建和迭代搜索算法作为路径规划的基础求解器。通过多目标优化方法自适应调整目标权重，以达到最大化兴趣点质量和任务完成率，同时最小化路径距离，受到用户时间限制、POI开放时间和任务依赖关系的约束。

Result: 实验结果表明，本文提出的方法在多个约束条件下取得了卓越性能。

Conclusion: 本文介绍了一种新颖的LLM辅助路径规划系统，采用LLM作为解析器来理解自然语言，识别任务，提取用户偏好并识别任务依赖关系，结合多步图构建和迭代搜索算法作为寻找最佳路径的基础求解器。通过多目标优化方法，自适应调整目标权重以最大化兴趣点（POI）质量和任务完成率，同时最小化路线距离，受用户时间限制、POI开放时间和任务依赖关系三个关键约束条件约束。实验结果表明，该方法在多个约束条件下达到了卓越性能。

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [5] [Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT](https://arxiv.org/abs/2509.12274)
*Mohammadreza Narimani,Ali Hajiahmad,Ali Moghimi,Reza Alimardani,Shahin Rafiee,Amir Hossein Mirzabe*

Main category: cs.AI

TL;DR: 本研究开发了智能气动喷雾温室系统，利用物联网和人工智能技术监测植物和环境条件。通过AI框架，实现了植物疾病的准确检测。研究表明物联网系统可以连续发布数据并调整控制参数，为植物提供最佳生长环境。AI框架中的VGG-19算法在识别健康叶片、干旱应激叶片和锈叶时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于提高温室环境的控制和植物状态监测，进而促进作物生产。利用物联网和人工智能技术，为用户提供实时数据和洞察，帮助他们做出更明智的管理决策。此外，通过AI框架的开发，旨在准确、快速地检测植物疾病，为植物生长和健康管理提供支持。

Method: 本研究开发和测试了一种智能气动喷雾温室系统，通过物联网和人工智能技术，连续监测格庄植物和环境条件。研发了基于物联网的平台来更有效地控制植物的环境条件，并利用VGG-19、InceptionResNetV2和InceptionV3算法开发了AI疾病检测框架，用于分析定期捕获的图像。通过将AI框架的表现与专家评估的疾病状态进行比较来评估其准确性。

Result: 研究初步结果显示，物联网系统能够连续在线发布温度、湿度、水流量和充电箱容量等数据给用户，并调整控制参数，为植物提供最佳生长环境。AI框架的结果表明，VGG-19算法能够在健康叶片、干旱应激叶片和锈叶之间以92%的最高准确性识别。

Conclusion: 研究结论表明，基于物联网和人工智能的智能气动喷雾温室系统能够有效监测植物的环境状况并为用户提供数据和洞察以进行决策管理。通过AI框架的研发和测试，能够准确检测植物疾病状态，具有潜力在植物生产中发挥重要作用。

Abstract: Controlling environmental conditions and monitoring plant status in
greenhouses is critical to promptly making appropriate management decisions
aimed at promoting crop production. The primary objective of this research
study was to develop and test a smart aeroponic greenhouse on an experimental
scale where the status of Geranium plant and environmental conditions are
continuously monitored through the integration of the internet of things (IoT)
and artificial intelligence (AI). An IoT-based platform was developed to
control the environmental conditions of plants more efficiently and provide
insights to users to make informed management decisions. In addition, we
developed an AI-based disease detection framework using VGG-19,
InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured
periodically after an intentional inoculation. The performance of the AI
framework was compared with an expert's evaluation of disease status.
Preliminary results showed that the IoT system implemented in the greenhouse
environment is able to publish data such as temperature, humidity, water flow,
and volume of charge tanks online continuously to users and adjust the
controlled parameters to provide an optimal growth environment for the plants.
Furthermore, the results of the AI framework demonstrate that the VGG-19
algorithm was able to identify drought stress and rust leaves from healthy
leaves with the highest accuracy, 92% among the other algorithms.

</details>


### [6] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant is an agentic, open-source Human-AI collaborative framework for scientific workflows that improves drafting efficiency and thematic consistency. Evaluation through Independent Human Review, Automated LLM Review, and Program Chair Oversight shows the importance of human-AI collaboration for factual correctness, methodological soundness, and ethical compliance. Limitations include hallucinated citations, challenges in adapting to dynamic paper structures, and incomplete integration of multimodal content.


<details>
  <summary>Details</summary>
Motivation: Addressing the fragmented and lack of human-centered workflows in AI-assisted research tools by introducing AIssistant to simplify the creation of scientific workflows. Aimed to enhance drafting efficiency and thematic consistency while ensuring human oversight for accuracy and ethical compliance.

Method: Introduction of AIssistant, an agentic, open-source Human-AI collaborative framework for scientific workflows. Conducted experiments on perspective and review research papers in machine learning. Integrated modular tools and agents for literature synthesis, section-wise experimentation, citation management, and automatic LaTeX paper text generation. Evaluation conducted through Independent Human Review, Automated LLM Review, and Program Chair Oversight to validate accuracy, coherence, and scholarly rigour.

Result: The results demonstrate improvements in drafting efficiency and thematic consistency with AIssistant. Identified key limitations include hallucinated citations, difficulties in adapting to dynamic paper structures, and incomplete integration of multimodal content.

Conclusion: AIssistant has shown improvements in drafting efficiency and thematic consistency, but human-AI collaboration is essential for maintaining factual correctness, methodological soundness, and ethical compliance. Key limitations include hallucinated citations, difficulty adapting to dynamic paper structures, and incomplete integration of multimodal content.

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [7] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 本文通过引入一种新的分解方法，在资源受限模型中提高了用户意图理解，甚至超越了大型多模态大语言模型的基础性能。通过结构化交互摘要和意图提取，成功解决了小型模型在意图推断方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 了解用户意图对智能代理开发至关重要，但在小型模型方面存在准确推断意图的挑战。资源受限模型运行在设备上，可提供隐私保护、低成本和低延迟的用户体验，但存在准确推断意图的困难。

Method: 首先进行结构化交互摘要，捕获每个用户操作的关键信息，然后使用在聚合摘要上运行的微调模型执行意图提取。

Result: 本文提出的方法在处理资源受限模型中的意图理解方面取得了显著改进，甚至超越了大型多模态大语言模型的基准性能。

Conclusion: 通过引入一种新的分解方法，本文解决了资源受限模型在用户意图推断方面的挑战，甚至超越了大型多模态大语言模型的基本性能。

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [8] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 这篇论文介绍了一项名为\sys的框架，用于增强Large Language Models（LLMs）在软件工程中的性能。通过在SWE-bench榜单上取得新的最先进成果，验证了该方法的有效性。在开放权重模型中，使用\sys训练的30B参数模型在lite排行榜上排名第一，在verified排行榜上排名第四，仅次于拥有超过10倍参数的模型（>350B）。


<details>
  <summary>Details</summary>
Motivation: 当前的Large Language Models在软件工程中面临复杂的挑战，需要对大型代码库进行推理和协调的工具使用。尽管测试时扩展（TTS）是提高性能的一种有前途的方法，但其增益严重依赖于模型输出的多样性。然而，现有的偏好优化算法通常针对单次任务设计，未能充分解决交互式编码代理所需的多轮推理和工具集成的复杂性。

Method: 通过引入\sys框架，将现有的偏好优化算法适应多轮、工具辅助的设置，扩展了偏好目标以明确保留策略熵，并泛化学习以优化多轮交互，而不是单轮响应。

Result: 该方法在SWE-bench榜单上取得了新的最先进成果，表现出卓越的性能。在开放权重模型中，使用\sys训练的30B参数模型在lite排行榜上排名第一，在verified排行榜上排名第四，仅次于拥有超过10倍参数的模型（>350B）。

Conclusion: 该论文介绍了一种名为\sys的熵增强框架，用于提高Large Language Models（LLMs）在软件工程中的性能。研究结果表明，该方法在SWE-bench榜单上取得了新的最先进成果，证明了在开放权重模型中的有效性。

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [9] [Enhancing Physical Consistency in Lightweight World Models](https://arxiv.org/abs/2509.12437)
*Dingrui Wang,Zhexiao Sun,Zhouheng Li,Cheng Wang,Youlun Peng,Hongyuan Ye,Baha Zarrouki,Wei Li,Mattia Piccinini,Lei Xie,Johannes Betz*

Main category: cs.AI

TL;DR: 研究提出了物理信息BEV世界模型（PIWM），通过使用Soft Mask和Warm Start技术，有效捕捉鸟瞰视图中的物理交互并提高预测准确性。在相同参数规模（400M）下，PIWM在加权总体得分上比基准模型提高了60.6%。最小的PIWM模型在加权总体得分上比最大的基准模型高出7.4%，推理速度快28%。


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between size and performance in deploying world models, aiming to create a compact model suitable for edge devices without sacrificing prediction accuracy.

Method: Proposed Physics-Informed BEV World Model (PIWM) using Soft Mask during training and Warm Start technique for inference. Comparison conducted at the same parameter scale (400M) and with the smallest PIWM (130M Soft Mask) against the largest baseline model (400M).

Result: PIWM achieves a 60.6% improvement in weighted overall score compared to the baseline at the same parameter scale. The smallest PIWM model outperforms the largest baseline model with a 7.4% higher weighted overall score and 28% faster inference speed.

Conclusion: PIWM outperforms baseline and larger models in capturing physical interactions in bird's-eye-view representations with improved prediction accuracy and faster inference speed.

Abstract: A major challenge in deploying world models is the trade-off between size and
performance. Large world models can capture rich physical dynamics but require
massive computing resources, making them impractical for edge devices. Small
world models are easier to deploy but often struggle to learn accurate physics,
leading to poor predictions. We propose the Physics-Informed BEV World Model
(PIWM), a compact model designed to efficiently capture physical interactions
in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training
to improve dynamic object modeling and future prediction. We also introduce a
simple yet effective technique, Warm Start, for inference to enhance prediction
quality with a zero-shot model. Experiments show that at the same parameter
scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.
Moreover, even when compared with the largest baseline model (400M), the
smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score
with a 28% faster inference speed.

</details>


### [10] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 研究指出，在推理语言模型中，常用的神经网络剪枝方法并不适用，因为这会导致性能下降。作者提出了一种简单的解决方案RAC，通过联合重构输入和模型的在线推理过程活跃值，显著提升了神经网络剪枝在推理语言模型中的性能。可在GitHub上找到作者提供的可复现实验代码。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型如DeepSeek-R1在推理过程中产生长的思路链，这使得它们在大规模部署时成本高昂。研究展示了神经网络剪枝等压缩技术在该领域的不足，并针对此问题提出了一种新的、简单的解决方案RAC。

Method: 在推理语言模型中，使用神经网络剪枝技术并非一种有效的压缩方法，因为它可能导致性能下降，并增加了模型生成推理 token 的负担。作者提出的解决方案是在剪枝过程中联合重构输入和模型的在线推理过程活跃值，通过这种“Reasoning-Aware Compression” (RAC) 方法改善性能。

Result: RAC方法能够有效改善神经网络剪枝在推理语言模型上的性能表现，提高了模型的性能。作者给出了在GitHub上可复现实验结果的代码。

Conclusion: 使用神经网络剪枝等压缩技术在推理语言模型上可能导致较大性能损失，并且在某些情况下可能使模型变得更慢。作者提出了一种新的解决方案，称为“Reasoning-Aware Compression” (RAC)，通过在剪枝过程中联合重构输入和模型的在线推理过程活跃值，显著提升了性能。

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [11] [Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT](https://arxiv.org/abs/2509.12471)
*Yiwen Lu,Lu Li,Dazheng Zhang,Xinyao Jian,Tingyin Wang,Siqi Chen,Yuqing Lei,Jiayi Tong,Zhaohan Xi,Haitao Chu,Chongliang Luo,Alexis Ogdie,Brian Athey,Alparslan Turan,Michael Abramoff,Joseph C Cappelleri,Hua Xu,Yun Lu,Jesse Berlin,Daniel I. Sessler,David A. Asch,Xiaoqian Jiang,Yong Chen*

Main category: cs.AI

TL;DR: PowerGPT is an AI-powered system that automates test selection and sample size estimation in clinical trial design. It significantly improved task completion rates, accuracy, and reduced completion time in sample size estimation. The system benefits both statisticians and non-statisticians by offering a scalable and efficient approach to enhance accessibility and accuracy in statistical power analysis for clinical research.


<details>
  <summary>Details</summary>
Motivation: The complexity and reliance on statistical expertise in sample size calculations pose barriers for researchers. PowerGPT aims to address these challenges by providing an AI-driven solution to automate test selection and sample size estimation, benefiting both statisticians and non-statisticians.

Method: PowerGPT integrates large language models with statistical engines to automate test selection and sample size estimation in trial design. It was evaluated in a randomized trial to assess its effectiveness in improving task completion rates, accuracy, and completion time in sample size estimation.

Result: In the randomized trial, PowerGPT showed significant improvements in task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs. 77.8% for sample size calculation), accuracy (94.1% vs. 55.4% in sample size estimation), and reduced completion time (4.0 vs. 9.3 minutes). These improvements were consistent across various statistical tests and bridged expertise gaps.

Conclusion: PowerGPT, an AI-powered system, significantly improved task completion rates, accuracy, and reduced completion time in sample size estimation for clinical trial design. It offers a scalable and efficient approach to enhance accessibility and accuracy in statistical power analysis for clinical research.

Abstract: Sample size calculations for power analysis are critical for clinical
research and trial design, yet their complexity and reliance on statistical
expertise create barriers for many researchers. We introduce PowerGPT, an
AI-powered system integrating large language models (LLMs) with statistical
engines to automate test selection and sample size estimation in trial design.
In a randomized trial to evaluate its effectiveness, PowerGPT significantly
improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.
77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size
estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3
minutes, p < 0.001). These gains were consistent across various statistical
tests and benefited both statisticians and non-statisticians as well as
bridging expertise gaps. Already under deployment across multiple institutions,
PowerGPT represents a scalable AI-driven approach that enhances accessibility,
efficiency, and accuracy in statistical power analysis for clinical research.

</details>


### [12] [Physical Complexity of a Cognitive Artifact](https://arxiv.org/abs/2509.12495)
*Gülce Kardeş,David Krakauer,Joshua Grochow*

Main category: cs.AI

TL;DR: 研究结合Soma Cube物理难题和认知问题解决策略，量化评估任务难度，通过优化试错搜索逐步降低时间复杂性，提出智能模型应视为算法库。


<details>
  <summary>Details</summary>
Motivation: 认知科学和理论计算机科学都致力于分类和解释任务的困难程度，研究智能机制以降低任务困难度。研究动机是将计算机复杂性概念与认知问题解决策略相结合，探讨不同策略如何影响任务的复杂性。

Method: 通过“物质原则”，将Soma Cube物理难题的计算复杂性映射到认知问题解决策略中，通过分析搜索树的出度来量化评估任务难度，逐步优化试错搜索，包括预处理、价值排序、变量排序和修剪。

Result: 通过研究Soma Cube物理难题和认知问题解决策略，量化评估任务难度，并系统探讨不同策略对任务复杂性的影响。结果表明通过优化试错搜索，可以降低任务的时间复杂性。

Conclusion: 研究将计算机复杂性的概念与认知问题解决策略相结合，通过分析Soma Cube物理难题的计算复杂性，量化评估任务难度，并系统研究不同策略如何修改复杂性。结果表明通过层层优化试错搜索，包括预处理、价值排序、变量排序和修剪，可以降低任务的有效时间复杂性。建议智能模型应视为一种算法库，利用心智和物质的能力。

Abstract: Cognitive science and theoretical computer science both seek to classify and
explain the difficulty of tasks. Mechanisms of intelligence are those that
reduce task difficulty. Here we map concepts from the computational complexity
of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies
through a ``Principle of Materiality''. By analyzing the puzzle's branching
factor, measured through search tree outdegree, we quantitatively assess task
difficulty and systematically examine how different strategies modify
complexity. We incrementally refine a trial-and-error search by layering
preprocessing (cognitive chunking), value ordering (cognitive free-sorting),
variable ordering (cognitive scaffolding), and pruning (cognitive inference).
We discuss how the competent use of artifacts reduces effective time complexity
by exploiting physical constraints and propose a model of intelligence as a
library of algorithms that recruit the capabilities of both mind and matter.

</details>


### [13] [A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights](https://arxiv.org/abs/2509.12524)
*Rohit Chakraborty,Subasish Das*

Main category: cs.AI

TL;DR: 本研究使用群集对应分析（CCA）鉴别了环形路口事故的四种模式，进而通过SHAP解释分析识别了造成伤害的驱动因素，揭示了不同条件下事故严重程度的差异及具体的事故机制。研究结果为公共安全分析提供了实用模板，支持现场筛选、对策选择和审计准备报告。


<details>
  <summary>Details</summary>
Motivation: 环形交叉路口减少了严重事故，但风险模式因条件而异。本研究的动机在于分析俄亥俄州环形交叉路口的事故，为公共安全分析提供实用模板。

Method: 本研究采用两步骤解释性工作流程，首先利用群集对应分析（CCA）识别相互关联的因素并获得四种事故模式，然后通过基于树的严重性模型并结合SHAP解释分析来量化不同模式中和跨模式的伤害驱动因素。

Result: 通过分析发现不同条件下环形路口事故的模式和驱动因素，提出了具体的驾驶行为对事故严重程度的影响，为现场筛选、对策选择和报告准备提供了支持。

Conclusion: 本研究分析了2017年至2021年俄亥俄州环形交叉路口的事故，发现了四种事故模式，并通过解释性工作流结合SHAP模型识别了造成伤害的驱动因素。研究结果表明，在黑暗、潮湿的路面和较高的限速条件下，与固定物体或角度事件同时发生时，事故的严重程度较高。而在明朗、低速条件下，事故的严重程度较低。研究还指出了入口处（未让行、接受间隙）、多车道内循环（不当转弯）、减速过程中（追尾）等不同模式下事故的具体机制。该研究工作流将模式发现与案例级解释相结合，支持现场筛选、对策选择和审计准备报告。在信息系统领域的贡献在于提供了一个可用的XAI实用模板，用于公共安全分析。

Abstract: Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This
study analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable
workflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors
and yields four crash patterns. A tree-based severity model is then interpreted
with SHAP to quantify drivers of injury within and across patterns. Results
show higher severity when darkness, wet surfaces, and higher posted speeds
coincide with fixed-object or angle events, and lower severity in clear,
low-speed settings. Pattern-specific explanations highlight mechanisms at
entries (fail-to-yield, gap acceptance), within multi-lane circulation
(improper maneuvers), and during slow-downs (rear-end). The workflow links
pattern discovery with case-level explanations, supporting site screening,
countermeasure selection, and audit-ready reporting. The contribution to
Information Systems is a practical template for usable XAI in public safety
analytics.

</details>


### [14] [zELO: ELO-inspired Training Method for Rerankers and Embedding Models](https://arxiv.org/abs/2509.12541)
*Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo*

Main category: cs.AI

TL;DR: 引入zELO训练方法，利用无监督数据训练了zerank-1和zerank-1-small这两个最先进的重新排序模型，取得了在多个领域最高的检索分数，超越了闭源专有的重新排序器。具有良好的多功能性和零-shot性能。


<details>
  <summary>Details</summary>
Motivation: 通过分析排名任务与Thurstone模型等效的静态性质，引入了zELO训练方法来优化检索性能。

Method: 使用zELO方法，利用无监督数据训练了一组最先进的开放权重重新排序模型zerank-1和zerank-1-small。

Result: 在多个领域包括金融、法律、代码和STEM等取得了最高的检索分数，在NDCG@10和召回率方面超越了闭源专有的重新排序器。同时在域外和私有客户数据集上保持了零-shot性能。

Conclusion: 引入了名为zELO的新型训练方法，通过分析排名任务与Thurstone模型等效的静态性质来优化检索性能。基于zELO方法，利用无监督数据训练了一组最先进的开放权重重新排序模型：zerank-1和zerank-1-small。这些模型在金融、法律、代码和STEM等多个领域取得了最高的检索分数，在NDCG@10和召回率方面超越了闭源专有的重新排序器。这些模型还表现出极高的多功能性，在域外和私有客户数据集上保持了零-shot性能。训练数据包括112,000个查询和每个查询100个文档，从未标注的查询和文档端到端训练，并在不到10,000小时内完成了训练。

Abstract: We introduce a novel training methodology named zELO, which optimizes
retrieval performance via the analysis that ranking tasks are statically
equivalent to a Thurstone model. Based on the zELO method, we use unsupervised
data in order train a suite of state-of-the-art open-weight reranker models:
zerank-1 and zerank-1-small. These models achieve the highest retrieval scores
in multiple domains, including finance, legal, code, and STEM, outperforming
closed-source proprietary rerankers on both NDCG@10 and Recall. These models
also demonstrate great versatility, maintaining their 0-shot performance on
out-of-domain and private customer datasets. The training data included 112,000
queries and 100 documents per query, and was trained end-to-end from
unannotated queries and documents in less than 10,000 H100-hours.

</details>


### [15] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 这项工作介绍了一个结构化框架，结合自动化和人工监督组件来处理广告本地化复杂性。通过结合多种技术，产生了适合在现实工作流程中使用的视觉连贯且语义准确的多语言广告。


<details>
  <summary>Details</summary>
Motivation: 广告针对多语言受众的本地化需求超出简单的文本翻译，需要跨不同语言和格式保留视觉一致性、空间对齐和风格完整性。

Method: 结合场景文本检测、修补、机器翻译和文本重新实施，以加速广告本地化评估工作流程。

Result: 通过定性结果在六个地区展示了方法的有效性，表明产生了适合在实际工作流程中部署的本地化广告。

Conclusion: 这项工作引入了一个结构化框架，结合自动化组件和人工监督以应对广告本地化的复杂性。在六个地区的定性结果表明，该方法产生了在语义上准确且视觉上连贯的本地化广告，适用于在现实工作流程中部署。

Abstract: Adapting advertisements for multilingual audiences requires more than simple
text translation; it demands preservation of visual consistency, spatial
alignment, and stylistic integrity across diverse languages and formats. We
introduce a structured framework that combines automated components with human
oversight to address the complexities of advertisement localization. To the
best of our knowledge, this is the first work to integrate scene text
detection, inpainting, machine translation (MT), and text reimposition
specifically for accelerating ad localization evaluation workflows. Qualitative
results across six locales demonstrate that our approach produces semantically
accurate and visually coherent localized advertisements, suitable for
deployment in real-world workflows.

</details>


### [16] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: The paper introduces Agentic AI, a proactive agent-assist tool, Minerva CQ, that significantly enhances agent efficiency and customer experience in voice-based customer support through real-time support and contextual reasoning.


<details>
  <summary>Details</summary>
Motivation: Despite advances in AI for contact centers, customer experience still faces challenges like high average handling time, low first-call resolution, and poor customer satisfaction due to agents' cognitive load. Existing AI-powered tools are often reactive and lack deeper contextual reasoning.

Method: The paper presents a case study of Minerva CQ, a real-time Agent Assist product, that integrates various technologies like real-time transcription, intent and sentiment detection, entity recognition, contextual retrieval, dynamic customer profiling, and conversational summaries to support agents in real-time.

Result: Minerva CQ, the AI agent-assist tool, deployed in voice-based customer support, acts as an AI co-pilot and has shown measurable improvements in agent efficiency and customer experience across multiple deployments.

Conclusion: Agentic AI introduces goal-driven, autonomous, and proactive agent-assist tools that improve agent efficiency and customer experience in voice-based customer support.

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [17] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: Match Chat是一个实时的、代理驱动的助手，旨在通过提供即时准确的响应来增强网球粉丝的体验。系统结合了生成人工智能和生成计算技术，并采用了代理为导向的架构（AOA）来预处理和优化用户查询，提高了系统的准确性和响应速度。系统通过交互式提示设计引导用户查询，优化了用户体验。同时，系统设计简单，并提供无需培训或技术熟悉度的用户界面，强调了平台的可扩展性和可靠性。在温布尔登锦标赛和美国网球公开赛上，Match Chat系统表现稳定，为大量用户提供了高效的服务。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提高网球粉丝的体验，通过提供实时、准确的响应来增强用户互动。通过引入生成人工智能和生成计算技术，系统旨在结合速度、精确性和可用性，为大规模用户群体提供高效、易用的服务。

Method: 系统结合了生成人工智能和生成计算技术，并采用了代理为导向的架构（AOA）来预处理和优化用户查询，提高了系统的准确性和响应速度。系统通过交互式提示设计引导用户查询，优化了用户体验。同时，系统设计简单，并提供无需培训或技术熟悉度的用户界面，强调了平台的可扩展性和可靠性。

Result: Match Chat系统在温布尔登锦标赛和美国网球公开赛上成功运行，为约100万用户提供了无缝访问流媒体和静态数据的体验。系统具有92.83%的准确率和平均6.25秒的响应时间，在每秒最多120个请求的负载下表现稳定。用户体验的优化和平台的可扩展性和可靠性得到了验证。

Conclusion: Match Chat是一个实时的、代理驱动的助手，旨在通过提供即时准确的响应来增强网球粉丝的体验。该系统融合了生成人工智能（GenAI）和生成计算（GenComp）技术，在网球单打比赛中合成关键见解。它在2025年的温布尔登锦标赛和美国网球公开赛上首次亮相，通过自然语言查询为约100万用户提供了无缝访问流媒体和静态数据。系统的架构基于以代理为导向的架构（AOA），结合规则引擎、预测模型和代理以在将用户查询传递给GenAI组件之前预处理和优化用户查询。Match Chat系统在每秒最多120个请求（RPS）的负载下，准确率为92.83%，平均响应时间为6.25秒。超过96.08%的查询都使用交互式提示设计进行引导，为用户体验提供了优先考虑清晰度、响应性和最小努力。该系统旨在掩盖架构复杂性，提供一个无摩擦力和直观界面，无需任何培训或技术熟悉度。在两次大满贯比赛部署期间，Match Chat保持了100%的正常运行时间，并支持了近100万独特用户，强调了平台的可扩展性和可靠性。这项工作介绍了面向实时消费者的人工智能系统的关键设计模式，强调了速度、精确性和可用性，突显了在动态环境中部署性能出色的代理系统的实用路径。

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [18] [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
*Minyu Chen,Guoqiang Li*

Main category: cs.AI

TL;DR: 本研究提出了DaSAThco框架，通过学习实例特征到定制启发式集合的映射，实现了训练一次、广泛适应的模型。实验结果显示DaSAThco达到了优越的性能，并展示了强大的领域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法虽然可以为特定问题族找到专门的配置，但缺乏泛化性，需要昂贵的重新优化适应新的问题类型。因此，需要一种能够学习实例特征与定制启发式集合之间映射的方法来解决这一挑战。

Method: 使用大型语言模型，通过系统定义的问题原型生成一系列多样化的专门的启发式方法，然后学习自适应选择机制形成最终的映射。

Result: 实验证明DaSAThco在性能上表现优越，并具有强大的领域外泛化能力。

Conclusion: 引入DaSAThco框架，通过从实例特征到定制启发式集合的可学习映射，实现了训练一次、广泛适应的模型，实现了出色的性能表现和强大的领域外泛化能力。

Abstract: The performance of Conflict-Driven Clause Learning solvers hinges on internal
heuristics, yet the heterogeneity of SAT problems makes a single, universally
optimal configuration unattainable. While prior automated methods can find
specialized configurations for specific problem families, this dataset-specific
approach lacks generalizability and requires costly re-optimization for new
problem types. We introduce DaSAThco, a framework that addresses this challenge
by learning a generalizable mapping from instance features to tailored
heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework
uses a Large Language Model, guided by systematically defined Problem
Archetypes, to generate a diverse portfolio of specialized heuristic ensembles
and subsequently learns an adaptive selection mechanism to form the final
mapping. Experiments show that DaSAThco achieves superior performance and, most
notably, demonstrates robust out-of-domain generalization where non-adaptive
methods show limitations. Our work establishes a more scalable and practical
path toward automated algorithm design for complex, configurable systems.

</details>


### [19] [Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis](https://arxiv.org/abs/2509.12611)
*Anmol Singhal Navya Singhal*

Main category: cs.AI

TL;DR: AD-FCoT combines analogical reasoning with chain-of-thought (CoT) prompting for sentiment prediction on historical financial news, outperforming baselines in sentiment classification accuracy and correlation with market returns. It requires no additional training data, leverages internal financial knowledge of models, and provides interpretable insights for real-world financial analysis.


<details>
  <summary>Details</summary>
Motivation: The existing methods struggle to capture the complex economic context of news and lack transparent reasoning, undermining their reliability. Renewed interest in enhancing AI techniques for sentiment analysis due to the rise of Large Language Models (LLMs) with strong text understanding capabilities.

Method: Proposed Analogy-Driven Financial Chain-of-Thought (AD-FCoT), a prompting framework that integrates analogical reasoning with chain-of-thought (CoT) prompting for sentiment prediction on historical financial news. It guides LLMs to draw parallels between new events and relevant historical scenarios with known outcomes, embedding analogies into a structured, step-by-step reasoning chain.

Result: AD-FCoT operates purely through prompting, requires no additional training data or fine-tuning, leverages the model's internal financial knowledge, and generates rationales that mirror human analytical reasoning. Experiments on thousands of news articles show that AD-FCoT performs well in sentiment classification accuracy and market returns correlation, providing interpretable insights for financial analysis.

Conclusion: AD-FCoT outperforms strong baselines in sentiment classification accuracy and achieves substantially higher correlation with market returns. Its generated explanations also align with domain expertise, providing interpretable insights suitable for real-world financial analysis.

Abstract: Financial news sentiment analysis is crucial for anticipating market
movements. With the rise of AI techniques such as Large Language Models (LLMs),
which demonstrate strong text understanding capabilities, there has been
renewed interest in enhancing these systems. Existing methods, however, often
struggle to capture the complex economic context of news and lack transparent
reasoning, which undermines their reliability. We propose Analogy-Driven
Financial Chain-of-Thought (AD-FCoT), a prompting framework that integrates
analogical reasoning with chain-of-thought (CoT) prompting for sentiment
prediction on historical financial news. AD-FCoT guides LLMs to draw parallels
between new events and relevant historical scenarios with known outcomes,
embedding these analogies into a structured, step-by-step reasoning chain. To
our knowledge, this is among the first approaches to explicitly combine
analogical examples with CoT reasoning in finance. Operating purely through
prompting, AD-FCoT requires no additional training data or fine-tuning and
leverages the model's internal financial knowledge to generate rationales that
mirror human analytical reasoning. Experiments on thousands of news articles
show that AD-FCoT outperforms strong baselines in sentiment classification
accuracy and achieves substantially higher correlation with market returns. Its
generated explanations also align with domain expertise, providing
interpretable insights suitable for real-world financial analysis.

</details>


### [20] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: 本研究提出了GBV-SQL框架，通过引入反向翻译验证和Gold Errors的形式分类，揭示了评估中存在的系统性问题，并在BIRD和Spider基准测试中取得显著的执行准确率提升。研究强调了对数据集策划的严格需求。


<details>
  <summary>Details</summary>
Motivation: 当前的Text2SQL生成存在语义鸿沟问题，为了解决这一挑战，提出了GBV-SQL框架。研究发现评估中存在的系统性问题主要源于基准数据质量低下，因此需要提出更严格的数据策划方法。

Method: GBV-SQL框架结合了Guided Generation和SQL2Text反向翻译的验证机制，利用专门的代理将生成的SQL重新翻译成自然语言，验证其与原始问题的逻辑一致性。研究介绍了Gold Errors的概念，认为评估中存在的困难在于基准数据的质量问题。

Result: 在BIRD基准测试中，GBV-SQL实现了63.23%的执行准确率，相对改进了5.8%。在Spider基准测试中，去除问题后的结果分别为96.5%（开发集）和97.6%（测试集）的执行准确率。

Conclusion: 提出了一种新的多代理框架GBV-SQL，用于改进Text2SQL生成中的语义鸿沟问题，通过引入SQL2Text反向翻译验证以及Gold Errors的形式分类，揭示了评估中存在的系统性问题，并在BIRD和Spider两个基准测试中取得显著的执行准确率提升。该研究强调了对数据集策划的严格需求。

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [21] [Mob-based cattle weight gain forecasting using ML models](https://arxiv.org/abs/2509.12615)
*Muhammad Riaz Hasib Hossain,Rafiqul Islam,Shawn R McGrath,Md Zahidul Islam,David Lamb*

Main category: cs.AI

TL;DR: 本研究提出了一种新技术，MB CWG，用于预测群体牛的体重增长。通过Random Forest模型比较了不同模型在月度体重增长预测中的表现，发现RF模型在所有数据集上均表现更好。研究结果表明，考虑天气和年龄因素可以显著提高体重增长预测的准确性，RF模型在各种情况下均优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 通过预测基于群体牛的体重增长，有助于大型养殖场精细化饲料策略，做出明智的繁殖选择，减少与气候变化和市场波动相关的风险。

Method: 本研究提出了一种名为MB CWG的新技术，利用查尔斯斯图尔特大学农场收集的历史数据，预测群体牛的一个月增重。研究采用了Random Forest（RF）模型，将其在每月体重增长预测中与支持向量回归（SVR）和长短时记忆（LSTM）模型进行了比较。使用四个数据集评估了模型的表现，包括来自108头群体牛的756个样本数据以及影响CWG的天气数据（降雨和温度）。

Result: RF模型在所有数据集上的表现均优于SVR和LSTM模型，当考虑天气和年龄因素时，R^2达到0.973，RMSE为0.040，MAE为0.033。结果表明，同时考虑天气和年龄因素显著提高了体重增长预测的准确性。

Conclusion: 研究结论表明，随着天气和年龄因素的考虑，RF模型在所有场景中均优于SVR和LSTM模型，显著提高了体重增长预测的准确性。RF模型在变化条件下预测牲畜体重增长具有潜力，突显了年龄和气候因素对群体体重趋势的影响。该研究还开发了一种创新的自动预处理工具，可用于生成MB CWG预测模型的基准数据集。该工具已公开在GitHub上，并可帮助准备当前和未来的分析研究数据集。

Abstract: Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock
farms, allowing farmers to refine their feeding strategies, make educated
breeding choices, and reduce risks linked to climate variability and market
fluctuations. In this paper, a novel technique termed MB CWG is proposed to
forecast the one month advanced weight gain of herd based cattle using
historical data collected from the Charles Sturt University Farm. This research
employs a Random Forest (RF) model, comparing its performance against Support
Vector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly
weight gain prediction. Four datasets were used to evaluate the performance of
models, using 756 sample data from 108 herd-based cattle, along with weather
data (rainfall and temperature) influencing CWG. The RF model performs better
than the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,
RMSE of 0.040, and MAE of 0.033 when both weather and age factors were
included. The results indicate that including both weather and age factors
significantly improves the accuracy of weight gain predictions, with the RF
model outperforming the SVR and LSTM models in all scenarios. These findings
demonstrate the potential of RF as a robust tool for forecasting cattle weight
gain in variable conditions, highlighting the influence of age and climatic
factors on herd based weight trends. This study has also developed an
innovative automated pre processing tool to generate a benchmark dataset for MB
CWG predictive models. The tool is publicly available on GitHub and can assist
in preparing datasets for current and future analytical research..

</details>


### [22] [ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal Language for Any LLM](https://arxiv.org/abs/2509.12625)
*Yong Xia,Jingxuan Li,YeTeng Sun,Jiarui Bu*

Main category: cs.AI

TL;DR: ECG-aBcDe is a novel ECG encoding method that enables direct fine-tuning of pre-trained LLMs without architectural modifications, enhances interpretability through attention heatmap extraction, and explicitly represents time-scale information. It achieves competitive performance on ROUGE-L and METEOR, with significant improvements in BLEU-4 scores, demonstrating the feasibility of integrating ECG analysis with LLMs.


<details>
  <summary>Details</summary>
Motivation: Current methods face challenges in transferability, time-scale information learning, and interpretability in ECG analysis using LLMs. The black-box nature of existing models hinders clinical adoption. The goal is to create a method that overcomes these limitations and provides a new paradigm for integrating ECG analysis with LLMs.

Method: Introducing ECG-aBcDe, a hybrid dataset of ECG language and natural language for universal ECG encoding, bidirectional convertibility between ECG and ECG language, and explicit representation of time-scale information to mitigate Transformer limitations.

Result: The method achieves competitive performance on ROUGE-L and METEOR, with significant improvements in BLEU-4 scores compared to existing methods. In-dataset and cross-dataset evaluations show improvements of 2.8 times and 3.9 times, reaching scores of 42.58 and 30.76, respectively. These results demonstrate the feasibility of the new paradigm.

Conclusion: ECG-aBcDe introduces a novel ECG encoding method that addresses challenges in transferability, time-scale information learning, and interpretability, achieving competitive performance on ROUGE-L, METEOR, and significant improvements in BLEU-4 scores. The method enables direct fine-tuning of pre-trained LLMs without architectural modifications and enhances interpretability through attention heatmaps extraction from ECG signals.

Abstract: Large Language Models (LLMs) hold significant promise for electrocardiogram
(ECG) analysis, yet challenges remain regarding transferability, time-scale
information learning, and interpretability. Current methods suffer from
model-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs
struggle to capture crucial time-scale information inherent in ECGs due to
Transformer limitations. And their black-box nature limits clinical adoption.
To address these limitations, we introduce ECG-aBcDe, a novel ECG encoding
method that transforms ECG signals into a universal ECG language readily
interpretable by any LLM. By constructing a hybrid dataset of ECG language and
natural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs
without architectural modifications, achieving "construct once, use anywhere"
capability. Moreover, the bidirectional convertibility between ECG and ECG
language of ECG-aBcDe allows for extracting attention heatmaps from ECG
signals, significantly enhancing interpretability. Finally, ECG-aBcDe
explicitly represents time-scale information, mitigating Transformer
limitations. This work presents a new paradigm for integrating ECG analysis
with LLMs. Compared with existing methods, our method achieves competitive
performance on ROUGE-L and METEOR. Notably, it delivers significant
improvements in the BLEU-4, with improvements of 2.8 times and 3.9 times in
in-dataset and cross-dataset evaluations, respectively, reaching scores of
42.58 and 30.76. These results provide strong evidence for the feasibility of
the new paradigm.

</details>


### [23] [Learn to Relax with Large Language Models: Solving Nonlinear Combinatorial Optimization Problems via Bidirectional Coevolution](https://arxiv.org/abs/2509.12643)
*Beidan Liu,Zhengqiu Zhu,Chen Gao,Yong Zhao,Wei Qi,Quanjun Yin*

Main category: cs.AI

TL;DR: 介绍了AutoCO方法，通过学习使用大型语言模型来放松约束，改变了非线性组合优化问题的解决方式。利用结构化LLM推理生成约束放松策略，建立了新颖的双向共同进化机制，确保在碎片化解空间中强化和多样化之间的最佳平衡。在三个具有挑战性的NCOP基准测试中验证了AutoCO方法的有效性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 非线性组合优化问题具有非凸特性，传统的约束放松方法依赖于专家驱动的迭代设计过程，缺乏系统自动化和可扩展性。最近的大型语言模型（LLM）优化方法虽然表现出解决问题的潜力，但主要作为被动约束验证器，无法处理NCOPs固有的复杂约束交互。

Method: 利用结构化的大型语言模型推理生成约束放松策略，采用统一的三元表示方案，建立了新颖的双向（全局-局部）共同进化机制，同时整合了进化算法和蒙特卡洛树搜索，确保在碎片化解空间中强化和多样化之间的最佳平衡。

Result: 在三个具有挑战性的NCOP基准测试上进行了全面实验，验证了AutoCO相对于基线的持续有效性和卓越性能。

Conclusion: 介绍了首个端到端的自动约束优化（AutoCO）方法，通过学习使用大型语言模型来放松约束，从而彻底改变了非线性组合优化问题（NCOPs）的解决方式。

Abstract: Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable
computational hurdle in practice, as their nonconvex nature gives rise to
multi-modal solution spaces that defy efficient optimization. Traditional
constraint relaxation approaches rely heavily on expert-driven, iterative
design processes that lack systematic automation and scalable adaptability.
While recent Large Language Model (LLM)-based optimization methods show promise
for autonomous problem-solving, they predominantly function as passive
constraint validators rather than proactive strategy architects, failing to
handle the sophisticated constraint interactions inherent to NCOPs.To address
these limitations, we introduce the first end-to-end \textbf{Auto}mated
\textbf{C}onstraint \textbf{O}ptimization (AutoCO) method, which revolutionizes
NCOPs resolution through learning to relax with LLMs.Specifically, we leverage
structured LLM reasoning to generate constraint relaxation strategies, which
are dynamically evolving with algorithmic principles and executable code
through a unified triple-representation scheme. We further establish a novel
bidirectional (global-local) coevolution mechanism that synergistically
integrates Evolutionary Algorithms for intensive local refinement with Monte
Carlo Tree Search for systematic global strategy space exploration, ensuring
optimal balance between intensification and diversification in fragmented
solution spaces. Finally, comprehensive experiments on three challenging NCOP
benchmarks validate AutoCO's consistent effectiveness and superior performance
over the baselines.

</details>


### [24] [Large Language Models Imitate Logical Reasoning, but at what Cost?](https://arxiv.org/abs/2509.12645)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: 本文对前沿大型语言模型的推理能力进行了长期研究，引入神经符号架构降低计算成本并保持性能，实现推理FLOPs数量的有效计算。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于评估大型语言模型在推理能力方面的表现，探讨性能提升的潜在机制，并提出一种神经符号架构以解决计算成本问题。

Method: 本文采用长期研究方法，评估了大型语言模型的推理能力，并引入神经符号架构以降低计算成本。通过对问题的标准化处理和求解器求解，实现了高效的推理过程。

Result: 研究表明，大型语言模型的推理能力可以通过适当的改进方法得到提升，神经符号架构在降低计算成本的同时能够保持良好的性能。推理FLOPs数量的近似计算方法在实验中表现良好。

Conclusion: 本文通过长期研究评估前沿大型语言模型的推理能力，在18个月内观察了三种主要模型在不同时间点的准确性和信实性，并探讨了模型性能的提升和改进方法。研究还介绍了一种神经符号架构，利用少于150亿参数的LLMs将问题转化为规范形式，并通过Z3求解器解决，以确定查询的可满足性。神经符号方法显著降低了计算成本，并保持了接近完美的性能表现。推理FLOPs数量的常见近似在所有实验中都在10%的范围内准确。

Abstract: We present a longitudinal study which evaluates the reasoning capability of
frontier Large Language Models over an eighteen month period. We measured the
accuracy of three leading models from December 2023, September 2024 and June
2025 on true or false questions from the PrOntoQA dataset and their
faithfulness to reasoning strategies provided through in-context learning. The
improvement in performance from 2023 to 2024 can be attributed to hidden Chain
of Thought prompting. The introduction of thinking models allowed for
significant improvement in model performance between 2024 and 2025.
  We then present a neuro-symbolic architecture which uses LLMs of less than 15
billion parameters to translate the problems into a standardised form. We then
parse the standardised forms of the problems into a program to be solved by Z3,
an SMT solver, to determine the satisfiability of the query. We report the
number of prompt and completion tokens as well as the computational cost in
FLOPs for open source models. The neuro-symbolic approach significantly reduces
the computational cost while maintaining near perfect performance. The common
approximation that the number of inference FLOPs is double the product of the
active parameters and total tokens was accurate within 10\% for all
experiments.

</details>


### [25] [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
*Hanqing Li,Kiran Sheena Jyothi,Henry Liang,Sharika Mahadevan,Diego Klabjan*

Main category: cs.AI

TL;DR: GRRAF is a training-free method combining retrieval-augmented generation with large language models for graph reasoning tasks. It achieves 100% accuracy in various tasks, including cycle detection, bipartite graph checks, and maximum flow, and scales effectively to graphs with up to 10,000 nodes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for graph reasoning tasks often require finetuning or rely on predefined algorithms, limiting their flexibility and efficiency. GRRAF aims to overcome these limitations by introducing a training-free approach that leverages retrieval-augmented generation and large language models.

Method: The paper introduces GRRAF, a training-free method that combines retrieval-augmented generation with large language models to address graph reasoning tasks. The target graph is stored in a graph database, and the model generates executable code queries to retrieve information, avoiding the need for extensive fine-tuning or predefined algorithms.

Result: Experimental evaluations on the GraphInstruct dataset show that GRRAF achieves 100% accuracy in most graph reasoning tasks, including cycle detection, bipartite graph checks, shortest path computation, and maximum flow. It also demonstrates high performance in subgraph matching. The method maintains consistent token costs regardless of graph sizes, indicating efficiency in handling graphs of varying complexities.

Conclusion: GRRAF achieves impressive results in various graph reasoning tasks, offering 100% accuracy in most cases and scaling effectively to large graphs with up to 10,000 nodes.

Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.

</details>


### [26] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: The paper proposes a hierarchical memory architecture with Hierarchical Hindsight Reflection (H$^2$R) to enable fine-grained knowledge transfer for LLM-based agents. Experimental results demonstrate improved generalization and decision-making performance compared to prior baselines.


<details>
  <summary>Details</summary>
Motivation: Existing approaches suffer from inefficient and coarse-grained knowledge transfer due to treating prior experiences and knowledge as monolithic units. The motivation behind this work is to enable fine-grained knowledge transfer by introducing a hierarchical memory architecture to improve the performance of LLM-based agents in multi-task scenarios.

Method: The paper proposes a hierarchical memory architecture that decouples high-level planning memory from low-level execution memory, along with a mechanism called Hierarchical Hindsight Reflection (H$^2$R) for distilling reusable and hierarchical knowledge from past interactions. This architecture allows LLM-based agents to access and utilize task-relevant knowledge efficiently.

Result: Experimental results on two benchmarks show that H$^2$R enhances generalization and decision-making performance, surpassing prior baselines like Expel.

Conclusion: Hierarchical memory architecture proposed in this work enables fine-grained knowledge transfer, leading to improved generalization and decision-making performance of LLM-based agents, outperforming prior baselines like Expel.

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [27] [LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning](https://arxiv.org/abs/2509.12875)
*Jiaqi Wang,Binquan Ji,Haibo Luo,Yiyang Qi,Ruiting Li,Huiyan Wang,Yuantao Han,Cangyi Yang,jiaxu Zhang,Feiliang Ren*

Main category: cs.AI

TL;DR: 本文提出了LTA-Thinker方法，通过增加生成的Latent Thought向量的方差分布和引入基于分布的方向优化范式来优化大型语言模型中的复杂推理，提高推理性能，取得了最先进的性能结果。


<details>
  <summary>Details</summary>
Motivation: 通过提高生成的Latent Thought向量的方差分布和改善信息效率，以解决大型语言模型中复杂推理的核心瓶颈问题。

Method: LTA-Thinker方法主要包括两方面的改进：一是构建基于可学习先验的Latent Thought生成架构，旨在增加生成的Latent Thought向量的方差分布；二是引入基于分布的方向优化范式，通过多目标联合训练策略改善信息效率和计算成本。该策略结合了标准监督微调（SFT）损失和两个新损失：语义对齐损失和推理焦点损失。

Result: 实验结果表明，LTA-Thinker方法在性能上优于其他基线方法，并显示出更高的性能上限和更好的扩展性能。

Conclusion: 提出了一种名为LTA-Thinker的Latent Thought增强训练框架，用于优化大型语言模型中的复杂推理，提高推理性能。实验证明，LTA-Thinker在各种基准测试中取得了最先进的性能，并展现出更高的性能上限和更好的扩展效果。

Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using
Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,
SoftCoT and its variant are effective in continuous latent space inference, the
core bottleneck still lies in the efficient generation and utilization of
high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger
variance in the generated Latent Thought distribution more closely approximates
the golden truth distribution, we propose a Latent Thought-Augmented Training
Framework--LTA-Thinker, which improves distributional variance and enhances
reasoning performance from two perspectives. First, LTA-Thinker constructs a
Latent Thought generation architecture based on a learnable prior. This
architecture aims to increase the variance distribution of generated Latent
Thought Vectors in order to simplify the overall structure and raise the
performance ceiling. Second, LTA-Thinker introduces a distribution-based
directional optimization paradigm that jointly constrains both distribution
locality and distribution scale. This mechanism improves information efficiency
and computational cost through a multi-objective co-training strategy, which
combines standard Supervised Fine-Tuning (SFT) loss with two novel losses:
Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent
Thought is highly relevant to the semantics of the question; Reasoning Focus
Loss, which utilizes a contrastive learning mechanism to guide the model to
focus on the most critical reasoning steps. Experiments show that LTA-thinker
achieves state-of-the-art (SOTA) performance among various baselines and
demonstrates a higher performance ceiling and better scaling effects.

</details>


### [28] [Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities](https://arxiv.org/abs/2509.12914)
*Tairan Fu,David Campo-Nazareno,Javier Coronado-Blázquez,Javier Conde,Pedro Reviriego,Fabrizio Lombardi*

Main category: cs.AI

TL;DR: The paper explores whether Large Language Models can generate random street addresses for European cities, concluding that they are not effective in doing so.


<details>
  <summary>Details</summary>
Motivation: To determine if Large Language Models can perform a specific task related to European cities.

Method: The paper investigates the capability of Large Language Models in generating random street addresses for European cities.

Result: The result shows that Large Language Models struggle to generate accurate random street addresses for European cities.

Conclusion: Large Language Models are not able to generate random street addresses for European cities effectively.

Abstract: Large Language Models (LLMs) are capable of solving complex math problems or
answer difficult questions on almost any topic, but can they generate random
street addresses for European cities?

</details>


### [29] [Population Estimation using Deep Learning over Gandhinagar Urban Area](https://arxiv.org/abs/2509.12926)
*Jai Singla,Peal Jotania,Keivalya Pandya*

Main category: cs.AI

TL;DR: 本研究提出了利用深度学习解决方案估算人口的方法，结合了卷积神经网络和人工神经网络。通过高分辨率影像和建筑分类，成功实现了对甘地纳加城市人口的准确估算。实验结果显示模型效果显著，整体F1分数达到0.9936。该方法可提供城市管理者用于资源管理的可扩展工具。


<details>
  <summary>Details</summary>
Motivation: 传统人口估算方法如调查和普查费时费力，依赖人力资源且成本高昂。因此，本研究旨在提出一种基于深度学习和高分辨率影像的新方法，以解决人口估算中的这些问题。

Method: 该研究采用了高分辨率卫星影像、数字高程模型和矢量边界相结合的方法，利用卷积神经网络对建筑进行分类，然后通过人工神经网络估算人口数量。在甘地纳加城市区域利用了约48k个建筑物轮廓，实现了建筑物级别的人口估算。

Result: 实验结果表明该模型在大规模数据集上表现良好，达到了0.9936的高F1分数。其结合了先进的地理空间分析方法，成功实现了对甘地纳加人口的准确估算。

Conclusion: 该研究提出了利用深度学习解决方案估算人口的方法，结合了卷积神经网络和人工神经网络，实现了对甘地纳加城市人口的准确估算。实验结果表明模型效果显著，整体F1分数达到0.9936。通过集成实时数据更新和标准化指标，该系统提供了一个自动化的方法来解决传统普查方法的局限性，为城市提供了可扩展和可复制的工具用于优化资源管理。

Abstract: Population estimation is crucial for various applications, from resource
allocation to urban planning. Traditional methods such as surveys and censuses
are expensive, time-consuming and also heavily dependent on human resources,
requiring significant manpower for data collection and processing. In this
study a deep learning solution is proposed to estimate population using high
resolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m
resolution and vector boundaries. Proposed method combines Convolution Neural
Network (CNN) architecture for classification task to classify buildings as
residential and non-residential and Artificial Neural Network (ANN)
architecture to estimate the population. Approx. 48k building footprints over
Gandhinagar urban area are utilized containing both residential and
non-residential, with residential categories further used for building-level
population estimation. Experimental results on a large-scale dataset
demonstrate the effectiveness of our model, achieving an impressive overall
F1-score of 0.9936. The proposed system employs advanced geospatial analysis
with high spatial resolution to estimate Gandhinagar population at 278,954. By
integrating real-time data updates, standardized metrics, and infrastructure
planning capabilities, this automated approach addresses critical limitations
of conventional census-based methodologies. The framework provides
municipalities with a scalable and replicable tool for optimized resource
management in rapidly urbanizing cities, showcasing the efficiency of AI-driven
geospatial analytics in enhancing data-driven urban governance.

</details>


### [30] [HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making](https://arxiv.org/abs/2509.12927)
*Xingxing Hong,Yungong Wang,Dexin Jin,Ye Yuan,Ximing Huang,Zijian Wu,Wenxin Li*

Main category: cs.AI

TL;DR: HLSMAC is a new cooperative MARL benchmark with 12 StarCraft II scenarios based on classical stratagems. It challenges agents with diverse strategic elements and introduces novel metrics for assessing performance. Experiments with state-of-the-art algorithms demonstrate its effectiveness in advancing multi-agent strategic decision-making.


<details>
  <summary>Details</summary>
Motivation: The existing benchmarks like SMAC primarily focus on micromanagement, limiting the evaluation of high-level strategic intelligence in MARL algorithms. The motivation is to introduce HLSMAC to provide a comprehensive evaluation of high-level strategic decision-making capabilities of agents in a cooperative MARL setting.

Method: Introducing HLSMAC, a new cooperative MARL benchmark with 12 StarCraft II scenarios based on classical stratagems. Designing scenarios to challenge agents with diverse strategic elements such as tactical maneuvering, timing coordination, and deception. Proposing novel metrics including ability utilization and advancement efficiency. Integrating state-of-the-art MARL algorithms and LLM-based agents for experiments.

Result: Comprehensive evaluation through experiments shows that HLSMAC is effective in advancing multi-agent strategic decision-making. The benchmark offers a robust testbed for assessing agents' performance with novel metrics and diverse strategic challenges.

Conclusion: HLSMAC is introduced as a new cooperative MARL benchmark with 12 StarCraft II scenarios based on classical stratagems to evaluate high-level strategic decision-making capabilities of agents. Novel metrics beyond win rate are proposed for assessing agents' performance within HLSMAC environment. Comprehensive experiments with state-of-the-art MARL algorithms and LLM-based agents show that HLSMAC is effective for advancing multi-agent strategic decision-making.

Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning
(MARL) algorithms. While StarCraft II-related environments have driven
significant advances in MARL, existing benchmarks like SMAC focus primarily on
micromanagement, limiting comprehensive evaluation of high-level strategic
intelligence. To address this, we introduce HLSMAC, a new cooperative MARL
benchmark with 12 carefully designed StarCraft II scenarios based on classical
stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a
specific stratagem and is designed to challenge agents with diverse strategic
elements, including tactical maneuvering, timing coordination, and deception,
thereby opening up avenues for evaluating high-level strategic decision-making
capabilities. We also propose novel metrics across multiple dimensions beyond
conventional win rate, such as ability utilization and advancement efficiency,
to assess agents' overall performance within the HLSMAC environment. We
integrate state-of-the-art MARL algorithms and LLM-based agents with our
benchmark and conduct comprehensive experiments. The results demonstrate that
HLSMAC serves as a robust testbed for advancing multi-agent strategic
decision-making.

</details>


### [31] [The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features](https://arxiv.org/abs/2509.12934)
*Jeremias Ferrao,Matthijs van der Lende,Ilija Lichkovski,Clement Neo*

Main category: cs.AI

TL;DR: FSRL is a transparent alignment framework that optimizes preferences effectively, comparably to current methods. Mechanistic analysis reveals a preference for style features over explicit alignment concepts. FSRL aims to offer interpretable model control and insight into alignment mechanisms.


<details>
  <summary>Details</summary>
Motivation: The prevailing RLHF approach leads to diffuse and opaque parameter changes, hindering the understanding of what the model has internalized. The need for a transparent alignment framework that enables interpretable model control and insight into alignment mechanisms drives the introduction of FSRL.

Method: Introducing Feature Steering with Reinforcement Learning (FSRL) as an alignment framework to train a lightweight adapter that modulates interpretable features from a Sparse Autoencoder (SAE) to steer behavior. Demonstrating the effectiveness of FSRL for preference optimization and comparing it with RLHF methods. Performing mechanistic analysis on the trained adapter to understand its policy and behavior patterns.

Result: FSRL is effective for preference optimization, comparable to RLHF methods, and exhibits a systematic preference for style features over explicit alignment concepts. It provides a tool for interpretable model control and diagnosing alignment mechanisms.

Conclusion: FSRL is introduced as a transparent alignment framework that effectively optimizes preferences and is comparable to current RLHF methods. Mechanistic analysis shows the trained adapter prioritizes style features over explicit alignment concepts, indicating a preference for stylistic presentation as a proxy for quality. FSRL aims to offer interpretable model control and insight into alignment mechanisms.

Abstract: Aligning large language models is critical for their usability and safety.
However, the prevailing approach of Reinforcement Learning from Human Feedback
(RLHF) induces diffuse, opaque parameter changes, making it difficult to
discern what the model has internalized. Hence, we introduce Feature Steering
with Reinforcement Learning (FSRL), a transparent alignment framework that
trains a lightweight adapter to steer behavior by modulating interpretable
features from a Sparse Autoencoder (SAE). First, we demonstrate that FSRL is an
effective method for preference optimization and is comparable with current
RLHF methods. We then perform mechanistic analysis on the trained adapter, and
find that its policy systematically promotes style features over explicit
alignment concepts, suggesting that the preference optimization process rewards
stylistic presentation as a proxy for quality. Ultimately, we hope that FSRL
provides a tool for both interpretable model control and diagnosing the
internal mechanisms of alignment.

</details>


### [32] [Black-box Model Merging for Language-Model-as-a-Service with Massive Model Repositories](https://arxiv.org/abs/2509.12951)
*Shilian Chen,Jie Zhou,Tianyu Huai,Yujiang Lu,Junsong Li,Bihao Zhan,Qianjun Pan,Yutao Yang,Xin Li,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 黑盒模型融合（BMM）是一种用于合并大型语言模型的方法，该论文提出了一种基于进化算法的无导数优化框架（Evo-Merging），通过推理时API查询实现有效的模型融合。该方法包括稀疏性去噪和符号感知缩放两个关键组件，实验表明在多项任务中取得了最先进的效果。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLMs）作为黑盒服务通过API接口提供，模型权重不可供端用户访问的挑战，提出解决这一挑战的黑盒模型融合（BMM）方法。

Method: 该论文提出了两个关键组件：1. 基于稀疏性的去噪，用于识别和过滤跨模型的不相关或冗余信息；2. 符号感知的缩放，动态计算相关模型的最佳组合权重，基于它们的性能表现。提供了对非对称稀疏化的正式理论的证明和理论分析。

Result: 在广泛的实验评估中，该方法在多项任务上取得了最先进的结果，明显优于现有强基准线。

Conclusion: 该论文提出了一种基于进化算法(Evo-Merging)的无导数优化框架，称为黑盒模型融合（BMM），用于有效地合并庞大的语言模型（LLMs），并且在多项任务中取得了最先进的结果，明显优于现有强基准线。

Abstract: Model merging refers to the process of integrating multiple distinct models
into a unified model that preserves and combines the strengths and capabilities
of the individual models. Most existing approaches rely on task vectors to
combine models, typically under the assumption that model parameters are
accessible. However, for extremely large language models (LLMs) such as GPT-4,
which are often provided solely as black-box services through API interfaces
(Language-Model-as-a-Service), model weights are not available to end users.
This presents a significant challenge, which we refer to as black-box model
merging (BMM) with massive LLMs. To address this challenge, we propose a
derivative-free optimization framework based on the evolutionary algorithm
(Evo-Merging) that enables effective model merging using only inference-time
API queries. Our method consists of two key components: (1) sparsity-based
denoising, designed to identify and filter out irrelevant or redundant
information across models, and (2) sign-aware scaling, which dynamically
computes optimal combination weights for the relevant models based on their
performance. We also provide a formal justification, along with a theoretical
analysis, for our asymmetric sparsification. Extensive experimental evaluations
demonstrate that our approach achieves state-of-the-art results on a range of
tasks, significantly outperforming existing strong baselines.

</details>


### [33] [Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning](https://arxiv.org/abs/2509.12958)
*Bihao Zhan,Jie Zhou,Junsong Li,Yutao Yang,Shilian Chen,Qianjun Pan,Xin Li,Wen Wu,Xingjiao Wu,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 该论文提出了一种隐私增强的持续学习框架（PeCL），通过引入Token级动态差分隐私策略和隐私引导的记忆整形模块，实现了对私密实体的强大保护同时最小化对非敏感知识的干扰。实验结果显示，PeCL在隐私保护和模型效用之间取得了优异的平衡，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统的隐私方法往往会导致模型效用显著下降，限制了持续学习模型在隐私敏感领域的部署。因此，研究旨在解决持续学习模型在隐私保护和模型效用之间的平衡问题。

Method: 论文提出了一种Token级动态差分隐私策略，根据单个Token的语义敏感性自适应分配隐私预算，以确保对私密实体的强大保护同时最小化对非敏感的普通知识的噪声注入。其次，结合了隐私引导的记忆整形模块，利用动态差分隐私机制的敏感性分析智能地忘记模型记忆和参数中的敏感信息，同时明确保留对于缓解灾难性遗忘至关重要的与任务无关的历史知识。

Result: PeCL在实验中表现出卓越的平衡能力，维持先前任务的高准确性的同时确保了强大的隐私保护。该框架优于基线模型，并在隐私保护和模型效用之间取得了良好的平衡。

Conclusion: 该论文提出了一种隐私增强的持续学习框架（PeCL），旨在克服传统隐私方法对于隐私保护和模型效用之间平衡的困难。研究结果表明，PeCL在维护高准确性的同时确保了强大的隐私保护，优于基线模型。

Abstract: Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.

</details>


### [34] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: 本文介绍了一种名为规划副驾驶的聊天机器人，通过整合多个规划工具并允许用户通过自然语言指令调用它们，弥补了大型语言模型（LLMs）在可靠的长期规划方面的不足。研究结果表明，规划副驾驶在执行规划任务时表现优异，远远超过了没有规划工具的相同LLMs，并且相比最近商业化LLM Chat GPT-5，规划副驾驶表现更出色。因此，专门的规划工具可能是使LLMs执行规划任务的有效方法。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于解决大型语言模型(LLMs)在长期规划方面的不足。通过引入规划副驾驶来整合多个规划工具，允许用户通过自然语言指令调用，利用MCP连接LLMs与外部工具和系统，从而进行可靠的长期规划。

Method: 通过引入规划副驾驶，该聊天机器人整合了多个规划工具，并允许用户通过自然语言指令调用它们。利用最新开发的模型上下文协议（MCP）连接LLMs与外部工具和系统，实现了无需进行领域特定微调的任何支持MCP的LLMs。作者对规划副驾驶使用三个开源LLMs的表现进行了实证评估，结果显示其明显优于没有规划工具的相同LLMs。此外，作者还进行了限定性的质性比较，发现规划副驾驶明显优于Chat GPT-5。

Result: 作者通过实验评估了规划副驾驶执行规划任务的能力，并与没有规划工具的LLMs进行了比较。结果显示规划副驾驶表现优异，并且比商业LLM Chat GPT-5表现更出色，尽管规划副驾驶依赖一个较小的LLM。

Conclusion: 本文介绍了一种名为规划副驾驶的聊天机器人，通过整合多个规划工具并允许用户通过自然语言指令调用它们，弥补了大型语言模型（LLMs）在可靠的长期规划方面的不足。研究结果表明，规划副驾驶在执行规划任务时表现优异，远远超过了没有规划工具的相同LLMs，并且相比最近商业化LLM Chat GPT-5，规划副驾驶表现更出色。因此，专门的规划工具可能是使LLMs执行规划任务的有效方法。

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [35] [Data-driven Methods of Extracting Text Structure and Information Transfer](https://arxiv.org/abs/2509.12999)
*Shinichi Honna,Taichi Murayama,Akira Matsui*

Main category: cs.AI

TL;DR: 成功需要满足少量基本条件，而失败则呈现多种形式。不同媒介中的结构原则不同：小说遵循反AKP，维基百科结合AKP和有序模式，学术论文按顺序遵循反AKP但在位置上保持混乱，电影则因类型而异。成功取决于不同媒介的特定结构约束，而失败在各个领域表现出不同形式。


<details>
  <summary>Details</summary>
Motivation: 测试不同结构模式在各种媒介中的表现，探讨成功和失败的条件。

Method: 对小说、在线百科全书、研究论文和电影进行测试，将文本表示为功能块序列，评估转换顺序和位置的收敛性。

Result: 研究结果显示随着媒介的变化，结构原则也不同：小说按顺序遵循反AKP，维基百科结合AKP和有序模式，学术论文按顺序遵循反AKP但在位置上混乱，电影则因类型而异。因此，成功取决于每种媒介特定的结构约束，失败在不同领域呈现出不同形式。

Conclusion: 不同媒介中的结构原则存在差异，小说遵循反阿娜·卡列尼娜原则（AKP），维基百科结合了AKP和有序模式，学术论文按顺序遵循反AKP但在位置上保持混乱，电影则因类型而异。因此，成功取决于每种媒介特定的结构约束，而失败在不同领域呈现出不同形式。

Abstract: The Anna Karenina Principle (AKP) holds that success requires satisfying a
small set of essential conditions, whereas failure takes diverse forms. We test
AKP, its reverse, and two further patterns described as ordered and noisy
across novels, online encyclopedias, research papers, and movies. Texts are
represented as sequences of functional blocks, and convergence is assessed in
transition order and position. Results show that structural principles vary by
medium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered
patterns, academic papers display reverse AKP in order but remain noisy in
position, and movies diverge by genre. Success therefore depends on structural
constraints that are specific to each medium, while failure assumes different
shapes across domains.

</details>


### [36] [A Visualized Framework for Event Cooperation with Generative Agents](https://arxiv.org/abs/2509.13011)
*Yuyang Tian,Shunqiang Mao,Wenchang Gao,Lanlan Qiu,Tianxing He*

Main category: cs.AI

TL;DR: 本论文提出了MiniAgentPro平台，包括地图编辑器和模拟播放器，用于评估代理的能力。测试结果显示在基本设置下代理表现良好，但在困难场景中存在挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs已经在模拟代理社区方面取得了革命性进展，但现有框架往往忽视对事件组织的系统评估，并且缺乏与物理环境的可视化集成，限制了代理在空间中导航和与物品进行真实互动的能力。

Method: 开发了MiniAgentPro可视化平台，引入了包含多种事件场景的测试集，使用GPT-4o进行评估代理的能力。

Result: MiniAgentPro平台具有直观的地图编辑器和模拟播放器，测试集包含八种不同事件场景，对代理能力进行评估。在基本设置中表现良好，但在困难场景中存在协调挑战。

Conclusion: 该论文提出了MiniAgentPro，这是一个可视化平台，具有直观的地图编辑器和模拟播放器，用于定制环境和模拟动画。通过该工具，引入了包含八种不同事件场景的综合测试集，用于评估代理的能力。使用GPT-4o进行评估，在基本设置中表现出良好性能，但在困难场景中强调了协调挑战。

Abstract: Large Language Models (LLMs) have revolutionized the simulation of agent
societies, enabling autonomous planning, memory formation, and social
interactions. However, existing frameworks often overlook systematic
evaluations for event organization and lack visualized integration with
physically grounded environments, limiting agents' ability to navigate spaces
and interact with items realistically. We develop MiniAgentPro, a visualization
platform featuring an intuitive map editor for customizing environments and a
simulation player with smooth animations. Based on this tool, we introduce a
comprehensive test set comprising eight diverse event scenarios with basic and
hard variants to assess agents' ability. Evaluations using GPT-4o demonstrate
strong performance in basic settings but highlight coordination challenges in
hard variants.

</details>


### [37] [Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets](https://arxiv.org/abs/2509.13131)
*Marylou Fauchard,Florian Carichon,Margarida Carvalho,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 本研究引入了一个新的基准测试，评估了大语言模型在匹配问题中的表现。研究发现，虽然大语言模型在某些约束下表现良好，但在保持一致性上存在困难。推理型大语言模型优于传统模型。不同的提示策略对大语言模型的表现产生不同影响。迭代提示与自动生成反馈的表现不是单调的。


<details>
  <summary>Details</summary>
Motivation: 先前关于大语言模型的研究已经展示了它们在处理复杂数学任务中表现出色的能力。然而，将大语言模型应用于需要在偏好和结构约束下进行推理的匹配问题仍然不够深入探讨。本研究的动机在于填补这一领域的空白，提出了一个新的基准测试以评估大语言模型在匹配问题中的表现。

Method: 本研究引入了一个由369个“大学录取问题”实例组成的基准测试，用于评估不同大语言模型在重要维度上的表现：可行性、稳定性和最优性。研究采用该基准测试来评估几种开放权重的大语言模型的性能。研究结果显示，尽管大语言模型可以满足某些约束条件，但它们在保持一致性方面存在困难。并且推理型大语言模型明显优于传统模型。研究还观察到，大语言模型对于不同的提示策略的反应不同，包括“Chain-of-Thought”、“In-Context Learning”和基于角色的提示，没有一种提示可以始终提供最佳性能。最后，研究报告了迭代提示与自动生成反馈的表现，并显示它们不是单调的，可能在尝试较多次后出现显著下降。

Result: 研究发现，大语言模型在某些约束下表现良好，但在保持一致性上存在困难。推理型大语言模型在匹配问题中优于传统模型。不同的提示策略对大语言模型的表现影响差异，迭代提示与自动生成反馈的表现可能会出现显著下降。

Conclusion: 本研究介绍了一个新的基准测试，用于评估大语言模型在匹配问题中的表现，结果显示大语言模型在满足某些约束方面表现良好，但在保持一致性上存在困难。研究也发现，具有推理能力的大语言模型在匹配问题上明显优于传统模型。对于不同的提示策略，大语言模型的反应不同，没有一种提示策略能够始终提供最佳性能。本研究还展示了迭代提示和自动生成反馈的性能结果，并指出性能可能在尝试较多次后出现显著下降。总体而言，本研究提供了一个新的视角，探讨了大语言模型在具有偏好约束的组合优化问题中的推理性能和提示策略的有效性。

Abstract: Recent advances in reasoning with large language models (LLMs) have
demonstrated strong performance on complex mathematical tasks, including
combinatorial optimization. Techniques such as Chain-of-Thought and In-Context
Learning have further enhanced this capability, making LLMs both powerful and
accessible tools for a wide range of users, including non-experts. However,
applying LLMs to matching problems, which require reasoning under preferential
and structural constraints, remains underexplored. To address this gap, we
introduce a novel benchmark of 369 instances of the College Admission Problem,
a canonical example of a matching problem with preferences, to evaluate LLMs
across key dimensions: feasibility, stability, and optimality. We employ this
benchmark to assess the performance of several open-weight LLMs. Our results
first reveal that while LLMs can satisfy certain constraints, they struggle to
meet all evaluation criteria consistently. They also show that reasoning LLMs,
like QwQ and GPT-oss, significantly outperform traditional models such as
Llama, Qwen or Mistral, defined here as models used without any dedicated
reasoning mechanisms. Moreover, we observed that LLMs reacted differently to
the various prompting strategies tested, which include Chain-of-Thought,
In-Context Learning and role-based prompting, with no prompt consistently
offering the best performance. Finally, we report the performances from
iterative prompting with auto-generated feedback and show that they are not
monotonic; they can peak early and then significantly decline in later
attempts. Overall, this work offers a new perspective on model reasoning
performance and the effectiveness of prompting strategies in combinatorial
optimization problems with preferential constraints.

</details>


### [38] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 这篇论文介绍了如何利用行动设计研究流程，在数字本地金融平台中设计和部署自主AI系统用于金融犯罪合规。该系统自动化了金融犯罪合规流程，强调可解释性、可追溯性和合规性设计，扩展了关于AI支持合规的信息系统文献，并展示了在监管环境中，通过嵌入负责任治理结构可以促进透明度和机构信任。


<details>
  <summary>Details</summary>
Motivation: 金融犯罪合规（FCC）的成本和复杂性不断上升，但往往没有显著提高效率。尽管人工智能提供潜力，但大多数解决方案仍然不透明且与监管预期不符。因此，设计和部署一个自主AI系统来优化金融犯罪合规流程具有重要意义。

Method: 本论文采用行动设计研究（ADR）流程，与金融科技公司和监管利益相关者合作，设计和部署了一种自主AI系统用于金融犯罪合规。通过艺术品中心建模，为自主代理人分配明确定界的角色，实现任务特定的模型路由和审计日志记录。

Result: 通过行动设计研究流程，成功设计和部署了一个自主AI系统，用于金融犯罪合规，在数字本地金融平台中能够自动化入职、监控、调查和报告流程，并强调可解释性、可追溯性和合规性设计。还提供了参考架构、实际原型，并探讨了自主AI如何重新配置金融犯罪合规工作流程的见解。

Conclusion: 这篇论文介绍了在数字本地金融平台中设计和部署的自主AI系统，用于金融犯罪合规。通过行动设计研究（ADR）流程与金融科技公司和监管利益相关者合作开发，该系统自动化了入职、监控、调查和报告流程，强调可解释性、可追溯性和合规性设计。对于自主代理人分配明确定界的角色并启用任务特定的模型路由和审计日志记录等方面进行建模。其贡献包括参考架构、实际原型和对自主AI如何在监管限制下重新配置金融犯罪合规工作流程的见解。结果扩展了关于AI支持合规的信息系统文献，展示了在负责任的治理结构内嵌入自动化如何支持高风险、受监管环境中的透明度和机构信任。

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [39] [G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models](https://arxiv.org/abs/2509.13203)
*Kanishk Garg,Saranya D.,Sanal Kumar,Saurabh Singh,Anupam Purwar*

Main category: cs.AI

TL;DR: 工作人员排班中存在复杂约束规则导致不可行性问题，提出了基于图的冲突集提取算法（G-CSEA）来解决这一问题，通过最小化冲突集生成不可约不可行子集（IIS）。


<details>
  <summary>Details</summary>
Motivation: 工作人员排班涉及各种基于规则的约束，可能导致模型不可行。当前的IIS提取方法存在问题，需要一种更高效的解决方案。

Method: 利用伪布尔约束和不等式关系建模，结合图论和冲突驱动子句学习的方法，构建了冲突集提取算法。

Result: 提出的G-CSEA方法能够有效解决基于伪布尔约束的工作人员排班模型中的不可行性问题，并成功提取冲突集用于生成IIS。

Conclusion: 提出了一种基于图的冲突集提取算法（G-CSEA），用于解决工作人员排班模型中的不可行性问题，并通过最小化冲突集产生不可约不可行子集（IIS）。

Abstract: Workforce scheduling involves a variety of rule-based constraints-such as
shift limits, staffing policies, working hour restrictions, and many similar
scheduling rules-which can interact in conflicting ways, leading to infeasible
models. Identifying the underlying causes of such infeasibility is critical for
resolving scheduling issues and restoring feasibility. A common diagnostic
approach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of
constraints that are jointly infeasible but become feasible when any one is
removed. We consider models formulated using pseudo-Boolean constraints with
inequality relations over binary variables, which naturally encode scheduling
logic. Existing IIS extraction methods such as Additive Deletion and
QuickXplain rely on repeated feasibility checks, often incurring large numbers
of solver calls. Dual ray analysis, while effective for LP-based models, may
fail when the relaxed problem is feasible but the underlying pseudo-Boolean
model is not. To address these limitations, we propose Graph-based Conflict Set
Extraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired
by Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs
an implication graph during constraint propagation and, upon detecting a
conflict, traces all contributing constraints across both decision branches.
The resulting conflict set can optionally be minimized using QuickXplain to
produce an IIS.

</details>


### [40] [Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy](https://arxiv.org/abs/2509.13234)
*Nadim Barakat,William Lotter*

Main category: cs.AI

TL;DR: 本研究评估了多模态大型语言模型（MLLMs）在糖尿病视网膜病变（DR）检测中的应用。研究发现MedGemma在DR检测中表现优于GPT-4o，模拟AI辅助和实际协作中的稳定性更佳。MLLMs有望改善DR筛查流程，而开放、轻量级模型如MedGemma对资源匮乏环境有重要价值，描述性输出提高了临床工作流的可解释性和医生信任。


<details>
  <summary>Details</summary>
Motivation: 当前FDA认可的系统主要提供二元引荐输出，这种较少的输出可能限制了临床信任和实用性。然而，确定最有效的输出格式以增强临床医生-AI性能是一个在规模上难以评估的经验挑战。

Method: 研究评估了多模态大型语言模型（MLLMs）在DR检测中的能力，以及它们模拟临床AI辅助在不同输出类型情况下的表现。实验包括基线评估、使用合成预测进行模拟AI辅助和实际的AI对AI协作。两个模型在IDRiD和Messidor-2上进行了测试，分别是GPT-4o（通用用途MLLM）和MedGemma（开源医学模型）。

Result: MedGemma在基线评估中表现优于GPT-4o，具有更高的敏感性和AUROC。在模拟AI辅助和实际协作中，MedGemma对调整预测表现更稳定，并且在实际协作中，GPT-4o在MedGemma的引导下取得了强大结果。研究结果表明MLLMs有望改善DR筛查流程，成为研究临床AI辅助的可伸缩模拟器。

Conclusion: 研究发现MedGemma在基线评估中优于GPT-4o，在DR检测中具有更高的敏感性和AUROC。模拟AI辅助中，MedGemma在调整预测方面表现更稳定，而GPT-4o在错误输入下性能下降。在实际协作中，GPT-4o在MedGemma的引导下取得强大结果。研究表明MLLMs可能改善DR筛查流程，成为研究不同输出配置下临床AI辅助的可伸缩模拟器。MedGemma等开放、轻量级模型在资源匮乏的环境中特别有价值，而描述性输出可能增强在临床工作流中的可解释性和临床人员信任。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI
systems can expand access to fundus photography screening. Current FDA-cleared
systems primarily provide binary referral outputs, where this minimal output
may limit clinical trust and utility. Yet, determining the most effective
output format to enhance clinician-AI performance is an empirical challenge
that is difficult to assess at scale. We evaluated multimodal large language
models (MLLMs) for DR detection and their ability to simulate clinical AI
assistance across different output types. Two models were tested on IDRiD and
Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source
medical model. Experiments included: (1) baseline evaluation, (2) simulated AI
assistance with synthetic predictions, and (3) actual AI-to-AI collaboration
where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at
baseline, achieving higher sensitivity and AUROC, while GPT-4o showed
near-perfect specificity but low sensitivity. Both models adjusted predictions
based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect
ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o
achieved strong results when guided by MedGemma's descriptive outputs, even
without direct image access (AUROC up to 0.96). These findings suggest MLLMs
may improve DR screening pipelines and serve as scalable simulators for
studying clinical AI assistance across varying output configurations. Open,
lightweight models such as MedGemma may be especially valuable in low-resource
settings, while descriptive outputs could enhance explainability and clinician
trust in clinical workflows.

</details>


### [41] [A Scenario-Driven Cognitive Approach to Next-Generation AI Memory](https://arxiv.org/abs/2509.13235)
*Linyue Cai,Yuyang Cheng,Xiaoding Shao,Huiming Wang,Yong Zhao,Wei Zhang,Kang Li*

Main category: cs.AI

TL;DR: 随着人工智能向人工通用智能 (AGI) 发展，当前记忆系统存在局限性，提出了一种基于场景的方法，引入COgnitive Layered Memory Architecture (COLMA)框架以解决这些问题，为实现AGI做出贡献。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能朝着人工通用智能 (AGI) 的发展，强调了对稳健和类人记忆系统的需求，当前的记忆架构存在适应性有限、多模态集成不足以及无法支持持续学习的问题。

Method: 引入一种基于场景驱动的方法论，从典型认知场景中提取功能性需求，并提出设计下一代AI记忆系统的统一设计原则。

Result: 通过引入COgnitive Layered Memory Architecture (COLMA)框架，提供了一种结构化基础，用于开发能够进行终身学习和具有类人推理能力的AI系统。

Conclusion: 提出COgnitive Layered Memory Architecture (COLMA)框架用于解决当前AI记忆系统的局限性，为发展具有终身学习和类人推理能力的AI系统奠定结构基础。

Abstract: As artificial intelligence advances toward artificial general intelligence
(AGI), the need for robust and human-like memory systems has become
increasingly evident. Current memory architectures often suffer from limited
adaptability, insufficient multimodal integration, and an inability to support
continuous learning. To address these limitations, we propose a scenario-driven
methodology that extracts essential functional requirements from representative
cognitive scenarios, leading to a unified set of design principles for
next-generation AI memory systems. Based on this approach, we introduce the
\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that
integrates cognitive scenarios, memory processes, and storage mechanisms into a
cohesive design. COLMA provides a structured foundation for developing AI
systems capable of lifelong learning and human-like reasoning, thereby
contributing to the pragmatic development of AGI.

</details>


### [42] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: 该论文介绍了RepIt框架，用于在大型语言模型中隔离特定概念表示，实现精确干预，消除不需要的广泛影响。通过实验验证，RepIt成功抑制特定概念的拒绝，保留其他地方的拒绝，并在标准基准测试中表现稳定。研究结果显示，RepIt框架具有良好的数据效率，并能针对模型过度泛化进行有针对性的干预。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）中，激活控制是一个不断增长的研究领域，然而现有的方法往往会引发比预期更广泛的影响。这就促使了本研究的动机，即通过隔离更纯净的概念向量，实现有针对性的干预，并以更细粒度的方式理解LLM的行为。

Method: RepIt框架用于隔离特定概念的表示，实现精确干预，消除不需要的广泛影响。通过在五个前沿的大型语言模型上的实验，实现了选择性抑制对特定概念的拒绝，同时保留其他地方的拒绝，从而生成可以回答与WMD相关问题的模型，并在标准基准测试中保持良好表现。结果表明校正信号局限于100-200个神经元，可以从几个例子中提取出坚固的目标表示。该框架的高效性使得可以使用少量计算资源和数据来进行操纵，以扩展到数据稀缺的主题并规避已有的基准测试。通过RepIt框架解开拒绝向量，实现了有针对性的干预，对抗模型过度泛化。

Result: 该论文提出的RepIt框架成功实现了隔离特定概念表示，使得可以对大型语言模型进行精确干预，消除不需要的广泛影响。实验结果展示，RepIt可以在几个前沿的LLMs上取得良好表现，有效抑制特定概念的拒绝而保留其他地方的拒绝，同时确保模型在标准基准测试中得分稳定。

Conclusion: 该论文提出了一种名为RepIt的简单且数据高效的框架，用于隔离特定概念的表示，以实现精确干预，消除不需要的广泛影响。通过在五个前沿的大型语言模型上进行实验，RepIt使我们能够精确干预，选择性地抑制针对性概念的拒绝，同时保留其他地方的拒绝，从而生成可以回答与WMD相关的问题的模型，同时在标准基准测试中得分稳定。研究还表明，校正信号仅局限于100-200个神经元，并且可以从仅仅十几个示例中提取出强大的目标表示。这种高效性引发了双重关注：可以利用适度的计算资源和数据执行操纵，以扩展至数据稀缺的主题，并规避现有的基准测试。通过使用RepIt解开拒绝向量，这项工作表明了有针对性的干预可以对抗过度泛化，为更细粒度地控制模型行为奠定了基础。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


### [43] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: Shapes of cognition introduces a new paradigm for computational cognitive modeling, particularly for LEIAs. It helps agents navigate complexity, recognize patterns, and minimize cognitive load. Specificity in modeling is crucial for building explainable and trustworthy agent systems. The principles of shapes can be applied to enhance knowledge-based and hybrid AI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind adopting shapes of cognition is to create explainable, extensible, and trustworthy agent systems, especially in critical domains. By applying specific objectives, hypotheses, and modeling strategies within a particular cognitive architecture, the aim is to validate hypotheses and develop practical agent systems.

Method: Shapes-based modeling involves the use of remembered constellations of sensory, linguistic, conceptual, episodic, and procedural knowledge to enable agents to navigate real-life complexity effectively. It also includes shapes-based recovery methods for handling atypical outcomes.

Result: The application of shapes-based modeling in LEIAs demonstrates the effectiveness of this paradigm in building useful and trustworthy agent systems. The principles of shapes can be extended to enhance knowledge-based and hybrid AI systems, providing a new approach to cognitive modeling.

Conclusion: Shapes of cognition serves as a new conceptual paradigm for computational cognitive modeling, particularly for Language-Endowed Intelligent Agents (LEIAs). It enhances agents' abilities to navigate complexity, recognize patterns, and minimize cognitive load. The specificity of shapes-based modeling is crucial for achieving practical goals in building useful and trustworthy agent systems.

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>
