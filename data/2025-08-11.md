<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 本文提出了自适应探索策略优化框架（AEPO），通过引入多答案生成策略和自适应探索奖励函数，训练出模型InfiGUI-G1-3B和InfiGUI-G1-7B，在多个GUI基准测试中取得了领先的结果，相对改进高达9.0%。


<details>
  <summary>Details</summary>
Motivation: 针对多模态大型语言模型在语义对齐方面的欠缺，本文提出了自适应探索策略优化，解决了探索效率低下导致模型无法学习困难语义关联的问题。

Method: 本文采用自适应探索策略优化框架（AEPO），结合多答案生成策略和基于效率原理 eta=U/C 推导的自适应探索奖励函数（AER），训练出模型InfiGUI-G1-3B和InfiGUI-G1-7B，取得了多项挑战性GUI定位基准测试的最新最佳结果。

Result: 通过AEPO训练的模型在多个具有挑战性的GUI定位基准测试中取得了显著的相对改进，与朴素的RLVR基线相比提高了高达9.0%。

Conclusion: 通过提出自适应探索策略优化（AEPO），本文解决了多模态大型语言模型在GUI定位方面的挑战，取得了显著的结果改进，证明了AEPO模型在多个GUI基准测试中的领先地位。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [2] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖的框架，通过结合主动推理原则和大型语言模型，将安全性集成到人工通用智能（AGI）系统设计中。提出的方法利用自然语言来表示和操纵信念，实现直接人类监督，确保安全性。该框架以多代理系统实现，通过明确分离信念和偏好、有界理性以及模块化结构等机制来保证安全性。最终提出了围绕ARC基准的研究议程，为验证框架的安全性质提供实验路径，为AGI的更安全发展提供了方法。


<details>
  <summary>Details</summary>
Motivation: 传统的AI安全方法存在局限性，需要在系统设计层面实现安全性，而非事后修补；利用自然语言作为信念表示的媒介，能够实现直接人类监督；基于主动推理原则的多代理系统为安全性提供了新思路。

Method: 结合主动推理原则与大型语言模型，将安全保证集成到系统核心设计中；利用自然语言表示和操纵信念，实现人类监督，并保持计算可行性；实现多代理系统，代理根据主动推理原则自组织，偏好和安全约束通过分层马尔可夫毯流动；确保安全性的具体机制包括：明确分离信念和偏好、资源感知的自由能最小化、通过模块化代理结构实现组合安全性。

Result: 提出的框架整合了安全保证到系统设计中，在安全性方面有显著贡献；以ARC基准为中心的研究议程提供了验证框架安全性质的实验路径；为AGI的发展提供了一种更安全的方法。

Conclusion: 该论文提出了一种新颖的框架，将主动推理原则与大型语言模型（LLMs）相结合，用于开发安全的人工通用智能（AGI）。作者认为，传统的人工智能安全方法，侧重于事后可解释性和奖励工程，存在根本性局限。他们提出了一种架构，通过透明的信念表示和分层价值对齐，将安全保证集成到系统的核心设计中。该框架利用自然语言作为表示和操纵信念的媒介，实现直接人类监督同时保持计算可行性。论文实现了一个多代理系统，其中代理根据主动推理原则自组织，偏好和安全约束通过分层马尔可夫毯进行流动。作者概述了确保安全性的具体机制，包括：（1）在自然语言中明确分离信念和偏好，（2）通过资源感知的自由能最小化实现有界理性，以及（3）通过模块化代理结构实现组合安全性。论文以围绕抽象与推理语料库（ARC）基准的研究议程结束，提出实验以验证他们的框架的安全性质。他们的方法为AGI的发展提供了一条内在更安全的道路，而非事后加入安全措施。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [3] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 这篇论文探讨了人类思维和神经网络之间的关系，认为神经网络表现出类似符号系统的能力，但并未否定人类思维的符号性质。作者提出了针对人类思维符号基础的新研究议程。


<details>
  <summary>Details</summary>
Motivation: 在人类思维和人工智能的关系方面存在诸多讨论，作者希望通过研究神经网络和符号系统之间的关系，探讨人类思维是否具有符号性质，从而提出新的研究议程。

Method: 作者通过比较神经网络和人类思维在组合思想、产生新颖性和快速学习等方面的能力，来考察人类思维是否应该被看作符号系统。同时，作者指出神经网络虽然具有类似的能力，但其训练数据常由符号系统生成，这也为人类思维解决的抽象问题提供了重要性质的角度。

Result: 论文提出了人类思维符号基础的新研究议程，对神经网络和符号系统之间的关系进行了探讨，为进一步研究人类思维的符号基础提供了新的视角。

Conclusion: 这篇论文认为现代神经网络和建立在其基础上的人工智能系统表现出类似人类思维的符号系统的能力。虽然神经网络通常是在由符号系统生成的数据上训练的，但这并不能否定人类思维所使用的认知过程和表征是符号性的。作者提出了一个关于人类思维符号基础的新研究议程。

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [4] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: Holistic-XAI (H-XAI) integrates causal rating methods with traditional XAI methods to address stakeholder-specific questions at both individual decision and overall model levels. It supports interactive, multi-method explanation processes, filling critical gaps in existing XAI methods.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods primarily serve developers and focus on justifying model outputs, neglecting diverse stakeholder needs. The shift towards Evaluative AI still concentrates on operational organizations. H-XAI aims to fill critical gaps by offering explanation as an interactive process, accommodating stakeholder goals.

Method: Introduces a unified framework, Holistic-XAI (H-XAI), that integrates causal rating methods with traditional XAI methods. Demonstrates the generality of the approach through two case studies on binary credit risk classification and financial time-series forecasting.

Result: H-XAI allows stakeholders to ask questions, test hypotheses, and compare model behavior against random and biased baselines. It combines instance-level and global explanations, adapting to different stakeholder goals such as understanding individual decisions, assessing group-level bias, and evaluating robustness under perturbations.

Conclusion: Holistic-XAI (H-XAI) introduces a unified framework that combines causal rating methods with traditional XAI methods to support explanation as an interactive, multi-method process, addressing stakeholder-specific questions at both the individual decision level and the overall model level.

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [5] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: 这项调查全面分析了具身体导航的安全性，包括攻击策略、防御机制和评估方法论。通过研究现有安全挑战、缓解技术、数据集和指标，探索未来研究方向和未解决问题。旨在推动开发更安全、可靠的具身体导航系统，并对增强社会安全和提高工业效率具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型不断进步和影响力增强，具身体AI的发展加速，尤其在导航场景中引起了重大关注。尽管具身体导航已获得应用，但将其整合到关键应用程序中引发了重大安全担忧。在动态、现实世界的环境中部署这些系统，确保其安全至关重要。

Method: 综合分析现有安全挑战、缓解技术、数据集和指标，同时探讨未来研究方向和解决关键差距的方法。

Result: 提供了对具身体导航安全性的综合分析，包括攻击策略、防御机制和评估方法论。探讨了安全挑战、缓解技术、不同数据集和评估指标的有效性和稳健性，以及未解决问题和未来研究方向。调查旨在引导未来研究开发更安全、更可靠的具身体导航系统。

Conclusion: 这项调查致力于从多个角度全面分析具身体导航中的安全性，包括攻击策略、防御机制和评估方法论。通过对现有安全挑战、缓解技术以及评估有效性和稳健性的各种数据集和指标进行全面审查，探讨了未解决问题和未来研究方向，包括潜在的攻击方法、缓解策略、更可靠的评估技术以及验证框架的实施。通过解决这些关键差距，这项调查旨在提供有价值的见解，引导未来研究朝着开发更安全、更可靠的具身体导航系统的方向。此外，这项研究的发现对增强社会安全和提高工业效率具有更广泛的影响。

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [6] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 该论文提出了一种基于知识图谱的工具检索框架，通过1-hop自我工具图的集合建模工具之间的直接和间接连接关系，提供更全面和上下文的工具选择。实验结果表明，在处理多步用户请求时，该方法在工具覆盖率方面优于非知识图谱基线。


<details>
  <summary>Details</summary>
Motivation: AI代理在识别和规划复杂用户查询中选择工具时，有效的工具检索是至关重要的。然而，现有文献中对此方面的研究仍未充分探讨。传统方法主要依赖于用户查询和工具描述之间的相似性，这在处理多步用户请求时显著限制了检索准确性。

Method: 论文提出了基于知识图谱的工具检索框架，利用1-hop自我工具图的集合来建模工具之间的直接和间接连接关系，从而为多步任务提供更全面和上下文的工具选择。在合成生成的内部数据集上对方法进行了评估，扩展了对连贯对话合成和工具检索基准的先前研究。

Result: 实验结果表明，基于知识图谱的方法在微平均完整性召回度指标上达到了91.85％的工具覆盖率，相比于强大的非知识图谱基线的89.26％，证实了结构信息在知识图中提供了对纯相似性匹配的补充信号，尤其适用于需要顺序工具组合的查询。

Conclusion: 该论文提出了基于知识图谱的工具检索框架，用于捕捉工具之间的语义关系和功能依赖关系，在处理多步骤用户请求时取得了更全面和上下文的工具选择。实验结果显示，该基于工具图的方法在微平均完整性召回度指标上达到了91.85％的工具覆盖率，比我们实验中最强的非知识图谱基线重新排序语义-词汇混合检索（89.26％）要更好。

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [7] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: 本研究提出了MedOrch框架，用于医学多模态决策制定，通过LLM中介智能体促进VLM智能体之间的协作。结果表明，不同VLM智能体之间的协作可以超越单个智能体的能力，且无需进行模型训练。


<details>
  <summary>Details</summary>
Motivation: 医学决策需要合作工作流程，设计AI多智能体系统可以加快和增强人类水平的临床决策。现有的多智能体研究主要关注仅限语言任务，但将其扩展到多模态情景仍具挑战性。盲目组合各种视觉-语言模型可能会加剧错误的结果解释。VLM相对于同等大小的大型语言模型（LLMs）在指令遵循和自我反思方面能力较弱，这大大限制了VLM在合作工作流程中的能力。

Method: 提出了MedOrch，一个中介引导的多智能体协作框架，使用LLM中介智能体以促进基于VLM的专家智能体之间的协作交流。通过利用多个开源通用和领域特定的VLM，展示了异构模型的优势。验证方法在五个医学视觉问答基准测试中验证了该框架的卓越性能。

Result: 通过提出的MedOrch框架，在医学多模态决策制定中展示了多智能体协作的重要性和优势。验证了多个VLM智能体之间的协作性能可以超越单个智能体的能力，而且无需进行模型训练。

Conclusion: 本研究提出了MedOrch，这是一个为医学多模态决策制定的中介引导多智能体协作框架。使用LLM中介智能体，实现多个基于VLM的专家智能体之间的协作交流和反思。研究结果表明，不同VLM智能体之间的协作可以超越任何单个智能体的能力。验证方法在五个医学视觉问答基准测试中得到验证，展示了卓越的协作性能，无需进行模型训练。研究结果强调了中介引导的多智能体协作在推动医学多模态智能方面的价值。

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [8] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: 本文提出了HIMA框架，在StarCraftII游戏中克服了现有基于LLMs的方法难以处理的动态，长周期任务。实验结果表明HIMA在战略性，适应性和计算效率方面优于现有技术，展示了结合专门的模仿模块与元级别调度的潜力。


<details>
  <summary>Details</summary>
Motivation: LLMs在行动序列预测方面表现出色，但在动态，长周期任务上存在挑战。对于StarCraftII等游戏，现有的LLMs方法往往难以应对资源约束和不断变化的战场情况。本研究的动机在于克服这些挑战，提出一种更有效的方法以改进行动序列预测的表现。

Method: 本文提出了分层多智能体框架HIMA，其中包括专门的模仿学习智能体和元控制器策略规划者（SP）共同实现在StarCraftII游戏中的高效行动序列预测。通过专家示范，各个智能体学习独特的策略，并由SP协调形成一个单一的，适应环境的计划。实验中使用了TEXTSCII-ALL测试平台，涵盖了StarCraftII中的所有种族配对组合。

Result: 实验结果表明，HIMA在战略清晰性，适应性和计算效率方面优于现有技术。将专门的模仿模块与元级别调度相结合，可以开发更强大，通用的AI代理。

Conclusion: 本文提出了一种分层多智能体框架（HIMA），在StarCraftII等游戏中克服了现有基于LLMs的方法在动态，长周期任务上的困难。通过专家示范，每个专门的智能体学习独特的策略，并由元控制器称为策略规划者（SP）进行协调。实验结果表明，HIMA在战略清晰性，适应性和计算效率方面优于现有技术，突显了将专门的模仿模块与元级别调度相结合，开发更强大，通用的AI代理的潜力。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [9] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 该论文提出了一个双重目的框架，利用参与式预算为LLM资源分配和推理能力评估。通过不同提示策略选择项目子集并与理性最大化进行对比，测试LLM是否能从开放式输入中推断偏好。研究结果强调了提示设计的重要性，显示LLM在机制设计中处理非结构化输入方面的潜力。


<details>
  <summary>Details</summary>
Motivation: LLM在处理复杂决策任务方面表现出色，但其资源分配能力和推理能力尚未得到充分探索。由于数据污染和现有基准测试的静态特性，评估它们的推理也变得困难。因此，本研究致力于利用PB框架对LLM进行资源分配，并以此作为测试其推理能力的基准。

Method: 该论文采用了参与式预算作为LLM资源分配的场景，并将其作为评估LLM推理能力的基准。通过三种不同的提示策略让LLM选择项目子集，并将其分配结果与理性最大化的预测进行对比。此外，还测试了LLM是否能从开放式输入中推断偏好，并通过比较推断偏好和真实投票的分配结果来评估LLM的能力。

Result: 研究结果显示，LLM在参与式预算环境下的资源分配结果与理性最大化的预测具有一定差距。此外，LLM展示了从自然语言输入中提取偏好的能力，表明其在处理非结构化输入方面具有潜力。

Conclusion: 该论文介绍了一个双重目的框架，利用参与式预算（PB）作为LLM资源分配的实践场景和评估其推理能力的自适应基准。通过三种提示策略让LLM在可行性约束下选择项目子集，评估其分配结果并测试其能否从自然语言选民输入或元数据中推断结构化偏好。研究结果强调提示设计的重要性，显示LLM在处理非结构化输入的机制设计方面具有潜力。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [10] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 认知想象力在人类思维中扮演关键角色，但目前被低估。提出使用语义模型来模拟认知想象力，以提升人工智能的能力。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在人类思维中扮演关键角色，但目前被低估，这影响人工智能的发展。提出通过语义模型来模拟认知想象力，以解决这一问题。

Method: 提出语义模型作为模拟认知想象力的工具，这种模型能够学习、基于概率因果关系，并确保虚构背景的一致性，实施了透明的方法，允许上下文被操作为一体化和一致的事实系统。

Result: 提出了基于语义模型的方法来模拟认知想象力，并指出其在人工智能领域的潜在应用价值。

Conclusion: 本文呼吁更多关注认知想象力作为人工智能领域下一个有希望的突破口，认为认知想象力在人类思维中发挥着关键作用，而目前这一作用被大大低估，导致众多问题并限制了人工智能的当前能力。提出了语义模型作为模拟认知想象力的工具，这种模型是一种新的数学模型方法，能够像神经网络一样学习，基于概率因果关系，通过保证虚构背景的一致性和实施透明的方法，实现认知想象力的模拟。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [11] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出了CA-DL8.5算法，通过增加beam search算法的restart-based方法，逐渐放宽修剪标准来改善解决方案质量，实现了精确和随时的决策树学习。实证结果表明，CA-DL8.5利用LDS策略在标准分类基准上始终提供最佳的随时性能，优于其他变种及Blossom算法。


<details>
  <summary>Details</summary>
Motivation: 早期的基于MILP、CP、SAT或动态规划的精确算法在停止搜索前往往很难快速找到高质量的决策树，因此需要提出更好的方法来解决这一问题。

Method: 提出了一种新的泛化框架来实现精确和随时决策树学习，并进行严格的实证比较。算法使用DL8.5的高效分支限界修剪和基于trie的缓存，结合逐渐放宽修剪标准来改善解决方案质量的beam search算法。

Result: 实证结果表明，CA-DL8.5利用LDS策略在标准分类基准上始终提供最佳的随时性能，优于其他变种及Blossom算法。

Conclusion: 本文提出了CA-DL8.5算法，通过增加beam search算法的restart-based方法，逐渐放宽修剪标准来改善解决方案质量，实现了精确和随时的决策树学习。经过严格的实证比较，发现CA-DL8.5与其他变种及Blossom算法相比，在保持完整性和最优性保证的情况下，利用LDS(有限差异)提供了最佳的随时性能。

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [12] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: 本文提出了一种利用深度强化学习（DRL）和鸟瞰图感知相结合的自动驾驶新方法。通过Mamba-BEV模型和ME$^3$-BEV框架，优化了动态城市驾驶场景中自动驾驶系统的性能，在CARLA模拟器上进行了充分实验验证。新方法提高了解释性，有效应对传统模块化方法和端到端学习系统的挑战，为实时自动驾驶提供了有前途的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统模块化方法在自动驾驶中存在误差传播和协调问题，而端到端学习系统虽然简化设计但面临计算瓶颈。因此，本研究旨在解决这些问题，提出一种结合DRL和鸟瞰图感知的新方法，以实现实时决策的增强。通过引入Mamba-BEV模型和ME$^3$-BEV框架，旨在优化动态城市驾驶场景下自动驾驶系统的性能。

Method: 使用深度强化学习和鸟瞰图感知相结合的方法，构建了Mamba-BEV模型和ME$^3$-BEV框架。Mamba-BEV模型整合了BEV感知和Mamba框架进行时间特征建模，ME$^3$-BEV框架利用Mamba-BEV模型进行端到端DRL。通过语义分割可视化高维特征，提高了模型的解释性。在CARLA模拟器上进行了实验验证新方法的性能。

Result: 通过实验验证，ME$^3$-BEV在碰撞率和轨迹准确性等多个指标上优于现有模型，证明了新方法的有效性和实用性。

Conclusion: 提出了一种利用深度强化学习（DRL）和鸟瞰图感知相结合的自动驾驶新方法。引入了Mamba-BEV模型，结合了基于鸟瞰图的感知和Mamba框架进行时间特征建模，使系统能够在统一坐标系中编码车辆周围环境和道路特征，准确建模长程依赖性。提出了ME$^3$-BEV框架，利用Mamba-BEV模型作为端到端DRL的特征输入，在动态城市驾驶场景中表现优异。通过语义分割来可视化高维特征，增强了模型的解释性。在CARLA模拟器上进行了大量实验，证明ME$^3$-BEV在碰撞率和轨迹准确性等多个指标上优于现有模型，为实时自动驾驶提供了有前途的解决方案。

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [13] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: 本文解决了一个开放性问题，证明了聚合-组合-读取图神经网络的逻辑表达能力严格超过C2逻辑。该结果适用于无向图和有向图。研究结果对图神经网络具有重要意义，同时在无穷逻辑的表达能力方面提供了纯粹的逻辑见解。


<details>
  <summary>Details</summary>
Motivation: 初始研究由Barcelo等人引入，表明分级模态逻辑(或C2逻辑的一种受限片段)描述了聚合-组合图神经网络的逻辑表达能力。之前留下了一个开放问题，即完整的C2逻辑是否描述了聚合-组合-读取图神经网络的逻辑表达能力。尽管已经有多次尝试，但这个问题仍未解决。

Method: 通过证明聚合-组合-读取图神经网络的逻...逻辑表达能力超过C2逻辑

Result: 聚合-组合-读取图神经网络的逻辑表达能力严格超过C2逻辑，适用于无向图和有向图。研究结果对图神经网络具有重要意义，同时在无穷逻辑的表达能力方面提供了纯粹的逻辑见解。

Conclusion: 本文解决了一个开放性问题，证明了聚合-组合-读取图神经网络的逻辑表达能力严格超过C2逻辑。该结果适用于无向图和有向图。除了对图神经网络的影响，本研究还在表达力方面对无穷逻辑提供了纯粹的逻辑见解。

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [14] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: 为了解决表格推理中对标记数据或数据增强的依赖以及LLMs性能不佳的问题，引入了PanelTR框架。实验证明，PanelTR在四个基准测试中表现优异，展示了在零-shot环境中具有灵活语义理解能力的潜力。


<details>
  <summary>Details</summary>
Motivation: 表格推理通常依赖于标记数据或复杂数据增强，限制了灵活性和泛化能力；LLMs通常表现不如简单监督模型。为了解决这些问题，引入了PanelTR框架。

Method: 介绍了PanelTR框架，利用LLM代理科学家进行鲁棒的表格推理。PanelTR的工作流程涉及代理科学家进行个体调查、自我审查以及参与协作同行审查讨论。

Result: 实验表明，PanelTR在四个基准测试中优于普通LLMs甚至可以与完全监督模型相媲美，同时不依赖于训练数据。

Conclusion: 结构化科学方法可以有效处理复杂任务，在零-shot环境中具有灵活的语义理解能力。

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [15] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: 研究引入了SKATE评估框架，通过让LLMs相互竞争生成和解决可验证任务来评估模型能力。SKATE是一种全自动、无需人类输入或领域专业知识的评估方法，可客观评估不同模型之间的能力差异，推动通用、可扩展的评估框架的发展。


<details>
  <summary>Details</summary>
Motivation: 当前的评估方法需要丰富的领域专业知识，限制了其可扩展性。作者引入SKATE框架的动机在于解决这一问题，提出一种更具可扩展性、开放性和客观性的评估方法。他们希望通过SKATE框架，能够实现自动化、无需人类输入或领域专业知识的评估，以推动LLM进步并与其同步发展评估框架。

Method: SKATE是一种评估框架，其中LLMs通过生成和解决可验证任务相互竞争。这种方法将评估视为一种游戏，模型既是任务设置者又是解决者，激励它们创建突出自身优势并暴露其他模型弱点的问题。通过使用可验证任务而不是LLM评判者，评分更加客观。利用TrueSkill排名系统，作者评估了六个前沿LLMs，并发现弱模型可以可靠区分并评分强模型，LLM系统能够产生与其自身能力相符的问题，SKATE能够自动显示模型之间的微观能力差异。

Result: 通过SKATE框架进行了LLMs的能力评估，发现弱模型能够可靠区分和评分强模型，LLM系统能够产生自我偏好行为，SKATE自动展示了模型之间微观能力差异。研究结果表明SKATE是一种可行的评估框架，有助于推动通用、可扩展的评估框架的发展。

Conclusion: SKATE是一种新颖的评估框架，利用大语言模型（LLMs）相互竞争生成和解决可验证的任务，具有自动化、数据无关和可扩展性等优势。通过SKATE，可以客观评估不同模型之间的能力差异。研究发现SKATE能够帮助识别模型之间的微观能力差异，推动了通用、可扩展的评估框架的发展。

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [16] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: 本研究通过利用机器学习方法对解决车辆路径问题（VRP）的质量进行预测，扩展了先前研究。结果表明，特征重要性有所变化，但某些特征始终表现为强预测因子。提出了一种统一框架，可以在不同场景下对特征影响进行排名。这些发现突显了特征重要性分析作为为解决VRP而开发元启发算法的指导机制的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于车辆路径问题（VRP）的复杂性以及其NP-难性质，传统上解决VRP的元启发算法依赖于经验研究开发的人工设计。但最近的研究表明，机器学习方法可用于组合优化问题中解决方案的结构特征，从而有助于设计更高效的算法，特别是用于解决VRP。本研究基于这一进展，通过使用多个能够预测VRP解决方案质量的分类器模型进行敏感性分析。

Method: 本研究通过使用能够预测VRP解决方案质量的多个分类器模型进行敏感性分析，扩展了先前研究。同时，利用可解释的人工智能，研究突显了这些模型如何做出决策的理解。

Result: 研究结果表明，特征重要性有所变化，但某些特征始终表现为强预测因子。此外，研究提出了一种统一框架，可以在不同场景下对特征影响进行排名，以阐明这一发现。

Conclusion: 本研究通过利用机器学习方法对解决车辆路径问题（VRP）的质量进行预测，以扩展先前研究。研究结果表明，尽管特征重要性有所变化，但某些特征始终表现为强预测因子。此外，我们提出了一种统一框架，可以在不同场景下对特征影响进行排名，以阐明这一发现。这些发现突显了特征重要性分析作为为解决VRP而开发元启发算法的指导机制的潜力。

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [17] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 研究通过RAG管道提高了LLMs在药物相互作用方面的能力，显著提高了模型的准确度，对年龄组、妊娠和联合用药禁忌的准确率有明显改善，从而提供更精确可靠的药物禁忌信息。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健领域，特别是在制药方面，确保准确可靠的信息至关重要。研究旨在应用LLMs解决禁忌反应问题，在药物摄入决策中提供更精准的信息。

Method: 研究采用了Retrieval Augmented Generation (RAG) pipeline来增强LLMs的能力，利用了OpenAI的GPT-4o-mini作为基础模型，以及text-embedding-3-small模型用于嵌入，结合Langchain来协调混合检索系统的重排序。利用来自公共数据库的Drug Utilization Review (DUR)数据，关注特定年龄组、妊娠和联合用药的禁忌症。

Result: 模型准确率得到显著提高，对年龄组、妊娠和联合用药禁忌的准确率分别达到0.94、0.87和0.89。

Conclusion: 研究通过实施RAG管道增强了LLM解决相互作用方面的能力，提高了模型的准确度和可靠性。结果表明，采用RAG框架可以显著降低处方和药物摄入决策中的不确定性，提供更精确可靠的药物禁忌信息。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [18] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: 本文提出了一种LLM作为评委系统的新方法，从精度至自信度驱动，强调自信度的重要性。通过引入TH-Score指标和LLM作为融合器框架，作者成功提高了校准性和评估的可靠性，在实验中取得了优异的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注精度，忽视了良好校准自信度的必要性，这对于自适应和可靠的评估流程至关重要。因此，作者提出了从精度至自信度驱动、风险感知的LLM作为评委系统的转变，强调确保可信赖和自适应评估的必要性。

Method: 通过识别当前LLM作为评委系统中的自信过度现象，作者引入TH-Score指标来衡量自信度与准确度的对齐度。此外，提出LLM作为融合器的框架，将LLM转化为可靠的、风险感知的评估者。

Result: 作者提出的方法在实验中显著改善了校准性并实现了优于现有基线的可靠性和准确性。

Conclusion: 本文提出了一种从精度至自信度驱动的风险感知LLM作为评委系统，强调对于可信赖和自适应评估，确保良好校准的自信度的必要性。通过识别当前LLM作为评委中的自信过度现象，作者引入了TH-Score指标来量化这一现象。此外，作者提出了LLM作为融合器的框架，将LLM转化为可靠的风险感知评估者。实验结果表明，这种方法显著改善了校准性，实现了优于现有基线的可靠性和准确性。

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [19] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: 本文提出了新的GeoLaux基准测试，用于评估MLLMs的几何推理能力，涵盖2186个几何问题，要求平均6.51个推理步骤，41.8%需要辅助线施工。作者设计了五维评估策略，发现模型在长步骤推理中性能下降，倾向于采取捷径解决证明问题，缺乏辅助线意识。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估MLLMs几何技能的基准测试忽视了辅助线施工，缺乏细致的过程评估，无法充分评估MLLMs的长步推理能力。本研究旨在填补这些空白，提出了GeoLaux基准测试，并设计了新的评估策略，以评估MLLMs在几何推理中的表现。

Method: 作者构建了GeoLaux基准测试，设计了五维评估策略，评估MLLMs的几何推理能力。作者进行了广泛实验，测试了13个主要MLLMs的性能表现，并发现了模型在长步骤推理中性能下降、解决证明问题时倾向于采取捷径，以及缺乏辅助线意识这三个关键发现。

Result: 作者提出了GeoLaux基准测试，包含2186个几何问题，实验结果表明模型在长步骤推理中性能下降，解决证明问题时倾向于采取捷径，缺乏辅助线意识。作者的研究为评估MLLMs在几何推理中的能力提供了新方法和指导。

Conclusion: 本文提出了GeoLaux基准测试，包括2186个几何问题，涵盖计算和证明问题，要求平均6.51个推理步骤，最多24个步骤，41.8%的问题需要辅助线施工。作者设计了一个新的五维评估策略，评估答案正确性、过程正确性、过程质量、辅助线影响和错误原因。实验证明在长步骤推理中，模型表现出明显的性能下降；相比计算问题，MLLMs在解决证明问题时倾向于采取捷径；模型缺乏辅助线意识，提升这一能力对整体几何推理改进特别有益。这些发现确立了GeoLaux作为评估MLLMs辅助线长步几何推理的基准测试，并作为能力提升指南。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [20] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 提出了一种贝叶斯归纳逻辑编程方法，从嘈杂数据中学习最小消息长度程序。在多个领域的实验中，该方法明显优于先前的方法，并表现出数据利用高效和对示例平衡的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 统一概率和逻辑学习是人工智能中的关键挑战。

Method: 平衡假设复杂性和数据拟合，通过显式支持更一般的程序的先验和支持准确程序的似然。

Result: 该方法表现出对数据的高效利用和对示例平衡的鲁棒性，能够从纯正例中学习。

Conclusion: 提出了一种贝叶斯归纳逻辑编程方法，从嘈杂数据中学习最小消息长度程序。实验结果表明，该方法在游戏玩法和药物设计等多个领域明显优于先前的方法。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [21] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 归纳逻辑编程旨在搜索能泛化训练数据和背景知识的假设，此研究通过引入一种打破假设空间对称性的方法，在多个领域进行实验，结果显示解决时间可从一个多小时减少到仅17秒。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程的目标是搜索一个能泛化训练数据和背景知识的假设，挑战在于搜索庞大的假设空间，因为存在许多逻辑等价的假设。

Method: 在归纳逻辑编程中引入一种方法来打破假设空间中的对称性。

Result: 实验证明该方法可以显著减少解决时间。

Conclusion: 引入一种方法来打破假设空间中的对称性，实验结果表明该方法将解决时间从一个多小时减少到仅17秒。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [22] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: 该技术报告介绍了PRISM Eval行为引诱工具（BET）和动态对抗优化技术，成功攻击了41种LLM模型中的37种，获得了100%的攻击成功率。提出了细粒度的健壮性度量和基本级别漏洞性分析方法，显示了攻击难度的巨大差异。合作评估展示了分布式健壮性评估的可行性。


<details>
  <summary>Details</summary>
Motivation: 针对LLM模型的攻击，提出了自动红队行动技术，并通过引入细粒度的健壮性度量和基本级别漏洞性分析，揭示了不同模型之间攻击难度的巨大差异。通过合作评估与第三方合作，展示了分布式健壮性评估在社区中的实际应用价值。

Method: 引入了PRISM Eval行为引诱工具（BET），采用动态对抗优化技术进行自动红队行动，对41种LLM模型中的37种进行攻击，实现了100%的攻击成功率。提出了细粒度的健壮性度量方法，估计了引发有害行为所需的平均尝试次数。引入了基本级别漏洞性分析以识别对特定危害类别最有效的越狱技术。与AI安全网络的可信第三方进行了合作评估，展示了对整个社区采用分布式健壮性评估的实际途径。

Result: 成功引入了自动红队行动技术和健壮性度量方法，实现了对LLM模型的高攻击成功率和漏洞分析。合作评估结果显示了分布式健壮性评估在社区中的实际可行性。

Conclusion: 该技术报告介绍了PRISM Eval行为引诱工具（BET），通过动态对抗优化实现了对41种先进LLM模型中37种的100%攻击成功率。除了二进制成功度量之外，提出了一个细粒度的健壮性度量，估计了引发有害行为所需的平均尝试次数，揭示了尽管存在普遍漏洞，攻击难度在各个模型之间相差超过300倍。还引入了基本级别的漏洞性分析，以确定哪些越狱技术对特定危害类别最有效。通过与AI安全网络的可信第三方的合作评估，展示了在整个社区中通过分布式健壮性评估的实际途径。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [23] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: 康纳特和阿什比的理论认为系统的良好调节器必须是该系统的模型，但我们在本文中挑战了这一观点。我们提出了一个新的观点，认为当代理执行调节任务时，观察者可以将其行为解释为有关环境的"信念"更新，从而提出了一个更普遍适用的模型概念和定理。我们的定理适用于系统调节环境或调节自身状态的情况，无论模型是否微不足道，有助于解决先前认为的明显反例。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是挑战康纳特和阿什比的经典理论，即系统的良好调节器必须是该系统的模型。通过研究人工生命中的多个系统，发现这一理论并不容易推广到更广泛的情况。我们试图探索一个不同的视角，认为当一个代理执行调节任务时，观察者可以将其行为解释为有关环境的"信念"更新，从而提出了一个更普遍适用的模型概念和定理。

Method: 我们展示了一个类似的直觉，无论系统中是否存在明显的模型，观察者可以将代理的行为解释为对环境有信念的更新。这个理论自然而然地将模型的概念更加复杂化，提出了一个更具普适性的定理。然而，这需要观察者视角的改变，观察者在该理论中发挥了至关重要的作用。

Result: 本文的研究表明，我们提出的新观点可以更好地解释代理的行为，并提出了一个比康纳特和阿什比更普遍适用的模型概念和定理。我们的定理适用于系统调节环境或调节自身状态的情况，无论模型是否微不足道。这有助于解决先前认为的明显反例。

Conclusion: 通过研究我们发现，每当一个代理能够执行调节任务时，观察者可以将其解释为对环境有"信念"，并根据感官输入进行"更新"。这种信念更新的概念提供了比康纳特和阿什比更复杂的模型概念，并提出了一个更广泛适用的定理。然而，这需要一个改变的观点，观察者在理论中扮演了一个重要角色：模型不仅仅是系统的属性，而是从外部强加在其上。我们的定理不论系统是在经典控制理论设置中调节其环境，还是在调节自身内部状态；模型都是针对其环境的。然而，模型可能是微不足道的，这是如何解决明显的反例的。

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [24] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: 该论文介绍了一种基于Transformer的机器学习模型AntiCheatPT_256，用于检测《反恐精英2》中的作弊行为。通过公开数据集CS2CD训练，模型取得了很高的准确度和AUC，为未来数据驱动的作弊检测研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 在线视频游戏中的作弊行为损害了游戏体验的完整性。传统反作弊系统面临着难以跟上不断变化的作弊方法的挑战，而且不能对用户系统实施侵入性措施。因此，本研究的动机是提出一种机器学习模型来检测作弊行为，以改善作弊检测的效果和应用性。

Method: 基于Transformer的机器学习模型AntiCheatPT_256用于检测作弊行为，训练了这个模型并在标记数据集CS2CD上进行了测试。针对类别不平衡问题，创建并增强了上下文窗口。最终实现了89.17%的准确度和93.36%的AUC。

Result: 在标记数据集CS2CD上训练的Transformer模型AntiCheatPT_256取得了89.17%的准确度和93.36%的AUC。这显示了该模型在作弊检测方面的有效性和性能。

Conclusion: 该论文介绍了一种基于Transformer的机器学习模型AntiCheatPT_256，用于检测《反恐精英2》中的作弊行为。通过介绍和公开发布了795场比赛的标记数据集CS2CD，创建了90,707个上下文窗口并对其进行增强，以解决类别不平衡问题。在这些窗口上训练的Transformer模型，在未增强的测试集上实现了89.17%的准确度和93.36%的AUC。该方法强调了可重现性和现实世界的适用性，为数据驱动的作弊检测提供了坚实的基准，为未来研究奠定了基础。

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [25] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 本论文提出了“解释性人工智能”作为一种与可解释人工智能不同的方法，以生成人工智能为解释伙伴，支持人类理解并提供上下文推理。研究开发了一个八维概念模型，通过实证验证表明用户更偏好上下文敏感、多模式的解释形式。结果表明AI系统需为人类理解而设计，而非仅注重算法透明性。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释人工智能(XAI)方法更注重算法透明性，并以抽象、非自适应的形式呈现解释，经常无法支持有意义的最终用户理解。因此，本论文引入了“解释性人工智能”作为一种补充范式，利用生成人工智能能力作为解释伙伴，从而更好地支持人类理解。

Method: 该论文使用了快速环境设计方法与医疗保健专业人员进行实证验证。通过开发了一个八维概念模型，区分了解释性人工智能的特点，其中包括叙事沟通、自适应个性化和逐步披露原则。实验结果表明用户更偏好上下文敏感、多模式的解释形式。

Result: 通过快速环境设计方法，研究证明用户更偏好上下文敏感、多模式的解释形式，而非技术透明性。这表明AI系统应该更注重为人类理解而设计，而非仅仅注重算法的透明性。

Conclusion: 该论文介绍了“解释性人工智能”作为一种补充范式，利用生成人工智能能力作为人类理解的解释合作伙伴，而非算法透明性的提供者。它强调在社会技术背景下支持人类决策的背景推理，而不仅是揭示算法决策过程以进行模型验证。研究提出了一个定义性的八维概念模型，通过叙事沟通、自适应个性化和逐步披露原则来区分解释性人工智能。通过与医疗保健专业人员进行的快速环境设计方法的实证验证，证明用户一贯偏好上下文敏感、多模式的解释而非技术透明性。研究结果表明，AI系统需设计成更易为人类理解而非算法内省，建立了一个全面的研究议程，推进以用户为中心的AI解释方法在各个领域和文化背景中的发展。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [26] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: Legal Knowledge Graphs were developed to improve access to legal information and support predictive justice in legal decision-making. Two approaches were used for construction, focusing on cases of violence against women. The KGs showed potential to enhance accessibility, enable complex queries, and benefit machine learning applications in legal decision processes.


<details>
  <summary>Details</summary>
Motivation: The lack of Legal Knowledge Graphs in the legal domain motivated the development of a legal KG focusing on cases of violence against women. The goal was to facilitate access to legal information, support advanced reasoning and machine learning applications, and enhance the decision process of legal experts.

Method: The paper developed a legal KG targeting legal cases of violence against women using two approaches: a systematic bottom-up approach customized for the legal domain and a new solution leveraging Large Language Models. These approaches integrated structured data extraction, ontology development, and semantic enrichment to produce tailored KGs.

Result: After comparing the two approaches for automated legal KG construction, the developed KGs were validated through competency questions. The KGs demonstrated the potential to improve accessibility to legal information for both humans and machines, enable complex queries, and serve as a valuable knowledge component for predictive justice tools.

Conclusion: Legal Knowledge Graphs (KGs) can improve accessibility to legal information, enable complex queries, and act as a valuable component for predictive justice in legal decision-making processes.

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [27] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 本文介绍了一个新型的动态机制“Fair Game”，旨在实现机器学习算法预测结果的公平性，并随着社会互动不断调整公平性目标。该框架结合审计员和去偏差算法，利用强化学习为公平性目标提供动态调整，构建了灵活且适应时间变化的公平机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 现有的公平机器学习方法受限于静态特性，提出的“Fair Game”机制旨在弥补当前公平机器学习在动态社会环境中的不足。通过模拟社会伦理和法律框架的演变，构建了灵活适应时间变化的公平机器学习系统的新框架。

Method: 结合审计员和去偏差算法，采用强化学习算法与环境交互以实现公平性目标的动态调整。通过修改审计员和不同偏差量化方式，使公平性目标能够随时间适应。

Result: 提出的“Fair Game”机制为构建公平机器学习系统提供了灵活且适应性强的方法，能够动态调整公平性目标，并通过审计员和去偏差算法的循环实现预测的公平性。

Conclusion: 提出了一种名为“Fair Game”的新颖动态机制，旨在确保机器学习算法的预测结果公平，并随着社会与算法的交互而不断调整预测。利用强化学习将审计员和去偏差算法置于一个循环中，使公平性目标可以随时间调整。该框架模拟了社会伦理和法律框架的演变，为构建机器学习系统提供了灵活且适应性强的方法。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [28] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: 提出了数据驱动框架，评估多赢家投票规则在不同偏好分布下的准则违反频率，展示神经网络在最小化准则违反方面的潜力，支持数据驱动研究在社会选择中的应用。


<details>
  <summary>Details</summary>
Motivation: 社会选择研究社区对于识别不同多赢家投票规则满足的属性表现出越来越大的兴趣，希望转变二元视角，从最坏情况分析给出的准则满足性出发。

Method: 提议一个数据驱动框架，评估不同多赢家投票规则在实践中如何频繁违反公理，分析多赢家投票规则与其在几种偏好分布下的准则性能之间的关系。展示神经网络作为投票规则能够在最小化准则违反方面胜过传统规则。

Result: 通过数据驱动框架分析多赢家投票规则在不同偏好分布下的准则违反情况，证明神经网络在最小化准则违反方面优于传统规则。

Conclusion: 数据驱动的方法可以帮助评估多赢家选举规则在不同偏好分布下违反准则的频率，并表明神经网络作为投票规则在最小化准则违反方面优于传统规则。研究结果表明，数据驱动的社会选择方法可以指导设计新的选举系统，并支持社会选择中数据驱动研究的持续发展。

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>
