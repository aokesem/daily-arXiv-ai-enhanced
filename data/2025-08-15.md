<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 这项研究涵盖了优化建模领域的最新进展，介绍了利用大型语言模型自动化数学建模的机会。研究对基准数据集的质量进行了分析，并发现了高误差率，清洗数据集并建立了新的评估排行榜。此外，通过在线门户整合资源，为社区提供帮助，并指出了当前方法的局限性和未来研究机会。


<details>
  <summary>Details</summary>
Motivation: 由于优化建模在解决实际问题中的广泛应用，但需要运筹学专业人员的大量专业知识，利用大型语言模型（LLMs）自动化数学建模过程开辟了新机会。

Method: 在整个技术栈上进行了综合性的审查，包括数据合成和精调、推理框架、基准数据集和性能评估。对基准数据集的质量进行了深入分析，并发现了惊人的高误差率。

Result: 清洗了基准数据集并构建了新的排行榜，建立了在线门户整合资源，发现了当前方法的局限性并提出未来研究机会。

Conclusion: 该研究综述了最近在优化建模领域的新进展，介绍了利用大型语言模型（LLMs）自动化数学建模过程的机会。研究清洗了基准数据集，并构建了一个公平评估性能的新排行榜。通过在线门户整合了清洗数据集资源、代码和论文库，造福社区。最后，识别了当前方法的局限性并勾勒了未来的研究机会。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [2] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: Amazon Nova AI Challenge focused on driving advances in secure AI for software development through a global competition among university teams. Teams developed automated red teaming bots and safe AI assistants, introducing novel techniques in safety alignment methods. The challenge led to significant advancements in AI safety, highlighting a collaborative effort to raise the bar for AI safety in software development.


<details>
  <summary>Details</summary>
Motivation: To address the safety challenges of AI systems for software development and ensure their secure implementation. The goal was to drive advances in secure AI by exploring novel approaches in safety alignment methods through a global competition among university teams.

Method: The Amazon Nova AI Challenge involved 10 university teams focusing on developing automated red teaming bots and safe AI assistants. Teams participated in head-to-head adversarial tournaments to test safety alignment methods. High-quality annotated data was provided to fuel iterative improvement. The challenge also included building a custom baseline coding specialist model, developing a tournament orchestration service, and creating an evaluation harness to support teams' efforts.

Result: University teams and the Amazon Nova AI Challenge team made significant advancements in addressing the safety challenges of AI for software development. The challenge led to the introduction of state-of-the-art techniques in safety alignment, model guardrails, jail-breaking, and probing of large language models. The collaborative effort aimed to raise the bar for AI safety in software development.

Conclusion: Amazon launched the Trusted AI track of the Amazon Nova AI Challenge, a global competition among university teams to drive advances in secure AI for software development. The challenge involved developing automated red teaming bots and safe AI assistants, providing a platform to evaluate safety alignment methods through adversarial tournaments. Teams introduced novel techniques in reasoning-based safety alignment, model guardrails, jail-breaking, and probing of large language models. The paper highlights the collaborative effort to address the safety challenges of AI in software development.

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [3] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 该论文介绍了一种利用关系抽取技术检测新闻文章中虚假信息的多Agent系统，包括机器学习代理、维基百科知识检查代理、连贯性检测代理和网络抓取数据分析代理。研究结果表明，该系统实现了95.3%的准确度和F1分数为0.964，显著优于传统方法和单个代理。系统通过加权聚合方法提升性能，且具有易扩展的模块化架构。


<details>
  <summary>Details</summary>
Motivation: 数字平台上虚假信息的大量传播对信息完整性造成了重大挑战。本论文的动机在于利用多Agent系统检测新闻文章中的虚假信息，针对标题和短文本片段进行检测。

Method: 论文提出了一种Agentic AI系统，利用关系抽取技术检测新闻文章中的虚假信息，结合了机器学习代理（逻辑回归）、维基百科知识检查代理（依赖命名实体识别）、连贯性检测代理（使用LLM提示工程）和网络抓取数据分析代理提出的四个Agent。系统通过模型上下文协议（MCP）协调各组件，实现共享上下文和动态学习。

Result: 研究结果显示，该多Agent合奏系统在F1分数为0.964的情况下实现了95.3%的准确度，明显优于单个代理和传统方法。通过加权聚合方法，系统实现了优越性能，且模块化架构使其易于扩展和保持决策过程的详细信息。

Conclusion: 该论文介绍了一种多Agent系统，利用关系抽取技术在新闻文章中检测虚假信息，在标题和短文本片段中实现了高准确度。通过整合四个Agent（机器学习代理、维基百科知识检查代理、连贯性检测代理和网络抓取数据分析代理），系统利用模型上下文协议（MCP）协调各组件，实现共享上下文和动态学习。结果表明，多Agent合奏系统实现了95.3%的准确度，F1分数达到0.964，显著优于单个Agent和传统方法。加权聚合方法优于算法阈值优化，模块化架构使系统易于扩展，同时保持决策过程的详细信息。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [4] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 该论文对领先的主体化人工智能框架进行了系统性审查和比较分析，评估了它们的架构原理、通信机制等各方面，并对代理通信协议进行了深入分析。研究结果建立了这些框架的基础分类，发现了主要限制、新兴趋势和未解挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型引领的主体化人工智能范式的兴起，意图建立对Agentic AI系统的基础分类，同时为未来研究方向提供指导。

Method: 提供了系统性审查和比较分析，评估领先Agentic AI框架的各方面，还进行了对代理通信协议的深入分析。

Result: 建立了领先Agentic AI框架的基础分类，揭示了该领域的主要限制、新兴趋势和未解挑战，同时提出了未来研究方向。

Conclusion: 该论文提供了对领先的“主体化人工智能”框架进行系统性审查和比较分析，评估它们的架构原理、通信机制、内存管理、安全防护措施以及与面向服务计算范式的一致性。同时识别了该领域的主要限制、新兴趋势和未解挑战。研究对象涉及的框架包括CrewAI、LangGraph、AutoGen、Semantic Kernel、Agno、Google ADK和MetaGPT。此外，还对解决代理通信问题进行了深入分析，并提出了未来研究方向以增强可扩展性、鲁棒性和互操作性。该研究为致力于推进下一代自主人工智能系统的研究人员和从业者提供了全面的参考。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [5] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 本研究比较了ODR和两个专有系统在BC-Small基准上的表现，发现三个系统在60个问题的测试集上均未获得准确性。通过引入三项战略改进，提出了ODR+模型，其取得了10%的成功率，是领先的系统。消融研究证实了这三项改进对ODR+的成功影响重大。


<details>
  <summary>Details</summary>
Motivation: 随着深度研究代理（DRAs）在处理用户自然语言提示方面取得显著进展，但目前的研究主要涉及专有闭源系统。鉴于目前仅有一个开源的DRA系统（ODR），本研究旨在比较ODR和现有专有系统在BrowseComp-Small基准上的表现，以推动学术界在这一领域的研究。

Method: 本研究通过在BrowseComp-Small基准上进行实验比较ODR和两个专有系统的性能，提出了ODR+模型，并针对改进效果进行了消融研究。

Result: 本研究发现在BC-Small测试集上，ODR、Anthropic和Google这三个系统在60个问题上的准确性均为0%。通过引入三项战略改进，ODR+模型在BC-Small基准上取得了10%的成功率，是当前闭源和开源系统中的领先者。消融研究表明这三项改进对ODR+的成功至关重要。

Conclusion: 本研究通过在BrowseComp-Small基准上对比ODR和两个专有系统的性能，发现三个系统在60个问题的测试集上均未获得准确性。我们提出了三项战略改进措施，并将其应用到ODR模型中，得到了ODR+模型，其在学术实验室中达到了10%的成功率，在闭源和开源系统中均处于领先地位。我们的消融研究表明，这三项改进均对ODR+的成功做出了贡献。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [6] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 本研究提出了一种名为Length Controlled Preference Optimization (LCPO)的方法，通过分析生成路径分布和过滤通过难度估计筛选生成轨迹，减少LRM的生成长度。LCPO方法通过平衡NLL损失相关的隐式奖励，有效学习长度偏好，实验证明可使LRM的平均输出长度减少超过50%。


<details>
  <summary>Details</summary>
Motivation: LRM的长输出增加了计算成本，可能导致过度思考，因此需要研究有效方法以减少LRM的生成长度，当前的高效推理方法通常要么降低推理质量，要么需要大量资源，存在平衡推理效果和效率的挑战。

Method: 通过分析生成路径分布并通过难度估计筛选生成的轨迹，分析不同偏好优化方法在基于Bradley-Terry损失框架下的目标收敛行为，提出Length Controlled Preference Optimization (LCPO)方法，直接平衡与NLL损失相关的隐式奖励，从而降低LRMs的生成长度。

Result: 在多个基准测试中，实验证明LCPO显著减少LRM的平均输出长度，同时保持推理性能。

Conclusion: 研究发现Length Controlled Preference Optimization (LCPO) 可以在有限的数据和训练条件下有效学习长度偏好，显著减少LRM的平均输出长度超过50%并保持推理性能。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [7] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI is a novel AutoML framework that enhances solution space exploration, integrates Retrieval-Augmented Generation, and improves pipeline evaluation speed. It outperforms existing methods by 3% on the primary AutoML benchmark and introduces a new benchmark, Kompete-bench, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing Large Language Model (LLM)-based AutoML systems, including constrained exploration strategies and severe execution bottlenecks. The goal is to enhance exploration diversity, improve recombination of partial solutions, and accelerate pipeline evaluation.

Method: Introducing KompeteAI, a novel AutoML framework with dynamic solution space exploration, a merging stage for composing top candidates, integration of Retrieval-Augmented Generation (RAG), and addressing the execution bottleneck with a predictive scoring model and accelerated debugging method.

Result: KompeteAI accelerates pipeline evaluation 6.9 times and outperforms leading methods by an average of 3% on the primary AutoML benchmark, MLE-Bench. Additionally, a new benchmark, Kompete-bench, is proposed to address limitations in MLE-Bench, where KompeteAI achieves state-of-the-art results.

Conclusion: KompeteAI is introduced as a novel AutoML framework with dynamic solution space exploration, integrating Retrieval-Augmented Generation (RAG) and addressing the execution bottleneck through a predictive scoring model and accelerated debugging method, outperforming leading methods by an average of 3% on the primary AutoML benchmark.

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [8] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 该论文介绍了将事件的熵势概念应用于人工智能中，为不确定性量化、决策制定和可解释性提供了新的途径。熵势框架在强化学习、贝叶斯推断、异常检测等方面展示了其潜力，为处理智能系统中的不确定性提供了理论上基础、可解释和多功能的方法。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于利用熵势概念来加强人工智能中的不确定性量化、决策制定和可解释性，通过引入事件为中心的度量标准，使其在人工智能领域中得以适用和发展。

Method: 该论文通过对事件的熵势概念进行适应，引入了以事件为中心的度量标准，形式化了熵势的原始定义和人工智能中的调整定义，强调条件期望以考虑反事实情景。论文探讨了在复杂人工智能模型中进行计算的实际考虑，涵盖强化学习、贝叶斯推断和异常检测等方面的概念示例，展示了熵势框架在管理人工智能中的不确定性方面的解释性和实用性。

Result: 该论文提出的熵势框架在智能系统中管理不确定性方面具有理论上的基础、可解释和多功能的优点，可以统一并加强智能系统中的不确定性建模。

Conclusion: 该论文展示了如何利用事件的熵势概念来增强人工智能中的不确定性量化、决策制定和可解释性，通过引入一种以事件为中心的度量标准来适应人工智能领域，形式化了熵势的原始定义和在人工智能中的调整定义，应用于政策评估、内在奖励设计、可解释人工智能和异常检测等方面，突出了该度量标准在智能系统不确定性建模中的潜力，提供了一种理论上基础、可解释和多功能的管理人工智能中不确定性的方法。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [9] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: The paper argues that the understanding and reasoning abilities attributed to LLMs are illusory and based on vague concepts, stating that these models cannot attain true understanding and reasoning due to essential limitations in their working principles.


<details>
  <summary>Details</summary>
Motivation: To clarify that the so-called "understanding ability" and "reasoning ability" of LLMs are illusions based on vague concepts.

Method: Not specified.

Result: The paper explains that LLMs cannot possess the ability of true correct reasoning.

Conclusion: LLMs cannot have true understanding and reasoning ability due to their essential limitations in working principles.

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [10] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 本研究提出了一种基于规则的可验证逐步奖励机制（VSRM），用于解决大推理模型（LRMs）中的过度思考问题。VSRM通过分配基于推理轨迹中中间状态表现的奖励，实现减少输出长度并保持推理性能的最佳平衡。实验证明，VSRM结合PPO和Reinforce++取得了显著结果，有效缓解了LRMs的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: LRMs在复杂推理任务中取得了重大进展，但经常面临过度思考的问题，浪费过多计算资源。现有的高效推理方法通常需要准确的任务评估来预设代币预算或选择推理模式，这限制了它们的灵活性和可靠性。因此，重新思考过度思考问题的本质对于解决LRMs的效率问题至关重要。

Method: 本研究重新审视了过度思考的本质，提出鼓励有效步骤并惩罚无效步骤的方案。通过引入VSRM机制，根据推理轨迹中中间状态的表现分配奖励，以实现该目标。研究采用了大量实验，证明了VSRM与PPO和Reinforce++的集成在数学推理基准上取得了显著结果。

Result: 研究结果表明，VSRM在大推理模型中取得了积极成果，实现了输出长度的显著缩减，同时保持了原始推理性能。进一步的实验验证了该方法在减少无效步骤的同时鼓励有效推理，从根本上缓解了过度思考问题。

Conclusion: 本研究提出了一种基于规则的可验证逐步奖励机制（VSRM），用于解决大推理模型（LRMs）中的过度思考问题。实验证明，该方法在减少输出长度的同时保持原始推理性能，达到了效率和准确性之间的最佳平衡。研究结果显示，VSRM结合PPO和Reinforce++在标准数学推理基准上取得了显著成果。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [11] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: 该论文描述了Dianping-Trust-Safety团队在META CRAG-MM挑战中取得的成就。他们通过结合监督微调、课程学习和强化学习，提高了答案准确性并降低了虚构内容，取得了优异的表现。


<details>
  <summary>Details</summary>
Motivation: 竞赛要求构建一个全面的检索增强生成系统，具有多模态多轮问答的能力。解决这些任务的关键在于整合结构化数据和外部知识源，以及实现上下文理解和信息整合。这些任务需要综合利用视觉大型语言模型、课程学习和强化学习等技术。

Method: 团队解决方案基于视觉大型语言模型，结合了来自GPT-4.1的知识提炼进行监督微调。他们还应用课程学习策略指导强化学习，提高了答案准确性并减少了虚构内容。在任务2和任务3中，他们还利用网络搜索API，融入外部知识，使系统能更好地处理复杂查询和多轮对话。

Result: 他们的解决方案在任务1中取得了第一名的显著领先优势，并在任务3中取得了第三名。他们展示了课程学习与强化学习在训练流程中的集成的有效性。

Conclusion: 该论文描述了Dianping-Trust-Safety团队针对META CRAG-MM挑战的解决方案。他们通过结合检索增强生成系统，实现了多模态多轮问答的能力。团队在竞赛中取得了第一和第三名的显着成绩，展示了他们的解决方案的有效性。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [12] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 本文针对多智能体强化学习（MARL）中的问题，提出了两种方法：HATRPO-W 和 HATRPO-G，用于改进 HATRPO 的表现。实验结果显示，这两种方法显著提高了性能，加快了收敛速度，并实现了更高的最终奖励，在广泛的 MARL 基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: MARL 需要在互动智能体之间实现协调和稳定的策略更新。现有的 HATRPO 方法在异构设置中可能会导致更新缓慢和局部最优解。因此，需要提出更灵活和有效的方法来解决这一问题。

Method: 提出了两种方法：HATRPO-W 和 HATRPO-G，用于在全局 KL 约束下优化阈值分配以及基于改进-散度比率对代理进行优先排序。通过连接顺序策略优化和约束阈值调度，实现了更灵活和有效的异构智能体设置中的学习。

Result: 实验证明，HATRPO-W 和 HATRPO-G 显著提升了 HATRPO 的性能，在多样的 MARL 基准测试中获得更快的收敛速度和更高的最终奖励。具体地，两种方法都超过了 22.5% 的性能提升，并且 HATRPO-W 表现出更稳定的学习动态。

Conclusion: 提出了两种方法来分配多智能体间的 KL 散度阈值：HATRPO-W 和 HATRPO-G，它们显着提高了 HATRPO 的性能，实现了更快的收敛速度和更高的最终奖励。在多样的 MARL 基准测试中，HATRPO-W 和 HATRPO-G分别提升超过 22.5%，其中 HATRPO-W 表现出更稳定的学习动态。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [13] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 研究探讨了大语言模型在想象性推理方面的能力。通过使用“Turtle Soup”游戏为基础，提出了综合研究框架，包括基准测试、智能体和评估协议。引入了TurtleSoup-Bench作为首个大规模、双语、互动的想象性推理基准测试，以及Mosaic-Agent来评估LLMs的表现。研究发现LLMs存在明显的能力限制和与人类性能的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试往往静态或专注于社交推理，无法捕捉到想象性推理过程中的动态、探索性质。为填补这一空白，引入了基于“Turtle Soup”游戏的综合研究框架，旨在探讨LLMs在想象性推理方面的表现。

Method: 研究使用了经典的“Turtle Soup”游戏作为基础，提出了一套综合的研究框架，包括基准测试、智能体和评估协议。引入了TurtleSoup-Bench作为首个大规模、双语、互动的想象性推理基准测试，以及Mosaic-Agent作为一种新型智能体来评估LLMs在此环境中的表现。为评估推理质量，开发了多维度协议，包括逻辑一致性、细节完整性和结论一致性。

Result: 研究揭示了领先的LLMs存在明显的能力限制和常见的失败模式，在想象性推理方面与人类存在明显的性能差距。

Conclusion: 研究发现大语言模型（LLMs）在想象性推理方面的能力有限，存在明显的性能上限和与人类相比的显著性能差距。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [14] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG addresses limitations of existing knowledge graph-based RAG methods by introducing a collaborative framework for knowledge aggregation and retrieval strategies. It significantly improves response quality and reduces retrieval redundancy by 46% in experiments across different QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge graph-based RAG methods face challenges like disconnected conceptual summaries and inefficient retrieval processes. LeanRAG aims to overcome these limitations by introducing a collaborative framework for knowledge aggregation and retrieval to enhance response quality and reduce redundancy in information retrieval.

Method: LeanRAG employs a semantic aggregation algorithm to form entity clusters and create explicit relations among aggregation-level summaries, establishing a navigable semantic network. It also utilizes a structure-guided retrieval strategy that anchors queries to relevant fine-grained entities and traverses the graph's semantic pathways efficiently.

Result: LeanRAG demonstrates superior performance in response quality compared to existing methods while reducing retrieval redundancy by 46%. The framework is evaluated on four QA benchmarks with different domains, showcasing its effectiveness across various tasks.

Conclusion: LeanRAG is introduced to address the limitations of knowledge graph-based RAG methods by incorporating a collaborative design for knowledge aggregation and retrieval strategies. It significantly outperforms existing methods in response quality and reduces retrieval redundancy by 46%.

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [15] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef框架结合医学本体和真实EHR数据，用于稳健的药物推荐。模型在基准测试中表现出色，在未知代码环境下也维持高准确性。引入了稀疏正则化方案和双曲空间嵌入，提高了泛化能力和稳健性。


<details>
  <summary>Details</summary>
Motivation: 真实世界的电子健康记录数据存在罕见的医疗实体和不完整记录，使得基于数据驱动模型在缺失或新颖条件下泛化困难。现有模型主要依赖于观察到的共现模式，导致泛化性能不佳。因此，为了解决这些问题，提出了结合医学本体和真实EHR数据的框架HiRef，以提高药物推荐的稳健性。

Method: 论文提出了一种统一框架HiRef，结合医学本体的层次语义和真实EHR数据推导的精细化共现模式。通过将本体实体嵌入双曲空间，提高了模型对未见代码的泛化能力。引入了基于先验引导的稀疏正则化方案，用于细化EHR共现图，进一步提升了模型的稳健性。进行了广泛实验和全面消融研究，证明了HiRef对未知医学代码的鲁棒性。

Result: 模型在EHR基准测试中表现强劲，对未知代码环境下的准确性也很高。通过广泛的实验和消融研究验证了模型的鲁棒性和对学习到的稀疏图结构和医学代码嵌入的深入分析。

Conclusion: 该论文提出了一种名为HiRef的框架，用于稳健的药物推荐，通过结合医学本体和真实EHR数据中推导的精细化共现模式，取得了显著的性能。模型在EHR基准测试（MIMIC-III和MIMIC-IV）上表现出色，并在模拟的未知代码环境下保持高准确性。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [16] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: 介绍了MM-Food-100K，一个包含10万样本的多模态食品智能数据集，提供可验证来源；使用Codatta贡献模型和AI辅助质量检查收集1.2百万质量接受的食品图像语料库；对ChatGPT等视觉语言模型进行微调，用于图像的营养预测，表现优于基准模型；发布免费访问，保留一部分用于商用并与贡献者分享收入。


<details>
  <summary>Details</summary>
Motivation: 为了创建一个多模态食品智能数据集，提供可验证来源和广泛的信息注释，用于图像的营养预测。

Method: 使用Codatta贡献模型，结合社区合作和可配置的AI辅助质量检查，收集了1.2百万质量接受的食品图像语料库。对大型视觉语言模型（ChatGPT 5、ChatGPT OSS、Qwen-Max）进行微调，以进行图像的营养预测。

Result: 微调在标准指标上比开箱即用的基准模型表现更好，主要结果报告在MM-Food-100K子集上。发布MM-Food-100K以供公开免费访问，并保留一部分用于商用访问与贡献者分享收入。

Conclusion: 介绍了MM-Food-100K，这是一个包含10万样本的多模态食品智能数据集。提供了可验证来源。由1.2百万接受质量的食品图像语料库精心筛选出来，用于各种信息的注释（例如菜名，创建地区）。语料库是在六周内从超过87000位贡献者那里收集而来，使用Codatta贡献模型，结合社区合作和可配置的AI辅助质量检查；每个提交都链接到一个安全的离链账本中的钱包地址，以实现可追溯性，在路线图上还有一个完整的在链协议。描述了模式、管道和质量保证，并通过在基于图像的营养预测上对大型视觉语言模型（ChatGPT 5、ChatGPT OSS、Qwen-Max）进行微调来验证效用。微调在标准指标上始终优于开箱即用的基准模型；我们主要在MM-Food-100K子集上报告结果。我们发布MM-Food-100K以供公开免费访问，保留约90%供潜在的商用访问，并与贡献者分享收入。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [17] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: 本文介绍了We-Math 2.0系统，旨在全面提升MLLMs的数学推理能力。实验结果表明MathBook-RL在多个基准测试上表现出色，并在MathBookEval上取得了强大的结果，表明其具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注数据集建设和方法优化，经常忽略两个关键方面：全面的知识驱动设计和以模型为中心的数据空间建模。为了克服MLLMs在复杂数学推理方面的困难，本文引入了We-Math 2.0系统，旨在全面提升MLLMs的数学推理能力。

Method: 本文提出了We-Math 2.0系统，其关键贡献有四个方面：（1）数学知识系统：构建了一个包含491个知识点和1819个基本原则的五级层次系统；（2）MathBook-Standard和Pro：开发了MathBook-Standard数据集，通过双重扩展确保了广泛的概念覆盖和灵活性。此外，定义了一个三维难度空间，并为每个问题生成7个渐进性变体，构建了MathBook-Pro，用于进行强大的训练；（3）MathBook-RL：提出了包含两个阶段的RL框架，包括冷启动微调，使模型与以知识为导向的思维链对齐，以及渐进对齐RL，利用平均奖励学习和动态数据调度实现跨难度级别的渐进对齐；（4）MathBookEval：引入了一个全面的基准测试，覆盖了所有491个知识点，具有多样化的推理步骤分布。

Result: 实验结果显示，MathBook-RL在四个广泛使用的基准测试上表现出色，并在MathBookEval上取得了强大的结果，表明其在数学推理方面具有良好的泛化能力。

Conclusion: 本文介绍了We-Math 2.0系统，该系统集成了结构化的数学知识系统、以模型为中心的数据空间建模和基于强化学习（RL）的训练范式，全面增强了MLLMs的数学推理能力。实验结果显示，MathBook-RL在四个广泛使用的基准测试上表现出色，并在MathBookEval上取得了强大的结果，表明其在数学推理方面具有良好的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [18] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: 本文提出了 FIRESPARQL 框架，用于解决 NLQ 转为 SPARQL 查询中的结构和语义错误。通过实验评估发现，fine-tuning 可获得最佳性能，在测试集上达到高准确度。


<details>
  <summary>Details</summary>
Motivation: 由于 LLM 方法在生成 SKG 特定内容和底层架构的 SPARQL 查询时遇到困难，提出了一种解决 NLQ 转为 SPARQL 查询过程中错误的框架。针对 LLM 生成的错误，如结构不一致和语义不准确，提出了 FIRESPARQL 方案。

Method: 使用 FIRESPARQL 框架，支持细调的 LLM 作为核心组件，可通过 RAG 提供上下文，并增加一个 SPARQL 查询校正层。在 SciQA 基准上进行了多种配置的实验评估，比较了性能与基线和最新方法。使用 BLEU 和 ROUGE 度量查询准确度，使用 RelaxedEM 度量查询结果准确度，并将结果与包含 NLQ、SPARQL 查询和其结果的金标准进行比较。

Result: 通过实验评估，证明了 fine-tuning 在查询准确度和结果准确度上达到了最佳性能，远超基线和最新方法。最终的模块化框架 FIRESPARQL 能有效解决 NLQ 转为 SPARQL 查询的问题。

Conclusion: 提出了一个名为FIRESPARQL的模块化框架，用于解决基于大型语言模型的 NLQ 转为 SPARQL 查询时存在的结构和语义错误。实验结果表明，通过 fine-tuning 可实现最高性能，测试集上查询准确度达到 0.90 ROUGE-L，结果准确度达到 0.85 RelaxedEM。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [19] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: This study explores the Spatial Exemplar Query (SEQ) scenario to enhance spatial search experiences, introducing SEQ-GPT, a spatial query system powered by Large Language Models (LLMs). The study aims to improve user experiences in spatial searches by enabling versatile searches based on user-specified examples through natural language interactions. Additionally, a tailored LLM adaptation pipeline is proposed to align natural language with structured spatial data and queries, offering insights into enhancing spatial search experiences.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the limitations of contemporary spatial services, such as online maps, in performing complex tasks like searching for a group of locations simultaneously. By exploring the Spatial Exemplar Query (SEQ) scenario, the study aims to improve the user experience in spatial searches by enabling more versatile searches based on user-specified examples using natural language capabilities of Large Language Models (LLMs).

Method: The study introduced SEQ-GPT, a spatial query system powered by Large Language Models (LLMs), and proposed a tailored LLM adaptation pipeline. The system utilizes natural language capabilities of LLMs to enhance spatial search experiences, allowing for interactive operations, query clarification, and dynamic search adjustments based on user feedback. The adaptation pipeline aligns natural language with structured spatial data and queries through dialogue synthesis and multi-model cooperation.

Result: The study provides an end-to-end demonstration of SEQ-GPT, showcasing how the system can broaden spatial search capabilities with realistic data and application scenarios. By introducing SEQ-GPT and the LLM adaptation pipeline, the study offers insights into enhancing spatial search experiences through natural language interactions and tailored adaptation processes.

Conclusion: Spatial Exemplar Query (SEQ) is explored in this study to enhance spatial search experiences by allowing users to search for multiple relevant locations simultaneously based on user-specified examples. SEQ-GPT, a spatial query system powered by Large Language Models (LLMs), was introduced to facilitate more versatile SEQ searches using natural language. Additionally, a tailored LLM adaptation pipeline was proposed to align natural language with structured spatial data and queries, enabling unique interactive operations in the SEQ process.

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [20] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: AI, specifically DxDirector-7B, is proposed to lead the full diagnosis process with minimal physician involvement, marking a shift where AI drives diagnosis to reduce physician workload significantly and enhance efficiency. It demonstrates superior accuracy and potential to serve as a substitute for specialists.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of AI in fully reducing physicians' workload and driving the entire diagnostic process from an ambiguous complaint. The aim is to enhance diagnostic efficiency by repositioning AI as the main driver of diagnosis.

Method: The paper proposes DxDirector-7B, an LLM with deep thinking capabilities, to act as the primary director in the diagnostic process while physicians serve as assistants. It establishes an accountability framework for misdiagnoses and evaluates its performance across rare, complex, and real-world cases.

Result: DxDirector-7B demonstrates superior diagnostic accuracy and reduces physician workload compared to existing medical LLMs and general-purpose LLMs. Fine-grained analyses validate its efficacy in multiple clinical departments, and expert evaluations suggest its potential as a substitute for medical specialists.

Conclusion: AI, particularly DxDirector-7B, can drive the full-process clinical diagnosis with minimal physician involvement and significantly reduce physician workload and enhance diagnostic efficiency. It marks a paradigm shift in the relationship between physicians and AI, positioning AI as the primary director.

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [21] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS is a multimodal framework that improves Chest X-Ray reasoning and medical AI safety. It outperforms baselines in accuracy, AUC, and LLM-J while managing computational costs. The method includes adaptive tool selection and probability-annotated decision paths, leading to a new paradigm in medical agentic systems.


<details>
  <summary>Details</summary>
Motivation: Existing tool-augmented agentic systems face limitations like black-box reasoning, poor multimodal integration, and inefficiency. PASS aims to overcome these challenges and enhance medical AI safety through adaptive tool selection and probability-annotated decision paths.

Method: Introducing PASS (Probabilistic Agentic Supernet Sampling) as a multimodal framework for Chest X-Ray reasoning. It adaptively samples agentic workflows over a multi-tool graph, leveraging learned task-conditioned distribution to select suitable tools for each layer. A three-stage training procedure is designed, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning.

Result: PASS significantly outperforms strong baselines in accuracy, AUC, and LLM-J metrics while balancing computational costs. It introduces a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.

Conclusion: PASS is a multimodal framework that addresses the challenges in Chest X-Ray reasoning, offering probability-annotated decision paths and enhancing medical AI safety. It outperforms strong baselines in accuracy, AUC, and LLM-J metrics while balancing computational costs, leading to a new paradigm shift in medical agentic systems.

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [22] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 本文研究了语言模型（LMs）与人类偏好对齐的重要性，介绍了一种改进的LM对齐方法 DPO，并通过结合静态和在线偏好数据来优化LM策略。作者提出了对齐阶段假设，将对齐过程分为偏好注入和偏好微调两个阶段，并提出了识别这两个阶段边界的有效算法。实验结果表明该算法有助于提高LM对齐的质量和效果。


<details>
  <summary>Details</summary>
Motivation: 本文强调了将LM模型与人类偏好进行对齐的重要性，并指出了静态和在线偏好数据之间可能存在的系统性效果差异。作者通过研究发现，online数据不一定总是最优的，因此提出了对齐阶段假设来解释这一现象。

Method: 本文主要通过对静态偏好数据进行直接优化LM策略的DPO方法，并结合在线采样来改进LM对齐效果。作者提出了对齐阶段假设，将对齐过程分为偏好注入阶段和偏好微调阶段，并通过理论和实证分析来表征这两个阶段。另外，作者还提出了一种有效的算法来识别这两个阶段之间的边界。

Result: 作者通过实验在 5 个LM模型和 2 种对齐方法上验证了对齐阶段假设和边界测量的普适性。结果显示，提出的算法有助于提高LM对齐的质量和效果。

Conclusion: 本文提出了一种有效算法识别LM对齐过程中不同阶段之间的边界，并在 Llama，Zephyr，Phi-2，Qwen 和 Pythia 等 5 个模型上进行了实验验证。通过理论和实证分析，该算法有助于区分偏好注入阶段和偏好微调阶段，从而提高LM对齐的质量。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [23] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出了ComMCS方法，用于改善大语言模型在数学等复杂领域推理能力方面的挑战。实验结果显示ComMCS在MATH-500和GSM8K基准上表现优异，比其他方法取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学等复杂领域的推理能力仍然存在挑战，现有基于价值的过程验证器效果受限于训练注释的估计误差。需要解决高方差造成的估计误差问题。

Method: 提出了ComMCS方法，通过线性组合当前和后续步骤的MC估计量构建无偏估计器，降低方差并保持无偏估计。在MATH-500和GSM8K基准上进行了实验验证。

Result: 提出的ComMCS方法在实验中表现优异，比其他方法在MATH-500和GSM8K基准上均取得显著提升。

Conclusion: 本文提出了一种名为ComMCS的方法，用于解决大语言模型在数学等复杂领域推理能力方面的挑战。通过结合当前和后续步骤的MC估计量，构建了一种无偏估计器，从而有效降低了方差，同时不增加额外的LLM推断成本。实证实验证明ComMCS在MATH-500和GSM8K基准上表现优异，比回归优化方法和基准方法分别提高了2.8点和2.2点。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [24] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: MSRS proposes a novel framework for multi-attribute steering in Large Language Models, reducing interference and achieving better control. It combines orthogonal subspaces for each attribute, a hybrid subspace composition strategy, and a dynamic weighting function for precise control. The token-level steering mechanism enables fine-grained behavioral modulation. MSRS outperforms existing methods and generalizes well to diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of jointly steering multiple attributes in Large Language Models, reducing interference and undesirable trade-offs. Aim to provide effective multi-attribute steering by isolating attribute influence within the model's representation space.

Method: Multi-Subspace Representation Steering (MSRS) is proposed to allocate orthogonal subspaces to each attribute, reducing inter-attribute interference. A hybrid subspace composition strategy combines attribute-specific and shared subspaces. A dynamic weighting function integrates these components for precise control. Token-level steering mechanism intervenes on semantically relevant tokens for fine-grained behavioral modulation.

Result: MSRS significantly reduces attribute conflicts, outperforms existing methods across various attributes, and generalizes effectively to diverse downstream tasks.

Conclusion: MSRS is a novel framework for multi-attribute steering in Large Language Models, reducing interference and achieving better control over various attributes. It outperforms existing methods and generalizes well to different tasks.

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [25] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: The paper introduces STEP, a conversational recommender system that effectively integrates external knowledge graph information and outperforms existing methods in recommendation precision and dialogue quality on public datasets.


<details>
  <summary>Details</summary>
Motivation: Existing conversational recommender systems struggle with capturing deep user preferences and integrating external knowledge graph information efficiently. Traditional approaches face challenges in integrating KG information into dialogue generation effectively, resulting in recommendations that may not meet user expectations.

Method: The paper introduces STEP, a conversational recommender system that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. It uses an F-Former to align dialogues with knowledge-graph entities through a three-stage curriculum and injects the fused representation into a pre-trained language model via two adaptive prefix prompts.

Result: Experimental results demonstrate that STEP achieves better precision in recommendation and higher dialogue quality compared to mainstream methods on two public datasets.

Conclusion: STEP, a conversational recommender system, outperforms existing methods in recommendation precision and dialogue quality on two public datasets.

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [26] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM is a language model-based ontology alignment framework that enhances semantic representations and achieves competitive performance in ontology matching, particularly in the biomedical domain. It outperforms traditional systems and recent methods, as evidenced in experiments on the OAEI Bio-ML track.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ontology matching in the biomedical domain by introducing GenOM, emphasizing its ability to enhance semantic interoperability and integration across heterogeneous knowledge sources.

Method: GenOM enriches the semantic representations of ontology concepts through generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments were conducted on the OAEI Bio-ML track to demonstrate GenOM's performance.

Result: The results of experiments on the OAEI Bio-ML track show that GenOM outperforms traditional ontology matching systems and recent LLM-based methods, confirming its competitive performance. Ablation studies further validate the effectiveness of semantic enrichment and few-shot prompting in GenOM.

Conclusion: GenOM is a large language model-based ontology alignment framework that enhances semantic representations and achieves competitive performance in ontology matching, surpassing traditional systems and recent LLM-based methods.

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [27] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 这项研究提出了Agentic Design Review System（AgenticDRS），通过多个代理协作分析设计，展示了其在评估图形设计和提供可操作反馈方面的功效。研究还构建了DRS-BENCH基准用于评估该框架，结果表明Agentic-DRS在该领域具有有效性。


<details>
  <summary>Details</summary>
Motivation: 图形设计的评估涉及多个方面，如对齐、构图、美学和颜色选择。评估设计需要综合个人专家评审员的反馈意见。该研究旨在引起对这一务实但尚未充分探索的研究方向的注意。

Method: 研究提出了一种Agentic Design Review System（AgenticDRS），其中多个代理通过图匹配和独特的提示扩展方法协作分析设计，同时构建了用于评估该框架的DRS-BENCH基准。通过与问题设置相适应的最新基线的彻底对比实验评估以及关键性消融实验，展示了Agentic-DRS在评估图形设计方面的有效性。

Result: 通过实验评估和与最新基线的对比，以及关键性消融实验，证明了Agentic-DRS在评估图形设计和生成可操作反馈方面的有效性。

Conclusion: 这项研究提出了一个Agentic Design Review System（AgenticDRS），通过多个代理协作分析设计，为从整体上评估图形设计提供了有效的框架，并展示其在评估图形设计和提供可操作反馈方面的功效。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [28] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 本研究提出了一种稀疏、目标感知的图神经网络表示方法，有效解决了在大规模网格环境中规划问题时普遍存在的稀疏性和内存需求过高等挑战。该方法表现出色，能够提高策略泛化和成功率，为解决实际的大规模规划任务提供了可行性基础。


<details>
  <summary>Details</summary>
Motivation: 现有的规划方法在处理大规模问题时存在内存需求过高、学习困难等问题，为解决这一挑战，本研究提出了一种更有效的图神经网络表示方法。

Method: 通过设计基于PDDL的新型无人机任务场景，在网格世界中模拟真实任务执行环境，提出了一种稀疏的、目标感知的图神经网络表示方法。

Result: 研究结果表明，该方法在大规模网格环境中取得了显著的成功，能够有效提高策略泛化和成功率。

Conclusion: 该研究提出了一种稀疏的、目标感知的图神经网络表示方法，有效地解决了在符号规划领域中使用深度强化学习和图神经网络时普遍存在的稀疏性和内存需求问题。实验证明该方法在大规模网格环境中表现出色，能够改善策略泛化和成功率。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [29] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 本研究关注AI生成内容对人类认知和行为的影响，引入了MhAIM数据集，提出了新的度量方式以评估用户对在线内容的判断和互动。设计了T-Lens系统，基于LLM，能够预测人类对多模态信息的反应，增强解释性和交互能力。研究结果强调了AI、人类认知和信息接收之间的复杂互动，为减轻AI驱动的错误信息风险提供了可行的策略。


<details>
  <summary>Details</summary>
Motivation: 以人为中心的方法研究AI生成内容对人类认知和行为的影响，弥补了现有研究主要关注内容是否真实的缺陷。在交易或股市等领域，预测人们的反应可能比验证内容的准确性更加关键。为了解决这一问题，引入了MhAIM数据集，并提出了新的度量方式和T-Lens系统，旨在增强LLMs的人类意识能力。

Method: 引入了MhAIM数据集，进行人类研究以评估用户对AI生成内容的反应。提出了新的度量方式：信任度、影响力和公开性，用于量化用户对在线内容的评判和互动。设计了T-Lens系统，基于LLM，结合了人类反应预测，通过HR-MCP（Human Response Model Context Protocol）实现与任何LLM的无缝集成。

Result: 通过人类研究发现，人们在包含文本和视觉内容时更容易识别AI生成内容，特别是在两者之间存在不一致性时。提出了三个新的度量方式：信任度、影响力和公开性，以量化用户对在线内容的评判和互动。设计了T-Lens系统，结合了LLM和人类反应预测，通过HR-MCP实现了与任何LLM的无缝集成。

Conclusion: 研究集中在AI生成内容对人类认知和行为的影响方面，提出了新的度量方式以评估用户对在线内容的判断和互动。引入了MhAIM数据集，研究表明人们在包含文本和视觉内容时更容易识别AI生成内容，特别是在两者之间存在不一致性时。提出了T-Lens系统，基于LLM设计，能够预测人类对多模态信息的反应，增强解释性和交互能力。研究为LLMs提供了人类意识能力，强调了AI、人类认知和信息接收之间的复杂互动，为减轻AI驱动的错误信息风险提供了可行的策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [30] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 研究介绍了临床试验自然语言推理基准测试，并评估了六种当代大型语言模型在此基准测试下的表现。结果显示，模型在GKMRV上准确率较高，但在主要推理任务上表现较差，暴露出结构和表征限制。该研究质疑了大型语言模型仅通过扩展数据和参数来获得有结构、可推广内部表征的假设。需要揭示LLM在高风险领域可靠性的框架。


<details>
  <summary>Details</summary>
Motivation: 质疑了大型语言模型仅通过扩展数据和参数来获得越来越有结构、可推广的内部表征的假设。主要推理任务上的低准确性表明当前LLM虽然具备相关临床知识，但在部署这些知识时缺乏结构化、可组合的内部表征。需要揭示LLM在高风险领域可靠性的框架。

Method: 介绍了一个临床试验自然语言推理基准测试，包括四个推理类别：因果归因、组成性基础、认识验证和风险状态抽象。每个项目与有针对性的地面知识和元级别推理验证探测（GKMRV）相配对，让我们能够区分事实访问的失败和推理的失败。评估了六种当代LLM模型，包括直接和思维链提示。

Result: 结果显示，模型在GKMRV上准确率接近最高（平均准确率为0.918），但在主要推理任务上表现不佳（平均准确率为0.25）。尽管准确率低，输出推断在样本间高度一致（平均为0.87），表明存在基础启发式和捷径的系统性应用。

Conclusion: 当前大型语言模型在临床试验自然语言推理基准测试中表现出高GKMRV准确性，但在主要推理任务上表现较差。尽管准确性较低，输出推断在样本间高度一致，表明存在基础启发式和捷径的系统性应用。研究揭示了当前LLM存在的基本结构和表征限制：虽然具备相关临床知识，但缺乏可靠地部署的结构化、可组合内部表征（例如，整合限制、权衡证据或模拟反事实）。通过使用GKMRV将知识与推理分离，使这种区分变得明确和可测量，为探究LLM在高风险领域的可靠性提供了有效框架。

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [31] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: 本文研究了XAI中的可访问性问题，强调了残障用户在XAI技术评估中的重要性。简化解释更易理解，多模式呈现有助于更公平的可解释性。采用文献综述和方法论概念验证作为研究方法。


<details>
  <summary>Details</summary>
Motivation: 尽管XAI的可解释性备受关注，但对残障用户的可访问性影响却鲜有探讨。本文旨在填补这一领域的研究空白，以促进更包容和公平的XAI设计。

Method: 本文采用了文献综述和方法论概念验证作为研究方法，分别从已有研究和实证研究的角度探讨了XAI的可访问性问题。

Result: 研究发现简化的解释对非视觉用户更易理解，而多模式呈现对更公平的可解释性是必要的。

Conclusion: 本文研究了可解释人工智能（XAI）中的可访问性差距，提出了包括残障用户在内的用户在评估XAI技术中的必要性。通过一项文献综述和一个四部分的方法论概念验证，证明了简化解释对于非视觉用户更易理解，而多模式呈现对于更公平的可解释性是必要的。

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>
