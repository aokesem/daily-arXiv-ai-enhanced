<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP is a framework that enables persistent, secure, and semantically searchable memory sharing among AI agents, addressing challenges in context preservation and collaboration. It demonstrates efficacy across domains and regulatory compliance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome ephemeral memory limitations in current AI agent architectures and enable effective collaboration and knowledge sharing across sessions and agent boundaries.

Method: SAMEP implements a distributed memory repository with vector-based semantic search, cryptographic access controls, and standardized APIs compatible with existing agent communication protocols.

Result: Experimental results show significant reductions in redundant computations, improvement in context relevance scores, and complete compliance with regulatory requirements, including audit trail generation.

Conclusion: SAMEP introduces a novel framework for persistent, secure, and semantically searchable memory sharing among AI agents, addressing critical challenges in context preservation, collaboration, and semantic discovery. It demonstrates effectiveness across various domains and regulatory compliance.

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [2] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: 通过实验研究表明，在多智能体强化学习中，通过内生成符号系统，智能体可以实现有效符号交流，无需外部归纳偏见。这一方法相较于传统交流方法具有更强的普适性和效率，提出了三项重要理论见解，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 该研究对传统方法引入归纳偏见以促进交流出现提出了根本性质疑，通过实验和研究，探讨了人类大脑对内部思维是否直接使用人类语言的观点，并结合“大语言模型（LLMs）”中的“软思维”能力进行了研究。

Method: 通过基于矢量量化变分自编码器（VQ-VAE）的“AI Mother Tongue”（AIM）框架进行实验，证明当智能体拥有内生符号系统时，其神经表征自然地展现出自发的语义压缩和纳什均衡驱动的语义收敛，实现了有效的符号化交流，不需要外部归纳偏见。

Result: 实验表明，使用内生符号系统的智能体在分布式多智能体强化学习中可以实现有效的符号交流，无需外部归纳偏见。相较于传统明确交流方法，这种方法具有更强的普适性和效率。

Conclusion: 通过实验表明，在分布式多智能体强化学习中，使用内生符号系统可以实现有效的符号交流，无需引入外部归纳偏见，相较于传统明确交流方法，具有更强的普适性和效率。研究提出了三项重要理论见解：神经交流假设、工具优先原则和语义可解释性范式。未来研究将探索集成分层量化变分自编码器以增强复杂表达能力，并研究“强化学习低层先训练”的潜力。这一发现为连接主义和符号主义提供了新的途径。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [3] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 本研究引入了一种Agentic AI视觉分类框架，集成了多模态代理、非视觉推理协调器和检索增强生成模块，解决了多代理人架构中的信任问题。在苹果叶病诊断中，应用信任感知编排和RAG模块，实现了零-shot设置下77.94％准确率提高，总体达到85.63％。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）在多代理架构上越来越倚重于融合视觉和语言理解，然而，在零-shot设置中尤其没有微调的情况下，我们如何能够相信这些代理是一个紧迫的问题。

Method: 引入了一种新颖的模块化Agentic AI视觉分类框架，结合了通用的多模态代理、非视觉推理协调器和检索增强生成模块。通过信心校准指标对编排器进行调节，并在苹果叶病诊断的基础上进行了三种配置的基准测试。通过信任感知的编排和RAG模块，在零-shot设置下实现了77.94％的准确率提高，总体准确率达到85.63％。

Result: 通过应用信任感知的编排和RAG模块，在零-shot设置下实现了77.94％的准确率提高，总体准确率为85.63％。GPT-4o在校准方面表现更好，而Qwen-2.5-VL显示出过度自信。

Conclusion: 本研究介绍了一种新颖的模块化Agentic AI视觉分类框架，通过集成通用的多模态agent、非视觉推理协调器和检索增强生成模块以解决多代理人架构的信任问题。利用信心校准指标（ECE、OCR、CCC），编排器调节对于代理的信任度。实验结果表明，在零-shot设置中，信任感知编排和RAG模块使准确率提高77.94％，总体准确率达到85.63％。GPT-4o表现出更好的校准，而Qwen-2.5-VL则显示出过度自信。此外，图像-RAG通过视觉上相似的案例实现了预测，通过迭代重新评估纠正代理人的过度自信。该系统将感知（视觉代理）与元推理（编排器）分开，实现可扩展和可解释的多agent AI。该方案可拓展到诊断、生物学和其他关键领域，所有模型、提示、结果和系统组件，包括完整的软件源代码，在Github上公开发布以支持再现性、透明度和社区基准测试。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [4] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: 研究分析了大型语言模型（LLMs）在符号推理、算术准确性和逻辑一致性任务中失败的原因，发现LLMs存在计算分裂综合症，缺乏原则性、组合推理的建筑支撑，提出未来模型改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在任务失败方面的原因，探讨指导和行动路径之间的分裂现象，确定当前LLMs的能力界限并提出未来模型的改进建议

Method: 结构性诊断、控制实验和架构分析

Result: 发现LLMs存在计算分裂综合症，缺乏组合推理的建筑支撑，理想提示下仍表现不稳定

Conclusion: 此篇论文提出了对大型语言模型（LLMs）在符号推理、算术准确性和逻辑一致性任务上失败的结构性诊断。他们发现LLMs存在“计算分裂综合症”，表现为指导和行动路径在几何上和功能上分离。论文指出LLMs在正确原则上表达，但在可靠应用这些原则上失败，这不是因为知识获取而是计算执行问题。研究结果显示LLMs缺乏用于原则性、组合推理的建筑支撑，这使得模型行为即使在理想提示下仍不稳定。作者建议未来的模型需要具备元认知控制、原则提升和有结构基础的执行。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [5] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data introduces a novel system for data acquisition and query handling in meteorology, outperforming existing systems in API call accuracy. It leverages a knowledge graph for enhanced performance in content retrieval, query handling, and data integration while mitigating the high cost of fine-tuning LLMs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore the effective utilization of LLMs via API calls in knowledge-intensive domains like meteorology. They address the limitations of existing LLM-based systems in handling complex queries and accessing domain-specific knowledge. The motivation is to provide intelligent data acquisition and query handling solutions for high-knowledge-demand domains.

Method: KG2data uses a virtual API to evaluate API call accuracy across metrics like name recognition failure, hallucination failure, and call correctness. It leverages a knowledge graph as persistent memory to enhance content retrieval, query handling, reasoning, relationship resolution, and data integration. The system mitigates the high cost of fine-tuning LLMs, making it adaptable to evolving domain knowledge and API structures.

Result: KG2data achieves superior performance in API call accuracy metrics (1.43%, 0%, 88.57%) compared to RAG2data and chat2data. It enhances content retrieval, query handling, reasoning, relationship resolution, and data integration in meteorological data analysis.

Conclusion: KG2data introduces a novel system integrating knowledge graphs, LLMs, ReAct agents, and tool-use technologies for intelligent data acquisition and query handling in meteorology. It outperforms RAG2data and chat2data in API call accuracy metrics, addressing the limitations of LLM-based systems in accessing domain-specific knowledge.

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [6] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文探讨了Web of Agents（WoA）的发展历程，介绍了现代协议对早期标准的演化响应，提出了四轴分类法进行分析，强调了智能焦点的范式转变对现代Agentic AI的重要性，并指出新协议不能单独构建稳健生态系统，下一个研究前沿将集中在解决社会技术挑战。


<details>
  <summary>Details</summary>
Motivation: 作者认为现有研究领域仍存在碎片化，需要整合不同社区的研究观点，提供一个统一的分析透镜以比较代理架构，揭示代理系统演化中的联系。

Method: 通过提供综合性进化概述，介绍了WoA的演变及现代协议的发展过程，引入了一个四轴分类法以系统化分析，阐明了智能焦点的范式转变，以及现代自主智能人工智能的基础性转变。

Result: 通过分析WoA的演变过程和现代协议的特点，揭示了智能焦点的转变对现代自主智能人工智能的基础性意义。从而强调了构建强大、开放、可信赖生态系统的重要性，并提出了下一个研究前沿方向。

Conclusion: 本文总结了Web of Agents（WoA）的发展历程及现状，指出新协议尽管重要，但仍不足以构建稳健、开放、可信赖的生态系统。作者强调下一个研究前沿在于解决持续存在的社会技术挑战，提出了关于去中心化身份、经济模型、安全性和治理方面的新议程。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [7] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖的基于规则的方法，通过在文法上执行变异操作生成与原始曲调相关的新曲调。研究证明了这种方法的可行性，分析了曲调随多次变异的变化，并评估了每种变异类型的影响。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于探索一种新颖的基于规则的方法来生成音乐。通过在文法上执行变异操作，可以生成与原始曲调相关的新曲调，从而研究曲调如何随变异而变化。通过分析编辑距离、结构复杂度和长度，以及每种变异类型的影响，可以深入了解生成音乐的过程。

Method: 研究通过解析曲调，找到路径组装结构，并使用Sequitur算法生成文法。然后在文法上执行变异操作，随机选择一种变异类型。最后扩展文法以生成新曲调。分析了曲调随多次变异逐渐变化的情况，使用编辑距离、结构复杂度和长度来评估变化。分析了每种变异类型的影响大小。审查了输出曲调的音乐方面特征。

Result: 研究证明了通过在文法上执行变异操作来生成新曲调的可行性。分析了曲调随多次变异如何逐渐变化，并研究了每种变异类型的影响。同时审查了输出曲调的音乐方面特征。

Conclusion: 该论文提出了一种基于规则的新方法，用于通过改变现有曲调生成音乐。研究通过解析每个曲调以找到路径组装（PA）结构，该结构表示曲调中的所有重复。使用Sequitur算法来实现此目的。研究结果是一个文法。然后在文法上执行变异操作，而不是直接在曲调上进行。有19种可能的变异类型，如添加、移除、交换或颠倒文法部分，可以应用于这些文法。系统在此步骤中随机选择一种变异，并自动操作文法。在变异操作之后，需要扩展文法，以产生一个新曲调。经过1次或多次变异后的输出将是一个与原始曲调相关的新曲调。研究考查了随着多次变异，曲调如何逐渐变化。编辑距离、结构复杂度和曲调长度被用来展示曲调在多次变异后的变化。此外，分析了每种变异类型的影响大小。最后，审查了输出曲调的音乐方面。需要注意的是，本研究仅专注于生成新的音高序列。研究基于爱尔兰传统曲调数据集，并使用整数列表来表示每个曲调的音高值。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [8] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: The paper reviews energy consumption scenarios of data centers and the impact on GHG emissions, discussing the potential impact of AI on CO2 emissions. AI may initially increase emissions but has the potential to reduce CO2 emissions in the long term by optimizing processes across industries.


<details>
  <summary>Details</summary>
Motivation: To address the impact of AI on CO2 emissions by 2035 and its potential to automate and optimize processes across various fields related to energy production, supply, and consumption.

Method: Technical review of energy consumption scenarios of data centers and the impact on GHG emissions, considering near-term projections up to 2030 and long-term outlook beyond 2035.

Result: AI may lead to an initial increase in CO2 emissions in the near term but could have a positive impact in the long term by reducing carbon footprint through automation and optimization processes in different industries.

Conclusion: AI may initially increase CO2 emissions due to the power-hungry nature of big data centers and the demand for AI applications, but in the long term, AI has the potential to significantly reduce CO2 emissions by automating and optimizing processes across industries.

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [9] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: 本文使用多种深度学习模型对IoT恶意攻击进行检测，其中BERT表现最佳，Multi-Head Attention具有良好的检测能力和可解释性，GraphSAGE训练时间最短但准确性略低。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于物联网系统的流量模式既是序列化的又是多样化的，为模型学习提供了丰富的时间模式。作者希望通过深度学习模型提高对恶意攻击的检测效果。

Method: 本文基于GraphSAGE、BERT、TCN、Multi-Head Attention、BI-LSTM等模型进行了恶意网络流量检测实验，通过模拟时间模式和检测特征显著性展示了这些模型的性能。

Result: 实验结果表明BERT在检测IoT恶意攻击方面表现最佳，Multi-Head Attention提供了可解释的结果，GraphSAGE具有较短的训练时间但准确性稍低。

Conclusion: 本文通过深度学习模型检测物联网恶意攻击，对深度学习和基于图的模型在恶意网络流量检测方面进行了全面评估。实验结果显示BERT具有最佳性能，准确率达到99.94％，同时具有高精度和召回率，F1得分和AUC-ROC得分均为99.99％。Multi-Head Attention提供了有前景的结果，具有良好的检测能力并提供可解释的结果。GraphSAGE模型在提供良好准确性的同时需要训练时间最短，但与其他模型相比，准确性、精度和F1得分较低。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [10] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 本研究探讨了如何有效地分类不适合机器学习的抽象数据以检测人工智能协助。通过构建神经网络友好的图像表述形式和明确编码用户探索/开发的时间序列表述，研究证明了在抽象任务中编码时间和空间量的重要性。研究结果表明，常见模型在适当预处理的情况下可以有效分类这些不适合机器学习的数据，并在测试性能上取得了成功。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在文本生成、医疗诊断和自动驾驶等复杂任务中变得越来越普遍，检测人工智能协助的重要性日益增加。尽管许多研究关注分类复杂但具体的数据类别，如图像，但在检测AI协助时所面临的数据往往不适合机器学习。因此，本研究旨在探讨如何有效地分类这些不适合机器学习的抽象数据。

Method: 研究构建了四种不同的神经网络友好的图像表述形式以及一个明确编码用户探索/开发的时间序列表述，以适用于其他抽象任务。研究对比了三种传统深度学习结构和一个并行的CNN-RNN结构的性能，证明了在抽象任务中编码时间和空间数量的重要性。

Result: 研究表明，在适当预处理的情况下，常见模型可以有效分类不适合机器学习的数据。通过在模型中加入时间序列信息，测试性能最大化，展示了编码时间和空间量对于在抽象任务中检测AI协助的重要性。

Conclusion: 在这项研究中，研究人员提出将探测人工智能协助作为分类任务，并证明常见模型在适当预处理的情况下可以有效分类抽象数据。研究强调了将时间序列信息融入模型以最大化测试性能的重要性，并展示了在抽象任务中编码时间和空间数量对探测人工智能协助的重要性。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [11] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: SigmaScheduling is a new method proposed in the paper to dynamically schedule decision points for mobile health interventions. It aims to improve the effectiveness of interventions targeting habitual behaviors by considering the uncertainty in predicted behavior times. The method was evaluated with real-world data and showed promising results, increasing the likelihood of decision points preceding brushing events in at least 70% of cases. SigmaScheduling has the potential to advance precision mHealth for time-sensitive behaviors like oral hygiene.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the research is to address the limitations of fixed interval scheduling for decision points in mobile health interventions. The current one-size-fits-all approach performs poorly for individuals with irregular routines, leading to ineffective interventions. By proposing SigmaScheduling, the aim is to improve timely decision-making and intervention effectiveness for habitual behaviors like daily toothbrushing.

Method: The paper introduces SigmaScheduling, a dynamic scheduling method based on the uncertainty in predicted behavior times. It schedules decision points closer to predicted behavior times when timing is more predictable and earlier when timing is less certain. The method was evaluated using real-world data from a 10-week trial of Oralytics with 68 participants.

Result: Results from the evaluation of SigmaScheduling using real-world data indicate that it increased the likelihood of decision points preceding brushing events in the majority of cases. This suggests that the method has the potential to enhance precision mHealth interventions, especially for targeting time-sensitive habitual behaviors such as oral hygiene or dietary habits.

Conclusion: SigmaScheduling method is proposed to dynamically schedule decision points for mobile health interventions, improving the effectiveness of interventions targeting habitual behaviors. Results show that SigmaScheduling increased the likelihood of decision points preceding brushing events in at least 70% of cases, indicating its potential to advance precision mHealth for time-sensitive behaviors like oral hygiene.

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [12] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 本研究评估了使用LLMs自动复制社交媒体数据的专家主题分析的可行性，发现少样本学习的LLM方法可以自动化主题分析，为定性研究提供可扩展的补充。在验证集上，使用两次提示的GPT-4o效果最佳，模型导出的主题分布与专家分类密切相似。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估使用LLMs复制社交媒体数据的专家主题分析的可行性，挑战LLMs在归纳性主题分析方面的困难。

Method: 研究采用两个Reddit数据集，评估五种LLMs对专家编码的效果，采用二元分类建模，利用零、一和少样本提示策略，并通过准确度、精确度、召回率和F1分数来评估性能。

Result: 在验证集上，使用两次提示的GPT-4o效果最佳，准确率为90.9%，F1分数为0.71。对于高患病率主题，模型导出的主题分布与专家分类密切相似。

Conclusion: 该研究发现，少数样本学习的大型语言模型方法可以自动化主题分析，为定性研究提供可扩展的补充。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [13] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY is an open-source toolkit for legal reasoning that helps non-experts explore and analyze argumentation frameworks. It uses visualizations and structured analysis to identify sources of ambiguity, explain argument acceptance, and support teleological legal reasoning in real-world cases.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the challenges non-experts face in identifying sources of ambiguity and explaining argument acceptance in legal reasoning using argumentation frameworks. AF-XRAY aims to provide a systematic way to generate critical attack sets to resolve undecided arguments and explore alternative resolutions.

Method: The paper introduces AF-XRAY, which offers layered visualizations, semantic role classification of attack edges, overlay visualizations of alternative solutions, and identification of critical attack sets to transform ambiguous scenarios into grounded solutions in legal reasoning. Real-world legal cases are used to demonstrate how the tool supports teleological legal reasoning by revealing the impact of different assumptions on conclusions.

Result: The result of this paper is the development of AF-XRAY, an effective toolkit that enables users to pinpoint specific causes of ambiguity in argumentation frameworks and explore alternative resolutions through visualizations and structured analysis.

Conclusion: AF-XRAY is an open-source toolkit that aids in exploring, analyzing, and visualizing abstract argumentation frameworks in legal reasoning, helping to identify sources of ambiguity and explain argument acceptance for non-experts.

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [14] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer是一个用于自动生成高质量导航指令的框架，NavInstrCritic是一个全面评估导航指令质量的系统。这些方法能够提高研究效率和可扩展性，并已通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前专家提供的指导数量有限，合成标注质量不足，难以用于大规模研究。因此，需要一种能够自动生成高质量导航指令，并进行全面评估的方法，以提高效率和可扩展性。

Method: NavComposer框架明确分解语义实体，并将它们重新组合成自然语言指令。其模块化架构允许灵活集成最先进的技术，同时对语义实体的明确使用增强了指令的丰富性和准确性。NavInstrCritic是一种全面的无标注评估系统，评估导航指令的对比匹配、语义一致性和语言多样性。

Result: 通过详尽的实验，证明NavComposer和NavInstrCritic方法的有效性。

Conclusion: NavComposer和NavInstrCritic提供了一个全新的框架，用于自动生成高质量的导航指令，并进行综合评估，以解决专家提供指示数量有限，合成标注质量不足的问题。该方法能够在大规模研究中更具效率和可扩展性。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [15] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: 本研究探讨了使用基于大型语言模型的多智能体系统，模拟多学科团队协作以提供更安全的治疗建议。研究发现，当前的大型语言模型单一智能体医生与多学科团队在治疗规划任务上表现相当。然而，一些模型提供的建议存在不必要的药物，导致不必要的药物和疾病之间或药物之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 现有的决策支持系统在处理多慢性病患者的治疗建议时面临可伸缩性限制。受全科医生在处理多重慢性病患者时偶尔召集多学科团队合作的启发，研究调查了使用基于大型语言模型的多智能体系统进行更安全治疗建议的可行性和价值。

Method: 设计了单一智能体和多智能体系统框架，通过启用大型语言模型智能体之间的讨论来模拟多学科团队的决策过程。评估了系统在多重慢性病患者治疗规划任务上的性能，使用基准案例进行比较。对于评估指标的定义超越了技术上的精确度和召回率，允许检查所提建议达到的临床目标和药物负担。

Result: 研究结果表明，当前的大型语言模型单智能体医生与多学科团队在治疗规划任务上表现相当。最高得分的模型提供了正确的建议，满足所有临床目标，但建议存在不完整性。部分模型还出现不必要的药物，导致药物与疾病或药物之间的不必要冲突。

Conclusion: 在处理多重慢性病患者的治疗建议时，使用基于大型语言模型的多智能体系统在解决治疗冲突方面具有潜在的价值。研究结果表明，当前的大型语言模型单智能体医生与多学科团队在治疗规划任务上表现相当。然而，一些模型提供的建议存在不必要的药物，导致药物之间或药物与疾病之间的不必要冲突。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [16] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: Protein language models pose risks in generating harmful protein sequences. To mitigate these risks, a KPO framework integrating prior knowledge and reinforcement learning was proposed. The framework effectively reduces the likelihood of producing hazardous sequences while maintaining functionality, providing a safety assurance for using generative models in biotechnology.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns of generating harmful protein sequences with protein language models, emphasizing critical biosafety and ethical challenges in the field of sequence generation.

Method: Knowledge-guided Preference Optimization (KPO) framework integrates prior knowledge via a Protein Safety Knowledge Graph, utilizing efficient graph pruning and reinforcement learning to minimize the risk of harmful protein generation.

Result: Experimental results demonstrate the effectiveness of the KPO framework in reducing the risk of producing hazardous protein sequences.

Conclusion: KPO framework effectively reduces the likelihood of generating harmful protein sequences while maintaining high functionality, providing a robust safety assurance for using generative models in biotechnology.

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [17] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: 本研究使用卷积神经网络（CNNs）和表格数据相结合的方法，预测了鸟类在不同气候下的分布情况，准确率达到85%。这为理解鸟类迁徙提供了可靠且可扩展的方法。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化导致许多栖息地发生了迁移，本研究旨在解决准确模拟鸟类在特定栖息地的存在情况的问题。

Method: 使用卷积神经网络（CNNs）和表格数据结合的方法，利用卫星图像和环境特征（如温度、降水量、海拔）预测鸟类在不同气候下的分布。CNN模型捕捉了景观的空间特征，如森林覆盖、水体和城市化，而表格方法则利用生态和地理数据。

Result: 通过结合卷积神经网络（CNNs）和表格数据，成功预测了鸟类在不同气候中的分布情况，准确率达到了85%。

Conclusion: 该研究通过结合卷积神经网络（CNNs）和表格数据的方法，成功模拟了鸟类在特定栖息地的存在情况，预测准确率达到了85%，为理解鸟类迁徙提供了可靠且可扩展的方法。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [18] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec框架是一个个性化锻炼推荐的通用框架，结合了知识追踪和语义表示，以及强化学习方法。研究指出现有锻炼推荐方法忽略了问题语义内容和学生学习进展，因此提出了ExRec框架以弥补这些缺失。通过验证，ExRec框架在在线数学学习中表现出有效性，并具有泛化能力，能够产生可解释的学生学习轨迹。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于现有的锻炼推荐方法在模拟学生表现时缺乏考虑问题的语义内容和学生学习的结构化进展。为了解决这一问题，研究提出了ExRec框架，旨在通过KT引导的RL方法实现有效个性化的推荐。

Method: ExRec框架通过使用知识追踪（KT）模型，结合语义表示和序列化结构化的学生学习进展，提出了一个个性化锻炼推荐的方法。研究中采用了端到端的流水线，包括注释问题的知识组件和学习它们的语义表示，训练KT模型，以及优化多种强化学习方法。此外，通过定制的基于模型的值估计（MVE）方法，对标准的Q学习连续RL方法进行了改进，直接利用KT模型的组件估计累积知识改进。

Result: 通过在在线数学学习中进行验证，研究表明ExRec框架在不同教育目标下具有有效性，并且对于新的、未见问题的泛化能力强，能够产生解释性强的学生学习轨迹。

Conclusion: 该论文介绍了ExRec框架，它是一个个性化锻炼推荐的通用框架，具有基于语义的知识追踪。研究表明，现有的锻炼推荐方法通过知识追踪（KT）模拟学生表现，但通常忽略了两个关键方面：（a）问题的语义内容和（b）学生学习的顺序结构化进展。为了解决这一问题，ExRec提出了一个端到端的流水线，从注释问题的知识组件(KCs)并学习它们的语义表示，到训练KT模型和优化多个强化学习（RL）方法。此外，通过定制的基于模型的值估计（MVE）方法，我们改进了标准的基于Q学习的连续RL方法，直接利用KT模型的组件估计累积知识改进。我们通过在在线数学学习的四个真实任务中使用各种RL方法验证了ExRec的有效性，展示了ExRec对于新的、未见问题的泛化能力，并能产生可解释的学生学习轨迹。综合来看，我们的研究结果凸显了KT引导的RL在教育中实现有效个性化的潜力。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [19] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于视觉语言模型的指挥官方法，用于解决无人地面车辆在自主对抗中的智能知觉到决策推理问题。实验表明，该方法在胜率上表现优异，超过了基准模型。


<details>
  <summary>Details</summary>
Motivation: 传统手工规则方法和当前的强化学习方法在复杂和瞬态的战场环境中变得脆弱，缺乏解释性。因此，提出了这一方法以应对这一挑战。

Method: 采用视觉语言模型进行场景理解和轻量级大型语言模型进行战略推理，实现统一的知觉和决策，具有强大的适应性和可解释性。

Result: 通过模拟和消融实验验证，该方法在胜率上超过了基准模型。

Conclusion: 提出了基于视觉语言模型的指挥官方法，解决了无人地面车辆在自主对抗中智能知觉到决策推理的问题，并获得了超过80%的胜率。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [20] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出了F2STrans方法，通过功能学习和风格学习改进LLMs在代码翻译中的性能。实验证明，该方法显著提高了代码翻译的性能，使Qwen-1.5B在20个不同场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码翻译中取得了显著进展，但确保翻译代码的正确性和可读性仍然是一个挑战，限制了它们在实际软件开发中的有效应用。因此，作者的动机是提出一种方法来解决这一挑战。

Method: 提出了F2STrans方法，包括功能学习和风格学习两个阶段，同时引入了新的代码翻译基准，进行功能和风格评估。

Result: 实验证明，作者的方法明显改善了代码翻译性能，使得Qwen-1.5B在多个场景中表现优于其他大型语言模型。

Conclusion: 作者提出了一种名为F2STrans的新方法，通过功能学习和风格学习两个关键阶段逐步改进LLMs在代码翻译中的性能。实验证明，该方法显著提高了代码翻译的性能，使Qwen-1.5B在20个不同场景中优于Qwen-32B和GPT-4。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [21] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS introduces an AI agent-based decentralized exchange for tokenizing physical gold into a stablecoin. Prototype shows fast token issuance, tight liquidity maintenance, and resilience to attacks. Governance model ensures transparency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Need to bridge physical asset custody with blockchain systems for decentralized trading of real-world alternative assets like gold. Meeting requirements for compliance, liquidity, and risk management.

Method: Utilizing multiple specialized AI agents for automating and securing tokenization and exchange of physical gold into a blockchain-based stablecoin. Combination of on-chain smart contracts for risk controls and off-chain AI agents for decision-making. Evaluation through simulation and controlled pilot deployment.

Result: Prototype achieves fast token issuance, tight liquidity maintenance, and resilience to attacks. Architecture scales to 5000 transactions per second with 10000 concurrent users. Governance model includes multi-signature agent updates and on-chain community voting on risk parameters for transparency and adaptability.

Conclusion: AI agent-based decentralized exchange for alternative assets can meet performance and safety requirements, democratizing access to traditionally illiquid assets. Governance model ensures transparency, adaptability, and system integrity.

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [22] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: 本文提出了神经符号人工智能的形式化定义，将推理定义为逻辑和信念函数的积分计算，抽象了关键神经符号人工智能系统。


<details>
  <summary>Details</summary>
Motivation: 神经符号人工智能领域缺乏对神经符号模型和推理的公认形式化定义，希望提出一个统一的定义。

Method: 引入了神经符号人工智能的形式化定义，定义了神经符号推理为逻辑和信念函数之积的积分计算。

Result: 研究结果表明，提出的神经符号人工智能定义抽象了关键的神经符号人工智能系统。

Conclusion: 提出了神经符号人工智能的形式化定义，将其关键要素抽象为一种推理方式。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [23] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: 该论文提出了一种基于不同质量属性的自主系统决策方法，利用二进制决策图和社会认识论概念，提高了决策的信任度和可靠性，解决了信息冲突的问题，实现了高效的自动推理。


<details>
  <summary>Details</summary>
Motivation: 自主系统在复杂环境中做出决策时面临信息冲突和合作决策等挑战。传统的共识或多数规则不足以解决信任度和可靠性问题，因此提出了基于信任度和质量属性的合作决策方法。

Method: 利用自主系统的质量属性和社会认识论概念定义决策规则，采用二进制决策图作为形式化模型，并制定规约规则以实现高效的合作自动推理。

Result: 通过引入质量属性和社会认识论概念，提出的方法在自主系统决策中提高了信任度和可靠性，有效地处理了信息冲突。利用二进制决策图等结构实现了高效的合作自动推理。

Conclusion: 提出了一种基于信任度和可靠性的智能系统决策方法。使用不同质量属性的自主系统进行决策，利用社会认识论概念定义决策规则，并结合二进制决策图进行形式化建模和推理。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [24] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: The paper proposes using Answer Set Programming (ASP) to accurately compute the actual maximum delay in integrated circuits, addressing performance optimization challenges in hardware design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of relying on upper bounds computed through Static Timing Analysis in hardware design, which may lead to suboptimal processor speeds and missed performance opportunities.

Method: The paper models the problem of computing the actual maximum delay using Answer Set Programming (ASP), a logic language with efficient solvers, and provides non-trivial encodings of the problem into ASP.

Result: Experimental results demonstrate that ASP is an effective approach for solving complex problems in hardware design, specifically in computing the actual maximum delay in integrated circuits.

Conclusion: Answer Set Programming (ASP) is proposed as a solution to compute the actual maximum delay in integrated circuits, aiming to address performance optimization challenges in hardware design.

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [25] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph is a novel KG reasoning mechanism that tackles score over-smoothing by segregating local and global information processing and introducing a coarse-to-fine optimization. It achieves state-of-the-art performance with significant improvements in reasoning quality and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing KG reasoning methods suffer from score over-smoothing, which blurs the distinction between correct and incorrect answers, hindering reasoning effectiveness. DuetGraph aims to overcome this issue by separating the processing of local and global information and optimizing entity partitions to improve reasoning quality.

Method: DuetGraph proposes a coarse-to-fine KG reasoning mechanism with dual-pathway global-local fusion. It segregates the processing of local and global information into two distinct pathways and introduces a coarse-to-fine optimization to alleviate over-smoothing and enhance inference quality.

Result: Extensive experiments on various datasets show that DuetGraph achieves state-of-the-art performance, with up to an 8.7% improvement in reasoning quality and a 1.8x acceleration in training efficiency.

Conclusion: DuetGraph is a new KG reasoning mechanism that addresses the issue of score over-smoothing and achieves state-of-the-art performance with significant improvements in reasoning quality and training efficiency.

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [26] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: AgentOps introduces a framework for observing, analyzing, optimizing, and automating the operation of agentic AI systems to manage uncertainty from Large Language Models. It outlines a six-stage process and highlights the role of automation in ensuring safe and effective operation of AI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the inadequacy of traditional software observability and operations practices in handling the uncertainty introduced by agentic AI systems powered by Large Language Models. It aims to address the unique forms of uncertainty arising from probabilistic reasoning, evolving memory states, and fluid execution paths.

Method: The paper presents the AgentOps Automation Pipeline, a six-stage process that includes behavior observation, metric collection, issue detection, root cause analysis, optimized recommendations, and runtime automation. It identifies the distinct needs of developers, testers, site reliability engineers (SREs), and business users in engaging with agentic AI systems at different points in their lifecycle.

Result: The result of the paper is the introduction of the AgentOps framework, which emphasizes automation as a crucial element in managing uncertainty and enabling self-improving AI systems for safe and adaptive operation within agentic systems.

Conclusion: AgentOps introduces a comprehensive framework for observing, analyzing, optimizing, and automating the operation of agentic AI systems. It addresses the challenges posed by uncertainty in Large Language Models (LLMs) and emphasizes the critical role of automation in managing uncertainty for safe and effective operation.

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [27] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: 该论文介绍了Opus Prompt Intention Framework，通过引入中间意图捕获层，提高了LLMs在复杂查询条件下生成逻辑和有意义的输出的能力。实验结果表明，应用该框架可以显著改进工作流生成质量，特别是在多意图查询的情况下。


<details>
  <summary>Details</summary>
Motivation: 目的是改进复杂的工作流生成过程，特别是在混合意图引出的情况下，通过引入Opus Prompt Intention Framework和中间意图捕获层，提高工作流生成的质量。

Method: 提出Opus Prompt Intention Framework，介绍Workflow Signal和Workflow Intention的概念，并从用户查询中提取工作流信号和工作流意图。通过LLMs驱动的工作流生成，构建可重现、可定制的LLM-based Intention Capture系统。

Result: 结果显示，Opus Prompt Intention Framework和中间意图捕获层显著改善了工作流生成的质量，特别是在混合意图引出的情况下。

Conclusion: 该论文介绍了Opus Prompt Intention Framework，旨在通过指令调整的大型语言模型（LLMs）来改进复杂的工作流生成。引入了介于用户查询和工作流生成之间的中间意图捕获层，实现Opus工作流意图框架，从用户查询中提取工作流信号，将其解释为结构化的工作流意图对象，并根据这些意图生成工作流。实验结果表明，该层使LLMs能够在查询复杂性增加时产生逻辑和有意义的输出。在包含1,000个多意图查询-工作流对的合成基准测试中，将Opus Prompt Intention Framework应用于工作流生成，在语义工作流相似性度量中产生一致的改进。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [28] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: 本文介绍了如何利用EW-QBAFs支持可争议的人工智能决策，提出了G-RAEs来解决边缘权重调整问题，并发展了迭代算法，实验证明方法的有效性。


<details>
  <summary>Details</summary>
Motivation: AI驱动的决策需要与人类偏好一致，探索如何利用EW-QBAFs来支持可争议性是本研究的动机。目的是解决如何调整边缘权重以提高特定论点的强度，从而支持可争议性。

Method: 引入竞争性问题来探讨如何调整边缘权重以达到期望的强度，提出了基于梯度的关系归因解释（G-RAEs）并发展了迭代算法来逐步调整边缘权重，通过在合成EW-QBAFs上的实验评估展示了方法的有效性。

Result: 通过实验验证了基于梯度的关系归因解释（G-RAEs）和迭代算法在解决边缘权重调整问题方面的有效性。

Conclusion: 本文介绍了如何利用Edge-Weighted Quantitative Bipolar Argumentation Frameworks（EW-QBAFs）来支持可争议的人工智能决策，通过引入竞争性问题和基于梯度的关系归因解释（G-RAEs）来解决边缘权重调整问题。通过实验验证，展示了该方法可以有效解决问题。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [29] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN is a VLM-based framework that mimics human cognitive processes and integrates fast and slow thinking systems for demand-driven navigation. It outperforms traditional data-driven methods by 15% in navigation accuracy and adaptability in unknown environments.


<details>
  <summary>Details</summary>
Motivation: Mobile robots need to navigate and interact in unknown environments based on human intent. Traditional data-driven DDN methods lack generalization in unseen scenarios. CogDDN aims to address this limitation by mimicking human cognitive processes.

Method: The paper proposes CogDDN, a VLM-based framework that emulates human cognitive and learning mechanisms, incorporating fast and slow thinking systems, selective object identification, and dual-process decision-making. It also includes Chain of Thought (CoT) reasoning to enhance the decision-making process.

Result: CogDDN, evaluated on the AI2Thor simulator with the ProcThor dataset, outperforms single-view camera-only methods by 15%, demonstrating improved navigation accuracy and adaptability.

Conclusion: CogDDN, a VLM-based framework, outperforms traditional data-driven DDN methods by integrating fast and slow thinking systems, selecting key objects based on human intent, and using dual-process decision-making. It improves navigation accuracy and adaptability by 15% in unknown environments.

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [30] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: 该研究介绍了一种神经符号框架，结合自然语言对话的易用性和目标解释的可验证保证。通过在100个经过不确定性过滤的示例上微调的轻量级模型，研究实现了超越GPT-4.1的零-shot性能，并减少了推断延迟。这为复杂物流中可证明、实时和符合用户的决策提供了实际途径。


<details>
  <summary>Details</summary>
Motivation: 物流运营商在面对生命关键决策时需要领域专业知识和快速连续的重规划。传统方法慢且假设理想化数学模型，无法处理不确定性。大型语言模型（LLMs）可以处理不确定性，但容易出现误解和幻觉，危及安全与成本。因此，研究旨在提出一种既能处理不确定性又能确保目标解释准确性的框架。

Method: 该研究引入了神经符号框架，利用自然语言对话将用户请求转换为结构化规划规范。在不确定性过滤的示例上进行微调的轻量级模型表现出良好的性能，超过了GPT-4.1的零-shot表现，并且推断延迟减少了近50%。此框架还采取交互式澄清循环以处理不确定性情况，以确保对目标的正确解释。

Result: 研究通过引入神经符号框架，表明了在复杂物流中实现可证明、实时和用户对齐的决策的实际途径。通过微调轻量级模型，实现了超越GPT-4.1零-shot性能并且减少推断延迟。

Conclusion: 该研究介绍了一种神经符号框架，将自然语言对话的易用性与对目标解释的可验证保证相结合。它将用户请求转换为结构化的规划规范，量化其在字段和标记级别的不确定性，并在信心低于自适应阈值时调用交互式澄清循环。研究表明，仅在100个经过不确定性过滤的示例上进行微调的轻量级模型，超过了GPT-4.1的零-shot性能，同时将推断延迟减少了近50%。这些初步结果突显了在复杂物流中实现可证明、实时和符合用户的决策的实际途径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [31] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 该论文介绍了一种新方法，通过结合文本和结构化数据的优势提高了代码建模的推理能力，弥补了现代LLM在分析代码结构、控制和数据流等方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 以往的方法在使用结构化数据和图神经网络进行建模时，存在生成能力和规模方面的不足。因此，本研究的动机是解决这一问题，结合现代LLM的规模和生成能力。

Method: 该论文采用了结合文本和结构化数据的方法，旨在提高对代码的推理能力。

Result: 通过该研究，可以更好地理解代码的结构化、分析特性，从而提高对代码的建模和理解能力。

Conclusion: 该论文介绍了一种新颖的方法，结合了将代码建模为文本和更结构化形式的优势。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [32] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI systems that 'think' in human language provide an opportunity for AI safety by monitoring their chains of thought for potential misbehavior. CoT monitoring, although imperfect, shows promise, and further research and investment in this area are recommended. Developers should consider the impact of their decisions on CoT monitorability.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to explore the potential of monitoring AI systems' chains of thought in human language to enhance AI safety. It aims to address the imperfections in current AI oversight methods and suggests CoT monitoring as a promising approach.

Method: The paper proposes monitoring the chains of thought (CoT) in AI systems that think in human language to identify the intent to misbehave. It acknowledges the imperfections in CoT monitoring but recommends further research and investment in this method alongside existing safety measures.

Result: The results suggest that monitoring chains of thought in AI systems that 'think' in human language can be beneficial for enhancing AI safety. Further research and investment in CoT monitoring are recommended, with developers urged to consider the impact of their decisions on CoT monitorability.

Conclusion: AI systems that 'think' in human language provide an opportunity for AI safety by monitoring their chains of thought for the intent to misbehave. CoT monitoring, although imperfect, shows promise and suggests further research and investment in this area. Developers should consider the impact of development decisions on CoT monitorability.

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [33] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR introduces Perspective-Aware AI in Extended Reality, using identity models from digital footprints to enhance immersive experiences. It integrates AI with XR to provide context-aware experiences based on user identity, demonstrating its utility through Unity-based scenarios.


<details>
  <summary>Details</summary>
Motivation: Current Extended Reality systems lack deep user modeling and cognitive context, hindering adaptive and immersive experiences. PAiR aims to address this limitation by integrating Perspective-Aware AI with XR to ground experiences in user identity.

Method: Introducing Perspective-Aware AI in Extended Reality (PAiR) as a foundational framework, utilizing reasoning-ready identity models learned from digital footprints to enable adaptive experiences. PAiR links dynamic user states with immersive environments in a closed-loop system.

Result: PAiR's architecture and system flow are detailed, and its utility is demonstrated through proof-of-concept scenarios in the Unity-based OpenDome engine, showcasing its potential in human-AI interaction within immersive systems.

Conclusion: PAiR framework integrates Perspective-Aware AI with Extended Reality to provide interpretable, context-aware experiences based on user identity, leading to a new direction for human-AI interaction in immersive systems.

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [34] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: 本文提出了一个受开放性进化理论启发的框架，重新审视了强化学习中有关代理定义、学习目标和奖励假设范围的核心概念。提出进化动力学可以在生物大脑中运作的观点，并强调奖励假设的限制以及代理概念缺乏正规解释的问题。主张整合生命起源理论观念，为理解生物系统中代理和资源受限强化学习提供基础。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的代理定义、学习目标和奖励假设是重要的概念，有待进行概念性修订。本文旨在重新审视这些核心概念，提出一个框架以挑战现有“教条”，并探讨进化动力学在生物学习模型中的应用。

Method: 提出一个框架，从开放性进化理论提取灵感，重新审视强化学习中的三个“教条”。通过重新审视每一假设，将理论与应用联系起来，探讨如何将进化动力学应用到生物学习模型中。

Result: 提出了一个新的框架，利用进化理论思想重新审视强化学习中的核心概念。通过对三个“教条”的重新审视和相关问题的讨论，为强化学习作为生物学习模型的进一步研究提供了理论基础。

Conclusion: 重点关注强化学习中有关代理定义、学习目标和奖励假设范围的三个核心原则，提出启发自开放性进化理论的框架以重新审视这三个“教条”。通过重新审视每一假设并解决相关问题，将论点与强化学习作为生物学习模型联系起来，阐明个体生命期内进化动力学可能在生物大脑中运作，而不限于代际过程。重点关注第二个“教条”，从进化的启示出发，丰富“适应而非搜索”视图。针对奖励假设的限制，运用进化适应性类比阐明标量奖励与多目标辩论。讨论对强化学习中探索的实际影响后，处理第一个问题，即代理范式缺乏正规解释。认为进化范式单独无法解决代理问题，但指向一个具有建设性方向。主张整合生命起源理论观念，其中维持和复制的热力学提供理解生物系统中代理和资源受限强化学习的有益基础。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [35] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: 该论文提出了名为DrafterBench的基准测试工具，旨在评估LLM代理在土木工程技术图纸修订中的表现。该工具包含大量任务和功能，能全面评估AI代理的能力，并提供了任务准确性和错误统计的详细分析。作者提供了工具的GitHub链接以及测试集的托管地址。


<details>
  <summary>Details</summary>
Motivation: 当前，LLM代理在解决实际问题中表现出巨大潜力，并有望成为工业任务自动化的解决方案。然而，需要更多基准测试来系统评估工业视角下的自动化代理，在土木工程中的代表性任务也需要被评估。因此，作者提出了DrafterBench，旨在全面评估LLM代理在技术图纸修订中的表现。

Method: 提出了DrafterBench作为基准测试工具，通过包含真实绘图文件任务、定制功能/工具和各种任务类型来评估LLM代理在土木工程技术图纸修订中的表现。基准测试工具还评估了AI代理在多个方面的能力，并提供了任务准确性和错误统计的详细分析。

Result: DrafterBench工具包含大量任务和功能，能全面评估AI代理的能力，并为工程应用中整合LLM提供改进目标。作者还提供了工具的GitHub链接以及测试集的托管地址。

Conclusion: 该论文提出了一个名为DrafterBench的基准测试工具，旨在全面评估LLM代理在土木工程技术图纸修订中的表现。DrafterBench包含12种任务类型，总共涵盖了来自真实绘图文件的任务，拥有46个定制功能/工具和1920个任务。该工具箱全面评估了AI代理在结构化数据理解、功能执行、指令遵循和批判性推理方面的能力。该基准测试提供了任务准确性和错误统计的详细分析，旨在为工程应用中整合LLM提供更深入的见解和改进目标。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [36] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: 本研究引入IFScale基准测试，评估了20个最先进模型在500个关键词包含指令的业务报告撰写任务上的表现。发现指令密度增加时模型的指令遵循表现逐渐降级，最佳模型的准确率仅为68%。研究结果揭示了模型大小和推理能力与性能降级模式、偏向早期指令以及指令遵循错误的关联，并可为设计指令密集提示提供实用信息。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅在单个或少量指令任务上评估模型，针对LLM系统中高指令密度的指令遵循能力尚未得到充分表征。作者的动机在于通过IFScale基准测试来衡量指令密度增加时模型的指令遵循表现，以填补这一研究空白。

Method: 本研究引入了IFScale基准测试，用于评估20个最先进模型在500个关键词包含指令的业务报告撰写任务上的表现。作者研究了指令密度增加时指令遵循表现的降级情况，发现模型大小和推理能力与性能降级模式、指向早期指令的偏向以及指令遵循错误的不同类别相关。

Result: 研究发现即使在最大指令密度下，最好的模型也仅达到68%的准确率。模型大小和推理能力与性能降级模式、指向早期指令的偏向以及指令遵循错误的不同类别相关。

Conclusion: 本研究介绍了IFScale，这是一个简单的基准测试，用于衡量指令密度增加时指令遵循表现的降级情况。作者评估了来自七家主要提供商的20个最先进模型，发现即使是最好的前沿模型在500个指令的最大密度下也只能达到68%的准确率。他们的分析揭示了模型大小和推理能力与三种不同的性能降级模式、对早期指令的偏向以及指令遵循错误的不同类别之间的相关性。研究结果可帮助设计现实世界应用中指令密集提示的方法，并突出了重要的性能-延迟权衡。作者开放源代码了基准测试和所有结果，以供进一步分析。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>
