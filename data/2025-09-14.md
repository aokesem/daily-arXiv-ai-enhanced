<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 贝叶斯推断广泛应用于不同领域，但通常假定需要精确输入以产生精确输出，这在实际应用中不切实际。本文通过将SME提供的区间估计信息转换为IT2模糊成员函数，扩展了IT2版本的贝叶斯定理。引入了一种新颖且保守的方法来避免潜在的不一致性，并提出了一种灵活的算法用于处理区间信息。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断在许多不同领域广泛应用于检验假设与观察数据的一致性。但是，现实应用中往往无法给出精确的输入值，本文旨在解决这一问题。通过将SME提供的区间范围估计转换为IT2模糊成员函数，扩展了贝叶斯定理，使得在输入概率方面更具灵活性。

Method: 本文使用了新颖且保守的方法开发了IT2版本的贝叶斯定理，避免了可能导致无效输出结果的输入IT2模糊成员函数的潜在不一致性。同时描述了一种灵活的算法，将SME提供的区间转换为IT2模糊成员函数，用于指定贝叶斯定理中的输入概率。该算法在解决将区间编码成词模糊成员函数方面进行了推广和扩展。

Result: 本文的关键成果是成功将贝叶斯定理扩展到区间模糊类型-2（IT2）版本，并提出了一种新颖且保守的方法来避免潜在的不一致性，同时描述了一种灵活的算法用于编码SME提供的区间信息。

Conclusion: 本文提出了一种将贝叶斯定理扩展到区间模糊类型-2（IT2）版本的方法，通过开发了一种新颖而保守的方法避免了潜在的不一致性，描述了一种灵活的算法来将SME提供的区间转换为IT2模糊成员函数，以用于指定贝叶斯定理中的输入概率。本算法推广和扩展了先前研究，主要解决了将区间编码成词模糊成员函数的问题。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [2] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖的框架，通过NLP和多模态LLMs将游戏设计文档转换为功能性Unity游戏原型。他们的方法结合了经过优化的LLaMA-3模型和自定义的Unity集成包，实现了GDD解析、游戏规范提取和Unity兼容代码合成的端到端系统。评估结果显示，他们的模型在多个方面比基线模型有显著改进，平均得分为4.8/5.0。生成的模板符合GDD规范，填补了AI辅助游戏开发的空白，将LLMs定位为有价值工具。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于填补AI辅助游戏开发中的关键空白，并探讨如何将自然语言处理和大型语言模型应用于游戏设计文档的转换和游戏原型生成。他们旨在提高模型在Unity代码生成方面的性能，并简化从游戏设计到实施的转变过程。

Method: 该论文提出了一种端到端系统，解析GDDs，提取结构化游戏规范，并合成Unity兼容的C#代码。他们的方法结合了针对Unity代码生成的经过优化的LLaMA-3模型和自定义的Unity集成包，简化了实施过程。评估结果显示，他们的经过优化模型在多个方面表现出卓越性能，包括编译成功率、GDD符合度、最佳实践采纳和代码模块化度量。

Result: 评估结果表明，该论文提出的方法在多个方面取得了显著改进，生成的模板符合GDD规范，提高了Unity代码生成的效率和质量。论文的经过优化模型比当前先进的大型语言模型表现更优异。

Conclusion: 该论文提出了一种通过自然语言处理（NLP）和多模态大型语言模型（LLMs）将游戏设计文档转换为功能性Unity游戏原型的新型框架。他们的方法结合了针对Unity代码生成的经过优化的LLaMA-3模型和自定义的Unity集成包，实现了从设计文档中提取结构化游戏规范并生成实现核心机制、系统和架构的Unity兼容C#代码。评估结果表明，他们的经过优化模型在编译成功、GDD符合度、最佳实践采纳和代码模块化指标等方面明显优于基准模型，平均得分为4.8/5.0。生成的模板在多种游戏类型中高度符合GDD规范，有效填补了AI辅助游戏开发中的关键空白，将LLMs定位为在游戏设计与实现之间简化过渡的有价值工具。

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [3] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个框架，通过多个大型语言模型代理分解建模任务并整合成完整MiniZinc模型，初步实验显示性能优越，提出了未来工作的全面路线图。


<details>
  <summary>Details</summary>
Motivation: 优化或满足问题的自然语言描述很难正确翻译为准确的MiniZinc模型，因为这个过程需要逻辑推理和约束编程专业知识。为了应对这一挑战，引入了一个以自主方法为基础的框架，通过大型语言模型代理分解建模任务，以减少总体复杂性。

Method: 采用自主方法进行建模任务的分解，每个大型语言模型代理专注于检测和生成特定类别的全局约束代码，然后由一个组装代理将这些约束片段整合成完整的MiniZinc模型。通过将问题分解为更小、定义明确的子任务，每个大型语言模型处理相对简单的推理挑战，可能减少总体复杂性。

Result: 初步实验表明该框架在性能上优于基准方法，同时提出了未来工作的全面路线图，重点突出潜在增强和改进方向。

Conclusion: 引入了一个以自主方法为基础的框架，通过多个专门化的大型语言模型代理进行建模任务的分解，最终组装成完整的MiniZinc模型，初步实验表明相对于单次提示和链式思维提示等基准方法，性能更佳。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [4] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: This paper addresses the challenge of model collapse in generative AI models using synthetic data by introducing a confidence-aware loss function called Truncated Cross Entropy (TCE). The method significantly delays model collapse, extending the model's fidelity interval by more than 2.3x, and generalizes across different modalities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenge of model collapse in generative AI models due to repeated training in synthetic data. The paper identifies model overconfidence as a key factor leading to collapse and aims to extend the model's fidelity interval before collapse.

Method: The paper proposes a confidence-aware loss function called Truncated Cross Entropy (TCE) to address model overconfidence in self-generated data. It provides a model-agnostic framework linking loss function design to model collapse mitigation.

Result: The proposed Truncated Cross Entropy (TCE) loss function significantly delays model collapse in recursive training, extending the model's fidelity interval by more than 2.3x. The method is validated both theoretically and empirically, showing generalization across modalities.

Conclusion: Loss function design plays a crucial role in mitigating model collapse in generative AI models using synthetic data.

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [5] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文讨论了可解释人工智能（XAI）中的不确定性解释和全局解释的重要性。选取了一种算法，涵盖不确定性、稳健性和全局可解释性概念，并检验其对信任的校准能力。同时，验证了一种较难理解但能提供直观视觉理解的算法是否能带来更高的用户满意度和人类可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然某些XAI领域已经研究透彻，但本文关注不确定性解释和全局解释，这些方面经常被忽视。许多研究者的主要努力是制定XAI方案的通用指南。

Method: 选择算法，涵盖不确定性、稳健性和全局可解释性概念，并测试其能力以校准信任。

Result: 研究得出从研究中得出的通用指南。

Conclusion: 提出了一种涵盖不确定性、稳健性和全局可解释性概念的算法，以验证其对信任的校准能力。检验了一种旨在提供更直观视觉理解的算法，尽管较难理解，但是否能提供更高用户满意度和人类可解释性。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [6] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文介绍了一种基于上下文条件的提示公式方法，用于优化几乎没有历史行为信息的冷启动用户在推荐任务中使用的大型语言模型。通过实证实验，发现在低数据设置中，通过最佳示例注入和指导结构化，可以显著提高这类模型在precision@k和NDCG分数方面的性能。研究结果表明，通过提示调整可以解决基于LLM的管道中冷启动推荐问题。


<details>
  <summary>Details</summary>
Motivation: 鉴于冷启动用户问题严重影响推荐系统的有效性，本文旨在解决这一问题并提高模型性能。

Method: 使用基于变压器的自回归LLM（BioGPT，LLaMA-2，GPT-4）进行系统实验，展示最佳示例注入和指导结构化如何改善模型性能。提出了一种上下文条件的提示公式方法，用于优化冷启动用户在推荐任务中使用的大型语言模型。

Result: 经验证实最佳示例注入和指导结构化可以显著提高模型性能，提示调整是解决基于LLM的冷启动推荐问题的一种有效方式。

Conclusion: 本文介绍了一种基于上下文条件的提示公式方法，用于优化几乎没有历史行为信息的冷启动用户在推荐任务中使用的大型语言模型。通过实证实验，发现在低数据设置中，通过最佳示例注入和指导结构化，可以显著提高这类模型在precision@k和NDCG分数方面的性能。研究结果表明，通过提示调整可以解决基于LLM的管道中冷启动推荐问题。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [7] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 本研究比较了人类、大型语言模型（LLMs）和贝叶斯代理在动态协商环境中的表现，发现贝叶斯代理通过积极优化获得最高剩余价值，但频繁拒绝交易；人类和LLMs可以实现类似的总体剩余价值，但通过不同的行为方式。性能平价可能掩盖了在过程和协调中的基本差异。


<details>
  <summary>Details</summary>
Motivation: 随着协调任务越来越被自主代理执行，评估这些代理的绩效和协商过程变得至关重要。不同代理展现不同优势：传统统计代理（如贝叶斯模型）在确定条件下表现出色，而大型语言模型能够跨领域泛化。因此，比较人类、LLMs和贝叶斯代理的表现以及其在动态、多代理环境中的协商过程对于深入了解代理行为具有重要意义。

Method: 通过在动态协商环境中比较人类、LLMs和贝叶斯代理的表现，采用直接、相同条件的比较方法，捕捉结果和行为动态。贝叶斯代理通过积极优化获得最高剩余价值，但频繁拒绝交易；人类和LLMs通过不同行为方式实现类似的总体剩余价值：LLMs更倾向于保守、让步的交易，人类则采用更具战略性、冒险性和关注公平性的行为。

Result: 贝叶斯代理在动态协商环境中通过积极优化获得最高剩余价值，但频繁拒绝交易；人类和LLMs可以通过不同行为方式实现类似的总体剩余价值：LLMs更倾向于保守、让步的交易，人类则采用更具战略性、冒险性和关注公平性的行为。性能平价可能掩盖了在过程和协调中的基本差异。

Conclusion: 本研究比较了人类、大型语言模型（LLMs）以及贝叶斯代理在动态协商设置中的表现，发现虽然贝叶斯代理通过积极优化提取了最高的剩余价值，但频繁拒绝交易；人类和LLMs在整体剩余价值上可以实现类似的表现，但通过不同的行为方式：LLMs更倾向于保守、让步的交易并很少遭到拒绝，而人类则采用更具战略性、冒险性和关注公平性的行为。性能平价可能掩盖了在过程和协调中的基本差异，这些差异对于在真实世界的协调任务中的实际部署至关重要。

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [8] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文使用机器学习技术开发了一种系统全面的管道来识别高风险银行客户，取得了第二名的比赛成绩。管道具有高AUROC均值和较低的标准偏差，能够有效帮助金融机构改善反洗钱行动的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 文章针对金融机构对反洗钱（AML）行动和措施的紧迫性，利用机器学习技术提高AML行动的效率和准确性。该研究旨在提出一种方法来识别高风险银行客户，在比赛中实现好成绩。

Method: 本文采用机器学习方法，通过16个步骤设计和统计分析构建了识别高风险银行客户的ML管道。使用SQL数据库存储数据，开发了SQL-based特征工程算法，连接预训练模型到数据库，提供了XAI模块以解释特征重要性。

Result: 提出的ML管道在比赛中取得了第二名的成绩，具有高AUROC均值和较低的标准偏差。该管道能够帮助金融机构识别高风险客户，提高反洗钱行动的效率。

Conclusion: 本文提出了一种系统全面的机器学习（ML）管道方法，用于识别数据集中的高风险银行客户，并在多项任务比赛中取得了第二名的成绩。提出的管道通过16个步骤的设计和统计分析确保了鲁棒性，采用了SQL数据库来存储数据，并开发了基于SQL的特征工程算法。最终管道实现了AUROC均值为0.961，标准偏差为0.005，同时提供了可解释人工智能（XAI）模块来推导特征重要性。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [9] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 本文探讨了人工智能空间推理能力的限制和人类空间智能的优势，提出了基于神经科学原则的计算框架来拓展Agentic空间推理能力。通过研究神经模型和计算框架，分析最新方法，识别发展中的关键缺陷，探讨新兴基准和数据集，探索潜在应用领域，并提出未来研究方向。希望为研究社区提供新的视角和结构化发展路径。


<details>
  <summary>Details</summary>
Motivation: 本文概述了目前人工智能在空间推理方面的限制，并指出了人类空间智能在非结构化环境中的优势。作者认为填补这一差距对于推动Agentic空间智能向着更好地与物理三维世界互动至关重要。因此，为了解决这一问题，提出了基于神经科学原则的计算框架，以拓展Agentic空间推理能力。

Method: 本文通过研究计算神经科学领域的空间神经模型，提出了一种基于神经科学原则的计算框架，并将核心生物功能映射到六个基本的计算模块：受生物启发的多模态感知、多感觉整合、以我为中心-以场地为中心的转换、人工认知地图、空间记忆和空间推理。通过这些模块，构建了一个适用于虚拟和物理环境中Agentic空间推理能力的视角框架。

Result: 本文提出的基于神经科学原则的计算框架为Agentic空间推理能力提供了新的视角，并通过分析最新方法和识别关键缺陷，为发展更加符合神经科学基础的空间推理模块提供了指导。此外，研究还探讨了新兴基准和数据集以及未来可能的应用领域，如机器人技术。最后，提出了强调泛化空间推理能力的潜在研究方向。

Conclusion: 本文探讨了人工智能在空间推理能力方面的限制以及人类空间智能的优势，提出了一种基于神经科学原理的计算框架以弥补这一差距。研究了空间神经模型，并介绍了六个关键的计算模块，为Agentic空间推理能力提供了新的视角。通过对最新方法进行分析和评估，发现了当前空间推理模块发展中的关键缺陷。讨论了新兴基准和数据集，探讨了从虚拟到实体系统（如机器人）等潜在应用领域。最后，提出了潜在的研究方向，强调了跨动态或非结构化环境中空间推理泛化的有前景的发展路线。希望这项工作能够为研究社区提供一个基于神经科学的视角和结构化的发展路径。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [10] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 本文提出了ProgD渐进多尺度解码策略，结合动态异质图模型，以全面捕捉未来社交互动的演变性质。实验证明，该方法在多智能体预测基准和多世界预测基准上取得了最佳表现，排名第一。


<details>
  <summary>Details</summary>
Motivation: 准确预测周围智能体的运动对自动驾驶车辆的安全规划至关重要。虽然近年来的研究已经将预测技术从单个智能体扩展到多个相互作用智能体的联合预测，但这些方法忽视了互动性质的演变。因此，为了解决这一局限性，本文提出了ProgD策略，通过动态异质图模型来逐步捕捉未来场景中的社交互动，以提高智能体未来运动的预测效果。

Method: 本文使用了渐进多尺度解码策略ProgD以及动态异质图来建模未来场景中的社交互动，设计了分解架构来处理未来场景中的时空依赖关系，并逐步消除多个智能体未来动作的不确定性。此外，还引入了多尺度解码过程，以改善未来场景建模和智能体未来动作的一致预测。

Result: 在实验中，所提出的ProgD方法在INTERACTION多智能体预测基准和Argoverse 2多世界预测基准上取得了最先进的表现，排名第一。该方法能够全面捕捉未来社交互动，提高未来场景建模的准确性和智能体未来运动的一致性预测。

Conclusion: 本文提出了一种名为ProgD的新型渐进多尺度解码策略，通过动态异质图模型来解决传统方法中忽视的未来互动的演变性质，实现了对未来场景中的社交互动的全面捕捉。最终，该方法在INTERACTION多智能体预测基准和Argoverse 2多世界预测基准上取得了表现优异的成绩，排名第一。

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [11] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 本文提出了一个基于区块链的分层架构，用于解决大规模代理生态系统中的监管和问责挑战，包括代理追踪、声誉评估和恶意行为预测模块。该方法建立了一个可信、弹性和可扩展的监管机制，为未来研究方向提供了参考。


<details>
  <summary>Details</summary>
Motivation: 本文针对大规模代理生态系统中的监管和问责挑战，提出了基于区块链的解决方案。由于代理的不可预测行为和异构能力，传统监管方式面临挑战，因此需要新的方法来确保可信、弹性和可扩展的监管机制。

Method: 本文使用基于区块链的分层架构，设计了三个关键模块，以解决大规模代理生态系统中监管和问责方面的挑战。通过代理行为追踪和仲裁、动态声誉评估、恶意行为预测等模块，建立了可信、弹性和可扩展的监管机制。

Result: 通过提出的区块链-enabled分层架构和关键模块，建立了一个系统的监管框架，能够应对大规模代理生态系统中的监管和问责挑战。这为未来研究提供了一个可靠的基础。

Conclusion: 本文提出了一个基于区块链的分层架构，用于监管代理之间的协作，包括代理层、区块链数据层和监管应用层。设计了三个关键模块：（i）代理行为追踪和仲裁模块，用于自动问责；（ii）动态声誉评估模块，用于信任评估；（iii）恶意行为预测模块，用于早期检测对抗性活动。该方法建立了一个系统性基础，以确保大规模代理生态系统中的可信、弹性和可扩展的监管机制。最后讨论了区块链-enabled监管框架在多代理系统中的未来研究方向。

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [12] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 本研究针对现有大型语言模型在多步推理和工具使用方面的困难，提出了一种从Jupyter笔记本中提取数据分析任务和解决方案的可扩展流程，并引入了NbQA数据集。通过开发Jupiter框架，将数据分析问题转化为搜索问题并应用MCTS算法生成多样化解决路径，以提高多步推理能力。实验结果显示在NbQA上取得了显著成果，提出的模型在多步推理任务上的表现达到或超越了其他先进模型和框架。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂数据分析任务中仍然面临多步推理和工具使用方面的困难，限制了它们在复杂数据分析任务上的有效性。因此，为了提高这一问题，研究提出了一种方法来解决这一挑战，并希望通过新的模型和框架实现更好的性能。

Method: 通过提出可扩展的流程从Jupyter笔记本中提取数据分析任务和解决方案，引入了NbQA数据集并开发了Jupiter框架，将数据分析问题转化为搜索问题并应用MCTS算法生成多样化解决路径，结合价值模型和节点访问计数以高效收集可执行的多步计划。

Result: 提出的方法在NbQA数据集上取得了显著的成果，Qwen2.5-7B和14B-Instruct模型的表现分别匹配或超越了GPT-4o和高级代理框架。进一步评估表明该方法在各种多步推理任务中表现出改进的泛化能力和更强大的工具使用推理能力。

Conclusion: 该研究提出了一种可扩展的流程，从现实世界的Jupyter笔记本和相关数据文件中提取基于工具的高质量数据分析任务及其可执行的多步解决方案。引入了NbQA数据集，展示了在实际数据科学场景中反映真实工具使用模式的标准化任务-解决方案对。通过Jupiter框架进一步增强了多步推理能力，将数据分析构建为搜索问题，应用Monte Carlo Tree Search（MCTS）生成用于价值模型学习的多样化解决路径。在推断过程中，Jupiter结合价值模型和节点访问计数，以最小的搜索步骤高效收集可执行的多步计划。实验结果表明，在NbQA上，Qwen2.5-7B和14B-Instruct模型解决了InfiAgent-DABench上77.82%和86.38%的任务，分别与GPT-4o和高级代理框架的性能相匹配或超越。进一步的评估表明在各种多步推理任务中，提供了改进的泛化能力和更强大的工具使用推理能力。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [13] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 本文通过比较研究了三种不同的方法来构建知识图谱三元组并与大型语言模型（LLMs）集成，用于问答系统。实验结果显示OpenIE提供了最全面的三元组覆盖，而GraphRAG在推理能力上表现最佳。最后讨论了每种方法的优势和局限性，并提出了未来改进知识图谱问答系统的方向。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法擅长从简洁文本中提取基于事实和局部上下文的信息，但在处理复杂、广泛文本时遇到限制，需要更深入分析文本和上下文的主题和整体理解。知识图谱是通过关系三元组结构化信息的强大工具，最近成为增强问答系统的新领头羊。

Method: 本文提出了三种不同构建知识图谱三元组并将其与大型语言模型（LLMs）集成以进行问答的方法：spaCy、Stanford CoreNLP-OpenIE和GraphRAG，均利用开源技术。作者通过分析它们的能力、发展状态以及它们对LLM问答性能的影响，对这些方法的有效性、可行性和适应性进行了全面技术比较研究。

Result: 作者评估了三种方法的有效性、可行性和适应性，并得出结论认为OpenIE提供了最全面的三元组覆盖，而GraphRAG在三者中表现出色。实验结果显示GraphRAG在推理能力上优于其他两种方法。

Conclusion: 实验结果表明OpenIE提供了最全面的三元组覆盖，GraphRAG在三种方法中展示了出色的推理能力。我们得出了对每种方法的优势和局限性的讨论，并提供了改进基于知识图谱的问答系统未来方向的见解。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [14] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 本文探讨了如何利用MCTS-derived trajectories改进基于偏好的强化学习中的策略优化。提出了一种分段GRPO训练方法，引入了基于树结构的优势估计设置，分析了新的奖励信号类别，并提出了解决挑战的方法。初步结果表明结构化优势估计可以稳定更新，但仍存在挑战和提出的解决方案。


<details>
  <summary>Details</summary>
Motivation: 受到在大型语言模型中使用MCTS的有效性的启发，探索了如何将传统用于训练价值或奖励模型的MCTS-derived trajectories重新用于改进基于偏好的强化学习中的策略优化。特别关注了Group Relative Policy Optimization（GRPO）算法，该算法可以实现无需价值网络的偏好一致性策略学习。

Method: 提出了一种基于MCTS-derived trajectories的分段GRPO训练范例，引入了一种新颖的基于树结构的优势估计设置，分析了前缀条件奖励信号的理论和实证结果，提出了解决挑战的启发式和统计解决方案。

Result: 初步结果表明，结构化优势估计可以稳定更新，并更好地反映组合推理质量，但也存在挑战，如优势饱和和奖励信号崩溃。提出了启发式和统计解决方案以减轻这些问题，并讨论了在分段或类似树状奖励结构下学习的开放挑战。

Conclusion: 通过使用MCTS推导的轨迹来改进基于偏好的强化学习中的策略优化，我们提出了一种分段GRPO训练范例，引入了一种基于树结构的优势估计设置。我们分析了这种方法产生的前缀条件奖励信号的理论和实证结果，并提出了应对挑战的启发式和统计解决方案。我们讨论了在分段或类似树状奖励结构下学习面临的挑战。

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [15] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: 本文提出了LightAgent，一个轻量级而功能强大的代理框架，解决了灵活性和简单性之间的折衷，能够有效地部署自学习代理。LightAgent集成了核心功能，并与主流聊天平台无缝集成，为开发人员提供了便利。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的迅速发展，Multi-agent Systems（MAS）在各种应用场景中取得了显著进展，但在设计用于代理部署的多功能、稳健和高效平台方面仍存在重大挑战。

Method: LightAgent集成了核心功能，如内存（mem0）、工具和思维树（ToT），保持极其轻量的结构。作为完全开源的解决方案，与主流聊天平台无缝集成，使开发人员能够轻松构建自学习代理。

Result: LightAgent已在https://github.com/wxai-space/LightAgent上发布，并且获得了积极的反馈和使用。

Conclusion: LightAgent是一个轻量级而功能强大的代理框架，解决了现有框架中灵活性和简单性之间的折衷，能够有效部署自学习代理。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [16] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 本文研究了为何在锦标赛中获胜的候选人提供认证解释的问题，确定了最小支持集的概念、大小和计算方法，并展示了在不同锦标赛规则下如何产生紧凑、认证且直观的解释。


<details>
  <summary>Details</summary>
Motivation: 研究锦标赛规则下候选人获胜的解释，探讨为何胜者会在锦标赛中获胜的问题，并强调了在形式化可解释人工智能中的重要性。

Method: 确定了最小支持集的概念，以及在常见锦标赛规则下计算最小支持集大小的多项式时间算法，说明了问题的 NP 完全性，还展示了最小支持集在生成紽解释方面的应用。

Result: 确定了各种锦标赛规则下最小支持集的大小和计算方法，提出了多项式时间算法，并展示了最小支持集在解释产生方面的应用。

Conclusion: 本文研究了在各种锦标赛规则下为何候选人脱颖而出的认证解释问题，并确定了最小支持集的大小和计算方法，展示了如何利用最小支持集来产生简洁、认证且直观的解释。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [17] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 本文研究了隐性空间协调对团队绩效的影响，探讨了在搜索和救援任务中，团队成员如何依赖移动模式来推断他人意图和协调行动。研究结果显示空间专业化对绩效有积极影响，自适应空间接近度表现为倒U型关系，时间动态指标能区分高低绩效团队。该研究对角色定位团队中的隐性空间协调提供了见解，并强调了平衡的自适应策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文探讨了在协作的在线搜索和救援任务中，团队成员在受限制的明示沟通下如何依赖移动模式来推断他人意图和协调行动的问题。通过研究隐性空间协调维度对团队绩效的影响，填补了现有文献中对此方面的研究空白。

Method: 本文分析了34个四人团队（136名参与者）在搜索和救援任务中的数据，研究了空间专业化、移动专业化和自适应空间接近度对团队绩效的影响。使用空间邻近性、分布模式和移动对齐性等指标来捕捉团队协作的关系方面。

Result: 研究结果表明空间专业化对绩效有积极影响，自适应空间接近度显示出倒U型关系，时间动态指标可以区分高低绩效团队。

Conclusion: 本文研究了三个空间协调维度对团队绩效的影响，发现空间专业化对绩效有积极影响，而自适应空间接近度表现出一定程度的倒U型关系。时间动态方面的指标能够区分高绩效团队和低绩效团队。研究结果揭示了角色定位团队中的隐性空间协调，并强调平衡的自适应策略的重要性，对培训和人工智能辅助团队支持系统具有重要启示。

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [18] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: 本文介绍了 TAM Bench，一个用于评估基于大型语言模型的代理的多元化、真实且结构化的基准测试库。通过自动收集和结构化机器学习挑战，并建立多维度的评估框架，构建了三个不同规模的基准测试子集。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖范围、领域多样性、难度建模和评估严谨性方面存在局限，无法充分展现这些代理在真实设置中的全部能力。因此，需要一个多元化、真实且结构化的基准测试库来评估LLMs代理在端到端机器学习任务上的性能。

Method: 1. 使用浏览器自动化和基于LLMs的任务获取系统自动收集和结构化机器学习挑战。
2. 利用排行榜驱动的难度建模机制估算任务复杂性，通过参与人数和得分分散性进行任务校准。
3. 结合性能、格式符合性、约束遵从性和任务泛化等多维度评估框架。

Result: 构建了 TAM Bench，提出了三个关键创新点：1. 浏览器自动化和LLMs任务获取系统；2. 排行榜驱动的难度建模机制；3. 多维度评估框架。通过150个策划的AutoML任务，构建了Lite、Medium和Full三个不同规模的基准测试子集。

Conclusion: 本文介绍了 TAM Bench，一个多元化、真实且结构化的基准测试库，用于评估基于大型语言模型（LLMs）的代理在端到端机器学习任务上的表现。通过自动收集和结构化来自Kaggle、AIcrowd和Biendata等平台的机器学习挑战，涵盖多种任务类型和数据形式，并利用排行榜驱动的难度建模机制和多维评估框架构建了三个不同规模的基准测试子集，适用于不同的评估场景。Lite 版本具有 18 个任务，平衡覆盖了各种数据形式和难度级别，适用于日常基准测试和比较研究。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [19] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 本文提出了一种资源高效语义探索的深度强化学习（DRL）架构，通过VLM通识集成和课程学习策略，实现了在探索中平衡效率和语义理解的挑战。实验结果表明代理的显著进展，包括提高的物体发现率和对语义丰富区域有效导航的能力。


<details>
  <summary>Details</summary>
Motivation: 在自主代理人中实现完全智能和自我引导探索需要更高级的认知能力。传统的强化学习方法由于嵌入在代理的小策略中的认知能力有限，往往在探索效率和语义理解之间难以平衡，导致在处理语义探索时常需人类驾驶员。因此，本文旨在解决这一挑战，提出一种专门设计用于资源高效语义探索的深度强化学习（DRL）架构。

Method: 本文采用了一种新颖的深度强化学习（DRL）架构，通过将Vision-Language Model（VLM）通识集成到分层奖励函数中，实现了资源高效的语义探索。针对VLM查询，将其建模为专用动作，使代理能够在必要时战略性地查询VLM以获得外部指导，从而节约资源。此外，结合课程学习策略，引导在不同复杂级别进行学习，以确保稳健和稳定的学习过程。

Result: 实验评估结果表明，该代理实现了显著提高的物体发现率，并发展出有效导航至语义丰富区域的学习能力。同时，它展示了在何时请求外部环境信息的策略掌握。

Conclusion: 本文通过提出一种专门设计用于资源高效语义探索的深度强化学习（DRL）架构，有效地解决了在探索中平衡高效性和语义理解的挑战。实验结果表明，该代理实现了显著提高的物体发现率，并在导航到语义丰富区域方面发展出学习能力。同时表现出了在何时请求外部环境信息的策略掌握。通过在自主代理人中嵌入常识语义推理的实践和可扩展方法，为在机器人领域追求完全智能和自我引导探索提供了一种新颖方法。

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [20] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: 提出了Template-Oriented Reasoning (TORSO)方法，旨在引导大型语言模型在各种任务中生成适当的响应，无需手动创建少样本示例。实验结果表明，TORSO在各种LLMs基准测试中表现出强大性能，并提供合理的合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用少样本提示生成响应，依赖于提供的示例，限制了模型固有推理能力的利用。手动构建特定任务的少样本提示成本高昂，可能导致不同任务之间的不一致性。

Method: 提出Template-Oriented Reasoning (TORSO)方法，引导大型语言模型利用内在推理能力生成响应。

Result: 实验证实TORSO在各种LLMs基准测试中取得了强大的性能，并提供了合理的合理性。

Conclusion: 介绍了Template-Oriented Reasoning (TORSO) 方法，通过引导大型语言模型利用内在推理能力在各种任务中生成适当的响应，无需手工创建少样本示例。实验证实TORSO在各种LLMs基准测试中取得了强大的性能，并提供了合理的合理性。

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [21] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 该论文分析了LLMs在法律领域中“幻觉”挑战，评估RAG缓解策略效果，并提出全面优化方案。强调人类监督的不可或缺性，倡导“咨询式”AI范式，将真实性和可追溯性视为重中之重，强调人工智能应用应作为专业判断的增强工具。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在法律领域中存在的幻觉问题，探讨其影响和潜在风险，以及现有RAG缓解策略的实际效果。关注人工智能应用的伦理道德和监管问题，强调人类在决策过程中的重要性。针对当前局限性提出全面优化方案，寻求更加可靠和可追溯的人工智能应用方式。

Method: 通过分析LLMs在法律应用中存在的幻觉挑战，检查其根源、表现形式，以及RAG缓解策略的效力，突出其局限性，并提出全面的优化方案。探讨伦理和监管影响，强调人类监督的不可或缺性。提出采用“咨询式”人工智能范式的观点，倡导优先考虑真实性和可追溯性，将人工智能视为一种增强而非替代专业判断的工具。

Result: 论文得出结论，强调解决法律领域LLMs中的幻觉问题不在于改进生成模型，而是转向“咨询式”人工智能范式，将真实性和可追溯性放在首位。倡导将人工智能视为专业判断的增强工具，而非取代人类角色。提出了对RAG缓解策略的限制以及全面的优化方案。

Conclusion: 该论文探讨了在应用于法律领域的LLMs中存在的“幻觉”（虚假信息）挑战，强调了RAG缓解策略的有效性，并指出其局限性，并提出了整体优化方案。论文探讨了伦理和监管问题，强调人类监督在此过程中的不可替代性角色。最终得出结论，解决方案不在于逐步改进生成模型，而在于采用“咨询式”人工智能范式，优先考虑真实性和可追溯性，作为一种工具来增强，而非替代专业判断。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [22] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM is a framework that transforms memory into an active, self-optimizing component for long-term multi-agent systems. It improves reasoning accuracy, reduces token overhead, and enables knowledge transfer across tasks. Evaluations demonstrate its effectiveness in memory management for multi-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: Efficient memory management is crucial for performance and scalability in long-term multi-agent systems. Existing methods have limitations such as noise accumulation, uncontrolled memory expansion, and limited generalization across domains. The goal is to overcome these challenges by presenting a verifiable and adaptive memory framework.

Method: SEDM, Self-Evolving Distributed Memory, is introduced as a framework that transforms memory into an active, self-optimizing component. It integrates verifiable write admission, self-scheduling memory controller, and cross-domain knowledge diffusion to address challenges in memory management in long-term multi-agent systems.

Result: SEDM improves reasoning accuracy, reduces token overhead, and enables knowledge transfer and enhancement of multi-hop reasoning. Evaluations on benchmark datasets show the effectiveness of SEDM compared to strong memory baselines.

Conclusion: SEDM is a scalable and sustainable memory mechanism for open-ended multi-agent collaboration, improving reasoning accuracy and reducing token overhead compared to existing methods. It enables knowledge transfer across heterogeneous tasks and enhances multi-hop reasoning through knowledge distilled from fact verification.

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [23] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 提出基于量子模型的图像字幕生成方法，通过在希尔伯特空间中解释组合张量模型表示，并使用变分量子电路学习表示，在图像编码中使用多热编码和角度/幅度编码。实验结果显示该方法在处理泛化能力要求高的任务中取得良好表现，特别是在多热编码下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前AI工具如视觉语言模型在组合泛化方面存在不足，之前研究表明基于组合张量的句子语义无法克服挑战。作者猜测量子模型的训练效率提高会改善这些任务的表现。

Method: 将组合张量模型的表示解释为希尔伯特空间中的表示，并使用变分量子电路在图像字幕生成任务中学习这些表示，使用了多热编码和角度/幅度编码两种图像编码技术。

Result: 在图像字幕生成任务中，提出的方法表现良好，在使用噪声的多热编码时获得良好的概念验证结果。在使用CLIP图像向量时表现更加复杂，但仍然优于经典组合模型。

Conclusion: 提出一种基于量子模型的方法，用于训练泛化能力强的图像字幕生成模型，相较于传统组合模型，取得更好的实验结果。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [24] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: 本研究提出了Auras算法系统共同设计的推理框架，用于优化具身体感知智能代理的推理频率。Auras通过分解感知和生成模块，并为其提供可控的管道并行性，从而实现高稳定的吞吐量。此外，面对增加并行性时出现的数据陈旧问题，Auras建立了一个公共上下文供感知和生成共享，从而保证了具身体智能代理的准确性。实验结果表明，Auras平均提高吞吐量2.54倍，同时达到原始准确度的102.7%，显示了其在克服顺序计算约束和提供高吞吐量方面的功效。


<details>
  <summary>Details</summary>
Motivation: Traditional sequential computation patterns face limitations in achieving the necessary 'thinking' frequency for real-world applications in dynamic environments. Embodied AI systems require seamless integration of perception and generation modules to process high-frequency input and output demands.

Method: Presented Auras, an algorithm-system co-designed inference framework, to optimize the inference frequency of embodied AI agents. Disaggregated the perception and generation modules and provided controlled pipeline parallelism for high and stable throughput. Established a public context for perception and generation to share, addressing the data staleness problem.

Result: Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.

Conclusion: Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [25] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 本文研究了大语言模型的持续扩展是否会带来递减收益的问题。通过提升执行能力来解决任务长度增加而导致的失败现象，并强调了模型规模扩展和顺序测试计算的重要性。大型模型能在更长任务中执行更多步骤，同时发现模型逐步准确性随步数增加而下降，并存在自我调节效应。近期的思考模型具有更好的执行能力，能够在单轮中执行更长任务。


<details>
  <summary>Details</summary>
Motivation: 着眼于执行能力，旨在解决大语言模型如何解决复杂推理问题却在处理更长的简单任务时失败的争论，并强调了模型规模扩展和顺序测试时间计算对长期任务的重要性。

Method: 通过观察单步准确性的边际增益如何在任务长度方面产生指数级改进，提出了通过明确提供解决长期任务所需的知识和计划来分离执行能力，探讨了大型语言模型在长任务中更高的执行正确率以及随着步数增加而趋于下降的逐步准确性情况。

Result: 大型语言模型能够在更长任务中执行更多步骤，即使小型模型的单步准确性达到100%，还发现模型在步数增加时逐步准确性下降，且存在自我调节效应。近期的思考模型则不具有自我调节效应，并且能够在单轮中执行更长任务。最后通过对前沿思考模型在单轮中执行任务长度的基准测试来做出结论。

Conclusion: 大语言模型的持续扩展是否会带来递减收益，论文提出通过提升执行能力来实现以解决任务长度增加而导致的失败现象，强调了模型规模扩展和顺序测试计算的巨大益处。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>
