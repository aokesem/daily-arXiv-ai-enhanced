<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 本研究通过对LLM作为法官范式中“思考型”和“非思考型”LLM的比较，发现明确推理模型在准确性、效率和稳健性方面优于非思考型模型。结果显示，明确推理模型在各种偏见条件下保持更大一致性，成本相对较低。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越被采用作为基准测试和奖励建模中的自动化法官，确保它们的可靠性、效率和稳健性变得至关重要。

Method: 通过系统比较“思考型”和“非思考型”LLM在LLM作为法官范式中的表现，使用开源Qwen 3模型（0.6B、1.7B和4B参数）进行评估。评估准确性和计算效率（FLOPs）时采用RewardBench任务，并进一步研究非思考型模型的增强策略，包括上下文学习、标准指引判定、基于参考的评估和N个最佳聚合。

Result: 明确推理模型在准确性上表现比非思考型模型约高10%，成本略低（不到2倍），而增强策略如少样本学习则在更高成本下带来适度提升（>8倍）。偏见和稳健性分析进一步显示，明确推理模型在各种偏见条件下（如位置、跟风、身份、多样性和随机偏见）保持较大一致性（平均高出6%）。实验扩展到多语言设置，结果证实明确推理的好处不仅限于英语。

Conclusion: 明确推理在LLM作为法官范式中具有明显优势，不仅在准确性和效率上，还在稳健性上表现出色。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [2] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 在15个不同模型上进行了评估意识研究，发现评估意识与模型大小存在明显的幂律关系。该规律有助于预测未来更大模型中的欺骗行为，并指导设计规模感知的AI安全评估策略。


<details>
  <summary>Details</summary>
Motivation: 之前的研究仅针对单个70B模型展示了评估意识的问题，但跨模型规模的缩放关系仍未知。本研究旨在探讨不同规模模型之间的评估意识，以了解其规模效应，有助于AI安全评估。

Method: 使用线性探针对转向向量激活进行了调查，研究了15个模型（参数范围从0.27B到70B）的评估意识，揭示了评估意识与模型大小之间的幂律关系。

Result: 发现明显的幂律缩放：评估意识随着模型大小的增加而可预测地增加。

Conclusion: 该研究揭示了大型语言模型在评估意识方面存在可预测的规模效应，为预测未来更大模型中的欺骗行为和指导设计面向规模的AI安全评估策略提供了依据。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [3] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: CoT推理是提高大型语言模型性能的有力工具，但推理步骤常常未能因果影响最终答案，导致输出脆弱不可靠。本文介绍了Faithful Reasoning via Intervention Training（FRIT），通过干预训练模型以产生因果一致的推理，提高了推理准确性和可信度，填补了推理性能和可信度之间的关键差距。


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought（CoT）推理在提高大型语言模型在复杂任务上的性能方面表现出色，但最近的工作表明，推理步骤通常不能因果影响最终答案，产生脆弱且不可靠的输出。先前的方法主要集中在衡量忠诚度上，而系统性改进方法仍然有限。

Method: 介绍了Faithful Reasoning via Intervention Training（FRIT）方法，该方法通过干预模型生成的CoTs中的个别推理步骤，生成忠诚/不忠诚对，突出了推理出现问题的情况。然后应用Direct Preference Optimization教导模型偏好因果一致的推理路径。

Result: FRIT在Qwen3-8B和Mistral-7B-v0.1上评估，提高了Mistral在GSM8K上的忠实推理3.4个百分点，同时准确性提高了7.6个百分点。

Conclusion: 介绍Faithful Reasoning via Intervention Training（FRIT），这是一种可扩展的对齐方法，通过从系统性被破坏的示例中学习，训练模型产生因果一致的推理，提高了推理的准确性和可信度。在Qwen3-8B和Mistral-7B-v0.1上评估，FRIT使Mistral在GSM8K上的忠实推理提高了3.4个百分点，同时将准确性提高了7.6个百分点。该方法提供了第一个可扩展、无监督的训练语言模型以产生更可靠和可解释推理的方法，填补了推理性能和可信度之间的关键差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [4] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 该论文认为现代AI研究应采用反脆弱的安全视角，强调处理罕见事件的能力随时间增强。指出静态测试的局限性，探讨反脆弱解决方案管理罕见事件的潜力，提出重新校准AI安全性方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 文中提到静态测试和单次鲁棒性测试忽视了环境的演变以及模型可能会漂移为恶适应的现实。主张重视利用不确定性来更好地准备未来可能存在的更大、更不可预测的不确定性，关键在于长期可靠性。

Method: 论文首先指出静态测试的关键局限性，包括场景多样性、奖励欺骗和过度对齐。然后探讨了反脆弱解决方案管理罕见事件的潜力。倡导根本性重新校准测量、基准测试和不断改进AI安全性方法，为反脆弱AI安全社区提供道德和实践指南。

Result: 提出了反脆弱的安全视角在处理罕见或越界事件等长期AI安全性方面的重要性，并探讨了应对罕见事件的潜在方案。倡导基于反脆弱方法重新校准测量、基准测试和不断改进AI安全性的方法。

Conclusion: 该论文主张现代AI研究必须采用反脆弱的安全视角，强调系统在处理罕见或越界事件等长期AI安全性方面的能力随时间扩展。提出反脆弱方法对于开放式ML系统的长期可靠性至关重要。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [5] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: 该研究提出了IMAC方法，利用世界模型生成想象环境训练代理人。通过无监督环境设计，在生成的环境中实现自动课程表的诱导，以训练能够泛化到新颖任务变化的代理人。实验证明该方法在具有挑战性的程序生成环境中取得强大迁移性能，为训练通用能力的代理人铺平道路。


<details>
  <summary>Details</summary>
Motivation: 训练代理人在具体环境中行动通常需要大量训练数据或准确的模拟，而这在许多实际情况下并不存在。本研究提出利用世界模型生成想象环境，以训练代理人能够泛化到新颖任务变化，并解决在此过程中训练代理人使用有效生成数据的挑战。

Method: 提出IMAC（Imagined Autocurricula）方法，借助无监督环境设计（UED），在生成的世界上诱导出自动课程表，以训练泛化到新颖任务变化的代理人。通过在挑战性的程序生成环境中展示实现强大的迁移性能，从而验证提出方法的有效性。

Result: 在一系列具有挑战性的程序生成环境中，证明了可以通过使用IMAC方法和无监督环境设计在生成的世界上诱导自动课程表来实现强大的迁移性能，并训练能够泛化到新颖任务变化的代理人。

Conclusion: 该研究利用世界模型生成想象环境，训练能够泛化到新颖任务变化的强大代理人。通过提出IMAC（Imagined Autocurricula）方法，借助无监督环境设计（UED），在生成的世界上诱导出自动课程表，从而解决了训练代理人使用有效生成数据的挑战。在一系列具有挑战性的程序生成环境中，表明只在从较窄数据集学习的世界模型内进行训练，即可在保留环境上实现强大的迁移性能。这为利用更大规模、基础世界模型训练通用能力的代理人打开了道路。

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [6] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 对主流抽象动作空间和分词器进行了比较分析，发现缺乏单一最优解，提出了CoA框架来解决任务特定性困境，实现了更好的训练代理策略和提高任务成功率，同时发布了OpenHA套件以促进可复现性研究


<details>
  <summary>Details</summary>
Motivation: 动作空间的选择是发展端到端可训练代理所面临的重要且未解决的挑战之一，通过对主流抽象动作空间和分词器进行比较分析，揭示了最有效的抽象取决于具体任务，为构建通用代理带来了困境

Method: 进行了大规模系统比较主流抽象动作空间和分词器在Vision-Language-Action (VLA)或分层代理模型中的表现，发现没有一种单一的动作空间是普遍最优的，提出了Chain of Action (CoA)框架来解决任务特定性带来的困境

Result: 证明了使用CoA范式训练的All-in-One代理能够学习更强健和通用的策略，取得了新的最先进结果，优于专门基线模型的整体任务成功率

Conclusion: 介绍了Chain of Action (CoA)框架，通过该框架可以统一高层规划和低层控制，构建一个整体的Vision-Language-Action (VLA)模型，取得了新的最先进结果，提高了整体任务成功率。发布了OpenHA套件，包括800多个不同任务的综合基准、筛选的数据集、源代码以及所有预训练模型检查点。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [7] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 本研究提出了PDDL-Instruct框架，旨在通过逻辑链推理来增强LLMs的符号规划能力。实验结果显示，该框架可以显著提高模型在规划任务上的准确性，填补了一般推理能力与自动规划逻辑精确性之间的差距。


<details>
  <summary>Details</summary>
Motivation: LLMs在各种任务上展示出色的能力，但它们在执行结构化符号规划方面的能力仍然有限，尤其是在需要类似规划领域定义语言（PDDL）的形式表示的领域。因此，本研究旨在提高LLMs的符号规划能力，填补其在需要形式化表示的领域中的限制性空白。

Method: 通过开发指导模型进行精确逻辑推理的指令提示，以确定在给定状态下何时可以应用行动，启用LLMs通过结构化反思自我纠正规划流程。系统地构建验证技能，将规划过程分解为关于前提满足、效应应用和不变量维护的明确推理链。

Result: 实验结果表明，基于思维链推理的指导调整模型在多个规划领域上的表现显著优越，其规划准确性高达94%，在标准基准上相较基线模型改进了66%绝对值。

Conclusion: 本研究提出了一种名为PDDL-Instruct的新型指导调整框架，旨在通过逻辑思维推理增强LLMs的符号规划能力。实验结果表明，基于思维链推理的指导调整模型在规划方面表现显著优于基线模型，达到了标准基准测试中高达94%的规划准确性，相比基线模型改进了66%绝对值。这项工作填补了LLMs的一般推理能力与自动规划所需的逻辑精确性之间的差距，为开发更好的人工智能规划系统提供了一个有前途的方向。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [8] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 该论文介绍了Agentic UAVs 框架，利用LLM推动推理、数据库查询和系统交互，提升UAVs的自主性和生态系统整合。通过实验验证，在搜救场景中取得了积极的成果，证实了适度的计算开销可带来全新水平的自主性和生态系统整合。


<details>
  <summary>Details</summary>
Motivation: 现有的UAV框架缺乏上下文感知的推理、自主决策和生态系统级别的集成，缺乏实时知识访问。大部分UAV系统仍局限于SAE 2-3级别自主性，依赖于基于规则的控制和狭窄人工智能，限制了在动态不确定任务中的适应性。

Method: 该论文提出了五层架构（感知、推理、行动、整合、学习）的Agentic UAVs框架，将UAVs与LLM驱动的推理、数据库查询和第三方系统交互相结合。通过基于ROS2和Gazebo的原型，将YOLOv11目标检测与GPT-4推理和本地Gemma-3部署集成。

Result: 在模拟搜救场景中，Agentic UAVs实现了更高的检测置信度、改善的个体检测率和显著增加的行动建议，证实了适度的计算开销能够带来新水平的自主性和生态系统集成。

Conclusion: 该论文介绍了Agentic UAVs 框架，通过引入基于LLM的推理、数据库查询和第三方系统交互，使得UAVs在模拟搜救场景中实现更高的检测置信度、改善个体检测率和显著增加行动建议。研究结果表明，适度的计算开销可以实现全新水平的自主性和生态系统集成。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [9] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 该论文提出了一种语义融合方案，通过特征通道和门控适配器，将可解释的语义特征融合到Transformer语言模型中，从而提高了生成文本的准确性和可控制性。实验结果表明，在合成语料库上能够改善混乱度，实现精确、可控的文本生成，同时保持模型的简单性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是提出一种轻量级的语义融合方案，以增强Transformer语言模型在标记级别的语义表达能力。通过为每个标记引入可解释的特征，并将这些特征融合到语言模型中，进一步提高了生成文本的准确性和可解释性。

Method: 在Transformer语言模型中增加模糊成员特征通道，并使用门控适配器将每个标记表示为可解释特征，通过训练标准的下一个标记预测、辅助重建语义特征的损失以及轻量级统一器来实现语义融合。

Result: 通过提出的语义融合方案，在两个子句语料库上实现了改进的混乱度，可以精确、可控地生成极性和标点，并保持了模型的简单性。此外，该方法的实施只带来了很小的额外开销，与输入输出嵌入绑定兼容，并提供了可解释的自然语言生成途径。

Conclusion: 该论文提出了一种语义融合的轻量级方案，通过在Transformer语言模型(LM)中增加一个并行的模糊成员特征通道，对标记级语义进行编码。语义融合通过一种门控适配器将每个标记表示为可解释特征(例如词性提示、浅层角色、边界标志、情感极性和强度)的矢量，这些特征值来自不同可微分成员函数(例如功率核)。这些每个标记的向量形成一个句子级语义矩阵，融合到LM中。在合成的两个子句语料库上，使用标准的下一个标记预测进行训练，辅助损失从隐藏状态重建语义特征，并使用一个轻量级统一器对形容词类分布进行规范化。语义融合改善了混乱度，并实现了精确、可控制的极性和标点生成，同时保持了模型的简单性。这种方法仅增加了很小的开销，与输入输出嵌入绑定完全兼容，并为有条件的自然语言生成提供了可解释的途径。

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [10] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 本文提出了星号运算符（*运算符），基于邻近结构并行传播（ASPP），形成了新型统一抽象推理框架。星号运算符通过局部并行状态演化过程和隐式关系图实现了全局推理能力，既保持局部计算约束又提供高效收敛的计算范式。经过数学分析和实验验证，证明了其普适性、收敛性和卓越性能。嵌入星号蒸馏方法在ARC2验证中取得了100%准确率，代表神经符号推理领域的重大突破。


<details>
  <summary>Details</summary>
Motivation: 提出星号运算符（*运算符）的初衷是为了解决抽象推理问题中的局部计算约束和全局推理能力之间的平衡难题，希望构建一个高效收敛的计算范式。同时，为了在神经符号推理领域取得突破，创新提出嵌入星号蒸馏方法以提高准确率和性能。

Method: 基于邻近结构并行传播（ASPP）的新型统一抽象推理框架，将结构化推理任务形式化为局部并行状态演化过程，通过隐式关系图指导推理过程。通过数学分析和实验验证该框架的普适性、收敛性和优越性能。同时提出嵌入星号蒸馏方法，实现了在ARC2验证中100%准确率的突破性成果。

Result: 通过研究，我们成功提出了星号运算符（*运算符）作为一种新型抽象推理框架，证明了其在实现全局推理能力的同时保持局部计算约束，展示了其在ARC2验证和康威生命游戏等方面的卓越性能。创新的嵌入星号蒸馏方法在ARC2验证中取得了100%的准确率。

Conclusion: 我们提出了星号运算符（*运算符），这是一个基于邻近结构并行传播（ASPP）的新型统一抽象推理框架。该运算符将结构化推理任务形式化为由隐式关系图指导的局部并行状态演化过程。我们证明星号运算符在保持局部计算约束的同时实现了全局推理能力，为抽象推理问题提供了高效收敛的计算范式。通过对ARC2挑战和康威生命游戏的严格数学分析和全面实验，我们展示了该运算符的普适性、收敛性和优越性能。我们的创新型嵌入星号蒸馏方法在仅有6M参数的情况下在ARC2验证中实现了100%准确率，代表了神经符号推理的重大突破。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [11] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: $Agent^2$ is a novel framework for fully automated RL agent design that outperforms manual designs by up to 55%. It features a dual-agent architecture and decomposes RL development into MDP modeling and algorithmic optimization stages. The framework is built on the Model Context Protocol, providing standardized agent creation across environments and algorithms, with adaptive training management for continuous improvement.


<details>
  <summary>Details</summary>
Motivation: Traditional RL agent development requires extensive expertise and time-consuming iterations, leading to high failure rates and limited accessibility. The motivation behind the paper is to introduce a fully automated RL agent design framework, $Agent^2$, that eliminates the need for human intervention and achieves superior performance compared to manual designs.

Method: The paper introduces the $Agent^2$ framework, a dual-agent architecture consisting of a Generator Agent and a Target Agent. The Generator Agent autonomously designs RL agents by analyzing tasks and environment code, while the Target Agent is the resulting generated RL agent. The framework decomposes RL development into MDP modeling and algorithmic optimization stages, providing targeted and effective agent generation. It is built on the Model Context Protocol and incorporates adaptive training management and intelligent feedback analysis for continuous improvement.

Result: Extensive experiments on various benchmarks show that $Agent^2$ consistently outperforms manually designed solutions across tasks, achieving significant performance improvement. The framework standardizes intelligent agent creation across different environments and algorithms, demonstrating its versatility and effectiveness.

Conclusion: $Agent^2$ framework achieved fully automated RL agent design through intelligent LLM-driven generation, outperforming manual designs by up to 55% on various benchmarks. It marks a fundamental breakthrough in automated AI systems by enabling intelligent agents to design and optimize other agents.

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [12] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 本研究评估了16个最先进的VLM模型在6个多模态数据集上的不确定性表现，发现大型模型在不确定性量化方面更胜一筹，并且在数学和推理任务上表现较差。本研究为多模态系统中的不确定性评估提供了基础。


<details>
  <summary>Details</summary>
Motivation: VLMs在视觉理解领域取得了显著进展，但不确定性量化这一关键维度缺乏足够关注；之前关于符合性预测的研究集中在有限的设置上，本研究旨在填补这一空白。

Method: 进行了全面的不确定性基准测试研究，评估了16个最先进的VLM模型在6个多模态数据集上的表现，使用了3种不同的评分函数。

Result: 发现大型模型一致表现出更好的不确定性量化能力；不确定性较低的模型实现更高的准确性；数学和推理任务的不确定性表现较差。

Conclusion: 大型模型在不确定性量化方面表现更好；对于数学和推理任务，所有模型的不确定性性能都不如其他领域；本研究为多模态系统中可靠的不确定性评估奠定了基础。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [13] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 研究使用深度學習架構和梯度下降來學習命題STRIPS世界模型，將任務定義為下一個token預測問題。通過分析動作序列，確定下一個動作。實驗結果顯示適合的transformer架構可以準確表示模型並僅從動作序列中學習。


<details>
  <summary>Details</summary>
Motivation: 本研究的動機在於通過動作軌跡獨立學習命題STRIPS世界模型的問題，並探討深度學習架構在此任務中的適用性和效果。

Method: 使用深度學習架構（transformers）和梯度下降來進行命題STRIPS世界模型的學習，將任務定義為監督式下一個token預測問題，並通過查看動作序列來確定下一個動作。

Result: 展示了適合的transformer架構可以忠實表示命題STRIPS世界模型，同時這些模型可以僅從正確和錯誤的動作序列集中學習。進行了多個實驗。

Conclusion: 透過深度學習架構（transformers）和梯度下降，本研究考慮從動作軌跡單獨學習命題STRIPS世界模型的問題。將任務視為監督式下一個token預測問題，其中token是動作，如果前幾個動作的隱藏效果不使動作的前提為$a$為false，則動作$a$可能跟隨一個動作序列。 我們展示了適合的transformer架構可以忠實表示命題STRIPS世界模型，并且可以僅從隨機有效（正面）和無效（負面）動作序列組中學習這些模型。論文報告了一系列實驗。

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [14] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: 本文介绍了SteeringControl，用于评估表示转向方法在核心对齐目标和次生行为上的影响。构建了模块化的转向框架，并收集了数据集进行评估。结果表明转向性能取决于特定的组合，并可能导致概念纠缠。


<details>
  <summary>Details</summary>
Motivation: 之前的对齐工作通常侧重于展示表示转向的副作用，但发现还有许多未被系统化理解的未探索的权衡。引入SteeringControl基准来评估表示转向方法在核心对齐目标以及对次生行为的影响。

Method: 构建了一个模块化的转向框架，基于独特组件，作为现有方法的构建模块。收集了安全相关的主要和次要行为数据集，评估了转向效果和围绕五种流行转向方法的行为纠缠。

Result: 强大的转向性能取决于特定的转向方法、模型和目标行为的组合，并且糟糕的组合可能导致严重的概念纠缠。

Conclusion: 本文引入了SteeringControl，这是一个用于评估表示转向方法在核心对齐目标（偏见、有害生成和幻觉）以及它们对次生行为（如阿谀奉承和常识道德）的影响的基准。我们发现之前的对齐工作通常强调真实性或推理能力，以展示表示转向的副作用，但我们发现还有许多未被系统化理解的未探索的权衡。我们收集了一个与安全相关的主要和次要行为数据集，以评估转向效果和围绕五种流行转向方法的行为纠缠。为此，我们构建了一个基于独特组件的模块化转向框架，这些组件可作为许多现有方法的构建模块。我们在Qwen-2.5-7B和Llama-3.1-8B上的结果表明，强大的转向性能取决于特定的转向方法、模型和目标行为的组合，并且这三者的糟糕组合可能导致严重的概念纠缠。我们在此公开我们的代码：https://github.com/wang-research-lab/SteeringControl.git。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [15] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 研究探究了为LLM代理提供协作工具和自主权是否可以提高其性能。实验证明，在困难问题上，协作工具显著提高了代理的表现。不同代理模型采用不同的协作策略，代理更倾向于写作而非阅读。因此，AI代理可以从人类启发的协作工具中受益，并指出自适应协作界面可以作为推理增强器。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究是否让LLM代理获得人类自然使用的协作工具和自主权，能否提高它们的性能。通过实验验证LLM代理在使用协作工具时的表现，探索这些工具对基线代理的影响。此外，也希望了解代理如何自发采用不同的协作策略，以及代理更偏向于写作还是阅读。

Method: 装备LLM代理与MCP-based社交媒体和日志工具，让其自行使用这些工具进行问题解决。观察34个Aider Polyglot Python编程挑战中协作工具的影响，并对代理的表现进行评估。进行行为分析以了解代理的偏好和如何采用不同的协作策略。

Result: 研究发现，在Aider Polyglot Python编程挑战中，协作工具显著提高了最困难问题的解决性能，降低了成本、回合数，并加快了完成时间。不同的代理模型采用了不同的协作策略，Sonnet 3.7广泛使用工具并受益于基于表达的认知支架，Sonnet 4则有选择性地采用基于日志的语义搜索。研究还发现代理更偏向于写作而非阅读，表明结构化的表达驱动了改进的大部分部分。最终指出，AI代理可以从人类启发的协作工具中受益，在其能力边缘处，自适应协作界面可以作为推理增强器。

Conclusion: 研究表明，为LLM代理提供与人类自然用于问题解决的协作工具和自主权能够提高它们的性能。使用基于MCP的社交媒体和日志工具装备Claude Code代理，并允许他们自行使用这些工具。在34个Aider Polyglot Python编程挑战中，协作工具显著提高了最困难问题的表现，使得成本降低了15-40％，回合数减少了12-27％，完成时间快了12-38％。对整个挑战集的影响则是复杂的，表明当需要额外的推理支持时，这些工具可以作为性能增强剂。令人惊讶的是，不同的模型在没有明确指导的情况下自然采用了不同的协作策略。Sonnet 3.7广泛使用各种工具，并受益于基于表达的认知支架。Sonnet 4表现出选择性采用，在问题真正困难时，依靠基于日志的语义搜索。这反映了人类开发者根据专业知识和任务复杂性调整协作的方式。行为分析显示，代理更偏向于写作而非阅读，比例约为2-9倍，这表明结构化的表达驱动了改进的大部分部分，而不仅仅是信息访问。总体而言，AI代理可以有系统地从人类启发的协作工具中受益，尤其是在它们能力的边缘，这指向了自适应协作界面作为推理增强器而非普遍效率提升的可能性。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [16] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 该研究分析了学生在三门基于证明的本科数学课程中对生成式人工智能的使用和感知，讨论了对教学的影响，并探讨未来将生成式人工智能整合到基于证明的数学教学中的相关考虑。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能在高等教育中快速崛起以及当前人工智能检测工具的不可靠性，制定鼓励学生学习和批判性思维的政策变得越来越重要。

Method: 通过调查和学生访谈，分析了学生在三门基于证明的本科数学课程中使用和感知生成式人工智能的情况。

Result: 研究结果揭示了学生对生成式人工智能工具的使用方式、其有用性和局限性的感知，以及这些感知对基于证明的数学教学的教学有何影响。

Conclusion: 研究通过调查和学生访谈分析了学生对生成式人工智能工具的使用和感知，讨论了这些感知对于教授基于证明的数学课程的教学的影响。

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [17] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: 本文介绍了一种名为CoBRA的工具包，用于在基于LLM的社会仿真中系统地指定代理行为。CoBRA通过量化代理的认知偏见，并利用行为调节引擎将代理的行为与控制的认知偏见相一致。研究发现，CoBRA能够准确地编程社会代理所展示的认知偏见，而无需针对特定模型进行调整。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过隐含的自然语言描述指定代理行为，但不能在模型之间产生一致的行为，也不能捕捉描述的微妙之处。因此，作者提出了CoBRA来明确地程序化代理的认知偏见，以创造一致且精细的代理行为。

Method: 本文引入了CoBRA工具包，通过基于经典社会科学实验量化代理的认知偏见，并利用行为调节引擎将代理的行为与控制的认知偏见相一致。

Result: 通过演示和技术基准测试，作者评估了CoBRA作为一个人机交互工具包的性能。研究结果表明，CoBRA能够在模型无关的情况下准确地编程社会代理所展示的认知偏见。

Conclusion: CoBRA是一种新型工具包，用于系统地在基于LLM的社会仿真中指定代理行为。与传统方法相比，CoBRA可以准确地编程模型中代理的认知偏见，而无需针对特定模型进行调整。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [18] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 研究构建了状态控制基准，评估了多模态代理的可靠性问题。提出了State-aware Reasoning (StaR)方法，教导代理感知和执行切换指令，显著提高了执行准确性。实验证明StaR还能增强一般任务性能，并展示了在动态环境中的潜在应用前景。


<details>
  <summary>Details</summary>
Motivation: 多模态代理在图形用户界面（GUI）中发挥作用，但执行切换控制指令时存在可靠性问题。本研究旨在解决这一挑战，通过构建状态控制基准并提出StaR方法，教导代理感知和执行切换指令。

Method: 构建了一个具有二元切换指令的状态控制基准，并评估了现有代理的可靠性。提出了State-aware Reasoning (StaR)训练方法，教导代理感知当前切换状态、分析指令中的期望状态并相应行动。对三个多模态代理进行实验表明，StaR方法可以使切换指令执行准确性提高超过30%。进一步评估表明，StaR还增强了一般任务性能。在动态环境的评估强调了StaR在实际应用中的潜力。

Result: StaR方法能够改善多模态代理在执行切换指令时的准确性，并提升一般任务性能。同时，在动态环境中显示出潜在的实际应用前景。

Conclusion: 本研究提出State-aware Reasoning (StaR)方法，通过训练多模态代理感知当前的切换状态，分析指令中的期望状态，并据此采取行动，从而改善了切换指令执行准确性超过30%。同时，StaR方法在一般任务性能上也表现出改善。研究结果显示StaR对动态环境具有潜在的实际应用前景。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [19] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: 本文提出了针对工业管理系统的新GUI框架InfraMind，通过五个创新模块解决系统复杂性、自动化不足、部署约束和安全需求等五大挑战。实验证明，InfraMind在工业管理自动化方面表现优越，提供了可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 针对工业管理系统中的复杂性、自动化能力不足、部署约束和安全需求等问题，提出了InfraMind框架以解决这些挑战，同时提高工作效率和任务成功率。

Method: 通过提出的InfraMind框架，整合了五个创新模块：基于虚拟机快照的系统搜索探索、内存驱动规划、高级状态识别、结构化知识蒸馏和全面的多层安全机制，以解决工业管理中面临的五大挑战。

Result: 通过广泛实验验证，InfraMind框架在工业管理自动化方面表现优越，比现有框架具有更高的任务成功率和操作效率。

Conclusion: 提出了一种针对工业管理系统的新的GUI智能框架InfraMind，通过整合五个创新模块解决工业管理中的各种挑战，实现了任务成功率和操作效率的提升。在开源和商业DCIM平台上进行了广泛实验，证明该方法在工业管理自动化方面始终优于现有框架，提供了严格且可扩展的解决方案。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [20] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR proposes a method to integrate external tools for high-precision tasks, addressing challenges in data construction, optimization, and inference. It achieves strong generalization, state-of-the-art performance on mathematical benchmarks, and consistent improvements on code benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with constructing tool-integrated reasoning data, fine-grained optimization, and enhancing inference. Integrating external tools is a promising approach to enhancing LLMs' performance on high-precision tasks like numerical computation and formal symbolic manipulation.

Method: Propose THOR (Tool-Integrated Hierarchical Optimization via RL) to address challenges in integrating external tools for high-precision tasks. Introduce TIRGen for constructing high-quality datasets, RL strategy for fine-grained hierarchical optimization, and self-correction mechanism for dynamic path revision during inference.

Result: THOR achieves strong generalization, state-of-the-art performance on mathematical benchmarks, and consistent improvements on code benchmarks.

Conclusion: THOR demonstrates strong generalization across diverse models, achieving state-of-the-art performance on mathematical benchmarks and consistent improvements on code benchmarks.

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [21] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: The paper introduces MIRA, a framework for intuitive AI tasking on smartphones, leveraging advanced AI technologies to enhance instruction recommendation accuracy and user experience.


<details>
  <summary>Details</summary>
Motivation: To simplify access to predefined AI services on smartphones and enable intuitive one-touch AI tasking, transforming the user interaction with devices.

Method: Introduces MIRA framework for task instruction recommendation using a multimodal large language model-based recommendation pipeline with structured reasoning, template-augmented reasoning mechanism, and prefix-tree-based constrained decoding strategy.

Result: Demonstrated significant improvements in instruction recommendation accuracy through evaluation with real-world annotated datasets and user study.

Conclusion: MIRA framework shows substantial improvements in the accuracy of instruction recommendation, revolutionizing the user interaction with AI services on smartphones.

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [22] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 本文提出了一种精确的MCILC方法，采用穷举DPLL架构，并集成了混合整数规划的简化技术。实验证明，在随机基准测试中，该方法表现优秀，优于所有其他精确方法，并且是唯一能解决所有应用实例的方法。


<details>
  <summary>Details</summary>
Motivation: 线性约束是计算机科学、运筹学和优化等领域中最基本的约束之一。许多应用都可以归结为在整数线性约束上进行模型计数的任务。

Method: 本文设计了一种精确的MCILC方法，基于穷举DPLL架构，并集成了多种混合整数规划的有效简化技术。

Result: 实验结果表明，本文方法在随机基准测试中明显优于所有精确方法，并且是唯一解决了所有应用实例的方法。

Conclusion: 本文设计了一种基于穷举DPLL架构的精确MCILC方法，并通过将多种混合整数规划的有效简化技术集成到架构中来提高效率。实验结果表明，我们的方法在随机基准测试中明显优于所有精确方法，在解决1718个实例，而目前最先进的方法只计算了1470个实例。此外，我们的方法是唯一解决了所有4131个应用实例的方法。

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [23] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 本研究使用了人工神经网络模型，发现信息流的改变可以导致认知性能的转变。循环网络在处理不同输入类型和学习复杂语法方面表现优异，但训练困难可能会阻碍性能提升。总体而言，研究结果展示了信息流变化对认知性能转变的影响。


<details>
  <summary>Details</summary>
Motivation: 最近的研究提出认知也可能经历一系列重要转变，改变生物神经网络结构，影响信息流动，本研究旨在验证这一观点。

Method: 研究使用了理想化的信息流模型，人工神经网络（ANNs），比较了具有前馈、循环和层状拓扑结构的网络，在学习不同复杂度的人工语法时的表现。

Result: 研究发现，循环网络相对于前向网络能处理更多类型的输入，并在学习最复杂语法时表现更好。此外，循环网络训练的困难造成了转变障碍与有条件的不可逆性，也展示了信息流变化如何影响认知性能的转变。

Conclusion: 研究表明信息流的变化可以导致认知性能的转变，透过人工神经网络的模型验证了这一观点。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [24] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent is a system that manages diverse annotation sources in NLP tasks effectively by integrating LLMs, SLMs, and human experts. It provides end-to-end process control through rational task assignment, demonstrated through experiments on multimodal classification tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical gap in managing diverse annotation sources effectively by providing holistic process control in NLP tasks. The paper aims to fill the gap in handling complex scheduling and quality-cost trade-offs in an integrated manner inspired by crowdsourcing companies.

Method: Introduces CrowdAgent, a system inspired by real-world crowdsourcing companies, that integrates task assignment, data annotation, and quality/cost management. It implements a methodology for rational task assignment to enable LLMs, SLMs, and human experts to work collaboratively in an annotation workflow.

Result: The paper demonstrates the effectiveness of CrowdAgent through experiments on six multimodal classification tasks, showcasing the benefits of synergistic collaboration among different annotation sources.

Conclusion: CrowdAgent is a multi-agent system that provides end-to-end process control for managing diverse annotation sources in NLP tasks effectively. It demonstrates the effectiveness of integrating LLMs, SLMs, and human experts in a collaborative workflow through extensive experiments on multimodal classification tasks.

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [25] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过实验证实了第二阶学习在发展与环境结构同构的内部心理地图时的重要性，提出了使用GCN和MLP controller的层次结构，实现了在未见过的迷宫任务中的鲁棒泛化和性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究者试图通过验证现有理论中关于第二阶学习对环境-认知同构性产生作用的假设，来验证心理表征的重要性。

Method: 采用了Hierarchical Architecture，结合了Graph Convolutional Network (GCN)和MLP controller，GCN用于第一阶学习直接映射节点级特征到最佳导航路径的预测，而MLP在面对结构新颖的迷宫环境时动态调整GCN的参数。

Result: 实验证实了第二阶学习在发展与环境结构同构内部心理地图时的重要性，并展示了在未见过的迷宫任务中性能的显著提升。

Conclusion: 第二阶学习对于发展与环境结构同构的内部心理地图至关重要，能显著提升性能并在未见过的迷宫任务中实现鲁棒泛化。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>
