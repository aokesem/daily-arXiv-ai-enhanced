{"id": "2508.10976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10976", "abs": "https://arxiv.org/abs/2508.10976", "authors": ["Martin Diller", "Sarah Alice Gaggl", "Philipp Hanisch", "Giuseppina Monterosso", "Fritz Rauschenbach"], "title": "Grounding Rule-Based Argumentation Using Datalog", "comment": null, "summary": "ASPIC+ is one of the main general frameworks for rule-based argumentation for\nAI. Although first-order rules are commonly used in ASPIC+ examples, most\nexisting approaches to reason over rule-based argumentation only support\npropositional rules. To enable reasoning over first-order instances, a\npreliminary grounding step is required. As groundings can lead to an\nexponential increase in the size of the input theories, intelligent procedures\nare needed. However, there is a lack of dedicated solutions for ASPIC+.\nTherefore, we propose an intelligent grounding procedure that keeps the size of\nthe grounding manageable while preserving the correctness of the reasoning\nprocess. To this end, we translate the first-order ASPIC+ instance into a\nDatalog program and query a Datalog engine to obtain ground substitutions to\nperform the grounding of rules and contraries. Additionally, we propose\nsimplifications specific to the ASPIC+ formalism to avoid grounding of rules\nthat have no influence on the reasoning process. Finally, we performed an\nempirical evaluation of a prototypical implementation to show scalability.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4e00\u9636ASPIC+\u5b9e\u4f8b\u63d0\u51fa\u4e86\u667a\u80fd\u57fa\u7840\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5c06\u5176\u8f6c\u6362\u4e3aDatalog\u7a0b\u5e8f\u5e76\u67e5\u8be2\u5f15\u64ce\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u57fa\u7840\u548c\u63a8\u7406\u8fc7\u7a0b\u3002\u901a\u8fc7\u7b80\u5316\u5904\u7406\u907f\u514d\u65e0\u5f71\u54cd\u89c4\u5219\u7684\u57fa\u7840\uff0c\u6700\u7ec8\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u5b9e\u9a8c\u8bc4\u4f30\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u89c4\u5219\u4ec5\u652f\u6301\u547d\u9898\u89c4\u5219\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u4e00\u9636\u5b9e\u4f8b\u8fdb\u884c\u57fa\u7840\u6b65\u9aa4\u3002\u57fa\u7840\u53ef\u80fd\u5bfc\u81f4\u8f93\u5165\u7406\u8bba\u5927\u5c0f\u6307\u6570\u589e\u957f\uff0c\u9700\u8981\u667a\u80fd\u7a0b\u5e8f\u5904\u7406\u3002\u76ee\u524dASPIC+\u7f3a\u4e4f\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u63d0\u51fa\u667a\u80fd\u57fa\u7840\u8fc7\u7a0b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u667a\u80fd\u57fa\u7840\u8fc7\u7a0b\uff0c\u5c06\u4e00\u9636ASPIC+\u5b9e\u4f8b\u8f6c\u6362\u4e3aDatalog\u7a0b\u5e8f\uff0c\u5e76\u67e5\u8be2Datalog\u5f15\u64ce\u4ee5\u83b7\u53d6\u57fa\u7840\u66ff\u4ee3\uff0c\u6267\u884c\u89c4\u5219\u548c\u76f8\u53cd\u547d\u9898\u7684\u57fa\u7840\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9ASPIC+\u5f62\u5f0f\u5316\u7279\u5b9a\u7b80\u5316\uff0c\u907f\u514d\u5bf9\u63a8\u7406\u8fc7\u7a0b\u6ca1\u6709\u5f71\u54cd\u7684\u89c4\u5219\u8fdb\u884c\u57fa\u7840\u3002\u8fdb\u884c\u4e86\u7ecf\u9a8c\u8bc4\u4f30\u4ee5\u5c55\u793a\u53ef\u6269\u5c55\u6027\u3002", "result": "\u901a\u8fc7\u667a\u80fd\u57fa\u7840\u8fc7\u7a0b\u7684\u63d0\u51fa\uff0c\u6210\u529f\u4fdd\u6301\u4e86\u57fa\u7840\u89c4\u6a21\u7684\u53ef\u63a7\u6027\uff0c\u786e\u4fdd\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u6b63\u786e\u6027\u3002\u5bf9ASPIC+\u5f62\u5f0f\u5316\u8fdb\u884c\u4e86\u7279\u5b9a\u7b80\u5316\uff0c\u907f\u514d\u4e86\u57fa\u7840\u5bf9\u63a8\u7406\u8fc7\u7a0b\u6ca1\u6709\u5f71\u54cd\u7684\u89c4\u5219\u3002\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u7684\u7ecf\u9a8c\u8bc4\u4f30\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u667a\u80fd\u57fa\u7840\u8fc7\u7a0b\uff0c\u4fdd\u6301\u4e86\u57fa\u7840\u7684\u89c4\u6a21\u53ef\u63a7\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u6b63\u786e\u6027\u3002\u901a\u8fc7\u5c06\u4e00\u9636ASPIC+\u5b9e\u4f8b\u8f6c\u6362\u4e3aDatalog\u7a0b\u5e8f\uff0c\u5e76\u67e5\u8be2Datalog\u5f15\u64ce\u4ee5\u83b7\u53d6\u57fa\u7840\u66ff\u4ee3\uff0c\u6267\u884c\u89c4\u5219\u548c\u76f8\u53cd\u547d\u9898\u7684\u57fa\u7840\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9ASPIC+\u5f62\u5f0f\u5316\u7279\u5b9a\u7b80\u5316\uff0c\u907f\u514d\u5bf9\u63a8\u7406\u8fc7\u7a0b\u6ca1\u6709\u5f71\u54cd\u7684\u89c4\u5219\u8fdb\u884c\u57fa\u7840\u3002\u901a\u8fc7\u5bf9\u4e00\u4e2a\u539f\u578b\u5b9e\u73b0\u8fdb\u884c\u7ecf\u9a8c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.11070", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11070", "abs": "https://arxiv.org/abs/2508.11070", "authors": ["Zahra Khotanlou", "Kate Larson", "Amir-Hossein Karimi"], "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching", "comment": null, "summary": "Decision makers are increasingly relying on machine learning in sensitive\nsituations. In such settings, algorithmic recourse aims to provide individuals\nwith actionable and minimally costly steps to reverse unfavorable AI-driven\ndecisions. While existing research predominantly focuses on single-individual\n(i.e., seeker) and single-model (i.e., provider) scenarios, real-world\napplications often involve multiple interacting stakeholders. Optimizing\noutcomes for seekers under an individual welfare approach overlooks the\ninherently multi-agent nature of real-world systems, where individuals interact\nand compete for limited resources. To address this, we introduce a novel\nframework for multi-agent algorithmic recourse that accounts for multiple\nrecourse seekers and recourse providers. We model this many-to-many interaction\nas a capacitated weighted bipartite matching problem, where matches are guided\nby both recourse cost and provider capacity. Edge weights, reflecting recourse\ncosts, are optimized for social welfare while quantifying the welfare gap\nbetween individual welfare and this collectively feasible outcome. We propose a\nthree-layer optimization framework: (1) basic capacitated matching, (2) optimal\ncapacity redistribution to minimize the welfare gap, and (3) cost-aware\noptimization balancing welfare maximization with capacity adjustment costs.\nExperimental validation on synthetic and real-world datasets demonstrates that\nour framework enables the many-to-many algorithmic recourse to achieve\nnear-optimal welfare with minimum modification in system settings. This work\nextends algorithmic recourse from individual recommendations to system-level\ndesign, providing a tractable path toward higher social welfare while\nmaintaining individual actionability.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u7b97\u6cd5\u6551\u6d4e\u6846\u67b6\uff0c\u901a\u8fc7\u5bb9\u91cf\u52a0\u6743\u4e8c\u90e8\u5339\u914d\u95ee\u9898\u6765\u4f18\u5316\u793e\u4f1a\u798f\u5229\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e09\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6700\u5927\u7a0b\u5ea6\u964d\u4f4e\u798f\u5229\u5dee\u8ddd\u5e76\u5e73\u8861\u6210\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u793e\u4f1a\u798f\u5229\uff0c\u5728\u7cfb\u7edf\u8bbe\u7f6e\u4e2d\u53d8\u52a8\u8f83\u5c0f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u4eba\uff08\u5bfb\u6c42\u8005\uff09\u548c\u5355\u4e2a\u6a21\u578b\uff08\u63d0\u4f9b\u8005\uff09\u60c5\u51b5\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u5f80\u5f80\u6d89\u53ca\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u3002\u9488\u5bf9\u591a\u65b9\u4e92\u52a8\u3001\u7ade\u4e89\u6709\u9650\u8d44\u6e90\u7684\u5b9e\u9645\u7cfb\u7edf\u73af\u5883\uff0c\u4e2a\u4f53\u798f\u5229\u7684\u4f18\u5316\u5ffd\u89c6\u4e86\u771f\u5b9e\u7cfb\u7edf\u7684\u591a\u4ee3\u7406\u6027\u8d28\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f15\u5165\u4e00\u4e2a\u8003\u8651\u591a\u65b9\u6551\u6d4e\u5bfb\u6c42\u8005\u548c\u6551\u6d4e\u63d0\u4f9b\u8005\u7684\u65b0\u6846\u67b6\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u4e09\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u672c\u7684\u5bb9\u91cf\u5339\u914d\u3001\u6700\u4f73\u5bb9\u91cf\u91cd\u65b0\u5206\u914d\u4ee5\u51cf\u5c11\u798f\u5229\u5dee\u8ddd\uff0c\u4ee5\u53ca\u5177\u6709\u6210\u672c\u611f\u77e5\u7684\u4f18\u5316\u5e73\u8861\u798f\u5229\u6700\u5927\u5316\u4e0e\u5bb9\u91cf\u8c03\u6574\u6210\u672c\u3002\u901a\u8fc7\u5efa\u6a21\u591a\u5bf9\u591a\u4e92\u52a8\u4e3a\u5bb9\u91cf\u52a0\u6743\u4e8c\u90e8\u5339\u914d\u95ee\u9898\uff0c\u4f18\u5316\u8fb9\u6743\u91cd\u4ee5\u5b9e\u73b0\u793e\u4f1a\u798f\u5229\u6700\u5927\u5316\uff0c\u540c\u65f6\u91cf\u5316\u4e2a\u4f53\u798f\u5229\u4e0e\u96c6\u4f53\u53ef\u884c\u7ed3\u679c\u4e4b\u95f4\u7684\u798f\u5229\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u793e\u4f1a\u798f\u5229\uff0c\u5e76\u4e14\u53ef\u4ee5\u5b9e\u73b0\u5728\u7cfb\u7edf\u8bbe\u7f6e\u4e2d\u6700\u5c0f\u7a0b\u5ea6\u7684\u4fee\u6539\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u591a\u65b9\u7b97\u6cd5\u6551\u6d4e\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3\u591a\u4e2a\u6551\u6d4e\u5bfb\u6c42\u8005\u548c\u6551\u6d4e\u63d0\u4f9b\u8005\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f18\u5316\u793e\u4f1a\u798f\u5229\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u7cfb\u7edf\u8bbe\u7f6e\u65b9\u9762\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u793e\u4f1a\u798f\u5229\u3002\u7814\u7a76\u5c06\u7b97\u6cd5\u6551\u6d4e\u4ece\u4e2a\u4f53\u5efa\u8bae\u6269\u5c55\u5230\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\uff0c\u4e3a\u63d0\u9ad8\u793e\u4f1a\u798f\u5229\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2508.11085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11085", "abs": "https://arxiv.org/abs/2508.11085", "authors": ["Qingqing Wang", "Liqiang Xiao", "Chang Chang"], "title": "Learn to optimize for automatic proton PBS treatment planning for H&N cancers", "comment": "27 pages, 4 figures", "summary": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u9006\u4f18\u5316\u5668\u548cPPO\u6846\u67b6\u7ed3\u5408\u7684\u81ea\u52a8\u5316\u6cbb\u7597\u8ba1\u5212\u65b9\u6cd5\uff0c\u5728\u4e34\u5e8a\u89c4\u5212\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6cbb\u7597\u8ba1\u5212\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6548\u679c\u548c\u6548\u7387\uff0c\u5e76\u5728\u5404\u79cd\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u5934\u9888\u90e8\u764c\u75c7\u7684\u8d28\u5b50PBS\u6cbb\u7597\u89c4\u5212\u6d89\u53ca\u4f17\u591a\u51b2\u7a81\u76ee\u6807\uff0c\u9700\u8981\u4eba\u7c7b\u89c4\u5212\u8005\u5728\u89c4\u5212\u8fc7\u7a0b\u4e2d\u5e73\u8861\u548c\u6ee1\u8db3\u591a\u4e2a\u4e34\u5e8a\u76ee\u6807\uff0c\u8fd9\u9700\u8981\u8017\u8d39\u5927\u91cf\u7684\u7cbe\u529b\u3002\u867d\u7136\u5df2\u7ecf\u4ed8\u51fa\u5927\u91cf\u52aa\u529b\u81ea\u52a8\u8c03\u6574\u76ee\u6807\u53c2\u6570\uff0c\u4f46\u9006\u4f18\u5316\u4ecd\u7136\u4f9d\u8d56\u4e8e\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u9006\u4f18\u5316\u5668\u548cPPO\u6846\u67b6\u7684\u6574\u5408\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5728\u4e34\u5e8a\u53ef\u63a5\u53d7\u7684\u89c4\u5212\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6cbb\u7597\u8ba1\u5212\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u9006\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e86PPO\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u5316\u6cbb\u7597\u8ba1\u5212\u3002\u4f18\u5316\u5668\u91c7\u7528L2O\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u5206\u5e03\u6765\u9884\u6d4b\u66f4\u65b0\u6b65\u9aa4\u3002\u4f5c\u8005\u8fd8\u5c06\u7528\u4e8e\u957f\u5e8f\u5217\u5904\u7406\u7684\u6280\u672f\u6574\u5408\u5230\u57fa\u4e8eTransformer\u7684L2O\u6846\u67b6\u4e2d\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709L2O\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002PPO\u6846\u67b6\u4f5c\u4e3a\u5916\u90e8\u5faa\u73af\u865a\u62df\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u7b56\u7565\u7f51\u7edc\u81ea\u4e3b\u8c03\u6574\u76ee\u6807\u53c2\u6570\uff0c\u5e76\u4f7f\u7528\u5242\u91cf\u9884\u6d4b\u5668\u521d\u59cb\u5316\u76ee\u6807\u53c2\u6570\u3002\u5185\u90e8\u5faa\u73af\u7684L2O\u9006\u4f18\u5316\u5668\u57fa\u4e8ePPO\u7b56\u7565\u7f51\u7edc\u7ec6\u5316\u7684\u76ee\u6807\u8ba1\u7b97\u53ef\u4ea4\u4ed8\u7684MU\u503c\u3002", "result": "\u672c\u7814\u7a76\u5171\u6536\u96c6\u4e8697\u4f8b\u60a3\u8005\u6570\u636e\uff0c\u5728\u4e0eL-BFGSB\u65b9\u6cd5\u6bd4\u8f83\u65f6\uff0c\u4f5c\u8005\u7684L2O\u9006\u4f18\u5316\u5668\u5206\u522b\u63d0\u9ad8\u4e8622.97%\u7684\u6548\u679c\u548c36.41%\u7684\u6548\u7387\u3002\u4f5c\u8005\u7684\u6846\u67b6\u751f\u6210\u7684\u8ba1\u5212\u5728\u5e73\u57472.55\u5c0f\u65f6\u5185\u5c55\u793a\u51fa\u6539\u5584\u6216\u76f8\u5f53\u7684\u5668\u5b98\u98ce\u9669\u5668\u5b98\u9650\u5236\u6548\u679c\uff0c\u4ee5\u53ca\u9488\u5bf9\u5904\u65b9\u5242\u91cf\u6c34\u5e73\u3001\u76ee\u6807\u4f53\u79ef\u6570\u91cf\u3001\u5c04\u7ebf\u89d2\u5ea6\u7b49\u4e0d\u540c\u56e0\u7d20\u7684\u4f18\u8d8a\u9776\u533a\u8986\u76d6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u9006\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86PPO\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u5316\u6cbb\u7597\u8ba1\u5212\uff0c\u80fd\u591f\u5728\u4e34\u5e8a\u53ef\u63a5\u53d7\u7684\u89c4\u5212\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8ba1\u5212\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8622.97%\u548c36.41%\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u53c2\u6570\u8bbe\u5b9a\u4e0b\u90fd\u8868\u73b0\u51fa\u66f4\u597d\u7684\u5668\u5b98\u98ce\u9669\u5668\u5b98\u5242\u91cf\u9650\u5236\u6548\u679c\u548c\u66f4\u4f18\u7684\u9776\u533a\u8986\u76d6\u3002"}}
{"id": "2508.11182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11182", "abs": "https://arxiv.org/abs/2508.11182", "authors": ["Matti Berthold", "Lydia Bl\u00fcmel", "Anna Rapberger"], "title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation", "comment": null, "summary": "In this work, we broaden the investigation of admissibility notions in the\ncontext of assumption-based argumentation (ABA). More specifically, we study\ntwo prominent alternatives to the standard notion of admissibility from\nabstract argumentation, namely strong and weak admissibility, and introduce the\nrespective preferred, complete and grounded semantics for general (sometimes\ncalled non-flat) ABA. To do so, we use abstract bipolar set-based argumentation\nframeworks (BSAFs) as formal playground since they concisely capture the\nrelations between assumptions and are expressive enough to represent general\nnon-flat ABA frameworks, as recently shown. While weak admissibility has been\nrecently investigated for a restricted fragment of ABA in which assumptions\ncannot be derived (flat ABA), strong admissibility has not been investigated\nfor ABA so far. We introduce strong admissibility for ABA and investigate\ndesirable properties. We furthermore extend the recent investigations of weak\nadmissibility in the flat ABA fragment to the non-flat case. We show that the\ncentral modularization property is maintained under classical, strong, and weak\nadmissibility. We also show that strong and weakly admissible semantics in\nnon-flat ABA share some of the shortcomings of standard admissible semantics\nand discuss ways to address these.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u62bd\u8c61\u53cc\u6781\u96c6\u5408\u6846\u67b6\u8be6\u7ec6\u7814\u7a76\u4e86ABA\u4e2d\u7684\u5f3a\u53ef\u5bb9\u6027\u548c\u5f31\u53ef\u5bb9\u6027\uff0c\u5f15\u5165\u4e86\u76f8\u5e94\u7684\u8bed\u4e49\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u5728\u975e\u5e73\u5766\u60c5\u51b5\u4e0b\u7684\u5e94\u7528\u3002\u7814\u7a76\u53d1\u73b0\u5f3a\u548c\u5f31\u53ef\u5bb9\u6027\u5728\u7ef4\u6301\u6a21\u5757\u5316\u5c5e\u6027\u65b9\u9762\u5177\u6709\u826f\u597d\u6027\u8d28\uff0c\u4f46\u4e5f\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u90e8\u5206\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u62d3\u5c55\u5bf9ABA\u4e2d\u53ef\u5bb9\u6027\u6982\u5ff5\u7684\u63a2\u8ba8\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u5f3a\u53ef\u5bb9\u6027\u548c\u5f31\u53ef\u5bb9\u6027\u8fd9\u4e24\u79cd\u66ff\u4ee3\u6982\u5ff5\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bed\u4e49\u548c\u7814\u7a76ABA\u4e2d\u7684\u5f3a\u53ef\u5bb9\u6027\uff0c\u8fdb\u4e00\u6b65\u5b8c\u5584\u5bf9\u975e\u5e73\u5766ABA\u6846\u67b6\u7684\u7406\u89e3\u3002\u6b64\u5916\uff0c\u5c06\u5f31\u53ef\u5bb9\u6027\u8bed\u4e49\u6269\u5c55\u5230\u975e\u5e73\u5766\u60c5\u51b5\uff0c\u6709\u52a9\u4e8e\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5206\u6790\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u62bd\u8c61\u53cc\u6781\u96c6\u5408\u4e3a\u57fa\u7840\u7684\u8bba\u8bc1\u6846\u67b6\u4f5c\u4e3a\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u8be6\u7ec6\u7814\u7a76\u4e86\u5f3a\u53ef\u5bb9\u6027\u548c\u5f31\u53ef\u5bb9\u6027\u7684\u6982\u5ff5\uff0c\u5e76\u5f15\u5165\u4e86\u76f8\u5e94\u7684\u8bed\u4e49\u3002\u7814\u7a76\u8fd8\u5bf9ABA\u4e2d\u5f3a\u53ef\u5bb9\u6027\u7684\u6027\u8d28\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u6269\u5c55\u4e86\u5f31\u53ef\u5bb9\u6027\u8bed\u4e49\u5728\u975e\u5e73\u5766ABA\u4e2d\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u6bd4\u8f83\u4e86\u5f3a\u548c\u5f31\u53ef\u5bb9\u6027\u8bed\u4e49\u4e0e\u6807\u51c6\u53ef\u5bb9\u6027\u8bed\u4e49\u7684\u4e0d\u8db3\u4e4b\u5904\uff0c\u5e76\u63a2\u8ba8\u4e86\u89e3\u51b3\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u548c\u5f31\u53ef\u5bb9\u6027\u5728\u975e\u5e73\u5766ABA\u4e2d\u4fdd\u6301\u4e2d\u5fc3\u7684\u6a21\u5757\u5316\u5c5e\u6027\uff0c\u4f46\u4e5f\u5b58\u5728\u4e00\u4e9b\u4e0e\u6807\u51c6\u53ef\u5bb9\u6027\u76f8\u540c\u7684\u7f3a\u9677\u3002\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u65b9\u6848\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u542f\u793a\u3002", "conclusion": "\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u6269\u5c55\u4e86\u5bf9\u57fa\u4e8e\u5047\u8bbe\u7684\u8bba\u8bc1\uff08ABA\uff09\u4e2d\u53ef\u5bb9\u6027\u6982\u5ff5\u7684\u7814\u7a76\u3002\u6211\u4eec\u7814\u7a76\u4e86\u62bd\u8c61\u8bba\u8bc1\u4e2d\u4e24\u79cd\u5907\u53d7\u5173\u6ce8\u7684\u66ff\u4ee3\u6807\u51c6\u53ef\u5bb9\u6027\u6982\u5ff5\uff0c\u5373\u5f3a\u53ef\u5bb9\u6027\u548c\u5f31\u53ef\u5bb9\u6027\uff0c\u5e76\u9488\u5bf9\u4e00\u822c\u7684\uff08\u6709\u65f6\u79f0\u4e3a\u975e\u5e73\u5766\u7684\uff09ABA\u5f15\u5165\u4e86\u76f8\u5e94\u7684\u9996\u9009\u3001\u5b8c\u5168\u548c\u57fa\u7840\u8bed\u4e49\u3002\u6211\u4eec\u4f7f\u7528\u62bd\u8c61\u53cc\u6781\u96c6\u5408\u4e3a\u57fa\u7840\u7684\u8bba\u8bc1\u6846\u67b6\uff08BSAFs\uff09\u4f5c\u4e3a\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u56e0\u4e3a\u5b83\u4eec\u7b80\u6d01\u5730\u6355\u6349\u4e86\u5047\u8bbe\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4e14\u8db3\u591f\u8868\u8fbe\u4e00\u822c\u975e\u5e73\u5766\u7684ABA\u6846\u67b6\u3002\u5c3d\u7ba1\u6700\u8fd1\u5df2\u7ecf\u7814\u7a76\u4e86\u5bf9\u4e8e\u5047\u8bbe\u4e0d\u80fd\u88ab\u63a8\u5bfc\u7684ABA\u53d7\u9650\u7247\u6bb5\u7684\u5f31\u53ef\u5bb9\u6027\uff0c\u4f46\u76ee\u524d\u4e3a\u6b62\u8fd8\u6ca1\u6709\u5bf9ABA\u8fdb\u884c\u5f3a\u53ef\u5bb9\u6027\u7684\u7814\u7a76\u3002\u6211\u4eec\u5f15\u5165\u4e86ABA\u7684\u5f3a\u53ef\u5bb9\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u826f\u597d\u6027\u8d28\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6269\u5c55\u4e86\u5bf9\u4e8e\u5e73\u5766ABA\u7247\u6bb5\u7684\u5f31\u53ef\u5bb9\u6027\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u975e\u5e73\u5766\u6848\u4f8b\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5728\u7ecf\u5178\u3001\u5f3a\u548c\u5f31\u53ef\u5bb9\u6027\u4e0b\uff0c\u4e2d\u5fc3\u7684\u6a21\u5757\u5316\u5c5e\u6027\u5f97\u4ee5\u4fdd\u6301\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u5728\u975e\u5e73\u5766ABA\u4e2d\u7684\u5f3a\u548c\u5f31\u53ef\u5bb9\u6027\u8bed\u4e49\u4e0e\u6807\u51c6\u53ef\u5bb9\u6027\u8bed\u4e49\u5b58\u5728\u4e00\u4e9b\u7f3a\u9677\uff0c\u5e76\u8ba8\u8bba\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.11252", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11252", "abs": "https://arxiv.org/abs/2508.11252", "authors": ["Youcheng Huang", "Bowen Qin", "Chen Huang", "Duanyu Feng", "Xi Yang", "Wenqiang Lei"], "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.", "AI": {"tldr": "LRMs excel in problem-solving but fall short in proactively asking for information. The study introduces a new dataset, evaluates LRMs' performance, uncovers overthinking and hallucination behaviors, and discusses the potential of supervised fine-tuning for enhancing LRMs' ability to ask for information.", "motivation": "Existing benchmarks only assess LRMs on well-defined problems, overlooking the need for genuine intelligence that includes the ability to ask for information. The paper aims to bridge this gap by introducing a new dataset with diverse incomplete problems.", "method": "The paper proposes a new dataset with incomplete problems to evaluate LRMs' ability to proactively ask for information. Systematic evaluation is conducted to reveal LRMs' shortcomings in this aspect and explore behaviors like overthinking and hallucination.", "result": "LRMs are found to be lacking in proactively seeking information. The study uncovers the behaviors of overthinking and hallucination in LRMs, emphasizing the importance of supervised fine-tuning for improving this capability.", "conclusion": "LRMs demonstrated remarkable problem-solving abilities in mathematics but lack the capability to ask for information proactively. The study highlights the behaviors of overthinking and hallucination in LRMs and sheds light on the challenges and potential of supervised fine-tuning in improving this ability."}}
{"id": "2508.11347", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11347", "abs": "https://arxiv.org/abs/2508.11347", "authors": ["Yifei Li", "Lingling Zhang", "Hang Yan", "Tianzhe Zhao", "Zihan Ma", "Muye Huang", "Jun Liu"], "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding", "comment": "10 pages, 5 figures, Accepted at KDD 2025, code available at\n  https://github.com/lyfxjtu/Dynamic-Embedding", "summary": "Traditional knowledge graph (KG) embedding methods aim to represent entities\nand relations in a low-dimensional space, primarily focusing on static graphs.\nHowever, real-world KGs are dynamically evolving with the constant addition of\nentities, relations and facts. To address such dynamic nature of KGs, several\ncontinual knowledge graph embedding (CKGE) methods have been developed to\nefficiently update KG embeddings to accommodate new facts while maintaining\nlearned knowledge. As KGs grow at different rates and scales in real-world\nscenarios, existing CKGE methods often fail to consider the varying scales of\nupdates and lack systematic evaluation throughout the entire update process. In\nthis paper, we propose SAGE, a scale-aware gradual evolution framework for\nCKGE. Specifically, SAGE firstly determine the embedding dimensions based on\nthe update scales and expand the embedding space accordingly. The Dynamic\nDistillation mechanism is further employed to balance the preservation of\nlearned knowledge and the incorporation of new facts. We conduct extensive\nexperiments on seven benchmarks, and the results show that SAGE consistently\noutperforms existing baselines, with a notable improvement of 1.38% in MRR,\n1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with\nmethods using fixed embedding dimensions show that SAGE achieves optimal\nperformance on every snapshot, demonstrating the importance of adaptive\nembedding dimensions in CKGE. The codes of SAGE are publicly available at:\nhttps://github.com/lyfxjtu/Dynamic-Embedding.", "AI": {"tldr": "Traditional KG embedding methods focus on static graphs, while real-world KGs are dynamically evolving. The paper introduces SAGE, a scale-aware framework for continual KG embedding, which dynamically updates embedding dimensions and balances learned knowledge with new facts. SAGE outperforms existing methods in experiments, demonstrating the significance of adaptive embedding dimensions in CKGE.", "motivation": "Real-world knowledge graphs (KGs) are dynamically evolving with constant additions of entities, relations, and facts. Existing CKGE methods lack consideration of varying scales of updates and systematic evaluation throughout the update process. The motivation is to address the dynamic nature of KGs and provide a scale-aware approach for efficient and effective continual updating of KG embeddings.", "method": "The paper proposes SAGE, which determines embedding dimensions based on update scales and expands the embedding space accordingly. It utilizes the Dynamic Distillation mechanism to balance the preservation of learned knowledge and the incorporation of new facts.", "result": "Extensive experiments on seven benchmarks demonstrate that SAGE consistently outperforms existing baselines, showing improvements in MRR, H@1, and H@10. Comparison with methods using fixed embedding dimensions highlights the optimal performance of SAGE, emphasizing the importance of adaptive embedding dimensions in CKGE.", "conclusion": "SAGE, a scale-aware gradual evolution framework for continual knowledge graph embedding (CKGE), outperforms existing baselines with significant improvements in Mean Reciprocal Rank (MRR), H@1, and H@10. The proposed method demonstrates the importance of adaptive embedding dimensions in CKGE by dynamically updating KG embeddings to accommodate new facts while maintaining learned knowledge."}}
{"id": "2508.11360", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCRAFT-GUI\u6846\u67b6\uff0c\u7ed3\u5408\u4e86GRPO\u6846\u67b6\u548c\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u9488\u5bf9GUI\u4ea4\u4e92\u73af\u5883\u4e2dRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5171\u548c\u5185\u90e8\u5728\u7ebf\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6539\u5584\uff0c\u9a8c\u8bc1\u4e86\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u8bfe\u7a0b\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3Reinforcement Learning (RL) \u65b9\u6cd5\u5728GUI\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5ffd\u89c6\u4e0d\u540cGUI\u4efb\u52a1\u4e4b\u95f4\u7684\u96be\u5ea6\u53d8\u5316\u548c\u5c06\u4efb\u52a1\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\u6298\u53e0\u4e3a\u5355\u4e00\u7c97\u7cd9\u5956\u52b1\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u57fa\u4e8eGroup Relative Policy Optimization (GRPO)\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6CRAFT-GUI\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u878d\u5408\u7b80\u5355\u89c4\u5219\u4fe1\u53f7\u548c\u6a21\u578b\u8bc4\u4f30\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u5b9e\u73b0\u66f4\u52a0\u7cbe\u7ec6\u7684\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cCRAFT-GUI\u65b9\u6cd5\u5728\u516c\u5171\u57fa\u51c6\u548c\u5185\u90e8\u5728\u7ebf\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u5747\u8f83\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u5347\uff0c\u5206\u522b\u63d0\u9ad8\u4e865.6%\u548c10.3%\uff0c\u9a8c\u8bc1\u4e86\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u8bfe\u7a0b\u5b66\u4e60\u76f8\u7ed3\u5408\u5728GUI\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86CRAFT-GUI\uff0c\u4e00\u4e2a\u57fa\u4e8eGroup Relative Policy Optimization (GRPO)\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3Reinforcement Learning (RL) \u65b9\u6cd5\u5728GUI\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u8003\u8651\u4e0d\u540c\u8f68\u8ff9\u7684\u96be\u5ea6\u53d8\u5316\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u7b80\u5355\u57fa\u4e8e\u89c4\u5219\u7684\u4fe1\u53f7\u548c\u6a21\u578b\u8bc4\u4f30\u7684\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u4f9b\u66f4\u4e30\u5bcc\u548c\u7ec6\u81f4\u7684\u53cd\u9988\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cCRAFT-GUI\u76f8\u8f83\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u516c\u5171\u57fa\u51c6Android Control\u4e0a\u63d0\u5347\u4e865.6%\uff0c\u5728\u5185\u90e8\u5728\u7ebf\u57fa\u51c6\u4e0a\u63d0\u5347\u4e8610.3%\uff0c\u5728GUI\u4ea4\u4e92\u4efb\u52a1\u4e2d\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u8bfe\u7a0b\u5b66\u4e60\u76f8\u7ed3\u5408\u80fd\u591f\u53d6\u5f97\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2508.11416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11416", "abs": "https://arxiv.org/abs/2508.11416", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "comment": null, "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AIM-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u4f9b\u5e94\u94fe\u7ba1\u7406\u60c5\u5883\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u7684LLM\u5b58\u5728\u51b3\u7b56\u504f\u5dee\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\u7b49\u7b56\u7565\u6709\u52a9\u4e8e\u7f13\u89e3\u8fd9\u4e9b\u504f\u5dee\uff0c\u5f3a\u8c03\u4e86\u5728\u5e93\u5b58\u51b3\u7b56\u60c5\u666f\u4e2d\u90e8\u7f72LLM\u65f6\u9700\u8981\u8003\u8651\u6f5c\u5728\u504f\u89c1\u3002\u5e0c\u671b\u8fd9\u4e9b\u53d1\u73b0\u80fd\u4e3a\u51cf\u8f7b\u4eba\u7c7b\u51b3\u7b56\u504f\u89c1\u3001\u5f00\u53d1\u4f9b\u5e94\u94fe\u7684\u4eba\u4e3a\u4e2d\u5fc3\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u9274\u4e8eLLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u60c5\u5883\u4e0b\u505a\u51fa\u5e93\u5b58\u51b3\u7b56\u7684\u80fd\u529b\u4ee5\u53ca\u4ee3\u7406\u7684\u51b3\u7b56\u504f\u89c1\u4ecd\u672a\u88ab\u6df1\u5165\u63a2\u8ba8\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86AIM-Bench\uff0c\u4ee5\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u4f9b\u5e94\u94fe\u7ba1\u7406\u60c5\u5883\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\u3002", "method": "\u5f15\u5165\u4e86AIM-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u591a\u6837\u5316\u7684\u5e93\u5b58\u8865\u8d27\u5b9e\u9a8c\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u51b3\u7b56\u884c\u4e3a\u3002\u7814\u7a76\u63a2\u7d22\u5e76\u8bd5\u56fe\u7f13\u89e3\u51b3\u7b56\u504f\u89c1\uff0c\u5305\u62ec\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\u7b49\u7b56\u7565\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4e0d\u540cLLM\u901a\u5e38\u8868\u73b0\u51fa\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u4e0d\u540c\u7a0b\u5ea6\u7684\u51b3\u7b56\u504f\u5dee\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u4e86\u4e00\u4e9b\u7b56\u7565\uff0c\u5982\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u51b3\u7b56\u504f\u89c1\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86 AIM-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u4f9b\u5e94\u94fe\u7ba1\u7406\u60c5\u5883\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\u7684\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u7684LLM\u901a\u5e38\u8868\u73b0\u51fa\u5404\u4e0d\u76f8\u540c\u7684\u51b3\u7b56\u504f\u5dee\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6240\u89c2\u5bdf\u5230\u7684\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u7f13\u89e3\u62c9\u5411\u4e2d\u5fc3\u6548\u5e94\u548c\u725b\u97ad\u6548\u5e94\u7684\u7b56\u7565\uff0c\u5373\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\u7684\u5b9e\u65bd\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u90e8\u7f72LLM\u8fdb\u884c\u5e93\u5b58\u51b3\u7b56\u60c5\u666f\u65f6\uff0c\u9700\u8981\u8ba4\u771f\u8003\u8651\u6f5c\u5728\u504f\u89c1\u7684\u5fc5\u8981\u6027\u3002\u5e0c\u671b\u8fd9\u4e9b\u89c1\u89e3\u5c06\u4e3a\u51cf\u8f7b\u4eba\u7c7b\u51b3\u7b56\u504f\u89c1\u4ee5\u53ca\u4e3a\u4f9b\u5e94\u94fe\u5f00\u53d1\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2508.11452", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "AI": {"tldr": "\u901a\u8fc7Inclusion Arena\u5b9e\u65f6\u6392\u884c\u699c\uff0c\u5229\u7528\u4eba\u7c7b\u53cd\u9988\u5bf9\u6a21\u578b\u8fdb\u884c\u6392\u540d\uff0c\u4ee5\u52a0\u901fLLMs\u548cMLLMs\u7684\u53d1\u5c55\u3002\u91c7\u7528Bradley-Terry\u6a21\u578b\u4e0e\u4e24\u9879\u5173\u952e\u521b\u65b0\uff0c\u786e\u4fdd\u8bc4\u4f30\u53cd\u6620\u5b9e\u9645\u4f7f\u7528\u573a\u666f\uff0c\u63d0\u9ad8\u8bc4\u5206\u7a33\u5b9a\u6027\uff0c\u5e76\u51cf\u5c11\u6076\u610f\u64cd\u7eb5\u7684\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u6392\u884c\u699c\u4f9d\u8d56\u4e8e\u9759\u6001\u6570\u636e\u96c6\u6216\u4f17\u5305\u901a\u7528\u63d0\u793a\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u63d0\u51faInclusion Arena\u586b\u8865\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528Bradley-Terry\u6a21\u578b\u5e76\u7ed3\u5408\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a(1) Placement Matches\uff0c\u7528\u4e8e\u5feb\u901f\u4f30\u8ba1\u65b0\u6574\u5408\u6a21\u578b\u7684\u521d\u59cb\u8bc4\u5206\uff0c(2) Proximity Sampling\uff0c\u4e00\u79cd\u667a\u80fd\u6bd4\u8f83\u7b56\u7565\uff0c\u4f18\u5148\u8003\u8651\u5177\u6709\u76f8\u4f3c\u80fd\u529b\u7684\u6a21\u578b\u4e4b\u95f4\u7684\u6bd4\u8f83\uff0c\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u83b7\u53d6\u5e76\u589e\u5f3a\u8bc4\u5206\u7a33\u5b9a\u6027\u3002", "result": "Inclusion Arena\u4ea7\u751f\u53ef\u9760\u548c\u7a33\u5b9a\u7684\u6392\u540d\uff0c\u6bd4\u4e00\u822c\u4f17\u5305\u6570\u636e\u96c6\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u4f20\u9012\u6027\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u6076\u610f\u64cd\u7eb5\u7684\u98ce\u9669\u3002", "conclusion": "\u63d0\u51faInclusion Arena\uff0c\u4e00\u4e2a\u5b9e\u65f6\u6392\u884c\u699c\uff0c\u901a\u8fc7\u76f4\u63a5\u4eceAI\u5e94\u7528\u7a0b\u5e8f\u6536\u96c6\u7684\u4eba\u7c7b\u53cd\u9988\u5bf9\u6a21\u578b\u8fdb\u884c\u6392\u540d\uff0c\u786e\u4fdd\u8bc4\u4f30\u53cd\u6620\u5b9e\u9645\u4f7f\u7528\u573a\u666f\uff0c\u65e8\u5728\u52a0\u901fLLMs\u548cMLLMs\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.11493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11493", "abs": "https://arxiv.org/abs/2508.11493", "authors": ["David H. Chan", "Mark Roberts", "Dana S. Nau"], "title": "Landmark-Assisted Monte Carlo Planning", "comment": "To be published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence", "summary": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in\nevery solution plan$\\unicode{x2013}$have contributed to major advancements in\nclassical planning, but they have seldom been used in stochastic domains. We\nformalize probabilistic landmarks and adapt the UCT algorithm to leverage them\nas subgoals to decompose MDPs; core to the adaptation is balancing between\ngreedy landmark achievement and final goal achievement. Our results in\nbenchmark domains show that well-chosen landmarks can significantly improve the\nperformance of UCT in online probabilistic planning, while the best balance of\ngreedy versus long-term goal achievement is problem-dependent. The results\nsuggest that landmarks can provide helpful guidance for anytime algorithms\nsolving MDPs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728\u968f\u673a\u9886\u57df\u4e2d\u5f88\u5c11\u88ab\u4f7f\u7528\u7684\u6982\u7387\u6027\u5730\u6807\uff0c\u5728MDPs\u4e2d\u6709\u6548\u5229\u7528\u5730\u6807\u4ee5\u63d0\u9ad8UCT\u7b97\u6cd5\u6027\u80fd\u7684\u7814\u7a76\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5408\u7406\u9009\u62e9\u7684\u5730\u6807\u53ef\u4ee5\u663e\u8457\u6539\u5584UCT\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8d2a\u5a6a\u5730\u6807\u5b9e\u73b0\u4e0e\u6700\u7ec8\u76ee\u6807\u5b9e\u73b0\u4e4b\u95f4\u9700\u8981\u5e73\u8861\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u7ecf\u5178\u89c4\u5212\u4e2d\u7684\u5730\u6807\u6761\u4ef6\u5728\u968f\u673a\u9886\u57df\u4e2d\u5f88\u5c11\u88ab\u5e94\u7528\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u63a2\u8ba8\u5728\u6982\u7387\u6027\u89c4\u5212\u4e2d\u5982\u4f55\u5229\u7528\u5730\u6807\u6765\u6539\u5584\u6027\u80fd\u3002", "method": "\u672c\u6587\u5f62\u5f0f\u5316\u4e86\u6982\u7387\u6027\u5730\u6807\u5e76\u8c03\u6574\u4e86UCT\u7b97\u6cd5\u4ee5\u5229\u7528\u5b83\u4eec\u4f5c\u4e3a\u5b50\u76ee\u6807\u6765\u5206\u89e3MDPs\u3002\u5173\u952e\u5728\u4e8e\u5728\u8d2a\u5a6a\u5730\u6807\u5b9e\u73b0\u548c\u6700\u7ec8\u76ee\u6807\u5b9e\u73b0\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5728\u57fa\u51c6\u9886\u57df\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u9009\u62e9\u7684\u5730\u6807\u53ef\u4ee5\u663e\u8457\u6539\u5584UCT\u5728\u5728\u7ebf\u6982\u7387\u89c4\u5212\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8d2a\u5a6a\u5730\u6807\u5b9e\u73b0\u4e0e\u957f\u671f\u76ee\u6807\u5b9e\u73b0\u4e4b\u95f4\u7684\u5e73\u8861\u56e0\u95ee\u9898\u800c\u5f02\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728\u7ecf\u5178\u89c4\u5212\u4e2d\u8d77\u5230\u91cd\u8981\u4f5c\u7528\u7684\u5730\u6807\u6761\u4ef6\uff0c\u5728\u968f\u673a\u9886\u57df\u4e2d\u5f88\u5c11\u88ab\u4f7f\u7528\u3002\u4f5c\u8005\u5f62\u5f0f\u5316\u4e86\u6982\u7387\u6027\u5730\u6807\u5e76\u8c03\u6574\u4e86UCT\u7b97\u6cd5\u4ee5\u5229\u7528\u5b83\u4eec\u4f5c\u4e3a\u5b50\u76ee\u6807\u6765\u5206\u89e3MDPs\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u9009\u62e9\u7684\u5730\u6807\u53ef\u4ee5\u663e\u8457\u6539\u5584UCT\u5728\u5728\u7ebf\u6982\u7387\u89c4\u5212\u4e2d\u7684\u6027\u80fd\uff0c\u800c\u8d2a\u5a6a\u4e0e\u957f\u671f\u76ee\u6807\u5b9e\u73b0\u4e4b\u95f4\u7684\u5e73\u8861\u56e0\u95ee\u9898\u800c\u5f02\u3002\u7814\u7a76\u7ed3\u679c\u6697\u793a\uff0c\u5730\u6807\u53ef\u4ee5\u4e3a\u89e3\u51b3MDPs\u7684\u4efb\u610f\u65f6\u95f4\u7b97\u6cd5\u63d0\u4f9b\u6709\u76ca\u6307\u5bfc\u3002"}}
{"id": "2508.11524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11524", "abs": "https://arxiv.org/abs/2508.11524", "authors": ["Wenkai Yu", "Jianhang Tang", "Yang Zhang", "Shanjiang Tang", "Kebing Jin", "Hankz Hankui Zhuo"], "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models", "comment": null, "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684LLM\u8f85\u52a9\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u548c\u4e24\u79cdLLM\u4f7f\u7528\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aLLM4Predict\u6bd4LLM4Inspire\u6548\u679c\u66f4\u663e\u8457\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u9488\u5bf9\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u4e2d\u7531\u4e8e\u5bf9\u8c61\u548c\u64cd\u4f5c\u589e\u52a0\u800c\u5bfc\u81f4\u7684\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u6311\u6218\uff0c\u4ee5\u53ca\u524d\u4eba\u5de5\u4f5c\u5ffd\u89c6LLMs\u4e0e\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6574\u5408\u7684\u95ee\u9898\u3002", "method": "\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u5c06LLMs\u4e0e\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u76f8\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u548c\u4e24\u79cdLLM\u4f7f\u7528\u8303\u5f0f\uff08LLM4Inspire\u548cLLM4Predict\uff09\u6765\u8f85\u52a9\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u7684\u89e3\u51b3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684LLM\u8f85\u52a9\u89c4\u5212\u5668\u80fd\u6709\u6548\u5b9a\u4f4d\u53ef\u884c\u89e3\uff0c\u5e76\u5728\u641c\u7d22\u7a7a\u95f4\u526a\u679d\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u6548\u3002LLM4Predict\u76f8\u5bf9\u4e8eLLM4Inspire\u5728\u878d\u5165\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684LLM\u8f85\u52a9\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u3002\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u5c06\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u878d\u5165LLM\u4e2d\u5bf9\u89c4\u5212\u6548\u679c\u6709\u663e\u8457\u63d0\u5347\uff0c\u6bd4\u8d77\u63d0\u4f9b\u4e00\u822c\u77e5\u8bc6\u7684LLM4Inspire\uff0c\u4fa7\u91cd\u4e8e\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684LLM4Predict\u5728\u641c\u7d22\u7a7a\u95f4\u526a\u679d\u65b9\u9762\u8868\u73b0\u66f4\u4e3a\u7a81\u51fa\u3002"}}
