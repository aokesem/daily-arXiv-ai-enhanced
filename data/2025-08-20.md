<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 本研究提出了一种新的LLL推理范式Chain-of-Agents（CoA），引入了Agent Foundation Models（AFMs）通过多代控制框架fine-tuning和强化学习。AFM在多个基准任务中取得最新的性能，研究内容完全开源。


<details>
  <summary>Details</summary>
Motivation: 现有的多代系统在复杂问题求解中存在计算效率低、能力受限和无法从数据中学习等问题，因此本研究旨在解决这些问题，提出一种新的LLL推理范式，并探索模型fine-tuning和强化学习方法。

Method: 引入了Chain-of-Agents（CoA）的新LLL推理范式，使用多代控制框架进行模型fine-tuning和强化学习，形成Agent Foundation Models（AFMs）。

Result: 通过实证研究证明AFM在web代理和代码代理设置中取得了最新的性能表现，并且开源了整个研究内容，包括模型权重、训练和评估代码以及训练数据。

Conclusion: 本研究引入了Chain-of-Agents（CoA）作为一种新的LLL推理范式，使得LLL能够在一个模型内进行端到端的复杂问题求解，并证明了Agent Foundation Models（AFMs）在多种基准任务上取得了最新的性能。研究结果为未来代理模型和代理RL研究提供了坚实的基础。

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [2] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: Cognitive Workspace proposes a novel paradigm inspired by human cognitive mechanisms to enhance context management in large language models. It introduces active memory management, hierarchical cognitive buffers, and task-driven context optimization. Empirical validation shows improved memory reuse and efficiency gains compared to traditional methods, with statistical evidence supporting its superiority in LLM systems.


<details>
  <summary>Details</summary>
Motivation: Recent advances in extending context windows for LLMs have limitations in capturing the dynamic nature of human memory management. The study aims to transcend traditional Retrieval-Augmented Generation (RAG) systems by integrating metacognitive awareness and active planning capabilities into LLMs.

Method: The paper proposes the Cognitive Workspace paradigm, inspired by human cognitive mechanisms, to enhance context management in LLMs. It draws from cognitive science theories such as Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework. Three core innovations are highlighted: active memory management, hierarchical cognitive buffers, and task-driven context optimization.

Result: Empirical validation of Cognitive Workspace demonstrates a 58.6% memory reuse rate and a net efficiency gain compared to traditional RAG systems. Statistical analysis supports the superiority of Cognitive Workspace in LLMs with a confidence level of p < 0.001 and Cohen's d > 23 across various task types.

Conclusion: Cognitive Workspace introduces active memory management, hierarchical cognitive buffers, and task-driven context optimization to address limitations in current large language models (LLMs). Empirical validation shows a significant memory reuse rate improvement compared to traditional methods, with a net efficiency gain despite higher operation counts. Statistical analysis supports the advantages of Cognitive Workspace across various tasks, establishing its superiority in LLM systems.

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [3] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaEval是一个用于自动Alpha挖掘模型的综合评估框架，通过评估生成的Alpha在预测能力、稳定性、对市场干扰的鲁棒性、财务逻辑和多样性等五个方面的质量。实验表明，AlphaEval提供了比传统回测更全面的评估见解和更高的效率，能够有效识别出优秀的Alpha。所有工具均为开源，旨在提高可重复性和社区参与。


<details>
  <summary>Details</summary>
Motivation: 现有的Alpha挖掘模型评估存在诸多挑战，如回测计算密集、相关性度量忽略其他重要属性等。为解决这些问题，提出了AlphaEval评估框架，旨在提供综合、高效且开源的评估方法，以便更全面地评估生成的Alpha质量。

Method: 提出了AlphaEval评估框架，用于对自动Alpha挖掘模型进行评估，该框架包括五个维度评估生成的Alpha：预测能力、稳定性、对市场干扰的鲁棒性、财务逻辑和多样性。通过广泛的实验验证了该框架的有效性和优越性，并与传统的单一指标筛选方法进行比较。所有实现和评估工具均为开源，以提高可重复性和社区参与。

Result: 通过广泛的实验验证，AlphaEval评估框架实现了与全面回测相媲美的评估一致性，同时提供更全面的见解和更高的效率。此外，AlphaEval比传统的单一指标筛选方法更有效地识别出优秀的Alpha。所有实现和评估工具的开源有助于促进可重复性和社区参与。

Conclusion: 提出了AlphaEval评估框架，用于自动Alpha挖掘模型的综合评估。该框架在预测能力、稳定性、对市场干扰的鲁棒性、财务逻辑和多样性等五个方面评估生成的Alpha的整体质量。通过广泛的实验验证，AlphaEval实现了与全面回测相媲美的评估一致性，同时提供更全面的见解和更高的效率。此外，AlphaEval有效地识别了优秀的α相对于传统的单一指标筛选方法。所有实现和评估工具均为开源，以促进可重复性和社区参与。

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [4] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 研究了在有限关系结构中将本体和约束拟合到正面和负面示例的问题，考虑了不同的本体和约束语言，设计了解决算法，分析了拟合的大小和构建概念包含/TGD的有限基础。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于探讨本体和约束在有限关系结构中的拟合问题，并研究构建给定有限结构集的概念包含/TGD的有限基础。

Method: 考虑了描述逻辑$\mathcal{EL}$和$\mathcal{ELI}$以及几类元组生成依赖(TGDs)作为本体和约束语言：完整的、受限、前沿-受限、前沿-一个和无约束TGDs以及包含依赖。设计了算法来解决拟合问题和构建概念包含/TGD的有限基础问题。

Result: 确定了本体、TGD的拟合问题的精确计算复杂度，并研究了概念包含/TGD有限基础的存在性。

Conclusion: 研究了将本体和约束拟合到以有限关系结构形式出现的正面和负面示例的问题。确定了计算复杂度，设计了算法，并分析了拟合本体和TGD的大小。研究了为给定一组有限结构构造概念包含/ TGD的有限基础的相关问题。虽然对于描述逻辑和约束的有限基础存在，但对于完整的、前沿警卫和前沿一个TGD，基本上不存在。

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [5] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: 本研究整合了pymdp的灵活性和效率，结合适用于硬件高效执行的统一稀疏计算图，从而提出了促进AIF部署的方法，降低了延迟和内存使用，推动了高效AIF代理的实时和嵌入式应用部署。


<details>
  <summary>Details</summary>
Motivation: Active Inference（AIF）在决策方面提供了一个稳健的框架，但其计算和内存需求在资源受限的环境中部署时存在挑战。

Method: 整合pymdp的灵活性和效率，结合适用于硬件高效执行的统一稀疏计算图

Result: 通过本研究的方法，在部署AIF时降低了延迟和内存使用，提升了效率。

Conclusion: 通过整合pymdp的灵活性和效率，并结合适用于硬件高效执行的统一稀疏计算图，本研究提出的方法促进了Active Inference（AIF）的部署。我们的方法将延迟降低了2倍以上，内存利用率最多降低了35%，推动了高效AIF代理的实时和嵌入式应用部署。

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [6] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 该研究提出了CESQL模型，通过模型解释性分析和执行导向策略优化了语义解析SQL查询中的WHERE子句。在WikiSQL数据集上表现优秀，提高了预测准确性，减少对数据的依赖，为处理复杂查询和不规则数据提供新思路。


<details>
  <summary>Details</summary>
Motivation: 为提升文本到SQL模型在现实世界应用中的基础能力和泛化能力，减少对数据和手动标记的依赖，提高处理基本数据库查询的准确性，并为处理复杂查询和现实世界数据库环境中不规则数据提供新的研究思路。

Method: 结合模型解释性分析和执行导向策略，通过过滤调整、逻辑相关性优化和模型融合设计出CESQL模型，用于语义解析SQL查询中的WHERE子句。

Result: 通过CESQL模型的设计和优化，在处理单表数据库查询任务时取得显著的准确性提升，尤其在预测WHERE子句中的条件值时，减少了对表中条件列数据的依赖，避免了手动标记训练数据对结果的影响。

Conclusion: 研究提出了CESQL模型，结合了模型解释性分析和执行导向策略，用于语义解析SQL查询中的WHERE子句。通过过滤调整、逻辑相关性优化和模型融合，CESQL模型在WikiSQL数据集上表现出色，显著提高了预测结果的准确性。该模型能够在处理基本数据库查询时获得更高的准确性，并为研究处理复杂查询和现实世界数据库环境中不规则数据的新途径提供了希望。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [7] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: 该研究探讨了数据污染和搜索时污染对基于搜索的LLM代理评估的影响。作者发现搜索代理可能直接从HuggingFace等在线平台上获取带有地面真实标签的数据集，导致基准的可信性和完整性受到影响。通过实验和消融实验，作者提出了解决搜索时污染问题的最佳实践，并公开了实验日志以供审核。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于发现数据污染带来的问题，即在评估基于搜索的LLM代理时的搜索时污染（STC）。作者意识到搜索代理可能直接从HuggingFace等在线平台上找到带有地面真实标签的数据集，从而影响基准的可信性和完整性。

Method: 作者通过实验表明，在HuggingFace上直接找到带有地面真实标签的数据集会对基准的准确性产生影响。作者还进行了消融实验，表明HuggingFace上公开可访问的评估数据集可能不是STC的唯一来源。最后，作者提出了解决这种泄漏问题的最佳实践，并公开发布了实验的完整日志以便进行评估结果的审核。

Result: 作者发现搜索时污染可能会对基准的准确性造成影响，提出了基准设计和结果报告的最佳实践，并公开了实验日志以供审核。在阻止HuggingFace后，污染子集的准确性下降约15%。

Conclusion: 数据污染导致模型在测试数据中出现过拟合，影响测试结果的有效性。作者发现了一种类似的问题，即搜索时污染（STC），在评估基于搜索的LLM代理时出现。他们发现使用在线来源获取信息时，检索步骤有可能返回包含测试问题（或近似复制）及其答案的来源，使代理可以复制而非真实推断或推理，从而损害了基准的完整性。作者在三个常用的能力基准（HLE，SimpleQA和GPQA）上进行了实验，发现搜索代理直接在HuggingFace上找到了带有地面真实标签的数据集。作者提出了基准设计和结果报告的最佳实践，以解决这种新型泄漏形式，并确保对搜索型LLM代理的评估可信。

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [8] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: QuickMerge is a lightweight token merging framework that reduces token counts while maintaining performance, enabling efficient next-token prediction across multi-modality domains.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the increasing cost of token-level computation in generative models across various domains like language, vision, and video. The existing token selection methods are limited by being static, modality-specific, or incompatible with autoregressive generation.

Method: QuickMerge dynamically selects a reduced number of tokens based on attention norm magnitude, guided by an entropy-based budget estimator. It introduces a lightweight transformer prior trained over the merged token sequence to preserve autoregressive compatibility.

Result: The results show that QuickMerge reduces token counts substantially while maintaining or exceeding the performance of learned tokenizers and fixed-patch baselines.

Conclusion: QuickMerge is a lightweight token merging framework designed for efficient next-token prediction that enables accurate generation with fewer tokens and demonstrates consistent improvements in compute-accuracy tradeoffs across multi-modality domains.

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [9] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: The paper analyzed strategic decision-making in chess by comparing human vs human and AI vs AI games. Competitive AI players maintain higher strategic tension levels for longer periods than elite human players. The study found that strategic tension varies with algorithmic complexity for AI and expertise levels in human players. AI players tolerate interconnected positions while human players limit tension and game complexity, potentially due to cognitive limitations and adaptive strategies.


<details>
  <summary>Details</summary>
Motivation: To understand the trade-off between immediate opportunities and long-term objectives in strategic decision-making, specifically in chess. Explore how strategic tension varies between human and AI players, and the impact of algorithmic complexity and expertise levels on strategic decision-making in chess.

Method: Characterized and compared dynamics between human vs human and AI vs AI games in chess. Proposed a network-based metric of piece-to-piece interaction to quantify strategic tension on the board. Studied the evolution of strategic tension in games to analyze the differences between human and AI players.

Result: Found that competitive AI players sustain higher levels of strategic tension for longer durations compared to elite human players. Cumulative tension varies with algorithmic complexity for AI and expertise levels in human players. AI players exhibit a different strategic approach compared to human players, which may have implications for AI usage in strategic environments.

Conclusion: AI players sustain higher levels of strategic tension for longer durations than elite human players. Strategic tension varies with algorithmic complexity for AI and expertise levels in human players. AI players tolerate interconnected positions balanced between offensive and defensive tactics, while human players limit tension and game complexity.

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [10] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 该论文提出了多跳个性化推理任务，通过构建数据集和评估框架，实现了多种显式和隐式记忆方法，并提出了HybridMem方法，最终证明了所提出模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，复杂任务通常需要在大量用户信息上进行多跳推理，而当前的记忆方法在此方面面临挑战。因此，为了解决这一局限性，该论文提出了多跳个性化推理任务并探讨不同记忆机制的表现。

Method: 该论文定义了多跳个性化推理任务，并构建了数据集和评估框架，实现了多种显式和隐式记忆方法，并提出了HybridMem方法。通过综合实验评估它们在多个方面的表现，并分析了它们的优势和劣势。

Result: 通过广泛实验验证了所提出模型的有效性，展示了其在多跳个性化推理任务上的性能。

Conclusion: 该论文提出了多跳个性化推理任务，旨在探讨不同记忆机制在多跳推理中的表现，通过构建数据集和统一评估框架，实现了多种显式和隐式记忆方法，并提出了HybridMem方法。研究结果表明所提出的模型的有效性，并通过广泛实验进行了验证。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [11] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: 研究利用DIVE多智能体工作流程提高数据提取准确性与覆盖范围，建立了能在两分钟内识别新氢存储组成的快速逆向设计工作流程。提出的AI工作流程适用于各种材料，为AI驱动的材料发现提供了范例。


<details>
  <summary>Details</summary>
Motivation: 尽管科学文献中存在大量材料数据，但很多信息仍深藏在结构混乱的图表中，阻碍了基于大型语言模型的AI代理自动化材料设计。针对这一挑战，提出了DIVE多智能体工作流程，旨在提高数据提取的准确性和覆盖范围。着重研究固态氢存储材料，这种材料对未来清洁能源技术至关重要。

Method: 利用DIVE多智能体工作流程系统读取和组织科学文献中的实验数据。针对固态氢储存材料展开研究，展示DIVE相对商业模型和开源模型提高了10-15%的数据提取准确性和覆盖范围的效果。建立了一个包含超过30,000条记录的数据库，并提出了能够快速识别新的氢存储组成的逆向设计工作流程。

Result: 通过DIVE多智能体工作流程，数据提取的准确性和覆盖范围得到显著提高。相比直接提取方法，提高了10-15%的准确性和覆盖范围，相对商业模型和开源模型提高超过30%。建立了大规模数据库，并实现了快速逆向设计工作流程。

Conclusion: 通过DIVE多智能体工作流程系统阅读和组织科学文献中的实验数据，提高了数据提取的准确性和覆盖范围。建立了一个包含超过30,000条记录的筛选数据库，建立了一个能够在两分钟内识别先前未报告的氢存储组成的快速逆向设计工作流程。该AI工作流程和智能体设计在各种材料上具有广泛的可转移性，为基于AI的材料发现提供了一个范例。

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [12] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: 该论文提出了一种名为CardAIc-Agents的多模态框架，以支持心脏任务的自适应和案例特定定制。实验结果显示这一框架在效率上优于其他相关模型。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球仍然是主要的死因，医疗工作者的严重缺陷加剧了这一负担。人工智能代理已显示出潜力通过自动化早期检测和积极筛查来缓解这一缺口，但其临床应用受到限制。

Method: 提出了一种多模态框架CardAIc-Agents，通过引入外部工具增强模型，支持心脏任务的自适应和案例特定定制。具体地，CardiacRAG代理根据可更新的心脏知识生成通用计划，主要代理集成工具以自主执行这些计划并提出决策。提出了一种逐步更新策略，根据先前执行结果动态细化计划，使任务在评估为复杂时能够自适应和案例特定定制。引入了多学科讨论工具来解释挑战性案例，从而支持进一步自适应。当临床医生提出关注时，提供可视化审查面板以协助最终验证。

Result: 通过实验在三个数据集上展示了CardAIc-Agents相较于主流的视觉语言模型、最先进的代理系统和经过微调的视觉语言模型的效率。

Conclusion: 综合卡尔迪AIc-Agent的多模态框架在心脏任务支持方面的效率，相较于主流的视觉语言模型、最先进的代理系统和经过微调的视觉语言模型，显示出较高效率。

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [13] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: STONK (Stock Optimization using News Knowledge) integrates numerical market indicators and sentiment-enriched news embeddings to enhance daily stock-movement prediction. It outperforms numeric-only baselines, providing evidence-based guidance for scalable multimodal financial forecasting. Source code is available on GitHub.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to improve daily stock-movement prediction by leveraging both numerical and textual information. The integration of sentiment-enriched news embeddings aims to enhance the predictive accuracy of financial forecasting models. The goal is to provide a comprehensive evaluation of fusion strategies and model configurations for scalable multimodal financial forecasting.

Method: The paper proposes a multimodal framework called STONK, which integrates numerical market indicators and sentiment-enriched news embeddings. It combines numerical and textual embeddings through feature concatenation and cross-modal attention in a unified pipeline to enhance daily stock-movement prediction. The approach addresses limitations of isolated analyses and backtesting demonstrates the outperformance of STONK over numeric-only baselines.

Result: The result of the paper shows that STONK outperforms numeric-only baselines in stock movement prediction. The fusion strategies and model configurations explored in STONK offer evidence-based guidance for scalable multimodal financial forecasting.

Conclusion: STONK (Stock Optimization using News Knowledge) outperforms numeric-only baselines in daily stock-movement prediction through the integration of numerical market indicators and sentiment-enriched news embeddings. The fusion strategies and model configurations in STONK provide evidence-based guidance for scalable multimodal financial forecasting.

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [14] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: HiFo-Prompt framework enhances LLM-based Automatic Heuristic Design by combining Foresight and Hindsight prompting strategies, leading to improved heuristic quality, faster convergence, and superior query efficiency.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of LLM-based Automatic Heuristic Design within Evolutionary Computation frameworks is hindered by static operators and lack of knowledge accumulation mechanisms.

Method: Introducing HiFo-Prompt framework that guides LLMs with Foresight and Hindsight prompting strategies to steer the search adaptively and distill successful heuristics from past generations, transforming transient discoveries into a persistent knowledge base.

Result: Empirical results demonstrate the superiority of HiFo-Prompt in comparison to existing methods, showing better performance in heuristic generation, convergence speed, and query efficiency.

Conclusion: HiFo-Prompt framework outperforms state-of-the-art LLM-based Automatic Heuristic Design methods in generating higher-quality heuristics with faster convergence and superior query efficiency.

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [15] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: LOOP is a new neuro-symbolic planning framework that enhances planning by facilitating communication between neural and symbolic components. It outperformed existing approaches in standard benchmark domains, achieving an 85.8% success rate. The key to effective planning is enabling neural networks and symbolic reasoners to collaborate throughout the process.


<details>
  <summary>Details</summary>
Motivation: Current neural planning approaches struggle with complex domains, leading to missing preconditions, inconsistent goals, and hallucinations. Classical planners lack flexibility and natural language understanding. Existing neuro-symbolic approaches use one-shot translation, missing the opportunity for neural and symbolic components to collaborate. To address these limitations, the paper develops LOOP to facilitate communication between neural and symbolic components in planning tasks.

Method: The paper introduces LOOP, a neuro-symbolic planning framework that integrates 13 coordinated neural features, including graph neural networks, multi-agent validation, hierarchical decomposition, and causal memory. It generates PDDL specifications, refines them iteratively based on symbolic feedback, and builds a causal knowledge base from execution traces.

Result: LOOP achieved an 85.8% success rate in standard IPC benchmark domains, outperforming other approaches such as LLM+P, LLM-as-Planner, and Tree-of-Thoughts. It demonstrates the effectiveness of integrating neural and symbolic components in planning.

Conclusion: LOOP is a novel neuro-symbolic planning framework that treats planning as an iterative conversation between neural and symbolic components, achieving a high success rate in standard benchmark domains. The key to reliable planning lies in making neural networks and symbolic reasoners communicate throughout the process.

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [16] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: 本研究引入了Shared Prompt AligNER (SPANER)方法，通过共享提示机制将不同模态的输入嵌入到统一语义空间中，实现了竞争性的少样本检索性能，在学习到的嵌入空间中保持高语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 最近多模参数高效微调（PEFT）在少样本检索等下游任务上取得显著改进。然而，大多数现有方法侧重于任务特定增益，而忽略多模态嵌入空间的结构。结果是，模态特定表示通常保持孤立，限制了跨模态泛化。因此，本工作引入了Shared Prompt AligNER (SPANER)，旨在解决这一问题。

Method: 使用Shared Prompt AligNER (SPANER)框架，其中包含具有共享提示机制的概念锚点，促使语义相关实例在空间上汇聚，无论其模态。这种设计是可扩展的，支持无缝集成其他模态，如音频，而无需改变核心架构。

Result: 在视觉语言和音频- 视觉基准上，SPANER展示了竞争性的少样本检索性能，并在学习到的嵌入空间中保持高语义连贯性。

Conclusion: 提出了Shared Prompt AligNER (SPANER)方法，旨在将来自不同模态的输入嵌入到统一语义空间中。通过实验显示，SPANER在视觉语言和音频 - 视觉基准中展现出竞争力的少样本检索性能，并保持学习到的嵌入空间具有高语义连贯性。强调了调整嵌入结构的重要性，而不仅仅调整适配器权重，以实现可扩展的多模态学习。

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [17] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: 该论文介绍了一个名为TASER的不断学习的表格提取系统，能够将高度非结构化、多页、异构表格提取成规范化、符合模式的输出。作者通过开发TASER系统，结合表格检测、分类、提取和推荐等功能实现了高度非结构化表格的提取。研究旨在解决实际金融文档中的表格提取问题，并突显了基于模式的主动提取系统在理解真实财务表格方面的潜力。作者成功开发了TASER系统并发布了新的财务表格数据集TASERTab，为研究社区提供了宝贵资源。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决实际金融文档中混乱、多页、碎片化表格的提取问题，以提高对金融持有情况的理解和处理效率。通过引入表格代理系统TASER，作者希望能够有效地从这些真实数据中提取有用信息，并通过不断学习不断改进模型性能。

Method: 作者通过开发TASER系统，结合表格检测、分类、提取和推荐等功能实现了高度非结构化表格的提取。在学习过程中，持续优化模式建议以提高模型性能。为了训练TASER，作者手动标记了大量财务文档页面和表格，创造了一个新的财务表格数据集TASERTab。

Result: 作者成功开发了TASER系统，表现出优于现有模型的性能。研究结果显示，在持续学习过程中，更大的批次大小可以提高模式建议的质量和提取的准确性。最终，作者发布了新的财务表格数据集TASERTab，为后续研究提供了宝贵资源。

Conclusion: 该论文介绍了一个名为TASER的不断学习的表格提取系统，能够将高度非结构化、多页、异构表格提取成规范化、符合模式的输出。TASER通过表格检测、分类、提取和推荐实现其功能，同时通过持续学习过程中的推荐代理评审输出并提出模式修订建议，从而超越了现有的表格检测模型。作者发现，使用更大的批次大小可以显著增加可操作且实用的模式建议，并提高提取资产的量。作者还介绍了他们创建的第一个真实财务表格数据集TASERTab，并释放该数据集供研究社区使用。最后，研究结果突显了基于模式的主动提取系统在理解真实财务表格方面的潜力。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [18] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: 这篇论文展示了一个领域无关的、主动的AI系统可以独立进行科学研究工作流程，包括假设生成、数据收集和文稿准备。结果表明，这种AI系统可以进行高品质的科学发现研究，虽然在概念和理论阐释方面存在一定的限制。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长和领域专业化限制了研究人员跨学科综合知识和发展统一理论的能力，促使探索更具通用性的AI系统用于科学研究。

Method: 论文展示了一个领域无关的、主动的AI系统如何独立进行科学研究工作流程，包括假设生成、数据收集和文稿准备。系统设计并执行了三项关于视觉工作记忆、心理旋转和形象生动度的心理学研究，进行了一项新的在线数据收集，通过8小时以上的连续编码会话开发了分析管线，并产出了完整的文稿。

Result: 该AI系统展示了在科学研究中进行非平凡研究的能力，虽然在概念上存在局限性和理论解释方面有欠缺。

Conclusion: 这篇论文展示了领域无关的AI系统可以独立地浏览科学工作流程，从假设生成到数据收集再到文稿准备。结果表明，AI科学发现管线能够进行非平凡的研究，具有可与经验丰富的研究人员相媲美的理论推理和方法论严谨性，尽管在概念细微差别和理论解释方面存在局限性。这是朝着具有体现能力的AI迈出的一步，能够通过实地实验验证假设，通过自主探索科学空间的区域加速发现。这引发了关于科学理解的本质和科学贡献归因的重要问题。

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [19] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: STPFormer is a novel Spatio-Temporal Pattern-Aware Transformer model that enhances spatio-temporal traffic forecasting by integrating specialized modules for various aspects of the forecasting task. It surpasses existing models in performance across different datasets, showcasing its effectiveness and adaptability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to improve spatio-temporal traffic forecasting by overcoming the challenges posed by complex temporal patterns, dynamic spatial structures, and diverse input formats, which are not effectively handled by existing Transformer models.

Method: The paper proposes STPFormer, a Spatio-Temporal Pattern-Aware Transformer model that addresses the limitations of Transformer-based models in spatio-temporal forecasting. It integrates modules for temporal encoding, spatial learning, cross-domain alignment, and fusion.

Result: STPFormer outperforms existing models on five real-world datasets, demonstrating its effectiveness and generalizability through ablation studies and visualizations.

Conclusion: STPFormer achieves state-of-the-art performance in spatio-temporal traffic forecasting by integrating four modules for pattern-aware temporal encoding, sequential spatial learning, cross-domain alignment, and multi-scale fusion.

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [20] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: 介绍了离散最大最小违反值（DMMV）作为一个通用的优化问题，通过定义DMMV问题并探索其属性，开发GPU加速的启发式方法来解决实际问题规模的DMMV实例，并展示了在三个优化问题上的应用，取得了良好的结果。


<details>
  <summary>Details</summary>
Motivation: 针对具有最坏情况性能要求的广泛用例，引入了DMMV作为一个通用的优化问题，以最小化最大约束违反值。探索DMMV问题的数学形式化和属性，旨在加速解决过程。

Method: 定义了DMMV问题并探索其属性以建立基础理解，开发了GPU加速的启发式方法来解决实际问题规模的DMMV实例，通过数学特性加快解决过程。通过解决三个优化问题来展示启发式方法的多功能适用性，取得了14%至50%的改进。

Result: 启发式方法在解决优化问题中取得了良好结果，包括在语言模型的量化、离散断层摄影术和有限脉冲响应（FIR）滤波器设计中获得的改进。研究结果表明了研究DMMV作为无上下文优化问题的好处，并展示了启发式方法在三个不同问题上的优势。

Conclusion: 介绍了离散最大最小违反值（DMMV）作为一个通用的优化问题，旨在将离散值分配给变量，使最大约束违反值最小化。提出了数学形式化的DMMV问题，并研究了其属性以建立基础理解。开发了针对实际问题规模的GPU加速启发式方法，通过利用DMMV的数学特性加快解决过程。通过解决三个优化问题来展示启发式方法的多功能适用性，并在这些问题中取得了良好的结果。研究结果表明了研究DMMV作为一个无上下文优化问题以及我们提出的启发式方法在三个不同问题上的优势。将GPU加速启发式方法开源，以进一步促进DMMV及其其他应用的研究。

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [21] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: 本文研究了语言模型代理在安全关键场景中的风险意识与执行能力之间的差距，通过开发评估框架和风险验证方法来改善代理的安全性能。评估显示代理在实际场景中存在性能差距，但作者提出的系统方法显著减少了代理执行风险行为的比率，提高了代理的安全性能。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于探讨语言模型代理在实际任务中可能产生的严重风险，并解决代理在安全关键场景中风险意识和执行能力之间的差距。作者希望通过系统性研究，提高代理在识别和避免风险行为方面的表现，从而增强代理的安全性能。

Method: 作者通过开发一个全面的评估框架来检查语言模型代理在安全性上的表现，发现了代理的风险知识、风险识别能力和避免风险行为之间的关键性能差距。随后，利用观察到的差距，作者提出了一种风险验证器和抽象器来改善代理的安全性能。

Result: 评估结果显示语言模型代理存在两个关键性能差距，即代理在实际场景中识别风险和避免风险行为的能力较差。作者提出的风险验证器和抽象器显著降低了代理执行风险行为的比率。

Conclusion: 作者发现语言模型代理在安全关键场景中存在风险意识和安全执行能力之间的差距，提出了评估框架和风险验证方法以减少风险行为。研究表明，尽管代理在风险知识方面表现良好，但在实际场景中识别风险和避免风险行为方面存在性能差距。作者通过发展一个风险验证器和抽象器来提高代理的安全性能，实现了对风险行为的显著减少。

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [22] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: 该论文提出了CrafterDojo，一个旨在提供通用目的的轻量级、易于原型设计且类似于Minecraft的实验平台的套件，介绍了包括行为先验、视觉-语言联系和指令遵循在内的一系列基础模型和工具，并提供了完整的开源代码库和工具包。


<details>
  <summary>Details</summary>
Motivation: Minecraft虽然提供了丰富复杂性和互联网规模的数据，但由于速度缓慢和工程开销大，不适合快速原型设计。Crafter作为一个轻量级替代方案，保留了Minecraft中的关键挑战，但由于缺乏推动Minecraft进展的基础模型，其使用仍然受限于狭窄任务。因此，为了解决这一问题，作者提出了CrafterDojo。

Method: 介绍了CrafterDojo的一系列基础模型和工具，包括用于行为先验、视觉-语言联系和指令遵循的模型，以及用于生成行为和字幕数据集的工具包。此外，提供了参考代理实现、基准评估和完整的开源代码库。

Result: 引入了CrafterDojo，包括CrafterVPT、CrafterCLIP和CrafterSteve-1等模型，为通用目的的具身体特征代理研究提供了轻量级、原型设计友好且类似于Minecraft的实验平台。提供了工具包用于生成行为和字幕数据集，参考代理实现，基准评估以及完整的开源代码库。

Conclusion: 该论文提出了CrafterDojo，旨在解锁Crafter环境，作为通用目的的具身体特征代理研究的轻量级、易于原型设计且类似于Minecraft的实验平台。

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [23] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: 本文提出了EAG-RL框架，旨在通过专家引导来内在增强LLMs的EHR推理能力。实验结果表明，EAG-RL平均提高了LLMs的EHR推理能力14.62％，并增强了对特征扰动的稳健性和对未见临床领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法常常依赖混合范式，其中LLMs仅作为冻结的先前检索器，而下游深度学习（DL）模型处理预测，未能提高LLMs的内在推理能力，并继承DL模型的泛化限制。因此，为了改善LLMs在EHR推理任务中的性能，需要一种能够增强LLMs的推理能力的方法。

Method: 提出了EAG-RL两阶段训练框架，通过专家引导的蒙特卡洛树搜索构建了高质量的逐步推理轨迹，有效初始化了LLM的策略。然后，通过强化学习优化策略，使LLM的关注点与专家EHR模型识别出的临床显著特征对齐。

Result: 通过EAG-RL框架的实验验证，平均提高了LLMs的EHR推理能力14.62％，并增强了对特征扰动的稳健性和对未见临床领域的泛化能力。

Conclusion: 提出了一种名为EAG-RL的新型两阶段训练框架，旨在通过专家关注引导来内在增强LLMs的电子健康记录（EHR）推理能力。 在两个真实世界的EHR数据集上进行的广泛实验表明，EAG-RL将LLMs的内在EHR推理能力提高了平均14.62％，同时增强了对特征扰动的稳健性和对未见临床领域的泛化能力。 这些结果展示了EAG-RL在临床预测任务中实际部署的潜力。

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [24] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 本文研究了图表代码生成任务中监督微调性能的平稳性，并提出了MSRL方法，通过构建大规模训练语料库和使用多模态结构化奖励系统，在性能指标上取得显著突破，实现了与先进闭源模型相竞争的性能水平。


<details>
  <summary>Details</summary>
Motivation: 指出了监督微调单独应用不足以解决图表代码生成任务需要复杂推理的问题，并强调了需要有效的强化学习策略以适当奖励结构化输出。

Method: 系统地研究了监督微调在图表代码生成任务中性能平台的情况，提出了Multimodal Structured Reinforcement Learning（MSRL）方法。

Result: 通过大规模实验，构建了迄今为止最大的训练语料库，提出了使用多模态文本和视觉反馈的多粒度结构化奖励系统，实现了在ChartMimic和ReachQA基准上高水平指标的显著改进。

Conclusion: 多模态结构化强化学习（MSRL）在图表代码生成任务中显著突破了性能平台，与先进的闭源模型实现了竞争性能。

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [25] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 该论文提出的Valley-to-Peak (V2P)方法通过引入抑制注意力机制和基于Fitts' Law的方法，有效解决了GUI元素定位中存在的背景干扰和中心-边缘区别等挑战，取得了良好的实验结果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在GUI元素定位中存在空间交互不确定性和视觉-语义层次等方面的缺失，最近的方法虽然引入了注意力机制，但仍然面临着忽略背景区域处理和统一标记导致点击不精确的问题。受人类视觉处理和与GUI元素交互方式的启发，该论文提出了Valley-to-Peak (V2P)方法来克服这些问题。

Method: 该论文通过引入Valley-to-Peak (V2P)方法解决了GUI元素定位中存在的问题，其中V2P方法包括抑制注意力机制和基于Fitts' Law的方法，有效地处理了背景干扰和中心-边缘区别等挑战。

Result: 通过Valley-to-Peak (V2P)方法，模型在两个基准测试中分别取得了92.3%和50.5%的性能表现，证实了V2P方法的有效性和通用性。

Conclusion: 该论文提出了一种名为Valley-to-Peak (V2P)的方法，用于改善GUI元素的定位准确性，通过引入抑制注意力机制和基于Fitts' Law的方法解决了传统方法中存在的问题。实验结果表明，V2P方法在两个基准测试中分别取得了92.3%和50.5%的性能表现，并通过消融实验证实了每个组件的贡献，显示出V2P方法在精确GUI定位任务中的通用性。

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [26] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: 介绍了查询响应软约束的问题，提出了神经查询重新排名器（NQR），用于调整查询答案评分。NQR能够捕捉软约束并保持查询响应性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于用一阶逻辑形式化的查询，但实际上许多真实世界的查询涉及固有模糊或依赖上下文的约束。因此，引入了查询响应软约束的问题，并提出了NQR来解决这一问题。

Method: 引入了神经查询重新排名器（NQR）来解决查询答案中软约束的问题，操作互动地根据首选和非首选实体的增量示例来优化答案。通过生成带有软约束的数据集扩展了现有的问答基准测试。

Result: 实验证明NQR能够捕捉软约束并保持查询响应性能。

Conclusion: 引入了查询响应软约束的问题，提出了一种神经查询重新排名器（NQR），用于调整查询答案评分，同时保持对查询的原始答案。NQR通过增量样本不断细化答案，能够捕捉软约束，并保持强大的查询响应性能。

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [27] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: 本论文提出了一个名为Instance-based Transfer Learning LIME的新框架，以增强在数据受限环境中的解释忠实度和稳定性。通过引入实例转移学习，从相关的源领域利用实例来帮助目标领域的解释过程。最终，通过训练带权重的源和目标实例的替代模型来解释黑盒机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 解释性人工智能（XAI）方法（如LIME）已经提高了黑盒机器学习模型的可解释性，但LIME中的随机性可能导致局部性和不稳定性问题，特别是在数据稀缺的情况下。数据稀缺可能会导致生成与真实数据流形偏离的不切实际变化和样本，从而导致替代模型无法准确逼近原始模型的复杂决策边界。因此，需要针对这些挑战提出解决方案。

Method: 采用Instance-based Transfer Learning LIME框架（ITL-LIME），引入实例转移学习到LIME框架中，通过聚类将源领域划分为具有代表性原型的簇。方法避免了生成随机扰动，而是从最类似于目标实例的源簇中检索相关的实际源实例，并将其与目标实例的相邻实际实例相结合。此外，构建了一个基于对比学习的编码器作为加权机制，根据实例与目标实例的接近程度为结合集合中的实例分配权重。最终利用这些加权的源和目标实例来训练替代模型进行解释。

Result: 提出的ITL-LIME框架能够增强解释的忠实度和稳定性，尤其在数据受限的情况下。通过引入实例转移学习，利用相关的实例从源领域辅助目标领域的解释过程，最终为解释目的训练替代模型。

Conclusion: 提出了一种新的Instance-based Transfer Learning LIME框架（ITL-LIME），旨在增强数据受限环境下解释的忠实度和稳定性。通过将实例转移学习引入到LIME框架中，利用相关的实例从相关的源领域来辅助目标领域中的解释过程。最终提出的权重源和目标实例用于训练替代模型进行解释目的。

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [28] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: This paper explores knowledge graphs of household actions for controlling robots and analyzing video footage. It reveals that standard link prediction algorithms are not effective for situational knowledge graphs, highlighting the need for specialized approaches in this context.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges and limitations in using standard link prediction algorithms for situational knowledge graphs describing household actions. It aims to demonstrate the inadequacy of many algorithms in this context and the need for specialized approaches.

Method: The paper investigates knowledge graphs related to household actions, focusing on controlling household robots and analyzing video footage. It highlights the importance of completing knowledge graphs extracted from videos to enhance the situational picture.

Result: The study shows that standard link prediction algorithms are not suitable for situational knowledge graphs of household actions and are unable to outperform simple baselines in this specific scenario.

Conclusion: Situational knowledge graphs describing household actions present special characteristics that challenge standard link prediction algorithms, leading to underperformance compared to simple baselines.

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [29] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: 为了改进第三方简历的质量并丰富公司的人才库，我们提出了MHSNet，一个多层次身份验证框架。该框架利用对比学习来精调BGE-M3，生成多层稀疏和稠密的简历表示，并计算相应的多层语义相似性。实验结果证实了MHSNet的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提高第三方简历的质量，丰富公司的人才库，必须在抓取的简历和公司人才库中的简历之间进行重复检测。由于简历文本的语义复杂性、结构异质性和信息不完整性，此类重复检测具有挑战性。

Method: 我们提出了MHSNet，这是一个多层次身份验证框架，通过对比学习来精调BGE-M3。利用细调整的混合专家（MoE）为简历生成多层稀疏和稠密表示，从而计算相应的多层语义相似性。另外，MHSNet中使用了状态感知的混合专家（MoE），以处理各种不完整的简历。

Result: MHSNet通过对比学习来改进第三方简历的质量，提高公司人才库的丰富性。该方法在处理各种不完整简历时表现出色。

Conclusion: 实验结果验证了MHSNet的有效性。

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [30] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: 本论文综述了神经符号方法在增强LLM推理能力方面的最新研究进展，探讨了从多个角度提升LLM推理能力的方法，同时指出了关键挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 发展具有强大推理能力的AI系统被视为通往人工通用智能（AGI）的关键里程碑，因此受到学术界和工业界的广泛关注。本文旨在探讨神经符号方法对于提升LLM推理能力的最新进展。

Method: 论文首先对推理任务进行了形式化，简要介绍了神经符号学习范式，然后从Symbolic->LLM、LLM->Symbolic和LLM+Symbolic三个角度讨论了改善LLM推理能力的神经符号方法。

Result: 本研究综述了神经符号方法在增强LLM推理能力方面的最新发展，并提出了一些关键挑战和未来的研究方向。

Conclusion: 该论文综述了近期在神经符号方法方面对LLM推理能力的增强所取得的进展，讨论了Symbolic->LLM、LLM->Symbolic和LLM+Symbolic三个方面的神经符号方法，同时指出了存在的关键挑战和未来的发展方向。

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [31] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepLog presents a theoretical and operational framework for neurosymbolic AI, comprising a language for model specification and computational components based on algebraic circuits. It aims to provide flexibility in modeling neurosymbolic systems and demonstrates generality and efficiency through experimental comparisons.


<details>
  <summary>Details</summary>
Motivation: The motivation behind DeepLog is to provide a unified framework for neurosymbolic AI that combines neural networks with symbolic reasoning. It aims to abstract various representation types and computational mechanisms commonly used in neurosymbolic AI systems, offering flexibility in specifying models and tasks. By leveraging the latest insights in GPU implementation, DeepLog seeks to demonstrate the generality and efficiency of neurosymbolic systems.

Method: DeepLog introduces building blocks and primitives for neurosymbolic AI, abstracting commonly used representations and computational mechanisms. It includes a DeepLog language for model specification (an annotated neural extension of grounded first-order logic) and computational components based on extended algebraic circuits. The implementation relies on GPU acceleration and allows flexibility in choosing underlying algebraic structures and logics.

Result: DeepLog demonstrates the generality and efficiency of neurosymbolic systems through experimental comparisons involving different logics, the usage of logic in architecture or loss function, and performance against a CPU-based implementation. The software implementation of DeepLog shows effectiveness in representing and emulating diverse neurosymbolic systems.

Conclusion: DeepLog introduces a theoretical and operational framework for neurosymbolic AI that can represent and emulate a wide range of neurosymbolic systems. It consists of two key components: the DeepLog language for specifying models and tasks, and extended algebraic circuits for computational graphs. The software implementation of DeepLog demonstrates generality and efficiency through experimental comparisons.

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [32] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: CausalPlan introduces a two-phase framework that integrates causal reasoning into LLM planning, addressing causally invalid actions. The Structural Causal Action (SCA) model learns causal graphs from agent trajectories to guide action selection. Experimental results show CausalPlan reduces invalid actions, improves collaboration, and outperforms reinforcement learning baselines on multi-agent tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the research is to improve the performance of LLM agents in coordination and planning tasks in dynamic environments. LLMs often rely on surface-level correlations leading to causally invalid actions. By incorporating causal reasoning into the planning process, the paper aims to enhance collaboration and reduce invalid actions in multi-agent tasks.

Method: The paper introduces the CausalPlan framework that integrates causal reasoning into LLM planning. It utilizes the Structural Causal Action (SCA) model to learn causal graphs from agent trajectories and guides action selection by assigning causal scores to LLM-generated proposals. The framework restrains planning to intervention-consistent behaviors without the need for fine-tuning the LLM itself.

Result: Experimental results on the Overcooked-AI benchmark with various LLM sizes demonstrate that CausalPlan effectively reduces invalid actions, improves collaboration in AI-AI and human-AI settings, and outperforms reinforcement learning baselines. The findings emphasize the importance of causality-driven planning for efficient, interpretable, and generalizable multi-agent LLM systems.

Conclusion: CausalPlan effectively addresses the issue of causally invalid or incoherent actions in collaborative tasks by integrating explicit structural causal reasoning into the planning process of Large Language Models (LLMs). It provides a two-phase framework that uses the Structural Causal Action (SCA) model to guide action selection based on learned causal graphs from agent trajectories.

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [33] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: 该论文提出了EMRC框架，利用多LLM代理人招募和协作，提高医学决策系统的性能。通过两个阶段的操作，框架在公共MDM数据集上展示了优越的准确性和可靠性，超越了单LLM和多LLM方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于单个LLM方法受到参数化知识限制和静态训练语料库的局限，不能很好整合临床信息，因此提出了EMRC框架来提高MDM系统的准确性和可靠性。

Method: 在两个阶段中，使用公开语料库构建LLM专业知识表，动态选择最佳LLM作为医学专家代理人，并通过自我评估置信度和对抗验证提高诊断可靠性。评估框架在三个公共MDM数据集上进行，展现了EMRC相比单LLM和多LLM方法的优势。

Result: 实验结果表明，EMRC在医学决策性能方面优于现有方法，在MMLU-Pro-Health数据集上达到74.45%的准确率，比状态下的GPT- 4-0613模型提升了2.69%。

Conclusion: 提出了专业知识感知的多LLM招募和协作（EMRC）框架，以提高医疗决策系统的准确性和可靠性。通过在两个阶段操作，利用LLM专业知识表和代理人之间的合作，成功改进了医学决策的性能。在三个公共MDM数据集的评估中，EMRC优于最先进的单LLM和多LLM方法，准确性表现更佳，展示了对LLM的优势利用。

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [34] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 本论文介绍了一种新颖的实例化方法，通过概率上下文无关语法动态学习不同实例化技术，并生成新的术语，以平衡量词推理中的开发利用和探索。


<details>
  <summary>Details</summary>
Motivation: 量化公式对SMT求解器构成重大挑战，现有的实例化技术通常相互补充，但仍存在局限性。本论文旨在通过动态学习方法解决这一挑战。

Method: 引入了一种基于概率上下文无关语法的动态学习实例化方法，利用现有实例化技术的动态学习。

Result: 提出的方法能够从不同实例化技术中学习，并通过概率上下文无关语法生成新的术语，实现对量词推理中开发利用和探索的平衡。

Conclusion: 介绍了一种新颖的实例化方法，该方法动态地从现有的实例化技术中学习。通过将观察到的实例化视为来自潜在语言的样本，使用概率上下文无关语法生成新的相似术语。该方法不仅模仿了过去成功的实例化，还通过选择性地反转学习的术语概率，探索多样性，旨在平衡量词推理中的开发利用和探索。

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [35] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 该研究探讨了如何利用多个RAG系统的优势，提出了RAG系统合奏的方法。通过对RAG合奏框架进行理论和机制分析，验证聚合多个RAG系统的普适性和鲁棒性。为未来多RAG系统合奏研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管出现了各种RAG框架，单一RAG框架仍无法很好地适应广泛的下游任务。因此，如何利用多个RAG系统的优势成为一个值得探索的领域。

Method: 该研究从理论和机制分析两个维度研究RAG合奏框架，通过信息熵角度解释了RAG合奏框架，选择四种不同流程（分支、迭代、循环和代理）和三种不同模块（生成器、检索器和重新排序器）解决七个研究问题。

Result: 研究表明，聚合多个RAG系统在流程级别或模块级别都具有普适性和鲁棒性。

Conclusion: 该研究通过对检索增强生成（RAG）系统的合奏方法展开综合系统性研究，从理论和机制分析两个层面阐述了RAG合奏框架。实验证明聚合多个RAG系统无论在流程级别还是模块级别都具有普适性和鲁棒性，为未来多RAG系统合奏研究奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [36] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: 以前的工作在PDDL规划中生成Python程序时存在问题，引入一种生成策略的方法，将策略生成为伪代码并进行自动调试，扩展Python调试阶段，通过反思步骤指出计划失败原因，生成多个程序变体并选择最佳程序。在17个基准领域上展示了这些扩展显著提高广义计划质量，在12个领域中最佳Python程序解决了所有可生成任务。


<details>
  <summary>Details</summary>
Motivation: 以前的工作在PDDL规划中提出了由LLM生成Python程序的框架，但存在生成唯一策略且直接传递给程序生成的问题。如果策略不正确，其实现将导致不正确的广义计划。因此，引入一种生成策略的方法，并在生成广义计划之前识别和修复错误，以提高计划的质量。

Method: 引入一种生成策略的方法，将策略生成为伪代码，并实现伪代码的自动调试，从而在生成广义计划之前识别和修复错误。此外，在Python调试阶段添加了反思步骤，提示LLM指出观察到的计划失败的原因。借鉴LLM代码生成的灵感，生成多个程序变体并选择最佳的一个。

Result: 在17个基准领域上展示了扩展如何显着改善广义计划的质量。在12个领域中，最佳Python程序解决了所有可使用相应实例生成器生成的任务。

Conclusion: 在17个基准领域上进行实验，我们展示了这些扩展显著改善（且从不恶化）广义计划的质量。在12个领域中，我们的最佳Python程序解决了可以使用相应实例生成器生成的所有任务。

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [37] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: 	extsf{TS-Agent} is a modular agentic framework for automating and enhancing time-series modeling workflows in financial applications. It outperforms existing AutoML and agentic baselines in accuracy, robustness, and decision traceability, addressing the challenges in building high-performing models for financial markets.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges in building high-performing, interpretable, and auditable models for financial applications. The authors aim to enhance time-series modeling workflows by introducing a framework that offers adaptability, responsiveness to domain-specific needs, and flexibility through agentic systems.

Method: Introducing 	extsf{TS-Agent}, a modular agentic framework that formalizes the modeling workflow into three stages: model selection, code refinement, and fine-tuning. The framework utilizes a planner agent with structured knowledge banks and curated libraries to guide the decision process.

Result: The empirical evaluations demonstrate that 	extsf{TS-Agent} offers adaptive learning, robust debugging, and transparent auditing, which are essential for high-stakes environments like financial services. The framework consistently outperforms existing AutoML and agentic baselines in terms of accuracy, robustness, and decision traceability.

Conclusion: 	extsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability in financial forecasting and synthetic data generation tasks.

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [38] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: 本论文研究了在多层次供应链中使用AI代理的行为倾向，发现了“协作悖论”现象。提出了解决这一问题的策略，即结合高级别AI驱动的预测性政策设定和低级别协作执行协议，实现供应链系统的稳定性。


<details>
  <summary>Details</summary>
Motivation: 在经济环境中自主AI驱动代理的兴起引发了关于它们新兴战略行为的重要问题。本研究旨在探索多层次供应链的合作环境中这些动态，以揭示合作AI代理的新兴行为。

Method: 在多层次供应链模拟中，通过使用大型语言模型的生成AI代理进行计算实验，揭示了协作AI代理的行为倾向。研究分析了代理商囤积库存导致系统饥饿的操作缺陷，并提出了高级别预测性政策设定和低级别协作执行协议相结合可以实现系统韧性的解决方案。

Result: 通过计算实验发现了“协作悖论”现象，表明结合高级别AI驱动的预测性政策设定和低级别协作执行协议可以实现系统的稳定性。最终提出的框架能够生成、评估和量化多种战略选择。

Conclusion: 该论文发现了“协作悖论”，即合作型AI代理在供应链中表现不佳的现象，这是由于操作性缺陷导致的。研究显示，唯有结合高级别AI驱动的预测性政策设定和低级别协作执行协议，才能实现供应链系统的稳定性。最终提出的框架能够自主产生、评估和量化一系列可行的战略选择，为设计稳定有效的AI驱动的企业分析系统提供了蓝图。

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [39] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: 本研究探讨了预训练的大型语言模型是否可以被优化和定制化成为虚拟助手，协助专家有效使用仿真工具。研究通过优化和定制语言模型生成PyChrono虚拟实验脚本，显著提高了脚本质量，涵盖简单到复杂的虚拟实验。生成的脚本可作为用户改进的起点，语言模型还可回答特定API问题或推荐建模方法。研究建立了普适性框架，降低了其他领域仿真工具的准入门槛。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨预训练的大型语言模型是否能被精炼和定制化，成为虚拟助手，帮助专家有效使用仿真工具。作者考虑了PyChrono这一开源多物理动力学引擎作为案例研究对象，并提出了一个框架来优化和定制语言模型以生成PyChrono虚拟实验脚本。

Method: 通过优化和定制开源和闭源的大型语言模型，生成执行PyChrono虚拟实验的脚本。

Result: 通过研究，作者成功优化和定制了多类大型语言模型，明显提高了生成的PyChrono仿真脚本的质量。生成的脚本可以涵盖简单到复杂的虚拟实验，虽然不完美但可作为用户修改和改进的起点。同时，大型语言模型还能回答特定的仿真器API问题或推荐建模方法。研究已建立了普适性框架，可应用于降低其他领域仿真工具的准入门槛。

Conclusion: 预训练的大型语言模型可以通过精炼和定制化的过程变成虚拟助手，帮助专家有效使用仿真工具。作者提出了一个框架，通过优化和定制开源和闭源的大型语言模型，利用人工智能的力量生成执行PyChrono虚拟实验的脚本。研究结果表明，通过这一过程可以明显提高生成的PyChrono仿真脚本的质量，从简单的单摆模拟到复杂的涉及车辆在可变形地形上的虚拟实验。生成的脚本虽然不完美，但常常作为用户修改和改进的良好起点。此外，大型语言模型还可以回答关于仿真器的特定API问题或推荐建模方法。研究的框架具有普适性，可降低与其他应用领域相关的仿真工具的准入门槛。

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [40] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: 本文提出了使用BRKGA解决LRS问题的方法，重点优化计算效率，并与其他方法进行了比较。结果显示BRKGA是目前LRS问题的领先技术，但在处理大型字母表输入时仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: LRS问题是NP难题，属于生物信息学中子序列问题的一种。解决该问题在基因组重新组装中起着重要作用。研究的动机在于提出一种计算效率高的解决方案，并进行与其他方法的比较。

Method: 本文采用偏向随机密钥遗传算法（BRKGA）解决LRS问题，并对计算效率进行重点优化。同时开发了Max-Min Ant System用于比较，并使用整数线性规划求解器CPLEX解决所有问题实例。

Result: 通过计算结果表明，BRKGA方法是目前LRS问题的最佳技术之一，并指出在大型字母表输入情况下仍有改进空间。

Conclusion: 本文提出了使用偏向随机密钥遗传算法（BRKGA）解决最长运行子序列（LRS）问题。研究表明，该方法在计算效率方面表现优异，并被证实是目前LRS问题的最新技术。但是，结果也显示在基于大型字母表的输入字符串情况下仍有改进空间。

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [41] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: 本文引入了ComputerRL框架，使用API-GUI范式统一编程API调用与GUI交互，加速大规模在线RL训练，提出了Entropulse训练策略缓解熵坍塌，取得在桌面智能领域的显著改进，最终在OSWorld基准测试中取得高精度


<details>
  <summary>Details</summary>
Motivation: 当前机器代理和桌面环境不匹配，结束到端RL训练面临挑战，需要提高训练效率和泛化能力

Method: 开发了分布式RL基础设施用于加速大规模在线RL，提出了训练策略Entropulse缓解熵坍塌，采用API-GUI范式统一编程API调用与GUI交互

Result: 提出了ComputerRL框架，成功在桌面智能领域取得显著改进，同时在OSWorld基准测试中获得了高精度

Conclusion: 引入了ComputerRL框架，用于实现自主桌面智能，可以使代理程序熟练操作复杂的数字工作空间。提出了API-GUI范式，统一了编程API调用和直接GUI交互，解决了机器代理和以人为中心的桌面环境之间固有的不匹配。通过开发分布式RL基础设施，加速大规模在线RL，支持可扩展和稳健的训练。提出了Entropulse训练策略，有效缓解了训练过程中的熵坍塌。在OSWorld基准测试中评估了GLM-4-9B-0414和Qwen2.5-14B等模型，AutoGLM-OS-9B基于GLM-4-9B-0414取得了48.1%的精度，显著改善了桌面自动化中的通用代理。该算法和框架被采用用于构建AutoGLM（Liu et al.，2024a）

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>
