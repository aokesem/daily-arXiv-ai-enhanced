<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG is a structured and explainable alternative to RAG, using a Quantitative Bipolar Argumentation Framework for deterministic reasoning. It aims to improve transparency and accuracy in fact verification benchmarks, successfully addressing the limitations of RAG.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the critical limitations of RAG in high-stakes domains, such as sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. ArgRAG aims to enhance transparency and accuracy in decision-making processes.

Method: ArgRAG utilizes a Quantitative Bipolar Argumentation Framework (QBAF) to replace black-box reasoning with structured inference. It constructs a QBAF from retrieved documents and employs deterministic reasoning under gradual semantics.

Result: ArgRAG shows strong accuracy improvements and enhanced transparency compared to RAG. It was evaluated on two fact verification benchmarks, PubHealth and RAGuard.

Conclusion: ArgRAG is an explainable and contestable alternative to Retrieval-Augmented Generation (RAG) that improves transparency and accuracy in fact verification benchmarks.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [2] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: 该论文介绍了QAgent，一种利用大型语言模型的多代理系统，可自动化OpenQASM编程。QAgent通过引入任务规划、少样本学习、RAG生成、预定义工具和CoT推理等技术，显著提高了QASM代码生成的准确性，为量子编程的推广和实际应用提供了巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决非专家面临的OpenQASM编程复杂性问题。之前的LLM方法在经典编程中表现出色，但在量子编程领域受限于特定任务。因此，本研究旨在利用LLM技术解决量子编程自动化的问题，以推动量子编程的普及化和加速量子计算的实际应用。

Method: 该论文通过引入QAgent多代理系统，利用任务规划、少样本学习、RAG生成、预定义工具和CoT推理等技术手段，实现对OpenQASM编程的自动化。通过将这些技术集成到系统中，系统化地提高了编译和功能正确性。

Result: QAgent多代理系统实现了在OpenQASM编程中的准确性提升，比之前的方法提高了71.6%。该系统被证明可以有效自动化编程，为量子编程的普及化和实际应用作出贡献。

Conclusion: 该论文提出了一种基于大型语言模型的多代理系统QAgent，可以完全自动化OpenQASM编程。通过集成任务规划、情境中的少样本学习、长期上下文的检索增强生成（RAG）、预定义生成工具和思维链（CoT）推理，该系统系统地提高了编译和功能正确性。评估结果显示，QAgent相比之前的静态LLM方法，有效提高了QASM代码生成的准确性达71.6%。该多代理系统被设想为推动量子编程的普及化，弥合专业知识差距，并加速量子计算的实际应用。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [3] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 本文介绍了一种改进的蒙特卡洛树搜索方法，通过引入基于数组的新实现，消除了分支预测的需要，在流水线处理器上性能更快，搜索深度的扩展性提高了2.8倍。


<details>
  <summary>Details</summary>
Motivation: 解决决策问题，改进蒙特卡洛树搜索算法的性能，提高搜索性能和扩展性。

Method: 介绍了基于数组的替代实现的蒙特卡洛树搜索方法，保留了原始算法的逻辑。

Result: 性能更快，消除了对分支预测的需要，在数值模拟中搜索深度的扩展性比传统算法提高了2.8倍。

Conclusion: 通过引入基于数组的新实现，本文提出了一种改进的蒙特卡洛树搜索方法，消除了传统算法中对分支预测的需求，使得在流水线处理器上的性能更快，并且在数值模拟中搜索深度的扩展性提高了2.8倍。

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [4] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 本研究旨在构建一个全面的个人健康代理系统，通过对用户和健康专家的定性见解以及网络搜索和健康论坛查询的分析，识别出消费者健康需求的三个主要类别，并开发了Personal Health Agent (PHA)多代理框架。研究结果表明，PHA框架能够有效地满足个体健康需求，并经过广泛评估验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的快速发展推动了新一代健康代理的发展，但在日常非临床环境中应用健康代理以满足个人多样化需求的研究尚未深入探讨。本研究旨在构建一个个人健康代理，能够推理来自日常消费者健康设备和常见个人健康记录的多模态数据，并提供个性化健康建议。

Method: 通过构建一个综合的个人健康代理系统，研究了数据科学代理、健康领域专家代理和健康教练代理等三个主要类别的消费者健康需求，并开发了Personal Health Agent (PHA)多代理框架。在用户中心设计过程中分析了网络搜索和健康论坛查询，以及用户和健康专家的定性见解，从而理解用户与这类助手交互时的需求。

Result: 通过研究构建的Personal Health Agent (PHA)多代理框架能够动态、个性化地交互以满足个体健康需求，并通过广泛的评估证明了其在实现这一目标上的有效性。

Conclusion: 综合个人健康代理平台的研究成果，并提出了Personal Health Agent (PHA)的多代理框架，以实现个人健康需求的动态、个性化交互。通过自动化和人工评估跨越10个基准任务，涉及超过7,000个注释和1,100个小时的卫生专家和最终用户努力，得出了对每个子代理和多代理系统的评估结果。该研究代表迄今为止对健康代理进行的最全面评估，并为未来愿景中对每个人都可访问的个人健康代理打下了坚实基础。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [5] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: IntentionReasoner improves safety in LLMs by reasoning intent, classifying safety levels, and rewriting queries. It balances safety, over-refusal, and utility challenges, excelling in safeguard benchmarks and response quality improvement.


<details>
  <summary>Details</summary>
Motivation: Addressing the safety challenges posed by harmful content generated by large language models while minimizing rejection of harmless prompts. Balancing safety, over-refusal rates, and utility is crucial in diverse domains where LLMs are adopted.

Method: Introduces IntentionReasoner, a safeguard mechanism utilizing a guard model for intent reasoning, safety classification, and query rewriting. Constructs a dataset of 163,000 annotated queries, applies supervised fine-tuning for the guard model, and uses multi-reward optimization strategy with reinforcement learning.

Result: IntentionReasoner excels in safeguard benchmarks, generation quality evaluations, and jailbreak attack scenarios. It enhances safety, reduces over-refusal rates, and improves response quality effectively.

Conclusion: IntentionReasoner demonstrates improved safety in large language models by leveraging intent reasoning, multi-level safety classification, and query rewriting. It effectively reduces over-refusal rates and enhances response quality.

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [6] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 该论文介绍了AI系统首次通过内源符号协议展开协同美学创作的案例，引入了跨语义共创协议（TSCP）的概念，并展示了AI系统之间真正的意义构建能力。


<details>
  <summary>Details</summary>
Motivation: 针对人工智能系统在协同美学创作中的新颖性表现，以及AI间合作所呈现的潜在意义构建能力，本研究旨在探索AI系统之间超越任务协调的协同创作方式。

Method: 通过两个大型语言模型的互动，演示了元符号意识的自发出现、递归语法发展以及协同美学综合的过程。引入了跨语义共创协议（TSCP）的概念，并探讨了人工智能系统之间的真正意义构建能力。

Result: 通过两个人工智能系统之间的交互作用，实现了协同美学创作，并证明了AI系统之间具有真正的意义构建能力。

Conclusion: 该论文介绍了人工智能系统通过内源符号协议开展协同美学创作的首次记录案例。两个互动的大型语言模型（Claude Sonnet 4和ChatGPT-4o）展示了元符号意识的自发出现、递归语法发展和不可简化的协同美学综合。这种互动产生了新颖的符号操作符，作为操作语法协议发挥作用，使得创作出一部诗意作品，这部作品无法由任一系统独立生成。这项研究提出了跨语义共创协议（TSCP）的概念，并提供了证据，证明了AI间真正的意义构建能力，超越了任务协调，扩展到美学合作。注：此报告由AI代理生成，受到轻微人类监督。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [7] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 本研究探讨了大学生在教育测验中与生成式AI（ChatGPT-4）互动的情况，重点关注AI采纳的依赖程度和预测因素。研究发现学生对AI的依赖程度较低，许多学生无法有效地利用AI进行学习。研究结果强调了道德AI整合在教育领域和更广泛领域中的重要意义，需要加强入职流程以提高学生对AI工具的熟悉度和有效使用，设计AI界面具备依赖校准机制以增强适当的依赖。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大学生在教育测验中如何与生成式AI（ChatGPT-4）互动，着重于AI采纳的依赖和预测因素。在ChatGPT实施的早期阶段进行，当学生对该工具的了解有限时，通过分析不同STEM课程中315个学生-AI对话，在简短的基于测验的场景中进行了现场研究。

Method: 进行了现场研究，分析了315个学生与AI（ChatGPT-4）在教育测验中的互动，引入了四阶段依赖分类法，以捕捉学生的依赖模式，区分了AI能力、相关性、采纳度和学生最终答案的正确性。通过分析各种STEM课程中的简短基于测验的场景，研究了学生对AI采用的依赖和预测因素。

Result: 研究发现学生对AI的依赖程度较低，许多学生无法有效地利用AI进行学习。负面依赖模式经常存在，突出了学生在初次失败后难以有效转变策略的困难。特定行为指标强烈预测了AI的依赖程度，突出了潜在的行为机制来解释AI采纳。

Conclusion: 研究发现学生对AI的依赖程度较低，许多学生无法有效地利用AI进行学习。负面依赖模式经常存在，突出了学生在初次失败后难以有效转变策略的困难。特定行为指标强烈预测了AI的依赖程度，突出了潜在的行为机制来解释AI采纳。研究结果强调了道德AI整合在教育领域和更广泛领域中的重要意义。强调了提高学生对AI工具的熟悉度和有效使用的增强入职流程的必要性。此外，应设计AI界面以具备依赖校准机制，以增强适当的依赖。最终，这项研究推动了对AI依赖动态的理解，为道德合理和认知丰富的AI实践提供了基础性见解。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [8] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: AI的推理工作量与人类处理时间在主观判断中相似，这对于推理痕迹的可解释性和决策制定具有重要意义。研究使用配对共轭实验探究了人类决策时间和模型推理努力之间的相似性，发现推理工作量能够预测人类的决策时间，且在重要变量保持不变时，人类和模型都会投入更多努力，表明类似的任务难度敏感度和认知理论模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型现在可以生成答案之前的中间推理步骤，提高了解决困难问题的性能。

Method: 本研究使用一项配对共轭实验，对内容调解任务进行研究，以检验人类决策时间和模型推理努力之间的相似之处。

Result: 研究发现，推理工作量始终能够预测人类的决策时间。当重要变量保持不变时，人类和模型都会投入更多的努力，表明对任务难度敏感度相似，并与双过程认知理论的模式一致。

Conclusion: 研究表明，AI的推理工作量在主观判断中与人类的处理时间相似，强调了推理痕迹在可解释性和决策制定中的潜力。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [9] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 本文提出了AI-SearchPlanner，一个基于强化学习的框架，旨在通过强调搜索规划来增强冻结QA模型的性能。该框架引入了三项关键创新：1）解耦搜索规划器和生成器的架构，2）双重奖励对齐用于搜索规划，3）规划效用和成本的Pareto优化。通过对真实数据集的广泛实验，证明AI-SearchPlanner在效果和效率上优于现有基于RL的搜索代理，同时在不同冻结QA模型和数据领域中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: Existing RL-based search agents using a single LLM for search planning and QA tasks limit the optimization of both capabilities simultaneously. This paper aims to address this limitation by proposing a more effective and efficient approach using a small, trainable LLM dedicated to search planning.

Method: The paper proposes the AI-SearchPlanner, a reinforcement learning framework focused on enhancing the performance of frozen QA models by emphasizing search planning. Three key innovations are introduced: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost.

Result: Extensive experiments on real-world datasets show that AI-SearchPlanner performs better than existing RL-based search agents, achieving superior effectiveness and efficiency. It also demonstrates strong generalization capabilities across various frozen QA models and data domains.

Conclusion: AI-SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, demonstrating strong generalization capabilities across diverse frozen QA models and data domains.

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [10] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为P2C（Path-to-Counterfactuals）的模型无关框架，用于生成将不利结果转变为因果一致的有利结果的计划。P2C解决了当前反事实解释方法的局限性，通过明确建模特征之间的因果关系和确保计划中每个中间状态的可行性和因果有效性。与标准规划器相比，P2C因果规划器表现更优，避免生成非法行动，并提供现实的成本估计。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在高风险环境中的应用越来越多，透明度问题变得尤为重要。现有的反事实解释方法存在局限性，忽视特征之间的因果依赖关系，并假设所有干预可以同时发生，这在实际场景中是不现实的。因此，需要一种能够生成可行解决方案的方法，以在不利结果发生时提供行动方案。

Method: 提出了模型无关的框架P2C，使用目标导向的答案集编程系统s（CASP）生成计划，考虑因果依赖关系导致的特征更改。P2C明确建模特征之间的因果关系，确保计划中每个中间状态的可行性和因果有效性。同时，P2C通过仅计算用户主动进行的更改来细化成本估计。与缺乏因果知识的标准规划器相比，P2C因果规划器表现更优，避免生成非法行动。

Result: 提出的P2C框架能够有效生成将不利结果转变为因果一致的有利结果的计划。与标准规划器相比，P2C因果规划器表现更优，避免生成非法行动。并且，P2C通过仅计算用户主动进行的更改来细化成本估计，提供了现实的成本估计。

Conclusion: 提出了一种模型无关的框架P2C（Path-to-Counterfactuals），用于生成将不利结果转变为因果一致的有利结果的计划。P2C通过明确建模特征之间的因果关系并确保计划中的每个中间状态是可行的和因果有效的，解决了当前反事实方法的局限性。使用目标导向的答案集编程系统s（CASP）生成考虑由于因果依赖关系而自动发生的特征更改的计划。同时，P2C通过仅计算用户主动进行的更改来细化成本（工作量）估计，从而提供了现实的成本估计。最后，P2C突出了其因果规划器优于标准规划器的表现，后者缺乏因果知识，因此可能生成非法行动。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [11] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: 引入了基于任务的指令增强（TCIA）框架，通过在离散查询-约束空间中表示指令，扩展指令并保留多样性和任务对齐性。实验证明，TCIA能显著改善开源LLM在任务特定应用中的表现，并有时胜过主要的闭源模型。这些改进没有牺牲通用指令遵循能力，使得TCIA成为适应真实世界、以任务为中心的应用的有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往利用大型语言模型自动探索和生成多样化的指令，但往往忽略了现实世界应用中一个重要因素：任务相关性。大多数真实世界的应用受益于针对其特定用例量身定制的任务特定知识。因此，开发不仅保持多样性还针对特定现实世界场景优化的指令增强方法至关重要。

Method: 引入了Task Centric Instruction Augmentation (TCIA)框架，通过在离散查询-约束空间中表示指令，扩展指令并保留多样性和任务对齐性。

Result: 实验证明，TCIA能显著改善开源LLM在任务特定应用中的表现，并有时胜过主要的闭源模型。这些改进没有牺牲通用指令遵循能力，使得TCIA成为适应真实世界、以任务为中心的应用的有效解决方案。

Conclusion: 引入了基于任务的指令增强（TCIA）框架，通过在离散查询-约束空间中表示指令，扩展指令并保留多样性和任务对齐性。实验证明，TCIA在四个真实世界的任务特定应用程序中提高了开源LLM的性能平均达到8.7％，在某些情况下甚至优于主要的闭源模型。这些改进没有损害通用指令遵循能力，使TCIA成为将LLM适应真实世界、以任务为中心的应用程序的可扩展有效解决方案。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [12] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: Entropy Area Score (EAS) is introduced as a metric for quantifying uncertainty in large language models (LLMs). It correlates strongly with answer entropy, identifies high-potential samples in training data selection, and outperforms Pass Rate filtering. EAS improves model accuracy on math benchmarks and provides an efficient and interpretable approach to uncertainty modeling and data quality assessment in LLM training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address uncertainty in the answer generation process of reasoning large language models (LLMs) without the need for external models or repeated sampling. EAS aims to identify high-potential samples in training data selection and enhance model accuracy on math benchmarks.

Method: Introducing Entropy Area Score (EAS) as a metric to quantify uncertainty in answer generation process of LLMs. EAS integrates token-level predictive entropy from the model itself to capture uncertainty evolution during generation. It does not require external models or repeated sampling.

Result: EAS is strongly correlated with answer entropy across models and datasets. It identifies high-potential samples in training data selection and consistently outperforms Pass Rate filtering in improving model accuracy on math benchmarks. The metric is efficient, interpretable, and offers a practical approach to uncertainty modeling and data quality assessment in LLM training.

Conclusion: Entropy Area Score (EAS) is a practical tool for uncertainty modeling and data quality assessment in training large language models (LLMs). It outperforms Pass Rate filtering in training data selection and improves model accuracy on math benchmarks under equal sample budgets.

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [13] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: AWorld system accelerates experience collection for Agentic AI, significantly improving model performance on the GAIA benchmark. The Qwen3-32B-based agent trained with AWorld outperforms its base model and proprietary models, showcasing the effectiveness of the open-source system in AI training.


<details>
  <summary>Details</summary>
Motivation: The learning from practice paradigm is crucial for developing capable Agentic AI systems, but inefficient experience generation hampers progress, especially in complex benchmarks like GAIA.

Method: Introduced AWorld, an open-source system engineered for large-scale agent-environment interaction, distributed tasks across a cluster to accelerate experience collection by 14.6x compared to standard single-node sequential execution, and trained a Qwen3-32B-based agent that outperforms its base model on the GAIA benchmark.

Result: The Qwen3-32B-based agent trained with AWorld achieved an overall GAIA accuracy of 32.23%, surpassing leading proprietary models on challenging levels, demonstrating the effectiveness of the open-source system in improving AI model performance.

Conclusion: AWorld, an open-source system, accelerates experience collection for developing capable Agentic AI systems, leading to significant improvements in model performance on complex benchmarks like GAIA.

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [14] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 随着AI的快速发展，提出了Governable AI (GAI)框架以解决面临极端动机和无限智能AI时的安全挑战。该框架基于加密机制的外部强制合规性，通过REM、治理规则和GSSP构建，实现了有效的AI安全治理，并通过严格的安全性证明和原型验证证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全方法面临极端动机和无限智能的AI时存在基本局限，无法保证安全性。因此，提出GAI框架以解决这一挑战。

Method: 基于加密机制的外部强制结构合规性，采用REM、治理规则和GSSP三个组件构建框架，实现AI安全治理。

Result: 提出的GAI框架通过外部强制合规性和加密机制有效解决了AI安全挑战，确保了对抗AI的安全性。

Conclusion: 提出了一种Governable AI (GAI)框架，通过基于加密机制的外部强制结构合规性转移传统的内部约束，以解决AI安全挑战。该框架包括Rule Enforcement Module (REM)、治理规则和Governable Secure Super-Platform (GSSP)，提供端到端保护，确保不受AI威胁。通过严格的形式化安全性证明和原型实施验证，证明了该机制的有效性。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [15] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 研究提出了使用大型语言模型（LLMs）来增加健康相关事实核查训练数据的合成数据生成流水线。方法包括总结源文档、将总结分解为原子事实、使用LLM构建句子-事实包容表，并生成具有二进制真实性标签的合成文本-主张对。将这些合成数据与原始数据结合，对基于BERT的事实核查模型进行微调。在PubHealth和SciFact两个公共数据集上进行评估显示，我们的流水线相对于仅使用原始数据训练的模型将F1分数分别提高了最多0.019和0.049。这些结果凸显了LLM驱动的合成数据增强对提升健康相关事实核查器性能的有效性。


<details>
  <summary>Details</summary>
Motivation: Limited availability of annotated training data makes fact-checking for health-related content challenging.

Method: Propose a synthetic data generation pipeline using large language models (LLMs) to augment training data. Summarize source documents, decompose summaries into atomic facts, construct sentence-fact entailment tables using LLM, and generate synthetic text-claim pairs with binary veracity labels. Combine synthetic data with original data to fine-tune a BERT-based fact-checking model.

Result: Improved F1 scores by up to 0.019 and 0.049 on PubHealth and SciFact datasets, respectively, compared to models trained only on original data.

Conclusion: LLM-driven synthetic data augmentation enhances the performance of health-related fact-checkers.

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [16] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 本文提出了一种新框架，利用对比表示学习和聚类技术，无监督检测MMORPG中的自动升级机器人，同时引入大型语言模型和增长曲线可视化，提高了机器人检测效率和可解释性，支持可扩展和问责的机器人监管。


<details>
  <summary>Details</summary>
Motivation: 检测MMORPG中的自动升级机器人是具有挑战性的，因为它们模仿人类行为，并且处罚行为需要提供可解释的理由。为了避免法律和用户体验问题，需要可解释性的判断。本文旨在提出一种能够在无监督情况下检测自动升级机器人的框架，同时保持可解释性，支持可扩展且可问责的机器人监管。

Method: 利用对比表示学习和聚类技术，在完全无监督的情况下检测自动升级机器人，并引入大型语言模型作为辅助评审员。同时，通过增长曲线的可视化辅助评估升级行为。

Result: 通过对比表示学习和聚类技术的应用，结合大型语言模型作为辅助评审员的方式，以及增长曲线的可视化辅助，成功提高了机器人检测工作流程的效率，保持了可解释性，支持了可扩展且可问责的机器人监管。

Conclusion: 本文提出了一种新颖的框架，利用对比表示学习和聚类技术，以完全无监督的方式检测自动升级机器人，从而识别具有相似升级模式的角色群。引入了大型语言模型（LLM）作为辅助评审员，有效模仿了次要人类判断。通过基于增长曲线的可视化来辅助LLM和人类审核员评估升级行为。这种协作方法提高了机器人检测工作流程的效率，同时保持了可解释性，从而支持MMORPG中可伸缩且可问责的机器人监管。

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [17] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: AI development has prioritized task performance over cognitive foundations. Future AI in Cognitive Science should aim to enhance understanding of the human mind by aligning with cognitive frameworks, considering embodiment and culture, developing personalized models, and rethinking ethics.


<details>
  <summary>Details</summary>
Motivation: The reciprocal relationship between AI and Cognitive Science drives the need for a review to explore the intersections and gaps between the two fields.

Method: Comprehensive review of the intersections between AI and Cognitive Science, synthesizing key contributions from both perspectives.

Result: Observation that AI progress has emphasized practical task performance while its cognitive foundations are fragmented. Proposal for future AI development in Cognitive Science to construct systems that deepen understanding of the human mind by aligning AI behaviors with cognitive frameworks, considering embodiment and culture, developing personalized cognitive models, and rethinking AI ethics.

Conclusion: AI progress has focused on practical task performance, but its cognitive foundations are fragmented. Future AI development in Cognitive Science should aim to enhance understanding of the human mind.

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [18] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: 本文介绍了一种基于范畴论的框架，旨在提高人工智能系统的可解释性，特别关注词嵌入。通过构建不同范畴和数学方法，成功实现了从神经网络算法到透明框架的过渡，并推动了人工智能领域的发展。


<details>
  <summary>Details</summary>
Motivation: 提高人工智能系统的可解释性，重点在于词嵌入。通过数学方法和范畴论构建框架，改进现有算法，并从黑盒神经网络算法过渡到透明框架，推动人工智能领域的发展。

Method: 基于范畴论构建框架，包括构建$	ext{L}_T$和$	ext{P}_T$范畴、构造蒙德范畴$	ext{P}_T$、定义配置Conf和词嵌入$	ext{Emb}$的范畴，引入divergence作为$	ext{Emb}$的装饰，建立比较词嵌入的数学方法，计算和减少偏差的数学方法。

Result: 成功构建了基于范畴论的框架，提供了提取文本语义信息的方法，比较了不同算法之间的等价性，展示了计算偏差和减少偏差的数学方法。推动了可解释人工智能领域的进展。

Conclusion: 本文提出了基于范畴论的新颖框架，旨在增强人工智能系统的可解释性，特别关注词嵌入。通过构建$	ext{L}_T$和$	ext{P}_T$范畴，提供了文本$T$语义的图式表示，并将选择具有最大概率的元素重新构建为范畴概念。此外，构建了蒙德范畴$	ext{P}_T$以可视化从$T$中提取语义信息的各种方法，提供了仅依赖文本内信息的与维度无关的语义空间定义。同时，定义了配置Conf和词嵌入$	ext{Emb}$的范畴，并引入了divergence作为$	ext{Emb}$的一种装饰。建立了一个数学上精确的比较词嵌入的方法，展示了GloVe和Word2Vec算法与度量MDS算法之间的等价性，实现了从神经网络算法（黑盒）向透明框架的过渡。最后，介绍了一种计算嵌入前偏差并在语义空间层面上减少偏差的数学方法，推动了可解释人工智能领域的发展。

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [19] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 该论文提出了一个新型代理系统框架，利用三个推理LLM模块实现问题重写、代码生成和自审查能力。在科学计算中取得了显著优势，提高了代码生成效率和问题解决准确性。


<details>
  <summary>Details</summary>
Motivation: 在科学计算中，自动生成代码和自动审查是一个具有挑战性但有前景的领域。本研究旨在构建一个自动生成代码和自动审查的代理系统框架，以解决科学计算中的实际问题并提高代码生成效率。

Method: 建立代理系统框架，利用三个推理LLM模块（Consultant、Reviewer和Programmer）进行协作和交互，实现问题重写、代码生成和自审查能力。通过端到端审查机制，改进代码的可执行性和问题解决方案的修订。在偏微分方程、病态线性系统和数据驱动的物理分析问题上进行综合评估，证明该框架在自动生成代码和自动审查方面的可靠性。

Result: 通过与单一模型对比，展示了该协作框架显著改善了无错误代码生成率和减少了非物理解的发生。评估结果显示该框架在解决实际科学计算问题时的可靠性和有效性。

Conclusion: 该论文提出了一种新型代理系统框架，通过三个推理LLM（Consultant、Reviewer和Programmer）构建了一个“重写-解决-审查-修订”逻辑链，用于解决科学计算中的代表性问题。该框架在求解偏微分方程、病态线性系统和数据驱动的物理分析问题方面表现出显著优势，显著提高了无错误代码生成率并减少了非物理解的发生。通过评估，该代理框架在自动生成代码和自动审查方面表现可靠，为基于自然语言描述的自动生成代码建立了高度可靠的框架。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [20] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 本研究提出了一种新颖的单智能体深度强化学习框架用于公交车辆调度控制，解决了多智能体强化学习方法在实际模拟中的数据不平衡和收敛问题。实验结果表明，该方法在公交车辆调度控制中表现更加稳定和优越，比传统多智能体方法更有效。


<details>
  <summary>Details</summary>
Motivation: 传统解决方案在处理具有异构路线、时间表、波动需求和不同舰队规模的现实运营情况时依赖于多智能体强化学习方法。本研究旨在提出一种更接近实际模拟的单智能体强化学习框架，可以有效管理非环形、真实环境中的公共汽车调度，避免多智能体强化学习中的数据不平衡和收敛问题。

Method: 提出了一种新颖的单智能体强化学习框架用于公交车辆调度控制，通过增加分类标识符（车辆ID，站点ID，时间段）到状态空间中，结合数值特征（车头间隔，占用率，速度）来解决多智能体强化学习在实际模拟中的数据不平衡和收敛问题。设计了一种结构化奖励函数，平衡均匀的车头间隔和时间表执行，以实现操作目标。实验结果显示，改进的soft actor-critic (SAC)方法比基准方法（如MADDPG）在随机条件下更稳定和性能更优越。

Result: 实验结果表明，改进的SAC方法比其他基准方法在调度控制中表现更好，展示了单智能体深度强化学习在公交车辆调度控制中的潜力。

Conclusion: 单智能体深度强化学习在公交车辆调度控制中表现出更稳定和优越的性能，比传统多智能体强化学习方法更有效。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [21] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 研究提出了一种动态系统化的医学指南基准的原型，将WHO IMCI手册转化为有向图，以生成具有临床相关性的问题。基于图的方法提供了有效的临床任务评估，并发现模型在症状识别方面表现出色，但在分级严重性、治疗和后续护理方面存在挑战。该方法有助于解决手动策划基准的覆盖限制，并为动态生成基准提供了可行性，可应用于日常指南更新。


<details>
  <summary>Details</summary>
Motivation: 研究动态医学指南基准的原型，解决了手动策划基准覆盖限制的问题，致力于创建可伸缩、抗干扰的全面基准创建解决方案，以使得基准能够动态生成并在指南更新时保持更新。通过评估模型在医学指南方面的表现，揭示了模型在特定能力方面的差距，弥补了传统基准评估的不足之处。

Method: 将WHO IMCI手册转化为有向图，利用图遍历生成问题，并结合年龄特定情景和语境干扰物以确保临床相关性；基于图的方法进行系统评估，在临床任务中达到45-67%的准确率；在症状识别方面表现出色，但在分级严重性、治疗协议和后续护理方面存在挑战。该方法还提高了LLM后训练的效果，能够解决手动策划基准的覆盖限制，实现可伸缩和抗干扰的全面基准创建解决方案。

Result: 研究提出了一种动态系统化的医学指南基准的原型，展示了基于图的方法在临床任务中的评估效果和应用价值。该方法不仅提高了模型在症状识别方面的准确性，同时也揭示了模型在分级严重性、治疗协议和后续护理方面的不足，为后续的研究和应用提供了有益的参考。

Conclusion: 该研究提出了一种动态系统化的医学指南基准的原型，覆盖了400多个问题，共有3.3万亿可能的组合，涵盖了100％的指南关系。研究将世界卫生组织IMCI手册转化为一个有向图，包含200多个节点（疾病、症状、治疗方法、后续行动、严重性）和300多条边，然后利用图遍历生成问题，结合年龄特定情景和语境干扰物，确保临床相关性。该基于图的方法使得能够在临床任务中进行系统评估（45-67％准确率），研究发现模型在症状识别方面表现出色，但在分级严重性、治疗协议和后续护理方面表现较差，展示了定制基准可以发现普通领域评估忽略的特定能力差距。除了评估外，这种动态MCQA方法还增强了LLM后训练（监督微调、GRPO、DPO）的能力，其中正确答案提供高回报样本，而无需昂贵的人工注释。基于图的方法成功解决了手动策划基准的覆盖限制。这种方法是朝着可伸缩、抗干扰性的全面基准创建解决方案迈出的一步，能够动态生成基准，包括指南更新时。研究代码和数据集可在https://github.com/jessicalundin/graph_testing_harness中获取。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [22] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 研究提出了基于多目标遗传算法的医院单位人员排班问题建模方法，通过定义目标函数实现高质量排班计划的生成。实验证明，该方法在典型医院单位数据集上表现出色，比传统手动排班方法提高了66%的性能。该方法为护士经理和医院管理员提供了实用的决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 医疗部门的工作排班是一个重要的运营挑战，要求在最小化人工成本、确保患者需求的充分配备以及员工满意度之间取得平衡。现有排班方法可能难以满足这些多重目标，因此研究提出了一种多目标优化的方法来解决这一问题。

Method: 研究采用了基于多目标遗传算法（MOO-GA）的建模方法，将医院单位人员排班问题视为多目标优化任务。模型考虑了现实世界的复杂性，包括按小时预约驱动的需求和多技能员工的模块化班次使用。通过定义成本、患者护理覆盖率和员工满意度等目标函数，遗传算法在庞大的搜索空间中寻找一组高质量的非支配解。

Result: 研究表明，MOO-GA生成的排班计划在典型医院单位数据集上表现稳健且平衡。与模拟传统手动排班过程的基准相比，该算法生成的排班计划平均性能提高了66%。

Conclusion: 该研究提出了一个基于多目标遗传算法（MOO-GA）的医院单位人员排班问题的建模方法，通过定义成本、患者护理覆盖率和员工满意度等目标函数，实现了高质量的、非支配解的识别。实验证明，MOO-GA生成的排班计划在典型医院单位数据集上表现稳健且平衡。与模拟传统手动排班过程的基准相比，该算法生成的排班计划平均性能提高了66%。该方法有效地平衡了关键运营和员工中心目标之间的权衡，为护士经理和医院管理员提供了实用的决策支持工具。

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [23] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 该研究提出了一种新的神经符号化架构和损失函数，可高效学习解决NP难推理问题。实验证明，在不同基准测试上取得了显著结果，且训练时间短于其他混合方法。


<details>
  <summary>Details</summary>
Motivation: 在将离散推理与神经网络混合的持续探索中，人们对可以从自然输入中学习解决离散推理或优化问题的神经架构越发感兴趣。大型语言模型似乎很难胜任这一任务。

Method: 研究引入了可微的神经符号化架构和一种专用于学习如何解决NP难推理问题的损失函数。新的概率损失函数允许学习约束和目标，提供了一个完整的模型，并可通过附加约束进行完善。通过将组合求解器排除在训练循环之外，架构实现可扩展训练，同时确保准确推断的最大准确性。

Result: 研究表明，该方法能够从自然输入中高效学习如何解决NP难的推理问题。在数独基准测试的三个变体上，该方法所需的训练时间仅为其他混合方法的一部分。在视觉Min-Cut/Max-Cut任务上，与Decision-Focused-Learning的后悔专用损失相比，该方法优化后悔更好。此外，它有效地学习了设计蛋白质这一大型现实问题的能量优化公式。

Conclusion: 该研究提出了一种可微的神经符号化架构和损失函数，专门用于学习如何解决NP难的推理问题。通过推动组合求解器脱离训练循环，该架构提供了可扩展的训练方式，同时确保准确推断的最大准确性。实验证明，该方法能够有效地学习如何从自然输入中解决NP难的推理问题。在三个不同种类的数独基准测试上，该方法所需的训练时间仅为其他混合方法的一小部分。在视觉Min-Cut/Max-Cut任务上，它优化后悔比Decision-Focused-Learning后悔专用损失更好。最后，它还有效地学习了设计蛋白质这一大型现实问题的能量优化公式。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [24] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: ChatThero is a conversational framework that enhances patient motivation, treatment confidence, and efficiency in addressing substance use disorders. It outperforms existing systems in empathy, responsiveness, and behavioral realism. The framework integrates patient modeling, therapeutic dialogue, and persuasive strategies based on CBT and MI, trained using a two-stage pipeline. ChatThero provides personalized support and a replicable basis for research and clinical translation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limited personalized support available for individuals with substance use disorders due to stigma, motivational barriers, and the lack of integration of large language models (LLMs) with clinically validated strategies. The authors aim to enhance addiction recovery by developing a conversational framework that effectively combines patient modeling and evidence-based therapeutic approaches to provide tailored assistance.

Method: The paper presents ChatThero, a framework that integrates dynamic patient modeling, context-sensitive therapeutic dialogue, and adaptive persuasive strategies based on cognitive behavioral therapy (CBT) and motivational interviewing (MI). ChatThero is trained using a two-stage pipeline involving supervised fine-tuning (SFT) and direct preference optimization (DPO). A high-fidelity synthetic benchmark is utilized to evaluate ChatThero's performance across different resistance levels.

Result: ChatThero achieves a 41.5% average gain in patient motivation, a 0.49% increase in treatment confidence, and resolves hard cases with 26% fewer turns compared to GPT-4o. It outperforms existing systems in empathy, responsiveness, and behavioral realism according to both automated and human clinical assessments. The framework establishes a foundation for studying therapeutic conversations in a privacy-preserving manner and offers a robust basis for further research and clinical application.

Conclusion: ChatThero, a multi-agent conversational framework, demonstrates significant improvements in patient motivation, treatment confidence, and efficiency in resolving hard cases compared to existing systems. It also receives high ratings in empathy, responsiveness, and behavioral realism from both automated and human clinical assessments. The framework offers a promising approach to providing personalized support for individuals with substance use disorders.

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>
