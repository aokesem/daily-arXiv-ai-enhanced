<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: 本文提出通过临床实践指南来改进HealthBench的评估方法，以培育具备临床可信度、伦理性、全球适用性的医疗语言模型。提出了相关的解决方案，包括证据加权评分、上下文覆盖逻辑等。


<details>
  <summary>Details</summary>
Motivation: 由于HealthBench依赖于专家意见而面临区域偏见、个别临床医生特异性等问题，特别是在低收入和中等收入国家存在着特定挑战，因此提出了通过临床实践指南来强调全球相关性和公平性的动机。

Method: 提出了一种通过将奖励函数与临床实践指南相结合的方法来解决HealthBench现有局限性的方案，包括强调证据加权评分、上下文覆盖逻辑以及重点考虑道德因素和延迟结果反馈等方面。

Result: 通过改进奖励函数的方法，旨在塑造更加值得信赖和全球具有影响力的医疗语言模型。

Conclusion: 提出了通过临床实践指南来强化奖励函数，以改进HealthBench在医疗语言模型评估中的局限性。旨在促进不仅在语言上优秀，而且在临床上值得信赖、符合伦理要求、具有全球影响力的医疗语言模型的发展。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [2] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文介绍了一种基于HyperTWTL约束的安全强化学习方法，通过动态Boltzmann softmax RL学习安全感知的最优策略。研究表明该方法在拾取和递送机器人任务案例中表现出有效性和可扩展性，并优于基准RL算法。


<details>
  <summary>Details</summary>
Motivation: 尽管存在许多关于时间逻辑约束安全强化学习（SRL）的文献，但在使用超属性进行安全感知强化学习方面存在重要的研究空白。本研究旨在填补这一空白，并探讨基于HyperTWTL的安全感知强化学习方法。

Method: 采用HyperTWTL约束来形式化透明度和安全性约束，提出了安全感知的强化学习方法，通过动态Boltzmann softmax RL学习最优策略。

Result: 提出的方法在拾取和递送机器人任务案例中显示出了有效性和可扩展性，并在实验中优于两种基准RL算法。

Conclusion: 研究提出了一种基于HyperTWTL的安全强化学习方法，通过动态Boltzmann softmax RL学习安全感知的最优策略，同时满足HyperTWTL约束。在拾取和递送机器人任务案例研究中展示了该方法的有效性和可扩展性，并与其他两种基准RL算法进行了比较，结果显示该方法的表现优于它们。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [3] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 本文讨论了人工智能在工业环境中的应用挑战，介绍了生成式，预测性和规范性人工智能以及面向对象的过程挖掘。作者指出人工智能需要过程智能来改进运营流程，并强调了成功结合OCPM和人工智能的机会。研究结果认为过程智能对于连接数据和过程以实现不同形式人工智能至关重要。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于现有组织在工业环境中应用人工智能时所面临的挑战。作者希望揭示人工智能必须结合过程智能才能在组织背景下取得成功的原因，并强调了OCPM对于连接数据和过程以实现不同形式人工智能的重要性。

Method: 该论文基于实证研究和理论框架，探讨了人工智能在工业环境中应用的困难和挑战。通过详细讨论生成式，预测性和规范性人工智能以及面向对象的过程挖掘（OCPM），阐述了过程智能对人工智能的重要性。作者提出了“过程智能（PI）”的概念，并说明了如何成功结合OCPM和不同形式的人工智能。

Result: 作者通过实证研究和理论分析，阐明了人工智能在工业环境中的应用困难和挑战，以及过程智能在提高运营流程效率方面的关键作用。他们提出了“过程智能（PI）”的概念，并探讨了成功整合OCPM和人工智能的机会。

Conclusion: 人工智能的普及影响了我们的工作方式，互动方式，商业运作方式以及研究方式。本文探讨了在工业环境中成功应用人工智能的困难，重点放在端到端的运营流程上。作者考虑了生成式，预测性和规范性人工智能，并详细阐述了诊断和改进此类流程的挑战。文章指出，人工智能需利用面向对象的过程挖掘(OCPM)。过程相关数据是结构化的，特定于组织，并且与文本不同，流程通常具有高度动态性。OCPM是连接数据和流程的缺失环节，并能够实现不同形式的人工智能。作者使用“过程智能（PI）”一词来指代以处理各种对象和事件类型为特点的以流程为中心的数据驱动技术的综合，使得人工智能在组织环境中得以实现。本文解释了为何人工智能需要过程智能来改善运营流程，并突出了成功结合OCPM和生成式，预测性以及规范性人工智能的机会。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [4] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 本文介绍了如何检测和处理多标准决策分析中的Rank Reversals问题，提出了三种测试方法并在Scikit-Criteria库中实现。讨论了这些方法的实施复杂性和设计考虑，以及它们在多标准决策方法中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 多标准决策分析中，Rank Reversals是一个严重的问题，可能会极大地影响多标准决策方法对特定备选方案的结果。因此，有必要有一种机制来衡量一种方法在一组备选方案上的性能。这个想法可以进一步发展，以建立不同方法解决问题效果的全局排名。

Method: 介绍了三种检测Rank Reversals的测试方法，并在Scikit-Criteria库中实现了这些测试。

Result: 在Scikit-Criteria库中实现了三种检测Rank Reversals存在的测试方法，并讨论了在实现这些测试时的复杂性和处理这些复杂性的设计考虑。

Conclusion: 本文介绍了三种检测Rank Reversals存在的测试方法，并在Scikit-Criteria库中实现了这些测试。讨论了在实现这些测试时出现的复杂性以及为处理这些复杂性所做的设计考虑。最后讨论了这些测试方法对多标准决策方法在问题解决中的重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [5] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 本文研究了在RDF图中使用SHACL进行验证，在更新过程中保持静态验证的问题。使用回归技术将静态验证问题简化为SHACL约束满足性问题，并分析了计算复杂性。提出了原型实现，展示了其在SHACL约束上执行静态验证和其他静态分析任务的行为。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究SHACL在RDF图中的验证问题，探讨如何在更新过程中保持对SHACL规范的静态验证。这为进一步推理关于不断演化的RDF图的服务提供了基础。

Method: 本文使用了将更新操作嵌入SHACL约束的回归技术，将静态验证问题简化到(a minor extension of) SHACL中约束的(不)满足性问题。对SHACL及其一些关键片段的静态验证问题的计算复杂性进行了分析。提出了一个原型实现用于执行静态验证和其他静态分析任务，并通过初步实验展示了其行为。

Result: 通过回归技术，将SHACL静态验证问题简化为SHACL约束的满足性问题。分析了静态验证问题的计算复杂性，并展示了一个原型实现，用于执行SHACL约束的静态验证和其他静态分析任务。

Conclusion: 本文研究了SHACL在RDF图中的验证问题，并提出了基于SHACL的更新语言，用于捕捉对RDF图的直观和现实修改。通过将更新操作嵌入SHACL约束，静态验证问题可以简化到SHACL的约束满足性问题。研究了SHACL静态验证问题的计算复杂性以及一些关键片段。最后，提出了一个原型实现，通过初步实验展示了其在SHACL约束上执行静态验证和其他静态分析任务的行为。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [6] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 本文讨论了人工智能算法对文化边缘群体的潜在影响，提出通过重新构建AI生产流程来减轻这些危害的观点。介绍了增强型人工智能生命周期模型，强调协同生产、多样性、平等、包容和跨学科合作的重要性。将生命周期模型与伦理框架联系起来，概述了扩展参与式治理的关键研究问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能算法对文化边缘群体产生不均衡影响，已有一系列方法来降低这种风险，但需要更全面的解决方案。基于对设计正义、广义学习理论和最新参与式人工智能研究的借鉴，提出了重新构建AI生产管道的观点，强调多样性、平等和跨学科合作的重要性。

Method: 结合设计正义、广义学习理论和最近关于参与式人工智能的实证研究，主张通过重新构建人工智能生产流程来减轻潜在风险。提出增强型人工智能生命周期模型，强调协同生产和跨学科合作，倡导分布式权威和迭代知识交流。

Result: 提出了增强型人工智能生命周期模型，包括五个相互关联的阶段，强调了协同生产、多样性、平等、包容和跨学科合作的重要性。将该生命周期模型与伦理框架联系起来，明确了需要进一步研究的关键问题。

Conclusion: 提出了重新构建人工智能生产流程的观点，强调采用协同生产、多样性、平等、包容和跨学科合作，为减轻人工智能算法对文化边缘群体的潜在危害。介绍了增强型人工智能生命周期，包括协同框架、协同设计、协同实施、协同部署和协同维护等五个相互关联的阶段。通过四个跨学科研讨会和分布式权威、迭代知识交流主题，构建了生命周期模型。最后，将提议的生命周期与几种主要的伦理框架联系起来，并概述了扩展参与式治理的关键研究问题。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [7] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 本文讨论了人类在评估数据标签中的不完美性，提出了五种互补评估方法，强调了外部效度的重要性，并呼吁重新思考注释质量和真相标准，以提高学生学习效果。


<details>
  <summary>Details</summary>
Motivation: 指出人类互评在验证数据标签方面存在的偏见、不可靠性和定义“真相”的不适用性。提出过于依赖人类互评的缺陷，阐释了对教育应用中的大量训练数据的需求，并探讨现有的评估方法对提高学习相关数据分类的影响。

Method: 提出了五种互补评估方法，包括多标签注释方案、基于专家的方法和闭环验证等。强调了外部效度的重要性，通过建立验证导师行为的程序并证明其在多个导师行为类别中的有效性。

Result: 通过提出五种互补评估方法，论文认为这些方法能够产生改善学生学习和提供更具可操作性见解的训练数据和模型。强调了外部效度的重要性，并呼吁重新思考注释质量和真相标准。

Conclusion: 这篇论文认为过度依赖人类互评可靠性作为注释数据质量的门卫，阻碍了以有效和可预测的方式对数据进行分类，从而提高学习效果。提出了五种互补评估方法，例如多标签注释方案、基于专家的方法和闭环验证等，以产生改善学生学习和提供更具可操作性洞见的训练数据和模型。强调外部效度的重要性，如通过建立验证导师行为的程序并证明其在多个导师行为类别（例如提供提示）中有效。呼吁学界重新思考注释质量和真相标准，将有效性和教育影响置于共识之上。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [8] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 本文探讨通过强制人工智能代理明确赋予人类权力，并以理想方式管理人类与人工智能代理之间的权力平衡，推广安全和幸福的想法。设计了一个参数化和可分解的目标函数，代表人类权力的不平等和风险厌恶的长期总和，并推导了计算该度量的算法。软最大化适当的人类权力合适的聚合指标可能是安全的，并有益于代理人工智能系统。


<details>
  <summary>Details</summary>
Motivation: AI安全中的关键概念是权力：权力寻求作为一种工具性目标，人类的突然或逐渐失能，人类与人工智能互动中的权力平衡以及国际人工智能治理。同时，作为追求多样目标的能力对幸福至关重要。通过强制人工智能代理明确赋予人类权力，并以理想方式管理人类与人工智能代理之间的权力平衡，探讨推广安全和幸福的想法。

Method: 使用一种基于公理的、部分公理化的方法，设计了一个参数化和可分解的目标函数，代表人类权力的不平等和风险厌恶的长期总和，考虑了人类的有限理性和社会规范，并广泛考虑了可能的人类目标。推导了计算该度量的算法，通过反向归纳或近似通过一种形式的多智能体强化学习从给定的世界模型中。软最大化这个度量在各种典型情况下的结果，描述了它可能暗示的工具性子目标。

Result: 设计了一个代表人类权力的不平等和风险厌恶的长期总和的参数化和可分解的目标函数，考虑了人类的有限理性和社会规范，广泛考虑了可能的人类目标。推导了计算该度量的算法，通过反向归纳或近似通过一种形式的多智能体强化学习从给定的世界模型中。软最大化适当的人类权力合适的聚合指标可能构成有益于代理人工智能系统的目标，比直接基于效用的目标更安全。

Conclusion: 软性最大化人类权力的合适聚合度量可能构成一种有益于代理人类系统的目标，比直接基于效用的目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [9] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS是一种新颖方法，通过结合多重重要性抽样和基于探索的优势函数，实现内部开发与外部数据的协同作用，以提升推理能力，解决能力边界问题。在数学推理基准测试和非分布式推理任务中表现优越，相对改进范围从21.1	o到69.2%。


<details>
  <summary>Details</summary>
Motivation: RL-PLUS提出的动机是解决RLVR在基本LLM模型的能力边界方面遇到的困难，以及避免RLVR可能导致的能力边界崩溃问题。通过内部开发和外部数据的协同作用，RL-PLUS旨在提高推理能力，实现比基本模型更强大的性能。

Method: RL-PLUS整合了两个核心组件：多重重要性抽样用于解决外部数据的分布不匹配问题，以及基于探索的优势函数，用于指导模型走向具有高价值且未被探索的推理路径。

Result: 通过理论分析和广泛实验，我们展示了RL-PLUS方法的优越性和泛化能力。实验结果显示，在数学推理基准测试和非分布式推理任务中，RL-PLUS实现了比现有RLVR方法更好的性能。在不同模型系列中，RL-PLUS取得了一致且显著的改进，平均相对改进范围从21.1	o到69.2%。

Conclusion: RL-PLUS是一种新颖的方法，通过内部开发和外部数据采用合作方式，以增强推理能力并突破基本模型的边界。在数学推理基准测试中，RL-PLUS优于现有的RLVR方法，在六个非分布式推理任务中表现出优越性能。与不同模型系列相比，RL-PLUS实现了一致和显著的进步，平均相对改进范围为21.1	o 69.2	o%。Pass@k曲线显示RL-PLUS有效解决了能力边界崩溃问题。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [10] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: 本文提出了MetaAgent，一种受学以致用原则启发的自主学习代理范式。MetaAgent通过不断优化推理和工具使用策略，构建内部工具和知识库，在知识发现任务中表现卓越，超越传统工作流模型和端到端训练代理，展示了自我进化代理系统的应用前景。


<details>
  <summary>Details</summary>
Motivation: 本文提出MetaAgent代理系统，旨在通过自主学习和不断优化提升任务解决能力，探索代理系统在知识发现任务中的应用潜力。借鉴学以致用的原则，MetaAgent能够动态学习、改进工具使用策略，逐步提升解决复杂任务的能力。

Method: MetaAgent使用了meta tool learning这一不断数据驱动的过程，通过自我反思、答案验证等方式，改进其推理和工具使用策略，无需更改模型参数或进一步后续训练。代理不仅生成自然语言求助请求，还构建内部工具和持久知识库，提升信息检索和整合能力。

Result: 经过多项挑战性知识发现基准测试的验证，MetaAgent在超越工作流基线模型的同时，达到或超越端到端训练的代理效果，证明了自我进化代理系统在知识发现任务中具有巨大潜力。

Conclusion: MetaAgent通过基于学以致用的原则提出了一种自主学习的代理范式，不断改进自身能力并实现知识发现任务。在多项知识发现基准测试中表现出色，超越了基于工作流的基线模型，并与端到端训练的代理相媲美，展示了自我进化代理系统在稳健、通用知识发现中的潜力。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [11] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 研究比较人类和LLM代理人在任务生成中的差异，发现LLMs生成的任务缺乏社交性、身体性，且偏向抽象。虽然LLMs生成的任务被认为更有趣和新颖，但存在与类人目标生成能力之间的脱节。因此，设计更符合人类认知特点的代理人时需结合内在动机和物理基础。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于研究LLMs在任务生成中是否遵循类似人类认知原则，以及探讨人类任务生成受心理驱动因素影响的情况。

Method: 研究通过比较人类与LLM代理人（GPT-4o）的任务生成实验来探讨人类认知与LLMs之间的差异。

Result: 研究发现LLMs生产的任务缺乏社交性、身体性，并且偏向抽象，与人类任务生成的行为模式存在差异。虽然LLMs生成的任务被认为更有趣和新颖，但存在与类人目标生成能力之间的脱节。

Conclusion: 研究发现，人类任务生成受心理驱动因素的影响，而大型语言模型（LLMs）生成的任务偏向抽象，缺乏社交性和身体性。虽然LLMs产生的任务被认为更有趣和新颖，但存在其语言能力与生成类人目标能力之间的脱节。因此，需要在设计更符合人类特点的代理人时，结合内在动机和物理基础。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [12] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: 研究评估了视觉语言模型在复杂图形推理任务中的表现，提出了首个以结构化图形推理为焦点的评估基准 ReasonBench。通过 DiaCoT 和 ReasonTune 双重优化策略，提高了 VLMs 的性能达到33.5%。


<details>
  <summary>Details</summary>
Motivation: 针对视觉语言模型在复杂图形推理方面的表现进行评估，因为现有研究仅关注简单图形，VLMs 在复杂图形推理和抽象问题解决中仍存在明显的不足。

Method: 提出了 ReasonBench 评估基准，包含来自现实智力测试的1,613个问题，涵盖了位置、属性、数量和多元任务的推理维度。评估了11种主流的 VLMs，揭示了当前模型的显著局限性。

Result: 提出了视觉语言模型的双重优化策略(DiaCoT 和 ReasonTune)，通过训练改进推理任务的适应性，从而提高了 VLM 性能。

Conclusion: 通过 ReasonBench 提出的双重优化策略(DiaCoT 和 ReasonTune)提高了视觉语言模型(VLMs)的性能达到33.5%。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [13] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文研究了LRMs存在的安全风险问题，提出了R1-Act方法，通过结构化推理过程显式触发安全知识，取得了显著的安全改进，同时保持推理性能，在多个LRM骨干和规模上得到验证。


<details>
  <summary>Details</summary>
Motivation: 调查了LRM安全风险的根本原因，发现模型已经拥有足够的安全知识，但在推理过程中未能激活。

Method: 提出了R1-Act方法，通过结构化推理过程显式触发安全知识。

Result: R1-Act方法在安全性能方面表现出色，胜过先前的对齐方法。

Conclusion: 本文研究了大型推理模型（LRMs）存在的安全风险问题，提出了一种后训练方法R1-Act，可以明显改善模型的安全性能，同时保持推理性能。该方法仅需1000个训练样本和90分钟的训练时间，能够在多个LRM骨干和规模上取得强大的效果。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [14] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI framework enhances reasoning in vision-language models by introducing visual verification, improving explanation quality. It follows a three-stage pipeline and is integrated without retraining. Evaluation on VCR benchmark shows performance improvement and human evaluations support the enhanced factual and helpful nature of explanations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the issue of hallucinations in explanations provided by vision-language models due to the lack of grounding in visual content. The absence of an explicit verification mechanism during multi-step reasoning leads to linguistically fluent yet visually ungrounded explanations.

Method: The paper proposes the CoRGI (Chain of Reasoning with Grounded Insights) modular framework, which consists of a three-stage pipeline: generating a textual reasoning chain, extracting visual evidence for reasoning steps, and synthesizing textual rationale with visual evidence for a grounded answer. The framework is integrated with existing VLMs without requiring end-to-end retraining.

Result: Evaluation on the VCR benchmark shows that CoRGI improves reasoning performance on representative VLM backbones. Ablation studies confirm the contribution of the verification module in enhancing explanations. Human evaluations indicate that CoRGI leads to more factual and helpful explanations, highlighting the importance of grounding reasoning steps in visual evidence.

Conclusion: CoRGI framework improves reasoning performance in vision-language models by introducing visual verification into the reasoning process. It enhances the factual and helpful nature of explanations provided by the models.

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [15] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 该论文提出了一种通过在主动推断中实现心灵理论（ToM）的新颖多智能体合作方法，扩展了基于推理树的规划算法，通过模拟任务评估了方法，在实验中表明ToM装备的代理能够更好地合作，避免碰撞和减少冗余努力，推动了人工智能领域的进展。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于提出一种新的多智能体合作方法，通过ToM实现代理之间的理解和合作，从而提高合作效率和避免冲突。同时，为人工智能的实际应用提供了新的思路和方法。

Method: 在主动推断中实现心灵理论（ToM），扩展基于推理树的规划算法以系统地探索联合策略空间，通过碰撞避免和觅食任务模拟评估方法。

Result: 通过碰撞避免和觅食任务模拟评估，结果显示ToM装备的代理能够比非ToM对照组更好地合作，避免碰撞和减少冗余努力。ToM代理通过推断他人的信念仅从可观察行为中实现合作。

Conclusion: 该论文提出了一种新颖的多智能体合作方法，通过在主动推断中实现心灵理论（Theory of Mind，ToM）。通过ToM的实现，使代理能够理解其他个体可能具有不同的知识和目标，从而使代理在规划自己的行动时能够推理其他人的信念。与先前的多智能体合作的主动推断方法不同，我们的方法既不依赖于特定任务的共享生成模型，也不需要显式通信，同时具有通用性。在我们的框架中，ToM装备的代理维护其自己和其他人的信念和目标的不同表示。我们将复杂的基于推理树的规划算法扩展到通过递归推理系统地探索联合策略空间。通过碰撞避免和觅食任务模拟对我们的方法进行评估。结果表明，与非ToM对照组相比，ToM装备的代理能够更好地合作，避免碰撞并减少冗余努力。关键是，ToM代理通过仅从可观察行为推断他人的信念来实现这一点。这项工作推动了人工智能在实际应用中的发展，并为ToM提供了计算上的洞见。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [16] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro is an open-source and free agent framework that addresses the limitations of current closed-source systems. It focuses on high-quality training data curation, novel test-time reflection strategies, and agent robustness. The framework outperforms previous systems and sets a new performance standard for accessible AI agents.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the limitations of current agent systems that are closed-source or rely heavily on paid APIs and proprietary tools, thus limiting accessibility and reproducibility for the research community. By introducing Cognitive Kernel-Pro as an open-source and free framework, the paper aims to make advanced AI agent development more accessible and democratized.

Method: The paper systematically investigates the curation of high-quality training data for Agent Foundation Models within Cognitive Kernel-Pro. It focuses on constructing queries, trajectories, and verifiable answers across key domains such as web, file, code, and general reasoning. The paper also explores novel strategies for agent test-time reflection and voting to enhance agent robustness and performance.

Result: Cognitive Kernel-Pro achieves state-of-the-art results among open-source and free agents, surpassing previous leading systems like WebDancer and WebSailor. The 8B-parameter open-source model of Cognitive Kernel-Pro establishes a new performance standard for high-capability AI agents.

Conclusion: Cognitive Kernel-Pro is an open-source and free multi-module agent framework that aims to democratize the development and evaluation of advanced AI agents. It outperforms previous leading systems and sets a new performance standard for accessible AI agents.

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [17] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型在数学领域的应用，强调了在正式数学推理方面取得进展困难的问题。文章关注于机器学习和数学认知交叉领域的核心问题，并呼吁研究延伸当前的限制。


<details>
  <summary>Details</summary>
Motivation: 本文针对大型语言模型在数学推理中的表现，以及在正式数学方面面临的困难展开讨论，旨在揭示当前在机器学习和数学认知交叉领域中的挑战和问题。

Method: 文章采用综述的方式，关注最新模型和基准，探讨了机器学习和数学认知交叉领域的关键问题。

Result: 通过深入探讨大型语言模型在数学推理中的表现和困难，文章提出了关键问题并对该领域的最新发展和未来方向进行了探讨。

Conclusion: 本文探讨了大型语言模型在数学领域的应用，并强调了在正式数学推理方面取得进展相对困难的问题。文章着重探讨了机器学习和数学认知交叉领域的三个核心问题，并呼吁研究延伸当前的限制。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [18] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard is a proactive safety framework for large language model agents that anticipates risks and triggers interventions based on user-defined thresholds, ensuring safety in different domains. It achieves high safety enforcement rates in embodied agent tasks and autonomous driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing rule-based enforcement systems lack foresight and struggle with long-horizon dependencies and distribution shifts, leading to reactive responses to unsafe behavior. Pro2Guard aims to proactively enforce safety by anticipating risks and intervening before violations occur.

Method: The paper proposes Pro2Guard, which abstracts agent behaviors into symbolic states, learns a Discrete-Time Markov Chain (DTMC) from execution traces, estimates the probability of reaching unsafe states at runtime, and triggers interventions based on user-defined thresholds. It incorporates semantic validity checks and PAC bounds to ensure statistical reliability and approximate the underlying ground-truth model.

Result: Pro2Guard extensively evaluated across embodied household agents and autonomous vehicles domains. In embodied agent tasks, it enforces safety early in up to 93.6% of unsafe tasks with low thresholds. In autonomous driving scenarios, Pro2Guard achieves 100% prediction of traffic law violations and collisions, anticipating risks up to 38.66 seconds ahead.

Conclusion: Pro2Guard is a proactive runtime enforcement framework that addresses the limitations of existing rule-based systems by anticipating future risks and triggering interventions to ensure safety in large language model agents across different domains.

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [19] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: 研究提出了MultiSHAP框架，通过Shapley Interaction Index对多模态AI模型的预测进行解释。该框架能提供实例级和数据集级解释，揭示了模型在个体样本和整体数据集上的跨模态效果和信息整合方式。实验证明其有效性和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型在任务中取得了出色表现，但缺乏可解释性和信任度，如何解释多模态AI模型中的跨模态交互仍然是一个主要挑战。现有的模型解释方法虽然提供了对跨模态关系的粗略洞察，但无法精确量化模态之间的协同效应，并且仅限于具有可访问内部权重的开源模型。

Method: 引入了MultiSHAP框架，利用Shapley Interaction Index对多模态预测进行解释，实现视觉和文本元素之间的相互作用的归因。

Result: 实验表明MultiSHAP能准确捕获跨模态推理机制，案例研究验证了其实际效用，可解释复杂多模态AI模型。

Conclusion: 提出了一种名为MultiSHAP的模型无关可解释性框架，利用Shapley Interaction Index来解释多模态预测，对细粒度的视觉和文本元素之间的相互作用进行归因，适用于开源和闭源模型。该方法提供了实例级解释和数据集级解释，能准确揭示模型在个体样本和整体数据集上的跨模态效应和信息集成模式。实验证明MultiSHAP能忠实地捕获跨模态推理机制，案例研究证明其实际实用性，可应用于解释复杂多模态AI模型。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [20] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 研究提出了一种多阶段大型语言模型驱动的框架，用于生成全面的预会诊问卷。该方法在信息覆盖率、诊断相关性、可理解性和生成时间上表现优越，在实际中有提升患者信息收集的潜力。


<details>
  <summary>Details</summary>
Motivation: 预会诊是有效医疗服务交付的关键组成部分，但从复杂庞大的电子病历中生成全面的预会诊问卷是一项具有挑战性的任务。直接的大型语言模型方法在信息完整性、逻辑顺序和疾病级综合等方面面临困难。因此，为了解决这一问题，提出了这一创新性的多阶段大型语言模型驱动框架。

Method: 提出了一种多阶段的大型语言模型驱动框架：第一阶段从电子病历中提取原子断言；第二阶段构建个性化因果网络并通过对电子病历语料库中的代表性网络进行聚类，综合疾病知识；第三阶段基于这些结构化表示生成定制的个人和标准化的疾病特定问卷。

Result: 方法在真实世界的电子病历数据集上进行了评估，并经临床专家验证，表现出卓越的性能，突出了其提升患者信息收集的实际潜力。

Conclusion: 该研究提出了一种多阶段的大型语言模型驱动框架，用于从电子病历中生成全面的预会诊问卷。通过构建个性化因果网络和综合疾病知识，生成定制的个人和标准化的疾病特定问卷。该框架在提高信息覆盖率、诊断相关性、可理解性和生成时间方面表现出卓越的性能，具有提升患者信息收集的实际潜力。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [21] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 研究提出了AVR-Eval相对质量度量方法和AVR-Agent多智能体系统，用于评估和生成多媒体内容。AVR-Agent生成的内容在比赛中表现较好，但在利用自定义资源和AVR反馈方面遇到困难。研究揭示了人类和机器内容创作方法之间的根本差异。


<details>
  <summary>Details</summary>
Motivation: 虽然AI优秀于生成文本、音频、图像和视频等内容，但生成互动性音频-视觉内容（如视频游戏）仍然具有挑战性。当前的LLM可以生成JavaScript游戏和动画，但缺乏自动化评估指标，面对通常需要团队数月制作的复杂内容（多镜头、多智能体）。为了解决这些问题，研究构建了新的度量和多智能体系统。

Method: 提出了AVR-Eval相对质量度量方法和AVR-Agent多智能体系统。AVR-Eval通过比较多媒体内容的AVR，结合文本模型评估，识别出好坏内容。AVR-Agent从多媒体资产库中选择相关资产，生成多个初始代码，利用AVR-Eval识别最佳版本，并通过AVR的多模态智能体反馈迭代改进代码。

Result: 实验结果显示，AVR-Agent生成的内容胜率明显较高。然而，模型在利用自定义资源和AVR反馈方面存在困难，未显示出更高的胜率。这表明当前编码模型未能有效利用高质量资源和音视频反馈，突显了人类和机器内容创作方法之间的根本差异。

Conclusion: 研究提出了AVR-Eval相对质量度量方法和AVR-Agent多智能体系统，用于评估多媒体内容质量和生成JavaScript代码。实验证明AVR-Agent生成的内容在比赛中获胜率显著较高，但在利用自定义资源和AVR反馈方面存在困难。研究揭示了人类和机器内容创作方法之间的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [22] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 本文提出了多频带变时滞格兰因果性（MB-VLGC）框架，通过明确建模频率相关的因果延迟弥补了现有方法的不足。实验证明该框架在各个领域的实验中表现优异，适用于各种时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 由于传统变时滞格兰因果性方法无法考虑因果交互不仅在时间延迟上变化，还在频率范围内存在差异，本研究旨在弥补这一不足，提出一种更具广泛适用性的方法。

Method: 对多频带变时滞格兰因果性（MB-VLGC）进行形式化定义，提出了一种有效的推断流程。

Result: 实验证明该框架在合成数据和真实数据上均优于现有方法，表明其在不同类型的时间序列数据上具有广泛适用性。

Conclusion: 提出了一种多频带变时滞格兰因果性（MB-VLGC）框架，通过明确建模频率相关的因果延迟，弥补了变时滞格兰因果性（VLGC）方法的不足。在多个领域的广泛实验证明，该框架在合成数据和真实数据上明显优于现有方法，适用于任何类型的时间序列数据。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [23] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出了一个融合了传统可解释人工智能技术、生成式人工智能模型和用户个性化的混合框架，以生成多模态、个性化的解释，满足用户需求。重新定义可解释性为动态沟通过程，针对用户角色和学习目标定制解释。文章讨论了教育领域中的可解释人工智能技术限制，并提出了未来研究方向。旨在推动更透明、支持以用户为中心的可解释人工智能发展。


<details>
  <summary>Details</summary>
Motivation: 许多人工智能驱动的自适应学习系统正在通过数据驱动的学习体验调整改变教育方式。然而，许多系统缺乏透明度，无法详细解释决策过程。目前大多数可解释人工智能技术侧重于技术输出，而忽视了用户角色和理解能力。基于这一背景，本文提出了一个融合传统和生成式人工智能模型的混合框架，旨在提供针对用户需求的多模态、个性化解释，重新定义可解释性并强调与用户角色和学习目标相关的动态沟通。

Method: 本文提出了一个融合了传统可解释人工智能技术、生成式人工智能模型和用户个性化的混合框架，以生成多模态、个性化的解释。重新定义了可解释性的概念，强调了动态沟通过程并针对用户角色和学习目标进行定制。文章还概述了框架设计，讨论了教育领域中可解释人工智能技术的限制，并提出了在准确性、公平性和个性化方面的研究方向。

Result: 通过提出新的混合框架，本文为教育领域的可解释人工智能技术提供了创新思路和解决方案。不仅重新定义了可解释性的概念，还强调了解释过程应当是动态的沟通过程，并应根据用户的角色和学习目标进行个性化定制。文章还指出了教育领域中可解释人工智能技术存在的限制，并提出了在准确性、公平性和个性化方面的未来研究方向。

Conclusion: 本文提出了一个融合了传统可解释人工智能技术、生成式人工智能模型和用户个性化的混合框架，以生成多模态、个性化的解释，满足用户需求。重新定义可解释性为针对用户角色和学习目标量身定制的动态沟通过程。文章概述了框架的设计、教育领域中关键的可解释人工智能技术限制，以及在准确性、公平性和个性化方面的研究方向。旨在朝着增强透明度、支持以用户为中心体验的可解释人工智能发展。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [24] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 在本论文中，提出了一个用户分割和上下文感知的解释层，通过一个多样化的解释方法的可视化解释系统来解决社交媒体平台中AI推荐的可解释性问题。论文提出的框架首次在单一流程中共同调整解释方式和粒度，并计划通过公开试点测试来验证其影响。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台的AI推荐缺乏用户理解推荐背后原因，因此需要解决可解释性问题，使解释与用户个性化需求保持一致。

Method: 提出了一个可视化解释系统，包括详细版本和简化版本，通过公开试点测试系统对决策制定和信任建立的影响。

Result: 论文提出的框架与解释系统旨在提高决策制定和信任建立的效果，并通过公开试点测试系统来验证其影响。

Conclusion: 该论文提出了一种面向用户分割和上下文感知的解释层，通过提出一个具有多种解释方法的可视化解释系统，解决了社交媒体平台中AI推荐推荐的可解释性问题。该框架是首个在单一流程中共同调整解释方式（可视化 vs. 数字化）和粒度（专家 vs. 普通用户）的系统。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [25] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 本文提出使用大型预训练多模态模型进行生成内容检测，通过线性分类器训练特征在不同模态下取得最先进的结果，保持计算效率高，快速训练，在少样本情况下也有效。实验结果显示在音频和图像领域的伪造检测表现优异，超过或匹配了强基准方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于恶意用户利用合成媒体传播虚假信息和散布深度伪造内容的情况日益严重，对于稳健和稳定的假检测器的需求迫在眉睫。现有大多数工作训练分类器区分真假信息，但这些工具通常仅在相同生成器家族和数据模式内泛化，对其他生成类别和数据领域的结果较差。因此，需要一个通用分类器。

Method: 使用大型预训练多模态模型来检测生成内容，利用这些模型的潜在编码自然地区分真假信息。通过这一发现，展示线性分类器在这些特征上训练可以取得最先进的结果，同时保持高效、快速和在少样本情况下也有效。

Result: 实验结果表明，提出的方法在音频和图像领域的伪造检测表现出色，超过或匹配了强基准方法。

Conclusion: 本文提出使用大型预训练多模态模型来检测生成内容，通过线性分类器训练这些特征可以在多种模态下实现最先进的结果，保持计算效率高，在少样本情况下也能有效。主要集中在音频和图像领域的伪造检测，表现优异，超过或匹配强基准方法。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>
