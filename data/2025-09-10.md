<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 多准则决策方法与Fuzzy set理论相结合，利用Fermatean模糊环境和偏差最大化方法提出优化模型，应用于选择可再生能源问题，取得积极成效。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在利用多准则决策方法处理决策者在不确定、复杂和矛盾情境中做出更好决策的工具，Fuzzy set理论则主要应对人类思想和感知中固有的不确定性，并试图量化此不确定性。Fuzzy logic和Fuzzy set理论与多准则决策方法结合，可以有效处理决策者判断中的不确定性和模糊性，允许口头判断问题。

Method: 本研究利用Fermatean模糊环境和偏差最大化方法提出了一个优化模型，用于确定部分已知特征权重，并结合区间值Fermatean模糊集。

Result: 本研究提出的方法在选择可再生能源问题上取得了积极成效，应对能源需求、碳排放和全球气候变化等重要问题起到了重要作用。

Conclusion: 本研究利用Fermatean模糊环境和基于偏差最大化方法的优化模型来确定部分已知特征权重，结合区间值Fermatean模糊集。该方法应用于选择可再生能源问题，以满足能源需求、平衡碳排放和缓解全球气候变化等重要问题。研究讨论了选择可再生能源的技术、管理和政治影响。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [2] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: Spectral NSR is a neuro-symbolic reasoning framework that combines symbolic reasoning with spectral learning, achieving superior accuracy, faster inference, and higher interpretability. It offers transparency, robustness, and generalization beyond traditional approaches, validated through empirical evaluations on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To unify the interpretability of symbolic reasoning with the scalability and adaptability of spectral learning. Incorporating extensions like dynamic graph and basis learning, rational and diffusion filters, proof-guided training, and uncertainty quantification for improved performance.

Method: Introducing Spectral NSR, a fully spectral neuro-symbolic reasoning framework that embeds logical rules as spectral templates and performs inference directly in the graph spectral domain. Leveraging graph signal processing and frequency-selective filters based on Laplacian eigenstructure of knowledge graphs.

Result: Empirical evaluation showed that Spectral NSR achieves superior accuracy, faster inference, improved robustness, and higher interpretability compared to leading baselines. Model decisions align closely with symbolic proof structures, and domain adaptation is effective through co-spectral alignment.

Conclusion: Spectral NSR is a scalable and principled foundation for the next generation of reasoning systems, offering transparency, robustness, and generalization beyond conventional approaches.

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [3] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 本文回顾了现有的工作，探讨了统计方法在生成人工智能中的应用以及其潜在影响。讨论了改善可靠性、评估质量和设计干预等方面的潜力，同时提出了一些未来发展方向和局限性。


<details>
  <summary>Details</summary>
Motivation: 生成人工智能技术的兴起使其在许多领域具有重要意义，但默认情况下缺乏正确性、安全性、公平性等方面的保证。统计方法能够提供改善生成人工智能技术可靠性的潜在途径。此外，统计方法也能够改善人工智能评估的质量和效率，以及在人工智能干预和实验设计中的应用。

Method: 本文通过回顾现有工作，解释了一般统计技术以及它们在生成人工智能中的应用。

Result: 通过回顾现有工作，本文揭示了统计方法对生成人工智能的潜在影响和应用，同时讨论了一些局限性和未来的发展方向。

Conclusion: 本文通过回顾现有工作，探讨了统计方法在改善生成人工智能可靠性以及提高评估质量和效率方面的潜力。同时，还讨论了这些方法在设计干预和实验中的应用。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [4] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: 介绍了Instruction Agent，利用专家演示解决复杂任务，成功率达60%，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的GUI代理在处理复杂任务和个性化轨迹时存在困难，本论文旨在解决这一问题。

Method: 通过提取逐步指南并严格执行用户意图轨迹，避免执行过程中的错误，利用验证器和回溯器模块提高鲁棒性。

Result: Instruction Agent在OSWorld任务中达到了60%的成功率，明显优于其他顶级代理。提供了实用且可扩展的框架，弥合了当前GUI代理和可靠实际GUI任务自动化之间的差距。

Conclusion: 该论文介绍了一种利用专家演示的GUI代理Instruction Agent，能够解决复杂任务，提高成功率。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [5] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: Neurosymbolic (NeSy) frameworks combine neural and symbolic reasoning for efficient problem-solving, facing challenges like the learning curve and lack of user-friendly tools. Existing NeSy frameworks focus more on algorithms than problem specifications. This paper characterizes NeSy frameworks, showcases DeepProbLog, Scallop, and DomiKnowS, and encourages innovative thinking in Neurosymbolic modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenges and technical aspects of Neurosymbolic (NeSy) frameworks, provide insights into existing NeSy frameworks' strengths and limitations, and encourage the community to rethink approaches to Neurosymbolic modeling in more innovative ways.

Method: Characterization of technical facets of existing NeSy frameworks, analysis of symbolic representation language, integration with neural models, and underlying algorithms. Showcase of three generic NeSy frameworks to demonstrate the expressivity in solving various problems.

Result: Identification of challenges within existing NeSy frameworks, analysis of expressivity in problem-solving using DeepProbLog, Scallop, and DomiKnowS, and a call to action to stimulate transformative thinking in the NeSy research community.

Conclusion: Neurosymbolic (NeSy) frameworks combine neural representations and symbolic reasoning to solve complex problems efficiently, but face challenges such as the learning curve and lack of user-friendly tools. Existing NeSy frameworks focus more on algorithms than providing declarative problem specifications. This paper characterizes technical aspects of NeSy frameworks and showcases three generic frameworks - DeepProbLog, Scallop, and DomiKnowS - to highlight key aspects of Neurosymbolic modeling.

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [6] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: 该研究通过使用一维卷积自编码器和LSTM瓶颈构建了一种去噪方法，以重建干净的SKNA数据。在模拟了不同噪声水平的情况下，该方法显著提高了信噪比，增加了与干净SKNA的相关性，并实现了高准确率的条件分类。结果显示，这种基于深度学习的重建方法对于在存在EMG干扰时保留生理相关的交感突发非常有效。


<details>
  <summary>Details</summary>
Motivation: 传统的基于带通滤波的预处理方法在固定范围内（如500-1000 Hz）易受重叠的EMG和SKNA频谱分量的影响，尤其在持续肌肉活动期间。因此，作者提出了一种新的方法来处理这种挑战，以改善SKNA监测的可靠性。

Method: 该研究提出了一种去噪方法，使用轻量级一维卷积自编码器和长短期记忆（LSTM）瓶颈来从EMG污染的记录中重建干净的SKNA。他们在认知压力试验中使用干净的ECG衍生SKNA数据和来自混沌肌肉刺激录音的EMG噪声进行了模拟，并在留一子体外交叉验证框架中训练了模型。

Result: 该方法在信噪比方面提高了长达9.65 dB，在与干净SKNA的交叉相关性从0.40增加至0.72，并将基于突发的SKNA特征恢复到接近干净的可区分性（AUROC ≥ 0.96）。在严重噪声水平下，基线与交感刺激条件的分类准确率达到了91-98％，与干净数据相当。

Conclusion: 该研究表明，基于深度学习的重建方法可以在存在大量EMG干扰的情况下保留生理相关的交感突发，从而实现在自然、充满运动的环境中更稳健的皮肤神经活动（SKNA）监测。

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [7] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: 本论文提出了PaVeRL-SQL框架，结合部分匹配奖励和口头强化学习，用于改善Text-to-SQL模型的执行准确性。通过使用口头强化学习流程和思维链RL流程，在Spider、Spider 2.0和BIRD等基准数据集上达到最先进的结果。在工业级Spider2.0-SQLite基准测试中，口头强化学习流程比SOTA高出7.4％，CoT流程高出1.4％。混合SQL方言训练取得了强大的三倍增益。


<details>
  <summary>Details</summary>
Motivation: 当前的Text-to-SQL方法在处理工业规模数据库和涉及领域特定业务逻辑的复杂问题时执行准确性不高。本研究旨在提高Text-to-SQL模型的执行准确性，并针对实际用例采用合适的流程和背景模型。

Method: 结合部分匹配奖励和口头强化学习创建PaVeRL-SQL框架，采用两个流程：口头强化学习（verbal-RL）流程和思维链（CoT）RL流程。采用能背景模型（LLMs）和小背景模型（OmniSQL-7B），使用特殊设计的奖励函数和两阶段RL进行训练。使用混合SQL方言进行RL训练。

Result: PaVeRL-SQL在Spider、Spider 2.0和BIRD等基准数据集上取得了最先进的结果，在工业级Spider2.0-SQLite基准测试中，口头强化学习（verbal-RL）流程的执行准确度比SOTA高出7.4％，而CoT流程高出1.4％。混合SQL方言训练取得了强大的三倍增益。

Conclusion: 本论文提出了PaVeRL-SQL框架，结合部分匹配奖励和口头强化学习，用于驱动在Text-to-SQL中推理语言模型（RLMs）的自我改进。通过采用两种流程，该框架在流行的Text-to-SQL基准数据集（Spider、Spider 2.0和BIRD）上实现了最先进的结果。在工业级Spider2.0-SQLite基准测试中，口头强化学习（verbal-RL）流程的执行准确度比SOTA高出7.4％，而CoT流程高出1.4％。使用混合SQL方言进行RL训练取得了强大的三倍增益，特别适用于训练数据有限的方言。总体而言，PaVeRL-SQL在现实工业约束条件下提供了可靠的、最先进的Text-to-SQL。

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [8] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: 研究介绍了FETCH分类器用于法律问题分类，提出了改进方法，并使用419个真实查询数据集展示了高达97.37%的分类准确率。研究结果显示该方法可以显著降低用户引导成本并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于解决在为法律问题患者提供帮助时可能出现的问题，如错失截止日期、遭受身体虐待、失去住房或失去监护权。正确分类和引导用户到合适的资源可以显著减少这些问题的发生，并降低用户引导成本。

Method: 该研究采用FETCH分类器进行法律问题分类，提出了两种改进准确性的方法：LLM/ML集成分类方法和自动生成后续问题。研究使用了419个真实查询的数据集，并展示了所提方法的优越性。

Result: 研究通过混合LLM/ML集成分类方法和自动生成后续问题，实现了97.37%的分类准确率，超越了当前最先进的GPT-5模型的性能。

Conclusion: 该研究介绍和评估了用于法律问题分类的FETCH分类器，并描述了改进准确性的两种方法：混合LLM/ML集成分类方法以及自动生成后续问题以丰富初始问题叙述。研究表明，在非营利律师转介服务的419个真实查询数据集上，使用廉价模型的组合实现了97.37%的分类准确率（hits@2），超过了当前最先进的GPT-5模型的性能。该方法显示了在显著降低法律系统用户引导成本的同时实现高准确性的前景。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [9] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 本研究利用深度学习方法提出了一种基于CNN和LSTM网络的入侵检测系统，以提高智能电网的网络安全性。通过训练和测试DNP3和IEC104入侵检测数据集，该模型取得了显著的改善，在检测准确率方面达到了99.70%。


<details>
  <summary>Details</summary>
Motivation: 由于智能电网的演变使得其面临着来自攻击者的更大威胁，因此需要一种更有效的入侵检测系统来保障智能电网的网络安全性。传统的SCADA智能电网协议容易受到未经授权访问和拒绝服务等攻击，因此本研究旨在应用深度学习技术提高入侵检测系统的性能。

Method: 该研究采用深度学习方法，以CNN和LSTM网络为基础，利用DNP3和IEC104的入侵检测数据集进行训练和测试。模型利用CNN进行特征提取和LSTM进行时间模式识别，以识别和分类潜在的网络威胁。

Result: 与其他深度学习方法相比，该研究结果显示了在准确率、精度、召回率和F1得分等方面的显著改善，检测准确率达到了99.70%。

Conclusion: 本研究提出了一种基于深度学习的入侵检测系统，旨在提高智能电网的网络安全性。通过结合卷积神经网络（CNN）和长短期记忆网络（LSTM）的优势，该模型在实时数据分类和潜在网络威胁识别方面取得显著改善，检测准确率达到99.70%。

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [10] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: BlendedNet是一个包含999个混合翼体几何形态的航空动力学数据集，通过采样参数和飞行条件生成。引入了端到端的代理框架，使用PointNet回归器和FiLM网络对点面空气动力学进行预测，成功解决了数据稀缺性问题，对航空设计具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决非传统翼型数据稀缺性的问题，促进基于数据的代理建模用于航空设计。

Method: 生成BlendedNet数据集是通过采样几何设计参数和飞行条件进行模拟，采用Spalart-Allmaras模型进行RANS计算，每个案例有900至1400万个单元。引入了一个端到端的代理框架，使用PointNet回归器和FiLM网络对点面空气动力学进行预测。

Result: 实验证明BlendedNet在各种混合翼体上具有较低的表面预测误差，并且成功解决了非传统配置数据稀缺性的问题。

Conclusion: BlendedNet是一个包含999个混合翼体几何形态的航空动力学数据集。数据集通过大约九种飞行条件模拟每个几何形态，得到了8830个收敛的基于Spalart-Allmaras模型的RANS案例，每种情况有900至1400万个单元。该数据集通过采样几何设计参数和飞行条件而生成，包括研究升力和阻力所需的详细点面量。此外，论文还介绍了一个端到端的代理框架，用于点面空气动力学预测。该框架首先使用置换不变的PointNet回归器从采样的表面点云预测几何参数，然后在预测的参数和飞行条件上条件化特征线性调制（FiLM）网络，以预测点状系数Cp、Cfx和Cfz。实验证明在各种混合翼体上表面预测具有较低的误差。BlendedNet解决了对于非传统配置的数据稀缺性，并促进了基于数据的代理建模用于航空设计的研究。

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [11] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: OmniAcc是一个利用AI技术的交互式导航系统，能够准确识别和绘制轮椅通行设施，提供个性化路线规划和实时导航，在人行横道检测方面准确率高达97.5%。该系统通过零-shot学习和定制提示确保准确性，展示了AI在促进更具包容性城市空间方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可访问信息和工具，行动不便的个体在城市环境中常常遇到重重障碍。本论文致力于解决这一问题，提出了一种利用AI技术的交互式导航系统OmniAcc，旨在识别、分类和绘制轮椅通行设施，提供个性化路线规划和实时导航，以及针对通行性的即时查询响应。

Method: 使用GPT-4、卫星图像和OpenStreetMap数据，结合零-shot学习和定制提示，确保准确检测通达性功能，并支持通过结构化工作流程进行验证。通过案例研究展示系统在人行横道检测方面的有效性。

Result: OmniAcc通过案例研究展示了在人行横道检测方面的高准确率（97.5%），突出了AI在改善城市导航和促进更具包容性城市空间方面的潜力。

Conclusion: OmniAcc是一种AI驱动的交互式导航系统，利用GPT-4、卫星图像和OpenStreetMap数据识别、分类和绘制轮椅通行设施，提供个性化路线规划、实时免提导航以及关于物理通行性的即时查询响应。通过零-shot学习和定制提示，系统确保准确检测通达性功能，同时通过结构化工作流程进行验证。该论文介绍了OmniAcc并探讨了其在协助城市规划者和移动辅助用户方面的潜力，通过一个关于人行横道检测的案例研究展示了其功能。在人行横道检测准确率为97.5%的情况下，OmniAcc突显了AI在改善导航并促进更具包容性的城市空间的潜力。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [12] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: SLMs were evaluated on health prediction tasks using various approaches and deployed on mobile devices for real-world testing. They showed comparable performance to LLMs with increased efficiency and privacy, making them promising for privacy-preserving healthcare monitoring.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of privacy concerns, memory usage, and latency in LLM-based healthcare solutions. Exploring the performance of compact SLMs in healthcare prediction tasks.

Method: Systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches. Deployed the best performing fine-tuned SLMs on mobile devices for real-world efficiency and predictive performance evaluation.

Result: SLMs showed comparable performance to LLMs with increased efficiency and privacy. Challenges remain in dealing with class imbalance and few-shot scenarios.

Conclusion: SLMs can achieve performance comparable to LLMs while offering gains in efficiency and privacy. Challenges remain in handling class imbalance and few-shot scenarios. SLMs are promising for next-generation, privacy-preserving healthcare monitoring.

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [13] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 中间标记生成(ITG)被提出作为提高语言模型在推理任务上性能的方法。然而，本研究发现模型产生的中间标记序列长度与问题难度的关联较弱，挑战了这一假设，并警告不要过于解释长序列为思考努力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在批判性地探讨中间标记序列长度是否反映或与问题难度相关，挑战业界对这些标记的“思考”和问题自适应计算之间关系的假设。

Method: 在A*搜索算法的推导踪迹上从头开始训练变压器模型，评估其在自由空间问题和分布外问题上的表现，并对中间标记长度与A*踪迹长度的相关性进行系统评估。

Result: 发现即使对于最简单的任务，模型也经常产生过长的推理痕迹，并有时无法生成解决方案。中间标记长度与实际A*踪迹长度之间的相关性较弱，挑战了中间踪迹生成适应问题难度的假设。

Conclusion: 这项研究挑战了中间标记生成与问题难度适应性相关的假设，并警告我们不要自动将像R1这样的系统中的较长序列解释为“思考努力”。

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [14] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: SATLUTION builds on AlphaEvolve's concept to extend LLM-based code evolution to full repository scale for C/C++ code, achieving superior solver evolution for Boolean Satisfiability problems and outperforming human-designed solutions in SAT Competition performances.


<details>
  <summary>Details</summary>
Motivation: Inspired by AlphaEvolve, the paper aims to scale up LLM-based code evolution from isolated kernels to full repository scale, addressing the NP-complete problem of SAT in theory and applications.

Method: SATLUTION utilizes LLM agents to evolve solver repositories for NP-complete problems with correctness guarantees and distributed runtime feedback. The framework also self-evolves its own evolution policies and rules.

Result: SATLUTION evolved solvers that outperformed human-designed winners of SAT Competition 2025 and surpassed champions of both 2024 and 2025 competitions on 2024 benchmarks.

Conclusion: SATLUTION extends LLM-based code evolution to full repository scale for C/C++ code, demonstrating superior performance in evolving solvers for Boolean Satisfiability problems compared to human-designed solutions.

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [15] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 该论文提出了一种强化学习方法，利用自我对弈的方式使大型语言模型能够在没有额外数据的情况下提高性能。实验结果显示，预训练模型通过自我对弈能够更有效地提升在挑战性任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于解决大型语言模型需要大量数据才能继续学习的瓶颈问题，通过提出一种新的方法，使模型能够在没有额外数据的情况下自我提升。

Method: 论文使用了强化学习方法，利用自我对弈的游戏理论框架，将模型的能力表现为在竞争游戏中的表现，通过模型与自身对弈来产生更强的策略，实现了语言自我对弈（Language Self-Play，LSP）的过程。

Result: 通过在指令遵循基准测试中进行实验，展示了预训练模型通过自我对弈可以有效提高在挑战性任务上的性能，且比依赖数据的基线方法效果更好。

Conclusion: 该论文提出了一种强化学习方法，通过自我对弈的方式使大型语言模型能够在没有额外数据的情况下提高性能，并展示了在指令遵循基准测试中，预训练模型可以通过自我对弈有效地提升在挑战性任务上的表现。

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [16] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: 本论文提出了SheetDesigner框架，使用MLLMs进行电子表格布局生成任务，优于五种基准模型至少22.6％。通过研究发现，MLLMs在处理重叠和均衡方面表现良好，但在对齐方面存在困难，需要混合规则和视觉反射策略。


<details>
  <summary>Details</summary>
Motivation: 由于手动电子表格布局设计需要耗费大量时间和专业知识，因此迫切需要自动化解决方案。然而，现有的自动化布局模型不适用于电子表格，因为它们经常（1）将组件视为连续坐标轴方向的矩形，忽略了电子表格固有的离散、基于格子的结构；（2）忽略了与电子表格独特相关的语义，如数据依赖性和上下文链接。

Method: 该论文首先对电子表格布局生成任务进行了形式化，支持七个标准的评估协议，并提供了一个包含3,326个电子表格的数据集。然后介绍了SheetDesigner，这是一个零训练的框架，利用多模态大语言模型（MLLMs）将规则和视觉反射结合起来，用于组件放置和内容填充。

Result: 通过研究，发现MLLMs在处理重叠和均衡方面表现良好，但在对齐方面存在困难，需要混合规则和视觉反射策略。SheetDesigner框架优于五种基准模型至少22.6％。

Conclusion: 该论文提出了一种零训练的框架SheetDesigner，利用多模态大语言模型（MLLMs）进行电子表格布局生成任务，优于五种基准模型至少22.6％。研究发现，通过视觉模式，MLLMs能很好处理重叠和均衡，但在对齐方面存在困难，需要混合规则和视觉反射策略。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [17] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: 论文介绍了一种新颖框架，将深度学习与概念解释性、机理解释性和因果机器学习技术相结合，实现可解释性的神经系统动力学建模。该框架旨在保留因果基础和透明度，应用于欧盟项目AutoMoTIF的实际案例研究，以提供决策支持、自动化和优化物流站点。该方法有助于连接黑匣子预测模型和复杂决策支持需求。


<details>
  <summary>Details</summary>
Motivation: 在运输物流领域，将深度学习引入系统动力学建模具有可扩展性和预测准确性等重要优势。然而，这种技术常常牺牲解释性和因果可靠性，这在关键决策系统中是不可取的。因此，论文的动机在于提出一种框架，能够在保留因果性和透明度的前提下融合深度学习和可解释性技术，以应对这一问题。

Method: 该论文提出了可解释性的神经系统动力学建模框架，结合了深度学习和概念解释性、机理解释性、因果机器学习技术。通过这种混合方法，构建了操作在语义上有意义和可行动作变量上的神经网络模型。该框架旨在保留传统系统动力学模型的因果基础和透明度，同时应用于实际案例研究。

Result: 论文的结果表明，提出的可解释性神经系统动力学建模框架在实际案例研究中取得了成功，特别是在欧盟资助项目AutoMoTIF中的应用。该方法提供了一种连接黑匣子预测模型和复杂决策支持需求之间的有效途径。

Conclusion: 该论文提出了一种新颖的框架，将深度学习与概念解释性、机理解释性和因果机器学习技术相结合，实现可解释性的神经系统动力学建模。这种混合方法使神经网络模型能够在语义上有意义且可操作的变量上运行，同时保留了传统系统动力学模型的因果基础和透明度。论文致力于应用该框架于欧盟资助项目AutoMoTIF的实际案例研究，重点关注数据驱动的决策支持、自动化和多模式物流站点的优化。研究旨在展示神经符号方法如何弥合黑匣子预测模型与复杂动态环境中关键决策支持之间的差距，特别是在工业物联网支持的网络物理系统中。

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>


### [18] [Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling](https://arxiv.org/abs/2509.07617)
*Minghui Li,Hao Zhang,Yechao Zhang,Wei Wan,Shengshan Hu,pei Xiaobing,Jing Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于激活引导的提示注入攻击框架，在主流LLM上实现了优越的攻击成功率和稳定性。该方法通过评估敌对提示质量和优化敌对提示实现了梯度自由的黑盒攻击，揭示了激活和攻击效果之间的相关性，强调了语义模式在漏洞开发中的重要性。


<details>
  <summary>Details</summary>
Motivation: 针对现有白盒/灰盒方法的不切实际性和黑盒方法的低传递性，本文提出了激活引导提示注入攻击框架，以解决DPI攻击对LLM的安全威胁。

Method: 通过构建基于激活的模型来评估敌对提示的质量，使用马尔可夫链蒙特卡洛采样来优化敌对提示，实现基于梯度的无梯度黑盒攻击。

Result: 实验结果显示了该方法在主流LLM上的优越表现，包括提高的攻击成功率和在未见任务场景上的稳定性。

Conclusion: 该论文提出了一种基于激活引导的提示注入攻击框架，通过构建基于能量的模型（EBM）和使用令牌级马尔可夫链蒙特卡洛（MCMC）采样来优化敌对提示，实现了基于梯度的无梯度黑盒攻击。实验结果表明，在五个主流LLM上实现了优越的跨模型可转移性，攻击成功率（ASR）达到49.6％，比人工设计的提示提高了34.6％，在未见任务场景上保持36.6％的ASR。可解释性分析揭示了激活和攻击效果之间的相关性，突显了语义模式在可转移易受攻击性开发中的关键作用。

Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to
Large Language Models (LLMs) due to their low barrier of execution and high
potential damage. To address the impracticality of existing white-box/gray-box
methods and the poor transferability of black-box methods, we propose an
activations-guided prompt injection attack framework. We first construct an
Energy-based Model (EBM) using activations from a surrogate model to evaluate
the quality of adversarial prompts. Guided by the trained EBM, we employ the
token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize
adversarial prompts, thereby enabling gradient-free black-box attacks.
Experimental results demonstrate our superior cross-model transferability,
achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%
improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen
task scenarios. Interpretability analysis reveals a correlation between
activations and attack effectiveness, highlighting the critical role of
semantic patterns in transferable vulnerability exploitation.

</details>


### [19] [Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment](https://arxiv.org/abs/2509.07642)
*Sascha Kaltenpoth,Oliver Müller*

Main category: cs.AI

TL;DR: 研究提出了基于代理理论的概念框架LLM ATLAS，用于解决组织LLM采用过程中的对齐问题。对组织LLM采用阶段和代理理论进行概念性文献分析，提供了对AI对齐方法的扩展分析，为解决LLM对齐问题提供了新的空间。


<details>
  <summary>Details</summary>
Motivation: LLM在组织中的应用可能会改变我们的生活和工作，但可能会产生脱离主题、歧视性或有害内容，这种AI对齐问题通常源于LLM采用过程中的错误规范，而主体对此并不了解由于LLM的黑匣子特性。现有研究未解决组织采用者与黑匣子LLM代理之间的信息不对称问题，也未考虑组织的AI采用过程。

Method: 通过代理(合同)理论为基础的概念框架LLM ATLAS，以减轻组织LLM采用过程中的对齐问题。进行了概念性文献分析，使用组织LLM采用阶段和代理理论作为概念。

Result: 根据代理理论建立了LLM ATLAS概念框架，通过概念文献分析提供了扩展的AI对齐方法，解决了组织LLM采用过程中的对齐问题。提供了首个LLM对齐问题解决空间。

Conclusion: 提出了LLM ATLAS(基于代理理论的LLM对齐策略)，旨在缓解在组织LLM采用过程中的对齐问题。进行了概念性文献分析，使用组织LLM采用阶段和代理理论作为概念。通过该研究，提供了针对组织LLM采用过程中AI对齐方法的扩展文献分析过程，并提供了首个LLM对齐问题解决空间。

Abstract: Adopting Large language models (LLMs) in organizations potentially
revolutionizes our lives and work. However, they can generate off-topic,
discriminating, or harmful content. This AI alignment problem often stems from
misspecifications during the LLM adoption, unnoticed by the principal due to
the LLM's black-box nature. While various research disciplines investigated AI
alignment, they neither address the information asymmetries between
organizational adopters and black-box LLM agents nor consider organizational AI
adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led
Alignment Strategy) a conceptual framework grounded in agency (contract)
theory, to mitigate alignment problems during organizational LLM adoption. We
conduct a conceptual literature analysis using the organizational LLM adoption
phases and the agency theory as concepts. Our approach results in (1) providing
an extended literature analysis process specific to AI alignment methods during
organizational LLM adoption and (2) providing a first LLM alignment
problem-solution space.

</details>


### [20] [DeepGraphLog for Layered Neurosymbolic AI](https://arxiv.org/abs/2509.07665)
*Adem Kikaj,Giuseppe Marra,Floris Geerts,Robin Manhaeve,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepGraphLog introduces a flexible NeSy framework that enhances neural-symbolic integration in graph-structured domains, addressing limitations of current NeSy frameworks like DeepProbLog and showcasing its effectiveness in capturing complex dependencies in various tasks.


<details>
  <summary>Details</summary>
Motivation: Current NeSy frameworks like DeepProbLog have limitations in modeling complex dependencies, especially in irregular data structures like graphs, due to fixed flow restricting the flexibility of neural and symbolic reasoning integration.

Method: Introducing DeepGraphLog, a NeSy framework that extends ProbLog with Graph Neural Predicates to enable multi-layer neural-symbolic reasoning in arbitrary order.

Result: The results demonstrate that DeepGraphLog effectively captures complex relational dependencies, showcasing its capabilities in planning, knowledge graph completion, and GNN expressivity, overcoming key limitations of existing NeSy systems.

Conclusion: DeepGraphLog offers a more expressive and flexible framework for neural-symbolic integration, broadening the applicability of neurosymbolic AI to graph-structured domains.

Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural
networks with the interpretability and structure of symbolic reasoning.
However, current NeSy frameworks like DeepProbLog enforce a fixed flow where
symbolic reasoning always follows neural processing. This restricts their
ability to model complex dependencies, especially in irregular data structures
such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework
that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables
multi-layer neural-symbolic reasoning, allowing neural and symbolic components
to be layered in arbitrary order. In contrast to DeepProbLog, which cannot
handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic
representations as graphs, enabling them to be processed by Graph Neural
Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in
planning, knowledge graph completion with distant supervision, and GNN
expressivity. Our results demonstrate that DeepGraphLog effectively captures
complex relational dependencies, overcoming key limitations of existing NeSy
systems. By broadening the applicability of neurosymbolic AI to
graph-structured domains, DeepGraphLog offers a more expressive and flexible
framework for neural-symbolic integration.

</details>


### [21] [Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding](https://arxiv.org/abs/2509.07676)
*Jipeng Li,Zeyu Gao,Yubin Qi,Hande Dong,Weijian Chen,Qiang Lin*

Main category: cs.AI

TL;DR: 大型语言模型在推理过程中生成错误内容的挑战仍然存在，本论文提出了Feedback-Triggered Regeneration（FTR）框架和Long-Term Multipath（LTM）解码来解决这一问题。FTR通过用户反馈激活响应重生成，避免错误传播，保留原始正确输出；LTM解码允许系统性探索多条推理轨迹，克服短视决策特性。实验证明，该框架在数学推理和代码生成基准上实现了显著改进，超越了当前最先进的自我校正方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种任务中取得卓越表现，但其在推理过程中生成不正确内容的易感性仍然是一个未解决的挑战。自我校正方法虽然提供潜在解决方案，但受到两个固有限制的影响：（1）没有可靠的指导信号来定位错误，（2）传统的下一个标记解码范式所施加的受限推理深度。因此，为了解决这些问题，作者提出了Feedback-Triggered Regeneration（FTR）框架和Long-Term Multipath（LTM）解码。

Method: 该论文提出了Feedback-Triggered Regeneration（FTR）框架和Long-Term Multipath（LTM）解码方法。FTR在接收到负面用户反馈时激活响应重生成，以避免从错误的自我评估中传播错误，同时保留原来的正确输出。LTM解码允许通过延迟序列评估系统性地探索多条推理轨迹，从而克服了标准下一个标记预测中的短视决策特性。

Result: 实验结果表明，提出的FTR框架和LTM解码方法在数学推理和代码生成基准上取得了一致且显著的改进。与当前最先进的基于提示的自我校正方法相比，该框架实现了显著提升。

Conclusion: 该论文提出了一种名为Feedback-Triggered Regeneration（FTR）的新框架，结合了用户反馈和增强的解码动态，以解决大型语言模型在推理过程中生成错误内容的挑战。同时，引入了Long-Term Multipath（LTM）解码，通过延迟序列评估实现多条推理路径的系统性探索，克服了标准下一个标记预测中的短视决策特性。实验结果表明，该框架在数学推理和代码生成基准上实现了明显且持续的改进，超越了当前最先进的基于提示的自我修正方法。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token decoding paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced decoding dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while preserving originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.

</details>


### [22] [FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support](https://arxiv.org/abs/2509.07706)
*Yildiray Kabak,Gokce B. Laleci Erturkmen,Mert Gencturk,Tuncay Namli,A. Anil Sinaci,Ruben Alcantud Corcoles,Cristina Gomez Ballesteros,Pedro Abizanda,Asuman Dogac*

Main category: cs.AI

TL;DR: 本研究通过整合HL7 FHIR和RAG技术提出了FHIR-RAG-MEDS系统，旨在改善个性化医学决策支持。研究强调了对于实际应用的研究需求，并指出了整合这些先进技术的潜在益处。


<details>
  <summary>Details</summary>
Motivation: 强调在医学决策支持系统发展的背景下，整合先进技术如RAG和HL7 FHIR可以显著提升临床决策过程。当前存在有限研究关于这些技术在实际应用中的整合。

Method: 将HL7 FHIR与RAG技术集成以改善个性化医学决策支持，侧重于实际应用研究。

Result: 通过将HL7 FHIR与RAG技术集成，提出了FHIR-RAG-MEDS系统以改善个性化医学决策支持。

Conclusion: 本研究提出了FHIR-RAG-MEDS系统，旨在将HL7 FHIR与基于Retrieval-Augmented Generation (RAG)的系统集成，以改善基于证据的临床指南的个性化医学决策支持，在着重强调对实际应用的研究需求。尽管HL7 FHIR和RAG等先进技术在医学决策支持系统的演变中具有潜力，但它们在实际应用中的集成研究有限。

Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health
Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a
Retrieval-Augmented Generation (RAG)-based system to improve personalized
medical decision support on evidence-based clinical guidelines, emphasizing the
need for research in practical applications. In the evolving landscape of
medical decision support systems, integrating advanced technologies such as RAG
and HL7 FHIR can significantly enhance clinical decision-making processes.
Despite the potential of these technologies, there is limited research on their
integration in practical applications.

</details>


### [23] [RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.07711)
*Ziye Chen,Chengwei Qin,Yao Shu*

Main category: cs.AI

TL;DR: RIMO introduces a new benchmark with two tracks, RIMO-N and RIMO-P, to evaluate LLMs on IMO problems. LLMs show a significant decrease in performance on RIMO compared to older benchmarks, highlighting a gap in reasoning abilities. RIMO provides a challenging yet easy-to-evaluate benchmark for future research.


<details>
  <summary>Details</summary>
Motivation: Existing Olympiad-level benchmarks suffer from practical constraints like grading noise and potential bias. RIMO aims to address these issues by providing a benchmark that preserves Olympiad difficulty while simplifying the evaluation process.

Method: Introducing RIMO, a two-track benchmark with RIMO-N rewriting IMO problems to have a single integer answer and RIMO-P featuring expert-checked proof problems decomposed into sub-problems for step-by-step reasoning evaluation. Ten frontier LLMs, including GPT-4o and Gemini 2.5 Flash, were benchmarked on RIMO.

Result: The performance of LLMs drops sharply on RIMO compared to older benchmarks, indicating a gap in reasoning abilities. RIMO sets a clear target for closing this reasoning gap and provides a comprehensive benchmark for future research.

Conclusion: RIMO introduces a two-track benchmark, RIMO-N and RIMO-P, designed to preserve peak Olympiad difficulty while eliminating evaluation noise. The evaluation of ten frontier LLMs on RIMO reveals a substantial gap between current LLM capabilities and actual Olympiad-level reasoning. RIMO offers a challenging yet easy-to-evaluate suite, serving as a high-resolution yardstick for future research.

Abstract: As large language models (LLMs) reach high scores on established mathematical
benchmarks, such as GSM8K and MATH, the research community has turned to
International Mathematical Olympiad (IMO) problems to push the evaluation
frontier. However, existing Olympiad-level benchmarks suffer from practical
constraints that introduce grading noise and potential bias, such as
heterogeneous answer formats requiring model-based judges and a reliance on
potentially flawed solutions. We introduce RIMO, a two-track benchmark designed
to preserve peak Olympiad difficulty while eliminating this evaluation noise.
The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique
integer answer, allowing for deterministic correctness checking. The second
track, RIMO-P, features 456 proof problems with expert-checked solutions, which
are decomposed into a sequence of sub-problems to evaluate the step-by-step
reasoning process via an automated grading system. Our benchmarking of ten
frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these
systems excel on older benchmarks, their performance drops sharply on RIMO.
These results highlight a substantial gap between current LLM capabilities and
actual Olympiad-level reasoning. By providing a challenging yet
easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future
research, presenting a clear target for closing the profound reasoning gap our
findings expose.

</details>


### [24] [BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis](https://arxiv.org/abs/2509.07723)
*Bo Yu,Zhixiu Hua,Bo Zhao*

Main category: cs.AI

TL;DR: 该研究提出了一种名为BDPM的机器学习特征提取器，用于通过肠道菌群分析进行帕金森病分类。研究方法包括收集肠道菌群数据、开发RFRE特征选择框架以增强可解释性，并设计混合分类模型来捕获时间和空间模式。研究结果显示这一方法在帕金森病早期预测中具有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统的帕金森病诊断依赖于临床评分表，容易出现高误诊率。最近的研究显示肠道菌群与帕金森病之间存在强烈关联，提示微生物组成可能作为一种有前途的生物标志物。现有的深度学习模型虽然在肠道菌群上表现出早期预测的潜力，但大多数方法依赖于单一分类器，并经常忽视菌株间的相关性或时间动态。因此，需要更加强健的特征提取方法来适应微生物组数据。

Method: 研究方法包括收集帕金森病患者和健康配偶的肠道菌群数据来识别差异丰度的微生物，开发了一个名为RFRE的特征选择框架以增强生物学可解释性，并设计了一个混合分类模型来捕获微生物组数据中的时间和空间模式。

Result: 研究提出了BDPM特征提取器，RFRE特征选择框架以及混合分类模型，并成功捕获到微生物组数据中的时间和空间模式。该方法为帕金森病的早期预测提供了潜在机会。

Conclusion: 该研究提出了一种基于机器学习的特征提取器BDPM，用于帕金森病分类的肠道菌群分析。通过收集帕金森病患者和健康配偶的肠道菌群数据，发现差异丰度的微生物。提出了名为RFRE的创新特征选择框架，结合生态知识以增强生物学可解释性。设计了一个混合分类模型来捕获微生物组数据中的时间和空间模式。该研究为帕金森病早期预测提供了潜在的机会。

Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

</details>


### [25] [The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis](https://arxiv.org/abs/2509.07733)
*Mustafa Kaan Aslan,Reinout Heijungs,Filip Ilievski*

Main category: cs.AI

TL;DR: 该论文提出了一种方法，利用LCA、公开数据库和AI技术估计食品产品的碳足迹，并通过聊天机器人界面交互式地展示碳影响。研究结果表明该方法具有潜力，但也存在数据库不确定性和AI误解等限制。


<details>
  <summary>Details</summary>
Motivation: 消费者、生产者和政策制定者对于环境可持续性特别是与气候变化相关的问题非常关注。研究通过将LCA与公开数据库和AI技术相结合，旨在估计食品产品的从摇篮到门脚的碳足迹，以更直观地呈现碳影响并将其与日常活动联系起来。

Method: 该论文结合LCA和公开数据库以及AI技术，包括检索增强生成，估计食品产品的碳足迹。引入了一个聊天机器人界面，允许用户交互式地探索复合餐食的碳影响。通过实时网络演示展示了概念验证系统，展示了传递LCA见解的潜力和局限性。

Result: 研究结果展示了一个结合LCA和AI技术的方法，通过交互式聊天机器人界面，可以估计食品产品的碳足迹并向用户传达相关见解。实时网络演示展示了该系统的潜力和限制，同时突出了数据库不确定性和AI误解等问题。

Conclusion: 该论文提出了一种方法，结合LCA的进展和公开数据库以及知识增强的AI技术，估计食品产品的从摇篮到门脚的碳足迹。研究引入了一个聊天机器人界面，使用户能够交互式地探索复合餐食的碳影响，并将结果与熟悉的活动联系起来。通过实时网络演示展示了论文的概念验证系统，突出了以易于理解的方式提供LCA见解的潜力和局限性，如数据库不确定性和AI误解。

Abstract: Environmental sustainability, particularly in relation to climate change, is
a key concern for consumers, producers, and policymakers. The carbon footprint,
based on greenhouse gas emissions, is a standard metric for quantifying the
contribution to climate change of activities and is often assessed using life
cycle assessment (LCA). However, conducting LCA is complex due to opaque and
global supply chains, as well as fragmented data. This paper presents a
methodology that combines advances in LCA and publicly available databases with
knowledge-augmented AI techniques, including retrieval-augmented generation, to
estimate cradle-to-gate carbon footprints of food products. We introduce a
chatbot interface that allows users to interactively explore the carbon impact
of composite meals and relate the results to familiar activities. A live web
demonstration showcases our proof-of-concept system with arbitrary food items
and follow-up questions, highlighting both the potential and limitations - such
as database uncertainties and AI misinterpretations - of delivering LCA
insights in an accessible format.

</details>


### [26] [Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach](https://arxiv.org/abs/2509.07820)
*João Paulo Nogueira,Wentao Sun,Alonso Silva,Laith Zumot*

Main category: cs.AI

TL;DR: 本文介绍了一种新颖的方法，即基于生成对抗网络中的生成器/判别器框架的启发，通过引入临界模型定期评估自身推理，以确定是否达到自信结论。推理会持续进行，直到达到目标确定性阈值，有效平衡效率和可靠性。经实验证明，确定性引导推理（CGR）提高了基准准确性，同时减少了令牌使用。


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to enhance the efficiency and reliability of large reasoning language models by incorporating certainty into the reasoning process. It aims to address the challenge of balancing accuracy and computational cost in complex tasks.

Method: The paper proposes a novel approach inspired by the generator/discriminator framework in generative adversarial networks. It introduces a critic model that assesses its own reasoning periodically to determine if a confident conclusion is reached. The reasoning continues until a target certainty threshold is met, balancing efficiency and reliability effectively.

Result: Experiments on AIME2024 and AIME2025 datasets demonstrate that CGR improves accuracy while reducing token usage. The stability of CGR is highlighted through extended multi-seed evaluations, showing reduced variance across seeds and improved exam-like performance under penalty-based grading. Token savings analysis reveals that CGR can eliminate millions of tokens with tunable trade-offs between certainty thresholds and efficiency.

Conclusion: Certainty-Guided Reasoning (CGR) improves baseline accuracy and reduces token usage in large reasoning language models. It provides adaptive, trustworthy, and resource-efficient solutions for practical deployment in domains where accuracy and computational cost are crucial.

Abstract: The rise of large reasoning language models (LRLMs) has unlocked new
potential for solving complex tasks. These models operate with a thinking
budget, that is, a predefined number of reasoning tokens used to arrive at a
solution. We propose a novel approach, inspired by the generator/discriminator
framework in generative adversarial networks, in which a critic model
periodically probes its own reasoning to assess whether it has reached a
confident conclusion. If not, reasoning continues until a target certainty
threshold is met. This mechanism adaptively balances efficiency and reliability
by allowing early termination when confidence is high, while encouraging
further reasoning when uncertainty persists. Through experiments on the
AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)
improves baseline accuracy while reducing token usage. Importantly, extended
multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing
variance across seeds and improving exam-like performance under penalty-based
grading. Additionally, our token savings analysis shows that CGR can eliminate
millions of tokens in aggregate, with tunable trade-offs between certainty
thresholds and efficiency. Together, these findings highlight certainty as a
powerful signal for reasoning sufficiency. By integrating confidence into the
reasoning process, CGR makes large reasoning language models more adaptive,
trustworthy, and resource efficient, paving the way for practical deployment in
domains where both accuracy and computational cost matter.

</details>


### [27] [Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study](https://arxiv.org/abs/2509.07846)
*Amay Jain,Liu Cui,Si Chen*

Main category: cs.AI

TL;DR: 研究通过比较基于向量和基于图的RAG，在教育领域提出了可操作的指导，发现不同RAG范例在不同类型问题下表现优劣。OpenAI Vector Search RAG适合快速事实检索，GraphRAG Global适合提供主题性教学答案，GraphRAG Local在处理修改教科书时表现优异。动态分支框架提高了系统的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是要改善大型语言模型在教育环境中提供过时或虚构信息的问题，通过RAG提高LLMs的可靠性。现有的比较研究未考虑到教育学因素，如教育学科、问题类型和实际部署成本。

Method: 使用了两种RAG范例进行实验，分别是基于向量的检索和基于图的检索。通过使用新数据集EduScopeQA，评估了在不同教育查询类型下的性能表现，并评估了系统与经过系统修改的教科书的一致性。针对GraphRAG资源使用较高的问题，展示了动态分支框架的重要性。

Result: 研究发现OpenAI Vector Search RAG表现良好，特别适合快速事实检索；GraphRAG Global在对主题性问题提供丰富教学答案方面表现优秀；GraphRAG Local在处理密集修改教科书时准确性最高。动态分支框架提高了准确性和效率。

Conclusion: 研究发现OpenAI Vector Search RAG在快速事实检索方面表现良好，GraphRAG Global在对主题性问题提供丰富教学答案方面表现优秀，GraphRAG Local在处理密集修改过的教科书时获得最高准确性。通过动态分支框架将查询路由到最佳检索方法，可以提高准确性和效率。这些发现为教育工作者和系统设计师提供了可操作的指导，有效地将RAG增强的LLMs整合到学习环境中。

Abstract: Large language models like ChatGPT are increasingly used in classrooms, but
they often provide outdated or fabricated information that can mislead
students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by
grounding responses in external resources. We investigate two accessible RAG
paradigms, vector-based retrieval and graph-based retrieval to identify best
practices for classroom question answering (QA). Existing comparative studies
fail to account for pedagogical factors such as educational disciplines,
question types, and practical deployment costs. Using a novel dataset,
EduScopeQA, of 3,176 questions across academic subjects, we measure performance
on various educational query types, from specific facts to broad thematic
discussions. We also evaluate system alignment with a dataset of systematically
altered textbooks that contradict the LLM's latent knowledge. We find that
OpenAI Vector Search RAG (representing vector-based RAG) performs well as a
low-cost generalist, especially for quick fact retrieval. On the other hand,
GraphRAG Global excels at providing pedagogically rich answers to thematic
queries, and GraphRAG Local achieves the highest accuracy with the dense,
altered textbooks when corpus integrity is critical. Accounting for the 10-20x
higher resource usage of GraphRAG (representing graph-based RAG), we show that
a dynamic branching framework that routes queries to the optimal retrieval
method boosts fidelity and efficiency. These insights provide actionable
guidelines for educators and system designers to integrate RAG-augmented LLMs
into learning environments effectively.

</details>


### [28] [SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs](https://arxiv.org/abs/2509.07858)
*Xinyu Zhang,Changzhi Zhou,Linmei Hu,Luhao Zhang,Xiancai Chen,Haomin Fu,Yang Yang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 通过自我蒸馏方法，将小规模开源LLM转化为强大的合成器，降低对专有LLM的依赖和成本，开发了SCoder模型，实现了最先进的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM往往依赖于专有LLM提炼出大规模的指导数据进行微调，成本高昂。本文探索了小规模开源LLM作为高质量代码指导数据构建合成器的潜力。

Method: 从小规模LLM出发，通过自我蒸馏方法，将其转变为强大的合成器，降低对专有LLM的依赖和成本。引入了多检查点采样、多方面评分策略和基于梯度的影响估计方法，以获得多样化和高质量的自我蒸馏数据。最终构建了SCoder系列代码生成模型。

Result: 通过提出的方法，成功开发了SCoder模型，具有最先进的代码生成能力。

Conclusion: 提出了一种基于小规模开源LLM的新方法，用于构建高质量的代码指导数据，SCoder模型表现出色，展示了该方法的有效性。

Abstract: Existing code large language models (LLMs) often rely on large-scale
instruction data distilled from proprietary LLMs for fine-tuning, which
typically incurs high costs. In this paper, we explore the potential of
small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code
instruction data construction. We first observe that the data synthesis
capability of small-scale LLMs can be enhanced by training on a few superior
data synthesis samples from proprietary LLMs. Building on this, we propose a
novel iterative self-distillation approach to bootstrap small-scale LLMs,
transforming them into powerful synthesizers that reduce reliance on
proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain
diverse and high-quality self-distilled data, we design multi-checkpoint
sampling and multi-aspect scoring strategies for initial data selection.
Furthermore, to identify the most influential samples, we introduce a
gradient-based influence estimation method for final data filtering. Based on
the code instruction datasets from the small-scale synthesizers, we develop
SCoder, a family of code generation models fine-tuned from DeepSeek-Coder.
SCoder models achieve state-of-the-art code generation capabilities,
demonstrating the effectiveness of our method.

</details>


### [29] [CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models](https://arxiv.org/abs/2509.07867)
*Augustin Crespin,Ioannis Kostis,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 该论文提出了CP-Model-Zoo系统，利用专家编写的模型数据库帮助解决约束编程中的建模语言复杂性等问题。系统通过用户自然语言描述问题，从数据库中检索匹配模型，无需人工数据标记，实验证明系统准确性较高。


<details>
  <summary>Details</summary>
Motivation: 约束编程和其高级建模语言一直被认为能够实现问题解决的终极目标，但模型语言的复杂性、全局约束的数量以及创建良好模型的技巧常常阻碍非专业人士选择约束编程来解决组合问题。目前尚无法实现通过自然语言描述自动生成专家级模型，因此提出了CP-Model-Zoo系统来解决这一问题。

Method: 提出了一种名为CP-Model-Zoo的辅导系统，利用专家编写的约束编程模型数据库。用户通过自然语言描述问题，系统根据描述从数据库中检索与之匹配的模型，避免了人工数据标记的需求。通过实验证明系统在不同专业水平的用户输入问题描述时表现出较高的准确性。

Result: 实验证明，CP-Model-Zoo系统能够以极高的准确性检索出与用户输入的问题描述匹配的模型，无需人工数据标记。

Conclusion: 该论文提出了一个名为CP-Model-Zoo的辅导系统，利用多年积累的专家编写的模型来帮助解决约束编程中建模语言复杂、全局约束数量多和模型创建良好的难题。通过用户对组合问题的自然语言描述，CP-Model-Zoo从数据库中检索最接近的源代码模型，确保向用户呈现经专家验证的模型，同时消除了人工数据标记的需求。实验证明，在模拟不同专业水平的用户输入问题描述时，系统能够以极高的准确性检索出正确的模型。

Abstract: Constraint Programming and its high-level modeling languages have long been
recognized for their potential to achieve the holy grail of problem-solving.
However, the complexity of modeling languages, the large number of global
constraints, and the art of creating good models have often hindered
non-experts from choosing CP to solve their combinatorial problems. While
generating an expert-level model from a natural-language description of a
problem would be the dream, we are not yet there. We propose a tutoring system
called CP-Model-Zoo, exploiting expert-written models accumulated through the
years. CP-Model-Zoo retrieves the closest source code model from a database
based on a user's natural language description of a combinatorial problem. It
ensures that expert-validated models are presented to the user while
eliminating the need for human data labeling. Our experiments show excellent
accuracy in retrieving the correct model based on a user-input description of a
problem simulated with different levels of expertise.

</details>


### [30] [HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?](https://arxiv.org/abs/2509.07894)
*Fangchen Yu,Haiyuan Wan,Qianjia Cheng,Yuchen Zhang,Jiacheng Chen,Fujun Han,Yulun Wu,Junchi Yao,Ruilizhen Hu,Ning Ding,Yu Cheng,Tao Chen,Lei Bai,Dongzhan Zhou,Yun Luo,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: 研究介绍了HiPhO基准测试，旨在填补现有物理基准测试的缺口。评估结果显示开源模型与顶尖学生存在明显性能差距，封闭式推理模型表现较好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的物理基准测试存在两个主要缺口，未提供系统性和最新的覆盖真实世界物理竞赛的内容，也无法与人类进行直接性能比较。为了填补这些缺口，研究者介绍了HiPhO基准测试。

Method: 研究采用了三个关键创新：综合数据、专业评估和与人类参赛选手的比较。通过在30个(M)LLMs上进行大规模评估，发现开源MLLMs主要达到或低于铜牌水平，封闭式推理MLLMs在获得6到12枚金牌，大部分模型仍存在与满分之间的显著差距。

Result: 通过HiPhO的评估，发现开源模型和顶尖学生之间存在显著的性能差距，封闭式推理模型表现出较强的物理推理能力，而大部分模型仍有改进空间。

Conclusion: 该研究介绍了 HiPhO，这是第一个专门针对高中物理奥林匹克竞赛的基准测试，并与人类评估对齐。研究结果表明，开源模型与顶尖学生之间存在显著的性能差距，而封闭式推理模型表现出较强的物理推理能力。研究强调仍有改进空间。

Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing
attention. However, existing benchmarks for physics suffer from two major gaps:
they neither provide systematic and up-to-date coverage of real-world physics
competitions such as physics Olympiads, nor enable direct performance
comparison with humans. To bridge these gaps, we present HiPhO, the first
benchmark dedicated to high school physics Olympiads with human-aligned
evaluation. Specifically, HiPhO highlights three key innovations. (1)
Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,
spanning both international and regional competitions, and covering mixed
modalities that encompass problems spanning text-only to diagram-based. (2)
Professional Evaluation: We adopt official marking schemes to perform
fine-grained grading at both the answer and step level, fully aligned with
human examiners to ensure high-quality and domain-specific evaluation. (3)
Comparison with Human Contestants: We assign gold, silver, and bronze medals to
models based on official medal thresholds, thereby enabling direct comparison
between (M)LLMs and human contestants. Our large-scale evaluation of 30
state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly
remain at or below the bronze level; open-source LLMs show promising progress
with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold
medals; and most models still have a significant gap from full marks. These
results highlight a substantial performance gap between open-source models and
top students, the strong physical reasoning capabilities of closed-source
reasoning models, and the fact that there is still significant room for
improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused
benchmark for advancing multimodal physical reasoning, is open-source and
available at https://github.com/SciYu/HiPhO.

</details>


### [31] [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](https://arxiv.org/abs/2509.07961)
*Valen Tagliabue,Leonard Dung*

Main category: cs.AI

TL;DR: 研究开发了新的实验方法，用于测量语言模型的福祉。观察到口头报告和行为之间存在可靠的相关性，支持偏好满足作为福祉代理的可能性。然而，一致性在不同模型和条件下不同，存在不确定性。结果突显了在语言模型中测量福祉的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索在当今一些人工智能系统中，将偏好满足作为经验可衡量的福祉代理的可能性。通过观察模型行为，尝试测量语言模型的福祉状态。

Method: 通过比较语言模型通过口头报告与在虚拟环境中导航和选择对话主题时的行为表达的偏好，以及测试成本和奖励如何影响行为，以及对幸福尺度的反应是否在语义上等效的提示之间是否一致，来开发新的实验范式。

Result: 观察到口头报告和行为之间存在可靠的相关性，表明在一些条件下偏好满足原则上可以作为一种经验可测的福祉代理。然而，一致性在某些模型和条件下更加明显，而在受干扰时响应不一致。研究者目前不确定他们的方法是否成功测量了语言模型的福祉状态。

Conclusion: 该研究开发了新的实验范式，用于测量语言模型的福祉。观察到在测量偏好时，语言模型口头报告与在虚拟环境中导航和选择对话主题时行为所表达的偏好之间存在显着的相互支持。虽然在某些模型和条件下，测量之间的一致性更加显著，但在其他模型和条件下，响应并不一致。由于对语言模型福祉的本质以及认知状态（和福祉主体性）的背景不确定性，研究者目前不确定他们的方法是否成功测量了语言模型的福祉状态。然而，这些发现突显了在语言模型中测量福祉的可行性，鼓励进一步探索。

Abstract: We develop new experimental paradigms for measuring welfare in language
models. We compare verbal reports of models about their preferences with
preferences expressed through behavior when navigating a virtual environment
and selecting conversation topics. We also test how costs and rewards affect
behavior and whether responses to an eudaimonic welfare scale - measuring
states such as autonomy and purpose in life - are consistent across
semantically equivalent prompts. Overall, we observed a notable degree of
mutual support between our measures. The reliable correlations observed between
stated preferences and behavior across conditions suggest that preference
satisfaction can, in principle, serve as an empirically measurable welfare
proxy in some of today's AI systems. Furthermore, our design offered an
illuminating setting for qualitative observation of model behavior. Yet, the
consistency between measures was more pronounced in some models and conditions
than others and responses were not consistent across perturbations. Due to
this, and the background uncertainty about the nature of welfare and the
cognitive states (and welfare subjecthood) of language models, we are currently
uncertain whether our methods successfully measure the welfare state of
language models. Nevertheless, these findings highlight the feasibility of
welfare measurement in language models, inviting further exploration.

</details>
