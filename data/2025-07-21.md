<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT is a novel graph-based architecture that improves the efficiency of managing traffic applications by reducing token consumption, average response latency, and improving efficiency in multi-query processing for urban mobility scenarios compared to existing chain-based systems like TrafficGPT.


<details>
  <summary>Details</summary>
Motivation: Current chain-based systems like TrafficGPT face limitations in sequential task execution, high token usage, and poor scalability in managing traffic applications. The motivation behind GraphTrafficGPT is to address these limitations by introducing a more efficient graph-based architecture for parallel task execution and dynamic resource allocation in traffic management scenarios.

Method: The paper proposes GraphTrafficGPT, a graph-based architecture that represents tasks and dependencies as nodes and edges in a directed graph. It introduces a Brain Agent for optimizing dependency graphs and coordinating specialized agents for data handling. The architecture utilizes context-aware token management and supports concurrent multi-query processing.

Result: Experimental results show that GraphTrafficGPT reduces token consumption by 50.2%, average response latency by 19.0%, and improves efficiency in handling multi-query processing by up to 23.0% compared to TrafficGPT.

Conclusion: GraphTrafficGPT is a novel graph-based architecture that improves the efficiency of managing traffic applications compared to current chain-based systems like TrafficGPT. It reduces token consumption, average response latency, and improves efficiency in handling multi-query processing for urban mobility scenarios.

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [2] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: 该论文介绍了PrefPalette框架，通过多属性决策原则实现个性化偏好预测，超越了当前的黑盒偏好模型。PrefPalette在Reddit等在线社交平台的评估中表现优异，提高了平均预测准确度，并揭示了不同社区的特定偏好。该框架不仅在预测性能上优秀，还提供了透明、可解释的洞察，为构建更可信赖、价值感知的个性化应用迈出了一步。


<details>
  <summary>Details</summary>
Motivation: 当前的偏好模型通常将人类判断视为黑盒，而实际需要了解背后的原因。因此，该论文的动机在于引入PrefPalette框架，以透明、可解释的方式个性化预测偏好，超越目前的模型局限。

Method: 该论文使用了多属性决策原则实现PrefPalette框架，包括生成合成训练数据以分离个体属性效应和基于注意力的偏好建模。通过这两种方法，PrefPalette能够捕捉驱动人类判断的多样评估框架，实现了属性维度决策的个性化预测。

Result: PrefPalette在45个社交群体的评估中表现出色，提高了平均预测准确度，并揭示了不同社区的特定偏好。除了预测性能的提升外，PrefPalette还为特定群体的偏好提供了启示，如学术社区注重冗长和刺激，冲突导向的社区重视讽刺和直接性，支持类社区强调同理心。

Conclusion: 该论文介绍了PrefPalette框架，通过将偏好分解为属性维度，并根据不同社区价值观个性化地预测偏好，超越了当前的偏好模型。在Reddit等在线平台上评估后，PrefPalette在平均预测准确度上比GPT-4o高出46.6%。通过模拟人们的判断结构，PrefPalette提供了更优秀的偏好建模和透明、可解释的洞察，为构建更可信赖、价值感知的个性化应用迈出了一步。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [3] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 本文介绍了一种新方法，通过限制领域和采用基于提示的提取方法，将大型语言模型转换为专家系统的符号表示。作者通过实验展示了生成知识库的依从性和连贯性，并提出了结合LLMs召回能力和符号系统精确性的透明混合解决方案。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是因为虽然大型语言模型（LLMs）在开放领域问答等系统中取得成功，但存在幻视或自信生成不正确或不可验证事实等缺点。因此，作者希望引入一种控制和透明的方法，以在专家系统的开发中解决这些问题。

Method: 本文的方法是通过限制领域和采用基于提示的提取方法，将LLMs生成的知识转化为符号表示，基于Prolog进行验证和修正。作者进行了定量和定性实验，使用Claude Sonnet 3.7和GPT-4.1来展示生成的知识库对事实的依从性和语义连贯性。最终提出了一种透明的混合解决方案，结合了LLMs的召回能力和符号系统的精确性。

Result: 通过本文的研究，作者成功展示了将大型语言模型转换为专家系统的新方法，该方法在限定领域和采用提示提取方法的情况下，能够生成可验证和可修正的知识表示。定量和定性实验表明生成的知识库具有对事实的强烈依从性和语义连贯性，同时确保了专家系统的可解释性、可扩展性和可靠性。

Conclusion: 本文介绍了一种使用大型语言模型（LLMs）开发专家系统的新方法，通过限制领域和采用良好结构的基于提示的提取方法，在Prolog中产生知识的符号表示。这种方法确保了开发的专家系统的可解释性、可扩展性和可靠性。作者通过对Claude Sonnet 3.7和GPT-4.1进行定量和定性实验，展示了生成的知识库对事实的强烈依从性和语义连贯性。文章提供了一种透明的混合解决方案，结合了LLMs的召回能力和符号系统的精确性，为在敏感领域构建可靠的人工智能应用奠定了基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [4] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 这篇论文讨论了关系学习在世界范围内尚未获得主导地位的原因，以及需要采取哪些措施来提高其重要性。作者指出现今世界数据主要为文本和图像，但实际应关注建模实体及其属性和关系。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于认为应该集中关注建模实体及其属性和关系，而不仅是对它们的感知或描述。同时指出目前的关系学习尚未在世界范围内站稳脚跟，需要采取措施提高其重要性。

Method: 该论文通过分析世界主要数据形式为文本和图像，但实际上世界是由实体（包括对象、事物和事件）及其属性和关系组成，从而指出了当前集中于建模像素和文字的现状，并介绍了关系学习领域的研究对象和数据形式。

Result: 该论文解释了关系学习尚未在世界范围内获得主导地位的原因，并提出了促进关系学习发展的措施。

Conclusion: 这篇论文讨论了关系学习尚未在世界范围内获得主导地位的原因，以及为了使其得到应有的重视，需要做些什么。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [5] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG is a system that combines dual-graph RAG integration to enhance automated construction compliance checking. It achieves high precision, recall, and F1 score, outperforming vector-only and graph-only RAG baselines. The system offers a transferable blueprint for navigating complex technical documents in engineering domains.


<details>
  <summary>Details</summary>
Motivation: Automated construction compliance checking faces challenges in information retrieval and question answering from safety regulations due to linguistic and structural complexity of regulatory text. Traditional retrieval-augmented generation (RAG) systems struggle with multi-hop queries that require synthesis of information across interlinked clauses. The motivation behind BifrostRAG is to address these challenges and enhance the reasoning capability of large language models over both text meaning and structure.

Method: The paper introduces BifrostRAG, a system that combines dual-graph RAG integration with explicit modeling of linguistic relationships and document structure. The system utilizes an Entity Network Graph and a Document Navigator Graph to enable hybrid retrieval mechanism combining graph traversal with vector-based semantic search. Evaluation was conducted on a multi-hop question dataset to demonstrate the system's performance.

Result: BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1 score of 87.3 percent, surpassing current leading approaches in automated construction compliance checking. Error analysis demonstrates the advantages of the hybrid retrieval mechanism over single-modality RAGs.

Conclusion: BifrostRAG is a dual-graph RAG-integrated system that significantly outperforms vector-only and graph-only RAG baselines in automated construction compliance checking. The hybrid retrieval mechanism achieves 92.8 percent precision, 85.5 percent recall, and an F1 score of 87.3 percent. It serves as a robust knowledge engine for LLM-driven compliance checking and offers a transferable blueprint for navigating complex technical documents in engineering domains.

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [6] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 研究探讨了基于最终答案的智能错误诊断方法，通过设计一个提供错误规则诊断的服务，在二次方程解题过程中应用该服务，结果显示最终答案评估可以诊断约29.4%的步骤，并且与教师诊断结果一致率高达97%。


<details>
  <summary>Details</summary>
Motivation: 许多智能辅导系统可以支持学生解决逐步任务，在学生将多个步骤组合成一个步骤时，可能存在大量可能路径，这种组合爆炸使错误诊断变得困难。使用最终答案来诊断一系列步骤可以缓解这种组合爆炸，因为可能的（错误的）最终答案通常少于（错误的）解决路径。

Method: 研究设计了一个服务，用于在学生结合多个步骤时提供错误规则诊断，将该服务应用于解二次方程过程中无法通过单一规则诊断的学生步骤数据集，结果表明最终答案评估可以诊断29.4%的步骤。

Result: 研究结果表明最终答案评估可以有效诊断学生解题步骤的约29.4%，并且与教师诊断结果高达97%的一致性。

Conclusion: 本研究探讨了基于最终答案的智能错误诊断的潜力，结果显示最终答案评估可以诊断解题步骤的29.4%，并且与教师诊断结果高达97%的一致性。这为进一步探索该方法奠定了基础。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [7] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 该研究提出了一种融合模型追踪和基于约束的建模方法的新方法，用于诊断学生在分步任务中的输入。研究结果表明系统的诊断与教师编码完全一致。


<details>
  <summary>Details</summary>
Motivation: 模型追踪和约束建模各有优势，但各自也存在局限。因此，研究动机在于结合两种方法的优势，提供更全面的学生输入诊断能力。

Method: 提出了一种结合模型追踪和基于约束的建模两种方法的新方法。设计了用于多步策略诊断的系统，并对这些诊断进行了评估。

Result: 通过对含有学生解二次方程步骤的数据集进行验证，结果显示系统诊断与教师编码完全吻合。

Conclusion: 该研究提出了一种融合模型追踪和基于约束的建模两种方法的方法，用于诊断学生在分步任务中的输入。研究结果表明，该系统的诊断与教师编码完全一致。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [8] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM is a log generation and summarization system that integrates contextual activity information across four dimensions, outperforming existing methods in accuracy and efficiency. It utilizes a lightweight LLM-based framework and achieves superior performance with a 1.5B-parameter model compared to a 70B-parameter SOTA baseline.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limitations in accuracy, efficiency, and semantic richness in activity log generation. DailyLLM aims to address these challenges by integrating contextual activity information across four dimensions using common smartphone and smartwatch sensors.

Method: DailyLLM proposes a lightweight LLM-based framework that integrates structured prompting with efficient feature extraction to enable high-level activity understanding. It utilizes a 1.5B-parameter LLM model for log generation and achieves a 17% improvement in BERTScore precision compared to the 70B-parameter SOTA baseline, with nearly 10x faster inference speed.

Result: Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art log generation methods and is suitable for deployment on personal computers and Raspberry Pi.

Conclusion: DailyLLM is a log generation and summarization system that outperforms existing methods in terms of accuracy and efficiency. It comprehensively integrates contextual activity information across four dimensions and can be efficiently deployed on personal computers and Raspberry Pi.

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [9] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [10] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 该论文提出了一个基于代理增强战略推理的混合架构，结合了启发式提取、语义激活和组合综合。通过语义相互依赖的过程激活和组合多个启发式，旨在实现更优于传统决策引擎的结果。通过Meta vs. FTC案例研究展示了该框架的效果，并初步验证了语义度量指标。论文的结果表明，该系统能够将不同的启发式融合为连贯且具有情境敏感性的叙事，具有更为综合和有效的表现。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于探索一种新的战略推理架构，能够有效结合不同来源的启发式，并将它们融合为具有情境敏感性的叙事。通过借鉴量子认知研究和语义相互依赖的概念，旨在实现更优于传统决策引擎的决策结果。研究对于战略决策领域的方法论和实践具有重要意义

Method: 论文使用了启发式提取、语义激活和组合综合的方法，通过语义相互依赖的过程激活和组合多个启发式。研究灵感来源包括经典军事理论和当代企业战略，还借鉴量子认知研究。与传统决策引擎的方式不同，该系统将冲突的启发式融合为连贯的叙事，在语义相互作用建模和修辞框架的引导下形成具有情境敏感性的结果。通过Meta vs. FTC案例研究展示了该框架，并初步验证了语义度量指标

Result: 通过Meta vs. FTC案例研究展示了该混合架构的效果，并通过语义度量指标进行了初步验证。结果表明，该系统能够将不同的启发式融合为连贯且具有情境敏感性的叙事，相较于传统决策引擎有着更为综合和有效的表现

Conclusion: 该论文提出了一个基于代理增强战略推理的混合架构，结合了启发式提取、语义激活和组合综合。通过语义相互依赖的过程激活和组合多个启发式，从经典军事理论到当代企业战略等各种来源汲取灵感。与选择最佳规则的传统决策引擎不同，该系统将冲突的启发式融合为连贯且具有情境敏感性的叙事，通过语义相互作用建模和修辞框架进行引导。我们通过Meta vs. FTC案例研究演示了该框架，初步验证了语义度量指标。讨论了一些限制和扩展（例如动态干扰调整）

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [11] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: 该论文介绍了一种名为EAGLE的轻量级框架，旨在解决现有T-GNNs在效率和效果方面的挑战。EAGLE通过结合短期时间新近性和长期全局结构模式，并利用自适应权重机制来平衡两者属性，实现了显著的性能提升。在七个真实世界的时间图上的实验中，EAGLE表现出优越的效果和效率，相对于最先进的T-GNNs，速度提升超过50倍。


<details>
  <summary>Details</summary>
Motivation: 现有的T-GNNs由于计算开销高而存在可扩展性和效率挑战。本论文的动机在于提出一种轻量级框架EAGLE，旨在解决现有方法的局限性，并在效率和效果上取得显著改进。

Method: 提出了EAGLE框架，包括基于时间感知和结构感知的模块，以及自适应权重机制来平衡两者的贡献。通过在七个真实世界的时间图上进行广泛实验来验证EAGLE的性能。

Result: 在七个真实世界的时间图上的实验表明，EAGLE相对于最先进的T-GNNs在效率和效果上都表现更优，且速度比基于Transformer的T-GNNs提高50倍以上。

Conclusion: 该论文提出了一种名为EAGLE的轻量级框架，结合了短期时间新近性和长期全局结构模式，通过自适应加权机制平衡这两者属性，显著提高了效率。实验证明，EAGLE在效果和效率上都优于最先进的T-GNNs，相比于基于Transformer的T-GNNs，速度提升超过50倍。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [12] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 本文介绍了一种因果知识转移框架，使RL代理能够在非稳态环境中零迁移地学习和分享紧凑的因果表示。研究发现异构目标的代理在新环境中能够填补部分差距，并且因果知识转移效果取决于环境复杂性和代理目标的异构性。


<details>
  <summary>Details</summary>
Motivation: 传统MARL中的知识转移方法在非稳态环境中泛化能力不足，因此本文的动机在于解决这一挑战。代理通常需要昂贵的重新训练才能适应环境的变化。通过引入因果知识转移框架，本文试图解决这一问题，使代理能够有效地适应新环境并实现目标最大化。

Method: 本文提出了一种因果知识转移框架，利用紧凑的因果表示来帮助RL代理学习和共享其路径，通过在线转移恢复动作宏的方法，实现在非稳态环境中的零迁移适应。这种方法通过查询一个查找模型获得本地上下文信息（碰撞）并应用恢复动作宏，有效填补了传统知识转移方法的不足。

Result: 研究结果显示，异构目标的代理在适应新环境时能够弥合随机探索和完全重新训练策略之间的差距约一半。此外，因果知识转移的影响取决于环境复杂性和代理异构目标之间的相互作用。

Conclusion: 本文引入了一种因果知识转移框架，使强化学习（RL）代理能够学习和共享非稳态环境中路径的紧凑因果表示。通过在线转移恢复动作宏，代理可以零迁移地适应新环境，从而填补了传统知识转移方法在MARL中泛化能力不足的缺口。研究结果表明，异构目标的代理能够填补随机探索和完全重新训练策略之间约一半的差距，并且因果知识转移的影响取决于环境复杂性和代理异构目标之间的相互作用。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [13] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 本论文提出了一种模型不可知潜空间创意框架，旨在解决大型语言模型在创意生成中的挑战。该框架无需手工规则且可适应不同领域、输入格式和创意任务，展示了作为通用共同创意者的潜力。初步结果表明该框架具有应用前景。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于创新性思想生成在人工智能中的重要性，以及现有大型语言模型在创意生成中的局限性。由于大型语言模型经常复制训练数据中的模式，无法生成既新颖又相关的内容，因此需要一种能够控制创意生成过程的方法。旨在提出一种能够跨领域、输入格式和创意任务适用的方法，以促进人工智能与人类合作的共同创意。

Method: 本论文使用了模型不可知潜空间创意框架来帮助大型语言模型在创意生成中克服复制训练数据模式的问题。与之前基于领域特定启发式和结构化提示管道的方法不同，该框架不需要手工规则，并能轻松适应不同领域、输入格式和创意任务。论文介绍了该方法的早期原型，概述了概念框架并展示了初步结果。

Result: 论文的初步结果表明提出的模型不可知潜空间创意框架具有潜力成为通用共同创意者，有望解决大型语言模型复制训练数据模式的问题。

Conclusion: 本论文提出了一种无需手工规则的模型不可知潜空间创意框架，旨在解决大型语言模型在创意生成中遇到的挑战。通过在连续嵌入空间中导航，实现了可控且可扩展的创意生成方式。初步结果表明这一框架潜力巨大，可作为人工智能与人类合作的通用共同创意者。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [14] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 研究提出了一种名为ADPC的新型视觉语言因果干预框架，用于神经疾病诊断辅助。通过因果干预消除混杂因素，实现了优异的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 早期识别和干预可以有效延缓MCI向痴呆症的发展，但由于多模态数据选择偏倚和变量之间复杂关系引起的混淆，诊断AD仍然是神经学领域面临的重要挑战。

Method: ADPC模型利用大型语言模型（LLM）总结临床数据，将参与者分类为认知正常（CN）、轻度认知障碍（MCI）和阿尔茨海默病（AD）类别。框架通过因果干预隐式地消除混杂因素。

Result: 实验结果展示了该方法在区分CN/MCI/AD病例方面的出色性能，达到了大多数评估指标的最先进水平。

Conclusion: 研究提出了一种名为阿尔茨海默病预测与交叉模态因果干预（ADPC）的新型视觉语言因果干预框架，用于诊断辅助。通过整合因果推理与多模式学习，展示了在神经疾病诊断中的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [15] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的基于时间和约束的逻辑扩展，结合了两种ASP基础扩展，用于解决动态系统中的非单调时间推理和数值约束处理，为ASP范式内处理高分辨率复杂动态系统建立了基础逻辑框架。


<details>
  <summary>Details</summary>
Motivation: 动态系统具有精细的时间和数值分辨率，对于基于逻辑的方法如ASP提出了显著挑战，因此有必要引入一个新的时间和约束扩展以解决这些挑战。

Method: 结合了两种基础ASP扩展：Here-and-There线性时间逻辑和具有约束的Here-and-There逻辑，实现了强大的非单调时间推理能力和对数值约束的直接集成和处理。

Result: 建立了适用于ASP范式内复杂动态系统的逻辑框架，为非单调时间推理与约束的结合提供了首个方法。

Conclusion: 本文引入并详细阐述了一种新颖的基于时间和约束的扩展，用于非单调时间推理与ASP的结合，为处理具有高分辨率的复杂动态系统建立了基础逻辑框架。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [16] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA is a new OM framework that uses Large Language Models to improve ontology matching performance without increasing communication overhead. It surpasses traditional and modern approaches in experiments, showcasing the effectiveness of targeted knowledge retrieval, prompt enrichment, and ontology refinement.


<details>
  <summary>Details</summary>
Motivation: Existing OM systems often rely on handcrafted rules or specialized models with limited adaptability. KROMA aims to address this limitation by leveraging LLMs to enhance semantic context in OM tasks and reduce communication overhead.

Method: Integrating bisimilarity-based concept matching, lightweight ontology refinement, and knowledge retrieval with context-augmented LLMs. The framework dynamically enriches semantic context with structural, lexical, and definitional knowledge to optimize performance and efficiency.

Result: Experimental results demonstrate that KROMA enhances ontology matching, surpassing traditional and modern approaches while keeping communication overhead comparable. The study validates the effectiveness of the proposed optimization techniques for ontology matching at scale.

Conclusion: KROMA, a novel OM framework that utilizes Large Language Models in a Retrieval-Augmented Generation pipeline, significantly improves ontology matching performance while maintaining communication overhead. It outperforms traditional OM systems and modern LLM-based approaches on benchmark datasets.

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [17] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: 本文介绍了Glucose-ML，包含10个公开可用的糖尿病数据集，旨在加速人工智能解决方案的发展。研究人员进行了比较分析和案例研究，展示同一算法在不同数据集上可能有显着不同的预测结果。提供了数据选择指导和血糖预测基准，为开发稳健AI解决方案提供了建议。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能算法在糖尿病管理领域的重要性增加，为加速稳健AI解决方案的发展，研究人员希望通过提供大量高质量糖尿病数据集来克服数据获取的障碍。通过比较分析和案例研究，指导算法开发者更好地利用这些数据集。

Method: 研究人员收集了同一算法在不同数据集上的预测结果，运用比较分析和案例研究的方法，提供了短期血糖预测基准。为算法开发者提供数据选择指导，并提供每个长期糖尿病数据集的直接链接和开放源码。

Result: 研究结果表明同一算法在不同数据集上可能有显着不同的预测结果，为开发稳健AI解决方案提出了建议。提供了Glucose-ML集合中每个糖尿病数据集的链接和开放源码。

Conclusion: 本文提出了Glucose-ML，一个包含10个公开可用的糖尿病数据集的集合，以加速透明、可重现和稳健的人工智能解决方案的发展。研究人员进行了比较分析，为算法开发者提供数据选择指导，并通过血糖预测案例研究展示了在Glucose-ML集合中所有10个糖尿病数据集上的短期血糖预测基准。研究结果表明，同一算法在不同数据集上开发/评估时可能具有显着不同的预测结果。最后，利用本研究结果提出开发糖尿病或更广泛健康领域稳健人工智能解决方案的建议。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [18] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: 该研究引入了生成-人工智能-支持的人体运动模拟（G-AI-HMS），通过集成文本到文本和文本到运动模型来增强模拟质量。研究结果表明，在八项任务的案例研究中，AI增强的运动在大多数情况下表现优于人类创建描述，通过提高空间精度、相似性对齐和整体时间相似性等方面来实现更佳的模拟效果。统计分析证实AI增强的提示显著改善了关节误差和时间不对齐问题，同时保持了相当的姿势准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动模拟方法通常存在运动保真度低的问题。本研究旨在引入生成-人工智能-支持的人体运动模拟（G-AI-HMS），通过集成文本到文本和文本到运动模型来提高模拟质量，从而解决现有方法的不足之处。研究动机是通过弥补现有方法的缺陷，提高运动模拟的质量，从而实现对工人行为、安全性和生产力在工业任务中的成本效益评估。

Method: 该研究方法采用生成-人工智能-支持的人体运动模拟（G-AI-HMS），通过集成文本到文本和文本到运动模型来提高模拟质量。研究使用大型语言模型将任务描述转化为运动感知语言，并通过计算机视觉验证AI增强的运动与真实人类动作的相似性。研究应用姿势估计算法提取实时视频中的关节地标，并使用运动相似度指标将其与AI增强的序列进行比较。在八项任务的案例研究中，研究结果表明，AI增强的运动在大多数情况下的误差率低于人类创建的描述，在空间精度方面表现更佳以及相似性对齐和整体时间相似性方面表现更佳。统计分析表明，AI增强的提示显著降低了关节误差和时间不对齐，同时保持了可比的姿势准确性。

Result: 研究结果表明，G-AI-HMS在大多数情况下比人类创建的描述在误差率、空间精度、相似性对齐和整体时间相似性等方面表现更佳。统计分析显示，AI增强的提示显著降低了关节错误和时间不对齐，并保持了可比的姿势准确性。

Conclusion: 该研究引入了生成-人工智能-支持的人体运动模拟（G-AI-HMS），通过集成文本到文本和文本到运动模型来增强物理任务的模拟质量。G-AI-HMS通过解决两个关键挑战：使用大型语言模型将任务描述转换为与MotionGPT的训练词汇对齐的运动感知语言，并使用计算机视觉验证AI增强的运动与真实人类动作的相似性。研究结果表明，在八项任务的案例研究中，AI增强的运动在大多数情况下都表现出比人类创建描述更低的错误率，基于空间精度在六项任务中表现更好，在姿势归一化后的四项任务中对齐性更好，在整体时间相似性方面的七项任务中表现更好。统计分析结果显示，AI增强的提示显著降低了关节错误和时间不对齐，同时保持了可比的姿势准确性。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [19] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 这项试点研究探讨了大型语言模型（LLMs）在解释NDE轮廓图方面的能力。研究评估了几种LLMs的能力，基于它们产生详细描述、识别缺陷、提供建议和展示总体准确性的能力。研究还使用专门设计的提示来提升图像描述的质量。研究发现，九种模型中有四种提供更好的图像描述，有效地涵盖了与桥梁状况相关的广泛主题。 ChatGPT-4和Claude 3.5 Sonnet产生了更有效的总结。这项研究建立的框架将LLMs整合到桥梁检查工作流程中，显示了LLM辅助分析提高效率而不损害准确性的潜力。


<details>
  <summary>Details</summary>
Motivation: Bridge maintenance and safety are crucial for transportation authorities. Interpreting NDE data is time-consuming and requires expertise, potentially delaying decision-making. Recent advancements in LLMs offer new ways to automate and improve this analysis, leading to increased efficiency in bridge inspection workflows.

Method: This pilot study explores the capabilities of Large Language Models (LLMs) in interpreting NDE contour maps. Several LLMs are evaluated based on their ability to produce detailed descriptions, identify defects, provide recommendations, and demonstrate overall accuracy. The study also uses prompts specifically designed to enhance image descriptions.

Result: The research indicates that four out of nine LLM models provide better image descriptions, covering a wide range of bridge condition topics effectively. LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more effective summaries. The framework established in this study integrates LLMs into bridge inspection workflows, showing the potential of LLM-assisted analysis to enhance efficiency without compromising accuracy.

Conclusion: LLMs have the potential to significantly improve efficiency and accuracy in interpreting NDE contour maps for bridge maintenance and safety assessments.

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [20] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: 本文介绍了一种自动强化学习框架CUDA-L1，用于CUDA优化，取得了显著性能提升。CUDA-L1在不同GPU架构上展示了卓越的加速效果，并发现了多种CUDA优化技术和原则。研究结果表明，强化学习可将性能不佳的模型转变为高效的CUDA优化器，为自动化CUDA操作优化提供新的可能性。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型的快速发展导致对GPU计算资源的指数级增长需求，当前SOTA模型在改善CUDA速度方面取得的成功率较低，因此迫切需要自动化CUDA优化策略。本研究旨在应对这一需求，引入了CUDA-L1框架，通过自动强化学习实现CUDA优化。

Method: 本文采用自动强化学习框架CUDA-L1进行CUDA优化，通过训练在不同GPU架构上达到了显著的加速效果。CUDA-L1不仅能够提高CUDA操作的性能，还展示了发现优化技术、揭示原则和识别性能瓶颈等优秀特性。训练的强化学习模型可以推广到新的核心上，为CUDA操作的自动优化提供了可能性。

Result: CUDA-L1在NVIDIA A100上训练，对KernelBench的250个CUDA核心实现了平均速度提升x17.7，峰值提升达到x449。此外，在H100、RTX 3090、L40、H800和H20等GPU架构上也取得显著的平均速度提升效果。CUDA-L1展示了强化学习如何将传统性能较差的模型转变为高效的CUDA优化器，为自动化优化CUDA操作提供了新的思路。

Conclusion: 本文介绍了CUDA-L1，一个用于CUDA优化的自动强化学习框架，能在多种GPU架构上实现显著性能提升。研究结果表明，CUDA-L1不仅在速度上取得了显著改进，还展示了多种引人注目的特性，如发现CUDA优化技术、揭示优化原则以及识别性能瓶颈等。通过速度提升作为奖励信号，CUDA-L1展示了强化学习如何将原始表现不佳的模型转变为高效的CUDA优化器。这一范式为自动化优化CUDA操作开辟了新的可能性，有望大幅提升GPU效率并缓解GPU计算资源的压力。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>
