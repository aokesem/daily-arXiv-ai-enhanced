{"id": "2508.00081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00081", "abs": "https://arxiv.org/abs/2508.00081", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "comment": null, "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\u6765\u6539\u8fdbHealthBench\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u57f9\u80b2\u5177\u5907\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3001\u4f26\u7406\u6027\u3001\u5168\u7403\u9002\u7528\u6027\u7684\u533b\u7597\u8bed\u8a00\u6a21\u578b\u3002\u63d0\u51fa\u4e86\u76f8\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u8bc1\u636e\u52a0\u6743\u8bc4\u5206\u3001\u4e0a\u4e0b\u6587\u8986\u76d6\u903b\u8f91\u7b49\u3002", "motivation": "\u7531\u4e8eHealthBench\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u610f\u89c1\u800c\u9762\u4e34\u533a\u57df\u504f\u89c1\u3001\u4e2a\u522b\u4e34\u5e8a\u533b\u751f\u7279\u5f02\u6027\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\u5b58\u5728\u7740\u7279\u5b9a\u6311\u6218\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u901a\u8fc7\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\u6765\u5f3a\u8c03\u5168\u7403\u76f8\u5173\u6027\u548c\u516c\u5e73\u6027\u7684\u52a8\u673a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u5956\u52b1\u51fd\u6570\u4e0e\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u6765\u89e3\u51b3HealthBench\u73b0\u6709\u5c40\u9650\u6027\u7684\u65b9\u6848\uff0c\u5305\u62ec\u5f3a\u8c03\u8bc1\u636e\u52a0\u6743\u8bc4\u5206\u3001\u4e0a\u4e0b\u6587\u8986\u76d6\u903b\u8f91\u4ee5\u53ca\u91cd\u70b9\u8003\u8651\u9053\u5fb7\u56e0\u7d20\u548c\u5ef6\u8fdf\u7ed3\u679c\u53cd\u9988\u7b49\u65b9\u9762\u3002", "result": "\u901a\u8fc7\u6539\u8fdb\u5956\u52b1\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5851\u9020\u66f4\u52a0\u503c\u5f97\u4fe1\u8d56\u548c\u5168\u7403\u5177\u6709\u5f71\u54cd\u529b\u7684\u533b\u7597\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u901a\u8fc7\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\u6765\u5f3a\u5316\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u6539\u8fdbHealthBench\u5728\u533b\u7597\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u5c40\u9650\u6027\u3002\u65e8\u5728\u4fc3\u8fdb\u4e0d\u4ec5\u5728\u8bed\u8a00\u4e0a\u4f18\u79c0\uff0c\u800c\u4e14\u5728\u4e34\u5e8a\u4e0a\u503c\u5f97\u4fe1\u8d56\u3001\u7b26\u5408\u4f26\u7406\u8981\u6c42\u3001\u5177\u6709\u5168\u7403\u5f71\u54cd\u529b\u7684\u533b\u7597\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00106", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eHyperTWTL\u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u5b89\u5168\u611f\u77e5\u7684\u6700\u4f18\u7b56\u7565\u3002\u7814\u7a76\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u62fe\u53d6\u548c\u9012\u9001\u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4f18\u4e8e\u57fa\u51c6RL\u7b97\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5b58\u5728\u8bb8\u591a\u5173\u4e8e\u65f6\u95f4\u903b\u8f91\u7ea6\u675f\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\u7684\u6587\u732e\uff0c\u4f46\u5728\u4f7f\u7528\u8d85\u5c5e\u6027\u8fdb\u884c\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u91cd\u8981\u7684\u7814\u7a76\u7a7a\u767d\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u63a2\u8ba8\u57fa\u4e8eHyperTWTL\u7684\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528HyperTWTL\u7ea6\u675f\u6765\u5f62\u5f0f\u5316\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u7ea6\u675f\uff0c\u63d0\u51fa\u4e86\u5b89\u5168\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u62fe\u53d6\u548c\u9012\u9001\u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u4e2d\u663e\u793a\u51fa\u4e86\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u4e24\u79cd\u57fa\u51c6RL\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperTWTL\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u5b89\u5168\u611f\u77e5\u7684\u6700\u4f18\u7b56\u7565\uff0c\u540c\u65f6\u6ee1\u8db3HyperTWTL\u7ea6\u675f\u3002\u5728\u62fe\u53d6\u548c\u9012\u9001\u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e0e\u5176\u4ed6\u4e24\u79cd\u57fa\u51c6RL\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u7684\u8868\u73b0\u4f18\u4e8e\u5b83\u4eec\u3002"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u4ecb\u7ecd\u4e86\u751f\u6210\u5f0f\uff0c\u9884\u6d4b\u6027\u548c\u89c4\u8303\u6027\u4eba\u5de5\u667a\u80fd\u4ee5\u53ca\u9762\u5411\u5bf9\u8c61\u7684\u8fc7\u7a0b\u6316\u6398\u3002\u4f5c\u8005\u6307\u51fa\u4eba\u5de5\u667a\u80fd\u9700\u8981\u8fc7\u7a0b\u667a\u80fd\u6765\u6539\u8fdb\u8fd0\u8425\u6d41\u7a0b\uff0c\u5e76\u5f3a\u8c03\u4e86\u6210\u529f\u7ed3\u5408OCPM\u548c\u4eba\u5de5\u667a\u80fd\u7684\u673a\u4f1a\u3002\u7814\u7a76\u7ed3\u679c\u8ba4\u4e3a\u8fc7\u7a0b\u667a\u80fd\u5bf9\u4e8e\u8fde\u63a5\u6570\u636e\u548c\u8fc7\u7a0b\u4ee5\u5b9e\u73b0\u4e0d\u540c\u5f62\u5f0f\u4eba\u5de5\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u73b0\u6709\u7ec4\u7ec7\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5e94\u7528\u4eba\u5de5\u667a\u80fd\u65f6\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u4f5c\u8005\u5e0c\u671b\u63ed\u793a\u4eba\u5de5\u667a\u80fd\u5fc5\u987b\u7ed3\u5408\u8fc7\u7a0b\u667a\u80fd\u624d\u80fd\u5728\u7ec4\u7ec7\u80cc\u666f\u4e0b\u53d6\u5f97\u6210\u529f\u7684\u539f\u56e0\uff0c\u5e76\u5f3a\u8c03\u4e86OCPM\u5bf9\u4e8e\u8fde\u63a5\u6570\u636e\u548c\u8fc7\u7a0b\u4ee5\u5b9e\u73b0\u4e0d\u540c\u5f62\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u6027\u3002", "method": "\u8be5\u8bba\u6587\u57fa\u4e8e\u5b9e\u8bc1\u7814\u7a76\u548c\u7406\u8bba\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5e94\u7528\u7684\u56f0\u96be\u548c\u6311\u6218\u3002\u901a\u8fc7\u8be6\u7ec6\u8ba8\u8bba\u751f\u6210\u5f0f\uff0c\u9884\u6d4b\u6027\u548c\u89c4\u8303\u6027\u4eba\u5de5\u667a\u80fd\u4ee5\u53ca\u9762\u5411\u5bf9\u8c61\u7684\u8fc7\u7a0b\u6316\u6398\uff08OCPM\uff09\uff0c\u9610\u8ff0\u4e86\u8fc7\u7a0b\u667a\u80fd\u5bf9\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u6027\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u201c\u8fc7\u7a0b\u667a\u80fd\uff08PI\uff09\u201d\u7684\u6982\u5ff5\uff0c\u5e76\u8bf4\u660e\u4e86\u5982\u4f55\u6210\u529f\u7ed3\u5408OCPM\u548c\u4e0d\u540c\u5f62\u5f0f\u7684\u4eba\u5de5\u667a\u80fd\u3002", "result": "\u4f5c\u8005\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u7406\u8bba\u5206\u6790\uff0c\u9610\u660e\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u56f0\u96be\u548c\u6311\u6218\uff0c\u4ee5\u53ca\u8fc7\u7a0b\u667a\u80fd\u5728\u63d0\u9ad8\u8fd0\u8425\u6d41\u7a0b\u6548\u7387\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\u3002\u4ed6\u4eec\u63d0\u51fa\u4e86\u201c\u8fc7\u7a0b\u667a\u80fd\uff08PI\uff09\u201d\u7684\u6982\u5ff5\uff0c\u5e76\u63a2\u8ba8\u4e86\u6210\u529f\u6574\u5408OCPM\u548c\u4eba\u5de5\u667a\u80fd\u7684\u673a\u4f1a\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u7684\u666e\u53ca\u5f71\u54cd\u4e86\u6211\u4eec\u7684\u5de5\u4f5c\u65b9\u5f0f\uff0c\u4e92\u52a8\u65b9\u5f0f\uff0c\u5546\u4e1a\u8fd0\u4f5c\u65b9\u5f0f\u4ee5\u53ca\u7814\u7a76\u65b9\u5f0f\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528\u4eba\u5de5\u667a\u80fd\u7684\u56f0\u96be\uff0c\u91cd\u70b9\u653e\u5728\u7aef\u5230\u7aef\u7684\u8fd0\u8425\u6d41\u7a0b\u4e0a\u3002\u4f5c\u8005\u8003\u8651\u4e86\u751f\u6210\u5f0f\uff0c\u9884\u6d4b\u6027\u548c\u89c4\u8303\u6027\u4eba\u5de5\u667a\u80fd\uff0c\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u8bca\u65ad\u548c\u6539\u8fdb\u6b64\u7c7b\u6d41\u7a0b\u7684\u6311\u6218\u3002\u6587\u7ae0\u6307\u51fa\uff0c\u4eba\u5de5\u667a\u80fd\u9700\u5229\u7528\u9762\u5411\u5bf9\u8c61\u7684\u8fc7\u7a0b\u6316\u6398(OCPM)\u3002\u8fc7\u7a0b\u76f8\u5173\u6570\u636e\u662f\u7ed3\u6784\u5316\u7684\uff0c\u7279\u5b9a\u4e8e\u7ec4\u7ec7\uff0c\u5e76\u4e14\u4e0e\u6587\u672c\u4e0d\u540c\uff0c\u6d41\u7a0b\u901a\u5e38\u5177\u6709\u9ad8\u5ea6\u52a8\u6001\u6027\u3002OCPM\u662f\u8fde\u63a5\u6570\u636e\u548c\u6d41\u7a0b\u7684\u7f3a\u5931\u73af\u8282\uff0c\u5e76\u80fd\u591f\u5b9e\u73b0\u4e0d\u540c\u5f62\u5f0f\u7684\u4eba\u5de5\u667a\u80fd\u3002\u4f5c\u8005\u4f7f\u7528\u201c\u8fc7\u7a0b\u667a\u80fd\uff08PI\uff09\u201d\u4e00\u8bcd\u6765\u6307\u4ee3\u4ee5\u5904\u7406\u5404\u79cd\u5bf9\u8c61\u548c\u4e8b\u4ef6\u7c7b\u578b\u4e3a\u7279\u70b9\u7684\u4ee5\u6d41\u7a0b\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u9a71\u52a8\u6280\u672f\u7684\u7efc\u5408\uff0c\u4f7f\u5f97\u4eba\u5de5\u667a\u80fd\u5728\u7ec4\u7ec7\u73af\u5883\u4e2d\u5f97\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u89e3\u91ca\u4e86\u4e3a\u4f55\u4eba\u5de5\u667a\u80fd\u9700\u8981\u8fc7\u7a0b\u667a\u80fd\u6765\u6539\u5584\u8fd0\u8425\u6d41\u7a0b\uff0c\u5e76\u7a81\u51fa\u4e86\u6210\u529f\u7ed3\u5408OCPM\u548c\u751f\u6210\u5f0f\uff0c\u9884\u6d4b\u6027\u4ee5\u53ca\u89c4\u8303\u6027\u4eba\u5de5\u667a\u80fd\u7684\u673a\u4f1a\u3002"}}
{"id": "2508.00129", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00129", "abs": "https://arxiv.org/abs/2508.00129", "authors": ["Agust\u00edn Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicol\u00e1s Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "comment": null, "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u68c0\u6d4b\u548c\u5904\u7406\u591a\u6807\u51c6\u51b3\u7b56\u5206\u6790\u4e2d\u7684Rank Reversals\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u6d4b\u8bd5\u65b9\u6cd5\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\u3002\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5b9e\u65bd\u590d\u6742\u6027\u548c\u8bbe\u8ba1\u8003\u8651\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u591a\u6807\u51c6\u51b3\u7b56\u5206\u6790\u4e2d\uff0cRank Reversals\u662f\u4e00\u4e2a\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u4f1a\u6781\u5927\u5730\u5f71\u54cd\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u5bf9\u7279\u5b9a\u5907\u9009\u65b9\u6848\u7684\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u6709\u4e00\u79cd\u673a\u5236\u6765\u8861\u91cf\u4e00\u79cd\u65b9\u6cd5\u5728\u4e00\u7ec4\u5907\u9009\u65b9\u6848\u4e0a\u7684\u6027\u80fd\u3002\u8fd9\u4e2a\u60f3\u6cd5\u53ef\u4ee5\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u4ee5\u5efa\u7acb\u4e0d\u540c\u65b9\u6cd5\u89e3\u51b3\u95ee\u9898\u6548\u679c\u7684\u5168\u5c40\u6392\u540d\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e09\u79cd\u68c0\u6d4bRank Reversals\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u6d4b\u8bd5\u3002", "result": "\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\u4e86\u4e09\u79cd\u68c0\u6d4bRank Reversals\u5b58\u5728\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u5b9e\u73b0\u8fd9\u4e9b\u6d4b\u8bd5\u65f6\u7684\u590d\u6742\u6027\u548c\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u6027\u7684\u8bbe\u8ba1\u8003\u8651\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e09\u79cd\u68c0\u6d4bRank Reversals\u5b58\u5728\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u6d4b\u8bd5\u3002\u8ba8\u8bba\u4e86\u5728\u5b9e\u73b0\u8fd9\u4e9b\u6d4b\u8bd5\u65f6\u51fa\u73b0\u7684\u590d\u6742\u6027\u4ee5\u53ca\u4e3a\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u6027\u6240\u505a\u7684\u8bbe\u8ba1\u8003\u8651\u3002\u6700\u540e\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u6d4b\u8bd5\u65b9\u6cd5\u5bf9\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u5728\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2508.00137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00137", "abs": "https://arxiv.org/abs/2508.00137", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "title": "SHACL Validation under Graph Updates (Extended Paper)", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728RDF\u56fe\u4e2d\u4f7f\u7528SHACL\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5728\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u9759\u6001\u9a8c\u8bc1\u7684\u95ee\u9898\u3002\u4f7f\u7528\u56de\u5f52\u6280\u672f\u5c06\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7b80\u5316\u4e3aSHACL\u7ea6\u675f\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u3002\u63d0\u51fa\u4e86\u539f\u578b\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u5176\u5728SHACL\u7ea6\u675f\u4e0a\u6267\u884c\u9759\u6001\u9a8c\u8bc1\u548c\u5176\u4ed6\u9759\u6001\u5206\u6790\u4efb\u52a1\u7684\u884c\u4e3a\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76SHACL\u5728RDF\u56fe\u4e2d\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u63a2\u8ba8\u5982\u4f55\u5728\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5bf9SHACL\u89c4\u8303\u7684\u9759\u6001\u9a8c\u8bc1\u3002\u8fd9\u4e3a\u8fdb\u4e00\u6b65\u63a8\u7406\u5173\u4e8e\u4e0d\u65ad\u6f14\u5316\u7684RDF\u56fe\u7684\u670d\u52a1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86\u5c06\u66f4\u65b0\u64cd\u4f5c\u5d4c\u5165SHACL\u7ea6\u675f\u7684\u56de\u5f52\u6280\u672f\uff0c\u5c06\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7b80\u5316\u5230(a minor extension of) SHACL\u4e2d\u7ea6\u675f\u7684(\u4e0d)\u6ee1\u8db3\u6027\u95ee\u9898\u3002\u5bf9SHACL\u53ca\u5176\u4e00\u4e9b\u5173\u952e\u7247\u6bb5\u7684\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u8fdb\u884c\u4e86\u5206\u6790\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u578b\u5b9e\u73b0\u7528\u4e8e\u6267\u884c\u9759\u6001\u9a8c\u8bc1\u548c\u5176\u4ed6\u9759\u6001\u5206\u6790\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u521d\u6b65\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u56de\u5f52\u6280\u672f\uff0c\u5c06SHACL\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7b80\u5316\u4e3aSHACL\u7ea6\u675f\u7684\u6ee1\u8db3\u6027\u95ee\u9898\u3002\u5206\u6790\u4e86\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u4e2a\u539f\u578b\u5b9e\u73b0\uff0c\u7528\u4e8e\u6267\u884cSHACL\u7ea6\u675f\u7684\u9759\u6001\u9a8c\u8bc1\u548c\u5176\u4ed6\u9759\u6001\u5206\u6790\u4efb\u52a1\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86SHACL\u5728RDF\u56fe\u4e2d\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eSHACL\u7684\u66f4\u65b0\u8bed\u8a00\uff0c\u7528\u4e8e\u6355\u6349\u5bf9RDF\u56fe\u7684\u76f4\u89c2\u548c\u73b0\u5b9e\u4fee\u6539\u3002\u901a\u8fc7\u5c06\u66f4\u65b0\u64cd\u4f5c\u5d4c\u5165SHACL\u7ea6\u675f\uff0c\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u53ef\u4ee5\u7b80\u5316\u5230SHACL\u7684\u7ea6\u675f\u6ee1\u8db3\u6027\u95ee\u9898\u3002\u7814\u7a76\u4e86SHACL\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u4ee5\u53ca\u4e00\u4e9b\u5173\u952e\u7247\u6bb5\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u578b\u5b9e\u73b0\uff0c\u901a\u8fc7\u521d\u6b65\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u5728SHACL\u7ea6\u675f\u4e0a\u6267\u884c\u9759\u6001\u9a8c\u8bc1\u548c\u5176\u4ed6\u9759\u6001\u5206\u6790\u4efb\u52a1\u7684\u884c\u4e3a\u3002"}}
{"id": "2508.00138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00138", "abs": "https://arxiv.org/abs/2508.00138", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u5bf9\u6587\u5316\u8fb9\u7f18\u7fa4\u4f53\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u63d0\u51fa\u901a\u8fc7\u91cd\u65b0\u6784\u5efaAI\u751f\u4ea7\u6d41\u7a0b\u6765\u51cf\u8f7b\u8fd9\u4e9b\u5371\u5bb3\u7684\u89c2\u70b9\u3002\u4ecb\u7ecd\u4e86\u589e\u5f3a\u578b\u4eba\u5de5\u667a\u80fd\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u5f3a\u8c03\u534f\u540c\u751f\u4ea7\u3001\u591a\u6837\u6027\u3001\u5e73\u7b49\u3001\u5305\u5bb9\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002\u5c06\u751f\u547d\u5468\u671f\u6a21\u578b\u4e0e\u4f26\u7406\u6846\u67b6\u8054\u7cfb\u8d77\u6765\uff0c\u6982\u8ff0\u4e86\u6269\u5c55\u53c2\u4e0e\u5f0f\u6cbb\u7406\u7684\u5173\u952e\u7814\u7a76\u95ee\u9898\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u5bf9\u6587\u5316\u8fb9\u7f18\u7fa4\u4f53\u4ea7\u751f\u4e0d\u5747\u8861\u5f71\u54cd\uff0c\u5df2\u6709\u4e00\u7cfb\u5217\u65b9\u6cd5\u6765\u964d\u4f4e\u8fd9\u79cd\u98ce\u9669\uff0c\u4f46\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u4e8e\u5bf9\u8bbe\u8ba1\u6b63\u4e49\u3001\u5e7f\u4e49\u5b66\u4e60\u7406\u8bba\u548c\u6700\u65b0\u53c2\u4e0e\u5f0f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u501f\u9274\uff0c\u63d0\u51fa\u4e86\u91cd\u65b0\u6784\u5efaAI\u751f\u4ea7\u7ba1\u9053\u7684\u89c2\u70b9\uff0c\u5f3a\u8c03\u591a\u6837\u6027\u3001\u5e73\u7b49\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002", "method": "\u7ed3\u5408\u8bbe\u8ba1\u6b63\u4e49\u3001\u5e7f\u4e49\u5b66\u4e60\u7406\u8bba\u548c\u6700\u8fd1\u5173\u4e8e\u53c2\u4e0e\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4e3b\u5f20\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u4eba\u5de5\u667a\u80fd\u751f\u4ea7\u6d41\u7a0b\u6765\u51cf\u8f7b\u6f5c\u5728\u98ce\u9669\u3002\u63d0\u51fa\u589e\u5f3a\u578b\u4eba\u5de5\u667a\u80fd\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u5f3a\u8c03\u534f\u540c\u751f\u4ea7\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u5021\u5bfc\u5206\u5e03\u5f0f\u6743\u5a01\u548c\u8fed\u4ee3\u77e5\u8bc6\u4ea4\u6d41\u3002", "result": "\u63d0\u51fa\u4e86\u589e\u5f3a\u578b\u4eba\u5de5\u667a\u80fd\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u5305\u62ec\u4e94\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u9636\u6bb5\uff0c\u5f3a\u8c03\u4e86\u534f\u540c\u751f\u4ea7\u3001\u591a\u6837\u6027\u3001\u5e73\u7b49\u3001\u5305\u5bb9\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002\u5c06\u8be5\u751f\u547d\u5468\u671f\u6a21\u578b\u4e0e\u4f26\u7406\u6846\u67b6\u8054\u7cfb\u8d77\u6765\uff0c\u660e\u786e\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u5173\u952e\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u4e86\u91cd\u65b0\u6784\u5efa\u4eba\u5de5\u667a\u80fd\u751f\u4ea7\u6d41\u7a0b\u7684\u89c2\u70b9\uff0c\u5f3a\u8c03\u91c7\u7528\u534f\u540c\u751f\u4ea7\u3001\u591a\u6837\u6027\u3001\u5e73\u7b49\u3001\u5305\u5bb9\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u4e3a\u51cf\u8f7b\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u5bf9\u6587\u5316\u8fb9\u7f18\u7fa4\u4f53\u7684\u6f5c\u5728\u5371\u5bb3\u3002\u4ecb\u7ecd\u4e86\u589e\u5f3a\u578b\u4eba\u5de5\u667a\u80fd\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u534f\u540c\u6846\u67b6\u3001\u534f\u540c\u8bbe\u8ba1\u3001\u534f\u540c\u5b9e\u65bd\u3001\u534f\u540c\u90e8\u7f72\u548c\u534f\u540c\u7ef4\u62a4\u7b49\u4e94\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u9636\u6bb5\u3002\u901a\u8fc7\u56db\u4e2a\u8de8\u5b66\u79d1\u7814\u8ba8\u4f1a\u548c\u5206\u5e03\u5f0f\u6743\u5a01\u3001\u8fed\u4ee3\u77e5\u8bc6\u4ea4\u6d41\u4e3b\u9898\uff0c\u6784\u5efa\u4e86\u751f\u547d\u5468\u671f\u6a21\u578b\u3002\u6700\u540e\uff0c\u5c06\u63d0\u8bae\u7684\u751f\u547d\u5468\u671f\u4e0e\u51e0\u79cd\u4e3b\u8981\u7684\u4f26\u7406\u6846\u67b6\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u6982\u8ff0\u4e86\u6269\u5c55\u53c2\u4e0e\u5f0f\u6cbb\u7406\u7684\u5173\u952e\u7814\u7a76\u95ee\u9898\u3002"}}
{"id": "2508.00143", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00143", "abs": "https://arxiv.org/abs/2508.00143", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "comment": "Accepted for presentation at NCME AIME-Con 2025", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u4eba\u7c7b\u5728\u8bc4\u4f30\u6570\u636e\u6807\u7b7e\u4e2d\u7684\u4e0d\u5b8c\u7f8e\u6027\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u4e92\u8865\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u5916\u90e8\u6548\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u601d\u8003\u6ce8\u91ca\u8d28\u91cf\u548c\u771f\u76f8\u6807\u51c6\uff0c\u4ee5\u63d0\u9ad8\u5b66\u751f\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u6307\u51fa\u4eba\u7c7b\u4e92\u8bc4\u5728\u9a8c\u8bc1\u6570\u636e\u6807\u7b7e\u65b9\u9762\u5b58\u5728\u7684\u504f\u89c1\u3001\u4e0d\u53ef\u9760\u6027\u548c\u5b9a\u4e49\u201c\u771f\u76f8\u201d\u7684\u4e0d\u9002\u7528\u6027\u3002\u63d0\u51fa\u8fc7\u4e8e\u4f9d\u8d56\u4eba\u7c7b\u4e92\u8bc4\u7684\u7f3a\u9677\uff0c\u9610\u91ca\u4e86\u5bf9\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\uff0c\u5e76\u63a2\u8ba8\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u63d0\u9ad8\u5b66\u4e60\u76f8\u5173\u6570\u636e\u5206\u7c7b\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e94\u79cd\u4e92\u8865\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u591a\u6807\u7b7e\u6ce8\u91ca\u65b9\u6848\u3001\u57fa\u4e8e\u4e13\u5bb6\u7684\u65b9\u6cd5\u548c\u95ed\u73af\u9a8c\u8bc1\u7b49\u3002\u5f3a\u8c03\u4e86\u5916\u90e8\u6548\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u5efa\u7acb\u9a8c\u8bc1\u5bfc\u5e08\u884c\u4e3a\u7684\u7a0b\u5e8f\u5e76\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u5bfc\u5e08\u884c\u4e3a\u7c7b\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u4e94\u79cd\u4e92\u8865\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8bba\u6587\u8ba4\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u6539\u5584\u5b66\u751f\u5b66\u4e60\u548c\u63d0\u4f9b\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u89c1\u89e3\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u3002\u5f3a\u8c03\u4e86\u5916\u90e8\u6548\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u601d\u8003\u6ce8\u91ca\u8d28\u91cf\u548c\u771f\u76f8\u6807\u51c6\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u8ba4\u4e3a\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u4e92\u8bc4\u53ef\u9760\u6027\u4f5c\u4e3a\u6ce8\u91ca\u6570\u636e\u8d28\u91cf\u7684\u95e8\u536b\uff0c\u963b\u788d\u4e86\u4ee5\u6709\u6548\u548c\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7c7b\uff0c\u4ece\u800c\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u3002\u63d0\u51fa\u4e86\u4e94\u79cd\u4e92\u8865\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f8b\u5982\u591a\u6807\u7b7e\u6ce8\u91ca\u65b9\u6848\u3001\u57fa\u4e8e\u4e13\u5bb6\u7684\u65b9\u6cd5\u548c\u95ed\u73af\u9a8c\u8bc1\u7b49\uff0c\u4ee5\u4ea7\u751f\u6539\u5584\u5b66\u751f\u5b66\u4e60\u548c\u63d0\u4f9b\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u6d1e\u89c1\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u3002\u5f3a\u8c03\u5916\u90e8\u6548\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u5982\u901a\u8fc7\u5efa\u7acb\u9a8c\u8bc1\u5bfc\u5e08\u884c\u4e3a\u7684\u7a0b\u5e8f\u5e76\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u5bfc\u5e08\u884c\u4e3a\u7c7b\u522b\uff08\u4f8b\u5982\u63d0\u4f9b\u63d0\u793a\uff09\u4e2d\u6709\u6548\u3002\u547c\u5401\u5b66\u754c\u91cd\u65b0\u601d\u8003\u6ce8\u91ca\u8d28\u91cf\u548c\u771f\u76f8\u6807\u51c6\uff0c\u5c06\u6709\u6548\u6027\u548c\u6559\u80b2\u5f71\u54cd\u7f6e\u4e8e\u5171\u8bc6\u4e4b\u4e0a\u3002"}}
{"id": "2508.00159", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u901a\u8fc7\u5f3a\u5236\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u660e\u786e\u8d4b\u4e88\u4eba\u7c7b\u6743\u529b\uff0c\u5e76\u4ee5\u7406\u60f3\u65b9\u5f0f\u7ba1\u7406\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u4e4b\u95f4\u7684\u6743\u529b\u5e73\u8861\uff0c\u63a8\u5e7f\u5b89\u5168\u548c\u5e78\u798f\u7684\u60f3\u6cd5\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53c2\u6570\u5316\u548c\u53ef\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4ee3\u8868\u4eba\u7c7b\u6743\u529b\u7684\u4e0d\u5e73\u7b49\u548c\u98ce\u9669\u538c\u6076\u7684\u957f\u671f\u603b\u548c\uff0c\u5e76\u63a8\u5bfc\u4e86\u8ba1\u7b97\u8be5\u5ea6\u91cf\u7684\u7b97\u6cd5\u3002\u8f6f\u6700\u5927\u5316\u9002\u5f53\u7684\u4eba\u7c7b\u6743\u529b\u5408\u9002\u7684\u805a\u5408\u6307\u6807\u53ef\u80fd\u662f\u5b89\u5168\u7684\uff0c\u5e76\u6709\u76ca\u4e8e\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002", "motivation": "AI\u5b89\u5168\u4e2d\u7684\u5173\u952e\u6982\u5ff5\u662f\u6743\u529b\uff1a\u6743\u529b\u5bfb\u6c42\u4f5c\u4e3a\u4e00\u79cd\u5de5\u5177\u6027\u76ee\u6807\uff0c\u4eba\u7c7b\u7684\u7a81\u7136\u6216\u9010\u6e10\u5931\u80fd\uff0c\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4e92\u52a8\u4e2d\u7684\u6743\u529b\u5e73\u8861\u4ee5\u53ca\u56fd\u9645\u4eba\u5de5\u667a\u80fd\u6cbb\u7406\u3002\u540c\u65f6\uff0c\u4f5c\u4e3a\u8ffd\u6c42\u591a\u6837\u76ee\u6807\u7684\u80fd\u529b\u5bf9\u5e78\u798f\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u5f3a\u5236\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u660e\u786e\u8d4b\u4e88\u4eba\u7c7b\u6743\u529b\uff0c\u5e76\u4ee5\u7406\u60f3\u65b9\u5f0f\u7ba1\u7406\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u4e4b\u95f4\u7684\u6743\u529b\u5e73\u8861\uff0c\u63a2\u8ba8\u63a8\u5e7f\u5b89\u5168\u548c\u5e78\u798f\u7684\u60f3\u6cd5\u3002", "method": "\u4f7f\u7528\u4e00\u79cd\u57fa\u4e8e\u516c\u7406\u7684\u3001\u90e8\u5206\u516c\u7406\u5316\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53c2\u6570\u5316\u548c\u53ef\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4ee3\u8868\u4eba\u7c7b\u6743\u529b\u7684\u4e0d\u5e73\u7b49\u548c\u98ce\u9669\u538c\u6076\u7684\u957f\u671f\u603b\u548c\uff0c\u8003\u8651\u4e86\u4eba\u7c7b\u7684\u6709\u9650\u7406\u6027\u548c\u793e\u4f1a\u89c4\u8303\uff0c\u5e76\u5e7f\u6cdb\u8003\u8651\u4e86\u53ef\u80fd\u7684\u4eba\u7c7b\u76ee\u6807\u3002\u63a8\u5bfc\u4e86\u8ba1\u7b97\u8be5\u5ea6\u91cf\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u5f52\u7eb3\u6216\u8fd1\u4f3c\u901a\u8fc7\u4e00\u79cd\u5f62\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4ece\u7ed9\u5b9a\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u3002\u8f6f\u6700\u5927\u5316\u8fd9\u4e2a\u5ea6\u91cf\u5728\u5404\u79cd\u5178\u578b\u60c5\u51b5\u4e0b\u7684\u7ed3\u679c\uff0c\u63cf\u8ff0\u4e86\u5b83\u53ef\u80fd\u6697\u793a\u7684\u5de5\u5177\u6027\u5b50\u76ee\u6807\u3002", "result": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ee3\u8868\u4eba\u7c7b\u6743\u529b\u7684\u4e0d\u5e73\u7b49\u548c\u98ce\u9669\u538c\u6076\u7684\u957f\u671f\u603b\u548c\u7684\u53c2\u6570\u5316\u548c\u53ef\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u8003\u8651\u4e86\u4eba\u7c7b\u7684\u6709\u9650\u7406\u6027\u548c\u793e\u4f1a\u89c4\u8303\uff0c\u5e7f\u6cdb\u8003\u8651\u4e86\u53ef\u80fd\u7684\u4eba\u7c7b\u76ee\u6807\u3002\u63a8\u5bfc\u4e86\u8ba1\u7b97\u8be5\u5ea6\u91cf\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u5f52\u7eb3\u6216\u8fd1\u4f3c\u901a\u8fc7\u4e00\u79cd\u5f62\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4ece\u7ed9\u5b9a\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u3002\u8f6f\u6700\u5927\u5316\u9002\u5f53\u7684\u4eba\u7c7b\u6743\u529b\u5408\u9002\u7684\u805a\u5408\u6307\u6807\u53ef\u80fd\u6784\u6210\u6709\u76ca\u4e8e\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u76ee\u6807\uff0c\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002", "conclusion": "\u8f6f\u6027\u6700\u5927\u5316\u4eba\u7c7b\u6743\u529b\u7684\u5408\u9002\u805a\u5408\u5ea6\u91cf\u53ef\u80fd\u6784\u6210\u4e00\u79cd\u6709\u76ca\u4e8e\u4ee3\u7406\u4eba\u7c7b\u7cfb\u7edf\u7684\u76ee\u6807\uff0c\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002"}}
{"id": "2508.00222", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS\u662f\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u91cd\u91cd\u8981\u6027\u62bd\u6837\u548c\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u5b9e\u73b0\u5185\u90e8\u5f00\u53d1\u4e0e\u5916\u90e8\u6570\u636e\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u80fd\u529b\u8fb9\u754c\u95ee\u9898\u3002\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u975e\u5206\u5e03\u5f0f\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u76f8\u5bf9\u6539\u8fdb\u8303\u56f4\u4ece21.1\to\u523069.2%\u3002", "motivation": "RL-PLUS\u63d0\u51fa\u7684\u52a8\u673a\u662f\u89e3\u51b3RLVR\u5728\u57fa\u672cLLM\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\u65b9\u9762\u9047\u5230\u7684\u56f0\u96be\uff0c\u4ee5\u53ca\u907f\u514dRLVR\u53ef\u80fd\u5bfc\u81f4\u7684\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\u3002\u901a\u8fc7\u5185\u90e8\u5f00\u53d1\u548c\u5916\u90e8\u6570\u636e\u7684\u534f\u540c\u4f5c\u7528\uff0cRL-PLUS\u65e8\u5728\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u6bd4\u57fa\u672c\u6a21\u578b\u66f4\u5f3a\u5927\u7684\u6027\u80fd\u3002", "method": "RL-PLUS\u6574\u5408\u4e86\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u591a\u91cd\u91cd\u8981\u6027\u62bd\u6837\u7528\u4e8e\u89e3\u51b3\u5916\u90e8\u6570\u636e\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4ee5\u53ca\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u7528\u4e8e\u6307\u5bfc\u6a21\u578b\u8d70\u5411\u5177\u6709\u9ad8\u4ef7\u503c\u4e14\u672a\u88ab\u63a2\u7d22\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86RL-PLUS\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u975e\u5206\u5e03\u5f0f\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cRL-PLUS\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709RLVR\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002\u5728\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u4e2d\uff0cRL-PLUS\u53d6\u5f97\u4e86\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u8303\u56f4\u4ece21.1\to\u523069.2%\u3002", "conclusion": "RL-PLUS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5185\u90e8\u5f00\u53d1\u548c\u5916\u90e8\u6570\u636e\u91c7\u7528\u5408\u4f5c\u65b9\u5f0f\uff0c\u4ee5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u5e76\u7a81\u7834\u57fa\u672c\u6a21\u578b\u7684\u8fb9\u754c\u3002\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRL-PLUS\u4f18\u4e8e\u73b0\u6709\u7684RLVR\u65b9\u6cd5\uff0c\u5728\u516d\u4e2a\u975e\u5206\u5e03\u5f0f\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u4e0e\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u76f8\u6bd4\uff0cRL-PLUS\u5b9e\u73b0\u4e86\u4e00\u81f4\u548c\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u8303\u56f4\u4e3a21.1\to 69.2\to%\u3002Pass@k\u66f2\u7ebf\u663e\u793aRL-PLUS\u6709\u6548\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2508.00271", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MetaAgent\uff0c\u4e00\u79cd\u53d7\u5b66\u4ee5\u81f4\u7528\u539f\u5219\u542f\u53d1\u7684\u81ea\u4e3b\u5b66\u4e60\u4ee3\u7406\u8303\u5f0f\u3002MetaAgent\u901a\u8fc7\u4e0d\u65ad\u4f18\u5316\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u6784\u5efa\u5185\u90e8\u5de5\u5177\u548c\u77e5\u8bc6\u5e93\uff0c\u5728\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u8d85\u8d8a\u4f20\u7edf\u5de5\u4f5c\u6d41\u6a21\u578b\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u4ee3\u7406\uff0c\u5c55\u793a\u4e86\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u672c\u6587\u63d0\u51faMetaAgent\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u4e3b\u5b66\u4e60\u548c\u4e0d\u65ad\u4f18\u5316\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\uff0c\u63a2\u7d22\u4ee3\u7406\u7cfb\u7edf\u5728\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002\u501f\u9274\u5b66\u4ee5\u81f4\u7528\u7684\u539f\u5219\uff0cMetaAgent\u80fd\u591f\u52a8\u6001\u5b66\u4e60\u3001\u6539\u8fdb\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u9010\u6b65\u63d0\u5347\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "MetaAgent\u4f7f\u7528\u4e86meta tool learning\u8fd9\u4e00\u4e0d\u65ad\u6570\u636e\u9a71\u52a8\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u3001\u7b54\u6848\u9a8c\u8bc1\u7b49\u65b9\u5f0f\uff0c\u6539\u8fdb\u5176\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u65e0\u9700\u66f4\u6539\u6a21\u578b\u53c2\u6570\u6216\u8fdb\u4e00\u6b65\u540e\u7eed\u8bad\u7ec3\u3002\u4ee3\u7406\u4e0d\u4ec5\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6c42\u52a9\u8bf7\u6c42\uff0c\u8fd8\u6784\u5efa\u5185\u90e8\u5de5\u5177\u548c\u6301\u4e45\u77e5\u8bc6\u5e93\uff0c\u63d0\u5347\u4fe1\u606f\u68c0\u7d22\u548c\u6574\u5408\u80fd\u529b\u3002", "result": "\u7ecf\u8fc7\u591a\u9879\u6311\u6218\u6027\u77e5\u8bc6\u53d1\u73b0\u57fa\u51c6\u6d4b\u8bd5\u7684\u9a8c\u8bc1\uff0cMetaAgent\u5728\u8d85\u8d8a\u5de5\u4f5c\u6d41\u57fa\u7ebf\u6a21\u578b\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u6216\u8d85\u8d8a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4ee3\u7406\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u5728\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "MetaAgent\u901a\u8fc7\u57fa\u4e8e\u5b66\u4ee5\u81f4\u7528\u7684\u539f\u5219\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u5b66\u4e60\u7684\u4ee3\u7406\u8303\u5f0f\uff0c\u4e0d\u65ad\u6539\u8fdb\u81ea\u8eab\u80fd\u529b\u5e76\u5b9e\u73b0\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u3002\u5728\u591a\u9879\u77e5\u8bc6\u53d1\u73b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e0e\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4ee3\u7406\u76f8\u5ab2\u7f8e\uff0c\u5c55\u793a\u4e86\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u5728\u7a33\u5065\u3001\u901a\u7528\u77e5\u8bc6\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00282", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u4ee3\u7406\u4eba\u5728\u4efb\u52a1\u751f\u6210\u4e2d\u7684\u5dee\u5f02\uff0c\u53d1\u73b0LLMs\u751f\u6210\u7684\u4efb\u52a1\u7f3a\u4e4f\u793e\u4ea4\u6027\u3001\u8eab\u4f53\u6027\uff0c\u4e14\u504f\u5411\u62bd\u8c61\u3002\u867d\u7136LLMs\u751f\u6210\u7684\u4efb\u52a1\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u548c\u65b0\u9896\uff0c\u4f46\u5b58\u5728\u4e0e\u7c7b\u4eba\u76ee\u6807\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u7684\u8131\u8282\u3002\u56e0\u6b64\uff0c\u8bbe\u8ba1\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7279\u70b9\u7684\u4ee3\u7406\u4eba\u65f6\u9700\u7ed3\u5408\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u3002", "motivation": "\u672c\u6587\u52a8\u673a\u5728\u4e8e\u7814\u7a76LLMs\u5728\u4efb\u52a1\u751f\u6210\u4e2d\u662f\u5426\u9075\u5faa\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u539f\u5219\uff0c\u4ee5\u53ca\u63a2\u8ba8\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u53d7\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u5f71\u54cd\u7684\u60c5\u51b5\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4eba\u7c7b\u4e0eLLM\u4ee3\u7406\u4eba\uff08GPT-4o\uff09\u7684\u4efb\u52a1\u751f\u6210\u5b9e\u9a8c\u6765\u63a2\u8ba8\u4eba\u7c7b\u8ba4\u77e5\u4e0eLLMs\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u751f\u4ea7\u7684\u4efb\u52a1\u7f3a\u4e4f\u793e\u4ea4\u6027\u3001\u8eab\u4f53\u6027\uff0c\u5e76\u4e14\u504f\u5411\u62bd\u8c61\uff0c\u4e0e\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u7684\u884c\u4e3a\u6a21\u5f0f\u5b58\u5728\u5dee\u5f02\u3002\u867d\u7136LLMs\u751f\u6210\u7684\u4efb\u52a1\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u548c\u65b0\u9896\uff0c\u4f46\u5b58\u5728\u4e0e\u7c7b\u4eba\u76ee\u6807\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u7684\u8131\u8282\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u53d7\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u4efb\u52a1\u504f\u5411\u62bd\u8c61\uff0c\u7f3a\u4e4f\u793e\u4ea4\u6027\u548c\u8eab\u4f53\u6027\u3002\u867d\u7136LLMs\u4ea7\u751f\u7684\u4efb\u52a1\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u548c\u65b0\u9896\uff0c\u4f46\u5b58\u5728\u5176\u8bed\u8a00\u80fd\u529b\u4e0e\u751f\u6210\u7c7b\u4eba\u76ee\u6807\u80fd\u529b\u4e4b\u95f4\u7684\u8131\u8282\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5728\u8bbe\u8ba1\u66f4\u7b26\u5408\u4eba\u7c7b\u7279\u70b9\u7684\u4ee3\u7406\u4eba\u65f6\uff0c\u7ed3\u5408\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u3002"}}
{"id": "2508.00323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00323", "abs": "https://arxiv.org/abs/2508.00323", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "comment": null, "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u4ee5\u7ed3\u6784\u5316\u56fe\u5f62\u63a8\u7406\u4e3a\u7126\u70b9\u7684\u8bc4\u4f30\u57fa\u51c6 ReasonBench\u3002\u901a\u8fc7 DiaCoT \u548c ReasonTune \u53cc\u91cd\u4f18\u5316\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86 VLMs \u7684\u6027\u80fd\u8fbe\u523033.5%\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\u8fdb\u884c\u8bc4\u4f30\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u7b80\u5355\u56fe\u5f62\uff0cVLMs \u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u548c\u62bd\u8c61\u95ee\u9898\u89e3\u51b3\u4e2d\u4ecd\u5b58\u5728\u660e\u663e\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86 ReasonBench \u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea\u73b0\u5b9e\u667a\u529b\u6d4b\u8bd5\u76841,613\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u4e86\u4f4d\u7f6e\u3001\u5c5e\u6027\u3001\u6570\u91cf\u548c\u591a\u5143\u4efb\u52a1\u7684\u63a8\u7406\u7ef4\u5ea6\u3002\u8bc4\u4f30\u4e8611\u79cd\u4e3b\u6d41\u7684 VLMs\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u663e\u8457\u5c40\u9650\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u91cd\u4f18\u5316\u7b56\u7565(DiaCoT \u548c ReasonTune)\uff0c\u901a\u8fc7\u8bad\u7ec3\u6539\u8fdb\u63a8\u7406\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86 VLM \u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7 ReasonBench \u63d0\u51fa\u7684\u53cc\u91cd\u4f18\u5316\u7b56\u7565(DiaCoT \u548c ReasonTune)\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u6027\u80fd\u8fbe\u523033.5%\u3002"}}
{"id": "2508.00324", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LRMs\u5b58\u5728\u7684\u5b89\u5168\u98ce\u9669\u95ee\u9898\uff0c\u63d0\u51fa\u4e86R1-Act\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u5b89\u5168\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2aLRM\u9aa8\u5e72\u548c\u89c4\u6a21\u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u8c03\u67e5\u4e86LRM\u5b89\u5168\u98ce\u9669\u7684\u6839\u672c\u539f\u56e0\uff0c\u53d1\u73b0\u6a21\u578b\u5df2\u7ecf\u62e5\u6709\u8db3\u591f\u7684\u5b89\u5168\u77e5\u8bc6\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u80fd\u6fc0\u6d3b\u3002", "method": "\u63d0\u51fa\u4e86R1-Act\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\u3002", "result": "R1-Act\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80dc\u8fc7\u5148\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5b58\u5728\u7684\u5b89\u5168\u98ce\u9669\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5R1-Act\uff0c\u53ef\u4ee5\u660e\u663e\u6539\u5584\u6a21\u578b\u7684\u5b89\u5168\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4ec5\u97001000\u4e2a\u8bad\u7ec3\u6837\u672c\u548c90\u5206\u949f\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u80fd\u591f\u5728\u591a\u4e2aLRM\u9aa8\u5e72\u548c\u89c4\u6a21\u4e0a\u53d6\u5f97\u5f3a\u5927\u7684\u6548\u679c\u3002"}}
{"id": "2508.00378", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI framework enhances reasoning in vision-language models by introducing visual verification, improving explanation quality. It follows a three-stage pipeline and is integrated without retraining. Evaluation on VCR benchmark shows performance improvement and human evaluations support the enhanced factual and helpful nature of explanations.", "motivation": "The motivation behind this paper is to address the issue of hallucinations in explanations provided by vision-language models due to the lack of grounding in visual content. The absence of an explicit verification mechanism during multi-step reasoning leads to linguistically fluent yet visually ungrounded explanations.", "method": "The paper proposes the CoRGI (Chain of Reasoning with Grounded Insights) modular framework, which consists of a three-stage pipeline: generating a textual reasoning chain, extracting visual evidence for reasoning steps, and synthesizing textual rationale with visual evidence for a grounded answer. The framework is integrated with existing VLMs without requiring end-to-end retraining.", "result": "Evaluation on the VCR benchmark shows that CoRGI improves reasoning performance on representative VLM backbones. Ablation studies confirm the contribution of the verification module in enhancing explanations. Human evaluations indicate that CoRGI leads to more factual and helpful explanations, highlighting the importance of grounding reasoning steps in visual evidence.", "conclusion": "CoRGI framework improves reasoning performance in vision-language models by introducing visual verification into the reasoning process. It enhances the factual and helpful nature of explanations provided by the models."}}
{"id": "2508.00401", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00401", "abs": "https://arxiv.org/abs/2508.00401", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "comment": null, "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u4e3b\u52a8\u63a8\u65ad\u4e2d\u5b9e\u73b0\u5fc3\u7075\u7406\u8bba\uff08ToM\uff09\u7684\u65b0\u9896\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u57fa\u4e8e\u63a8\u7406\u6811\u7684\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4efb\u52a1\u8bc4\u4f30\u4e86\u65b9\u6cd5\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u660eToM\u88c5\u5907\u7684\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u5408\u4f5c\uff0c\u907f\u514d\u78b0\u649e\u548c\u51cf\u5c11\u5197\u4f59\u52aa\u529b\uff0c\u63a8\u52a8\u4e86\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u8fdb\u5c55\u3002", "motivation": "\u8be5\u8bba\u6587\u7684\u52a8\u673a\u5728\u4e8e\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7ToM\u5b9e\u73b0\u4ee3\u7406\u4e4b\u95f4\u7684\u7406\u89e3\u548c\u5408\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u5408\u4f5c\u6548\u7387\u548c\u907f\u514d\u51b2\u7a81\u3002\u540c\u65f6\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u3002", "method": "\u5728\u4e3b\u52a8\u63a8\u65ad\u4e2d\u5b9e\u73b0\u5fc3\u7075\u7406\u8bba\uff08ToM\uff09\uff0c\u6269\u5c55\u57fa\u4e8e\u63a8\u7406\u6811\u7684\u89c4\u5212\u7b97\u6cd5\u4ee5\u7cfb\u7edf\u5730\u63a2\u7d22\u8054\u5408\u7b56\u7565\u7a7a\u95f4\uff0c\u901a\u8fc7\u78b0\u649e\u907f\u514d\u548c\u89c5\u98df\u4efb\u52a1\u6a21\u62df\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u78b0\u649e\u907f\u514d\u548c\u89c5\u98df\u4efb\u52a1\u6a21\u62df\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aToM\u88c5\u5907\u7684\u4ee3\u7406\u80fd\u591f\u6bd4\u975eToM\u5bf9\u7167\u7ec4\u66f4\u597d\u5730\u5408\u4f5c\uff0c\u907f\u514d\u78b0\u649e\u548c\u51cf\u5c11\u5197\u4f59\u52aa\u529b\u3002ToM\u4ee3\u7406\u901a\u8fc7\u63a8\u65ad\u4ed6\u4eba\u7684\u4fe1\u5ff5\u4ec5\u4ece\u53ef\u89c2\u5bdf\u884c\u4e3a\u4e2d\u5b9e\u73b0\u5408\u4f5c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e3b\u52a8\u63a8\u65ad\u4e2d\u5b9e\u73b0\u5fc3\u7075\u7406\u8bba\uff08Theory of Mind\uff0cToM\uff09\u3002\u901a\u8fc7ToM\u7684\u5b9e\u73b0\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u7406\u89e3\u5176\u4ed6\u4e2a\u4f53\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684\u77e5\u8bc6\u548c\u76ee\u6807\uff0c\u4ece\u800c\u4f7f\u4ee3\u7406\u5728\u89c4\u5212\u81ea\u5df1\u7684\u884c\u52a8\u65f6\u80fd\u591f\u63a8\u7406\u5176\u4ed6\u4eba\u7684\u4fe1\u5ff5\u3002\u4e0e\u5148\u524d\u7684\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u7684\u4e3b\u52a8\u63a8\u65ad\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u65e2\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u5171\u4eab\u751f\u6210\u6a21\u578b\uff0c\u4e5f\u4e0d\u9700\u8981\u663e\u5f0f\u901a\u4fe1\uff0c\u540c\u65f6\u5177\u6709\u901a\u7528\u6027\u3002\u5728\u6211\u4eec\u7684\u6846\u67b6\u4e2d\uff0cToM\u88c5\u5907\u7684\u4ee3\u7406\u7ef4\u62a4\u5176\u81ea\u5df1\u548c\u5176\u4ed6\u4eba\u7684\u4fe1\u5ff5\u548c\u76ee\u6807\u7684\u4e0d\u540c\u8868\u793a\u3002\u6211\u4eec\u5c06\u590d\u6742\u7684\u57fa\u4e8e\u63a8\u7406\u6811\u7684\u89c4\u5212\u7b97\u6cd5\u6269\u5c55\u5230\u901a\u8fc7\u9012\u5f52\u63a8\u7406\u7cfb\u7edf\u5730\u63a2\u7d22\u8054\u5408\u7b56\u7565\u7a7a\u95f4\u3002\u901a\u8fc7\u78b0\u649e\u907f\u514d\u548c\u89c5\u98df\u4efb\u52a1\u6a21\u62df\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u975eToM\u5bf9\u7167\u7ec4\u76f8\u6bd4\uff0cToM\u88c5\u5907\u7684\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u5408\u4f5c\uff0c\u907f\u514d\u78b0\u649e\u5e76\u51cf\u5c11\u5197\u4f59\u52aa\u529b\u3002\u5173\u952e\u662f\uff0cToM\u4ee3\u7406\u901a\u8fc7\u4ec5\u4ece\u53ef\u89c2\u5bdf\u884c\u4e3a\u63a8\u65ad\u4ed6\u4eba\u7684\u4fe1\u5ff5\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u8fd9\u9879\u5de5\u4f5c\u63a8\u52a8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\uff0c\u5e76\u4e3aToM\u63d0\u4f9b\u4e86\u8ba1\u7b97\u4e0a\u7684\u6d1e\u89c1\u3002"}}
{"id": "2508.00414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "Cognitive Kernel-Pro is an open-source and free agent framework that addresses the limitations of current closed-source systems. It focuses on high-quality training data curation, novel test-time reflection strategies, and agent robustness. The framework outperforms previous systems and sets a new performance standard for accessible AI agents.", "motivation": "The motivation behind this work is to address the limitations of current agent systems that are closed-source or rely heavily on paid APIs and proprietary tools, thus limiting accessibility and reproducibility for the research community. By introducing Cognitive Kernel-Pro as an open-source and free framework, the paper aims to make advanced AI agent development more accessible and democratized.", "method": "The paper systematically investigates the curation of high-quality training data for Agent Foundation Models within Cognitive Kernel-Pro. It focuses on constructing queries, trajectories, and verifiable answers across key domains such as web, file, code, and general reasoning. The paper also explores novel strategies for agent test-time reflection and voting to enhance agent robustness and performance.", "result": "Cognitive Kernel-Pro achieves state-of-the-art results among open-source and free agents, surpassing previous leading systems like WebDancer and WebSailor. The 8B-parameter open-source model of Cognitive Kernel-Pro establishes a new performance standard for high-capability AI agents.", "conclusion": "Cognitive Kernel-Pro is an open-source and free multi-module agent framework that aims to democratize the development and evaluation of advanced AI agents. It outperforms previous leading systems and sets a new performance standard for accessible AI agents."}}
{"id": "2508.00459", "categories": ["cs.AI", "68T07, 68T20", "I.2.6; I.2.7; I.2.3"], "pdf": "https://arxiv.org/pdf/2508.00459", "abs": "https://arxiv.org/abs/2508.00459", "authors": ["Andrea Asperti", "Alberto Naibo", "Claudio Sacerdoti Coen"], "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u5728\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\u56f0\u96be\u7684\u95ee\u9898\u3002\u6587\u7ae0\u5173\u6ce8\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6570\u5b66\u8ba4\u77e5\u4ea4\u53c9\u9886\u57df\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u547c\u5401\u7814\u7a76\u5ef6\u4f38\u5f53\u524d\u7684\u9650\u5236\u3002", "motivation": "\u672c\u6587\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u5728\u6b63\u5f0f\u6570\u5b66\u65b9\u9762\u9762\u4e34\u7684\u56f0\u96be\u5c55\u5f00\u8ba8\u8bba\uff0c\u65e8\u5728\u63ed\u793a\u5f53\u524d\u5728\u673a\u5668\u5b66\u4e60\u548c\u6570\u5b66\u8ba4\u77e5\u4ea4\u53c9\u9886\u57df\u4e2d\u7684\u6311\u6218\u548c\u95ee\u9898\u3002", "method": "\u6587\u7ae0\u91c7\u7528\u7efc\u8ff0\u7684\u65b9\u5f0f\uff0c\u5173\u6ce8\u6700\u65b0\u6a21\u578b\u548c\u57fa\u51c6\uff0c\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u548c\u6570\u5b66\u8ba4\u77e5\u4ea4\u53c9\u9886\u57df\u7684\u5173\u952e\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6df1\u5165\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u548c\u56f0\u96be\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u5173\u952e\u95ee\u9898\u5e76\u5bf9\u8be5\u9886\u57df\u7684\u6700\u65b0\u53d1\u5c55\u548c\u672a\u6765\u65b9\u5411\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\u76f8\u5bf9\u56f0\u96be\u7684\u95ee\u9898\u3002\u6587\u7ae0\u7740\u91cd\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u548c\u6570\u5b66\u8ba4\u77e5\u4ea4\u53c9\u9886\u57df\u7684\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u547c\u5401\u7814\u7a76\u5ef6\u4f38\u5f53\u524d\u7684\u9650\u5236\u3002"}}
{"id": "2508.00500", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00500", "abs": "https://arxiv.org/abs/2508.00500", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "comment": null, "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "AI": {"tldr": "Pro2Guard is a proactive safety framework for large language model agents that anticipates risks and triggers interventions based on user-defined thresholds, ensuring safety in different domains. It achieves high safety enforcement rates in embodied agent tasks and autonomous driving scenarios.", "motivation": "Existing rule-based enforcement systems lack foresight and struggle with long-horizon dependencies and distribution shifts, leading to reactive responses to unsafe behavior. Pro2Guard aims to proactively enforce safety by anticipating risks and intervening before violations occur.", "method": "The paper proposes Pro2Guard, which abstracts agent behaviors into symbolic states, learns a Discrete-Time Markov Chain (DTMC) from execution traces, estimates the probability of reaching unsafe states at runtime, and triggers interventions based on user-defined thresholds. It incorporates semantic validity checks and PAC bounds to ensure statistical reliability and approximate the underlying ground-truth model.", "result": "Pro2Guard extensively evaluated across embodied household agents and autonomous vehicles domains. In embodied agent tasks, it enforces safety early in up to 93.6% of unsafe tasks with low thresholds. In autonomous driving scenarios, Pro2Guard achieves 100% prediction of traffic law violations and collisions, anticipating risks up to 38.66 seconds ahead.", "conclusion": "Pro2Guard is a proactive runtime enforcement framework that addresses the limitations of existing rule-based systems by anticipating future risks and triggering interventions to ensure safety in large language model agents across different domains."}}
{"id": "2508.00576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00576", "abs": "https://arxiv.org/abs/2508.00576", "authors": ["Zhanliang Wang", "Kai Wang"], "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "comment": null, "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86MultiSHAP\u6846\u67b6\uff0c\u901a\u8fc7Shapley Interaction Index\u5bf9\u591a\u6a21\u6001AI\u6a21\u578b\u7684\u9884\u6d4b\u8fdb\u884c\u89e3\u91ca\u3002\u8be5\u6846\u67b6\u80fd\u63d0\u4f9b\u5b9e\u4f8b\u7ea7\u548c\u6570\u636e\u96c6\u7ea7\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e2a\u4f53\u6837\u672c\u548c\u6574\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6a21\u6001\u6548\u679c\u548c\u4fe1\u606f\u6574\u5408\u65b9\u5f0f\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u51fa\u8272\u8868\u73b0\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\uff0c\u5982\u4f55\u89e3\u91ca\u591a\u6a21\u6001AI\u6a21\u578b\u4e2d\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u73b0\u6709\u7684\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u4e86\u5bf9\u8de8\u6a21\u6001\u5173\u7cfb\u7684\u7c97\u7565\u6d1e\u5bdf\uff0c\u4f46\u65e0\u6cd5\u7cbe\u786e\u91cf\u5316\u6a21\u6001\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u5e76\u4e14\u4ec5\u9650\u4e8e\u5177\u6709\u53ef\u8bbf\u95ee\u5185\u90e8\u6743\u91cd\u7684\u5f00\u6e90\u6a21\u578b\u3002", "method": "\u5f15\u5165\u4e86MultiSHAP\u6846\u67b6\uff0c\u5229\u7528Shapley Interaction Index\u5bf9\u591a\u6a21\u6001\u9884\u6d4b\u8fdb\u884c\u89e3\u91ca\uff0c\u5b9e\u73b0\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u7684\u5f52\u56e0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMultiSHAP\u80fd\u51c6\u786e\u6355\u83b7\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\uff0c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6548\u7528\uff0c\u53ef\u89e3\u91ca\u590d\u6742\u591a\u6a21\u6001AI\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMultiSHAP\u7684\u6a21\u578b\u65e0\u5173\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u5229\u7528Shapley Interaction Index\u6765\u89e3\u91ca\u591a\u6a21\u6001\u9884\u6d4b\uff0c\u5bf9\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u8fdb\u884c\u5f52\u56e0\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u4f8b\u7ea7\u89e3\u91ca\u548c\u6570\u636e\u96c6\u7ea7\u89e3\u91ca\uff0c\u80fd\u51c6\u786e\u63ed\u793a\u6a21\u578b\u5728\u4e2a\u4f53\u6837\u672c\u548c\u6574\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6a21\u6001\u6548\u5e94\u548c\u4fe1\u606f\u96c6\u6210\u6a21\u5f0f\u3002\u5b9e\u9a8c\u8bc1\u660eMultiSHAP\u80fd\u5fe0\u5b9e\u5730\u6355\u83b7\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\uff0c\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u5176\u5b9e\u9645\u5b9e\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u89e3\u91ca\u590d\u6742\u591a\u6a21\u6001AI\u6a21\u578b\u3002"}}
{"id": "2508.00581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00581", "abs": "https://arxiv.org/abs/2508.00581", "authors": ["Ruiqing Ding", "Qianfang Sun", "Yongkang Leng", "Hui Yin", "Xiaojian Li"], "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation", "comment": "16 pages, 10 figures", "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5168\u9762\u7684\u9884\u4f1a\u8bca\u95ee\u5377\u3002\u8be5\u65b9\u6cd5\u5728\u4fe1\u606f\u8986\u76d6\u7387\u3001\u8bca\u65ad\u76f8\u5173\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u751f\u6210\u65f6\u95f4\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5728\u5b9e\u9645\u4e2d\u6709\u63d0\u5347\u60a3\u8005\u4fe1\u606f\u6536\u96c6\u7684\u6f5c\u529b\u3002", "motivation": "\u9884\u4f1a\u8bca\u662f\u6709\u6548\u533b\u7597\u670d\u52a1\u4ea4\u4ed8\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u4ece\u590d\u6742\u5e9e\u5927\u7684\u7535\u5b50\u75c5\u5386\u4e2d\u751f\u6210\u5168\u9762\u7684\u9884\u4f1a\u8bca\u95ee\u5377\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u76f4\u63a5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u3001\u903b\u8f91\u987a\u5e8f\u548c\u75be\u75c5\u7ea7\u7efc\u5408\u7b49\u65b9\u9762\u9762\u4e34\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8fd9\u4e00\u521b\u65b0\u6027\u7684\u591a\u9636\u6bb5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4ece\u7535\u5b50\u75c5\u5386\u4e2d\u63d0\u53d6\u539f\u5b50\u65ad\u8a00\uff1b\u7b2c\u4e8c\u9636\u6bb5\u6784\u5efa\u4e2a\u6027\u5316\u56e0\u679c\u7f51\u7edc\u5e76\u901a\u8fc7\u5bf9\u7535\u5b50\u75c5\u5386\u8bed\u6599\u5e93\u4e2d\u7684\u4ee3\u8868\u6027\u7f51\u7edc\u8fdb\u884c\u805a\u7c7b\uff0c\u7efc\u5408\u75be\u75c5\u77e5\u8bc6\uff1b\u7b2c\u4e09\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u6784\u5316\u8868\u793a\u751f\u6210\u5b9a\u5236\u7684\u4e2a\u4eba\u548c\u6807\u51c6\u5316\u7684\u75be\u75c5\u7279\u5b9a\u95ee\u5377\u3002", "result": "\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u7535\u5b50\u75c5\u5386\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u7ecf\u4e34\u5e8a\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u63d0\u5347\u60a3\u8005\u4fe1\u606f\u6536\u96c6\u7684\u5b9e\u9645\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7535\u5b50\u75c5\u5386\u4e2d\u751f\u6210\u5168\u9762\u7684\u9884\u4f1a\u8bca\u95ee\u5377\u3002\u901a\u8fc7\u6784\u5efa\u4e2a\u6027\u5316\u56e0\u679c\u7f51\u7edc\u548c\u7efc\u5408\u75be\u75c5\u77e5\u8bc6\uff0c\u751f\u6210\u5b9a\u5236\u7684\u4e2a\u4eba\u548c\u6807\u51c6\u5316\u7684\u75be\u75c5\u7279\u5b9a\u95ee\u5377\u3002\u8be5\u6846\u67b6\u5728\u63d0\u9ad8\u4fe1\u606f\u8986\u76d6\u7387\u3001\u8bca\u65ad\u76f8\u5173\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u751f\u6210\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5177\u6709\u63d0\u5347\u60a3\u8005\u4fe1\u606f\u6536\u96c6\u7684\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2508.00632", "categories": ["cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00632", "abs": "https://arxiv.org/abs/2508.00632", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "comment": null, "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86AVR-Eval\u76f8\u5bf9\u8d28\u91cf\u5ea6\u91cf\u65b9\u6cd5\u548cAVR-Agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u751f\u6210\u591a\u5a92\u4f53\u5185\u5bb9\u3002AVR-Agent\u751f\u6210\u7684\u5185\u5bb9\u5728\u6bd4\u8d5b\u4e2d\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u5229\u7528\u81ea\u5b9a\u4e49\u8d44\u6e90\u548cAVR\u53cd\u9988\u65b9\u9762\u9047\u5230\u56f0\u96be\u3002\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u5185\u5bb9\u521b\u4f5c\u65b9\u6cd5\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\u3002", "motivation": "\u867d\u7136AI\u4f18\u79c0\u4e8e\u751f\u6210\u6587\u672c\u3001\u97f3\u9891\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u5185\u5bb9\uff0c\u4f46\u751f\u6210\u4e92\u52a8\u6027\u97f3\u9891-\u89c6\u89c9\u5185\u5bb9\uff08\u5982\u89c6\u9891\u6e38\u620f\uff09\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5f53\u524d\u7684LLM\u53ef\u4ee5\u751f\u6210JavaScript\u6e38\u620f\u548c\u52a8\u753b\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u9762\u5bf9\u901a\u5e38\u9700\u8981\u56e2\u961f\u6570\u6708\u5236\u4f5c\u7684\u590d\u6742\u5185\u5bb9\uff08\u591a\u955c\u5934\u3001\u591a\u667a\u80fd\u4f53\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u65b0\u7684\u5ea6\u91cf\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86AVR-Eval\u76f8\u5bf9\u8d28\u91cf\u5ea6\u91cf\u65b9\u6cd5\u548cAVR-Agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002AVR-Eval\u901a\u8fc7\u6bd4\u8f83\u591a\u5a92\u4f53\u5185\u5bb9\u7684AVR\uff0c\u7ed3\u5408\u6587\u672c\u6a21\u578b\u8bc4\u4f30\uff0c\u8bc6\u522b\u51fa\u597d\u574f\u5185\u5bb9\u3002AVR-Agent\u4ece\u591a\u5a92\u4f53\u8d44\u4ea7\u5e93\u4e2d\u9009\u62e9\u76f8\u5173\u8d44\u4ea7\uff0c\u751f\u6210\u591a\u4e2a\u521d\u59cb\u4ee3\u7801\uff0c\u5229\u7528AVR-Eval\u8bc6\u522b\u6700\u4f73\u7248\u672c\uff0c\u5e76\u901a\u8fc7AVR\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u53cd\u9988\u8fed\u4ee3\u6539\u8fdb\u4ee3\u7801\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAVR-Agent\u751f\u6210\u7684\u5185\u5bb9\u80dc\u7387\u660e\u663e\u8f83\u9ad8\u3002\u7136\u800c\uff0c\u6a21\u578b\u5728\u5229\u7528\u81ea\u5b9a\u4e49\u8d44\u6e90\u548cAVR\u53cd\u9988\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u672a\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u80dc\u7387\u3002\u8fd9\u8868\u660e\u5f53\u524d\u7f16\u7801\u6a21\u578b\u672a\u80fd\u6709\u6548\u5229\u7528\u9ad8\u8d28\u91cf\u8d44\u6e90\u548c\u97f3\u89c6\u9891\u53cd\u9988\uff0c\u7a81\u663e\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u5185\u5bb9\u521b\u4f5c\u65b9\u6cd5\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86AVR-Eval\u76f8\u5bf9\u8d28\u91cf\u5ea6\u91cf\u65b9\u6cd5\u548cAVR-Agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u5a92\u4f53\u5185\u5bb9\u8d28\u91cf\u548c\u751f\u6210JavaScript\u4ee3\u7801\u3002\u5b9e\u9a8c\u8bc1\u660eAVR-Agent\u751f\u6210\u7684\u5185\u5bb9\u5728\u6bd4\u8d5b\u4e2d\u83b7\u80dc\u7387\u663e\u8457\u8f83\u9ad8\uff0c\u4f46\u5728\u5229\u7528\u81ea\u5b9a\u4e49\u8d44\u6e90\u548cAVR\u53cd\u9988\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u5185\u5bb9\u521b\u4f5c\u65b9\u6cd5\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\u3002"}}
{"id": "2508.00658", "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00658", "abs": "https://arxiv.org/abs/2508.00658", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "comment": "First draft", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u9891\u5e26\u53d8\u65f6\u6ede\u683c\u5170\u56e0\u679c\u6027\uff08MB-VLGC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u9891\u7387\u76f8\u5173\u7684\u56e0\u679c\u5ef6\u8fdf\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u5404\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u53d8\u65f6\u6ede\u683c\u5170\u56e0\u679c\u6027\u65b9\u6cd5\u65e0\u6cd5\u8003\u8651\u56e0\u679c\u4ea4\u4e92\u4e0d\u4ec5\u5728\u65f6\u95f4\u5ef6\u8fdf\u4e0a\u53d8\u5316\uff0c\u8fd8\u5728\u9891\u7387\u8303\u56f4\u5185\u5b58\u5728\u5dee\u5f02\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u5177\u5e7f\u6cdb\u9002\u7528\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5bf9\u591a\u9891\u5e26\u53d8\u65f6\u6ede\u683c\u5170\u56e0\u679c\u6027\uff08MB-VLGC\uff09\u8fdb\u884c\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u65ad\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u660e\u5176\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9891\u5e26\u53d8\u65f6\u6ede\u683c\u5170\u56e0\u679c\u6027\uff08MB-VLGC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u9891\u7387\u76f8\u5173\u7684\u56e0\u679c\u5ef6\u8fdf\uff0c\u5f25\u8865\u4e86\u53d8\u65f6\u6ede\u683c\u5170\u56e0\u679c\u6027\uff08VLGC\uff09\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u7c7b\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002"}}
{"id": "2508.00665", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00665", "abs": "https://arxiv.org/abs/2508.00665", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "comment": null, "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u878d\u5408\u4e86\u4f20\u7edf\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u3001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u548c\u7528\u6237\u4e2a\u6027\u5316\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4ee5\u751f\u6210\u591a\u6a21\u6001\u3001\u4e2a\u6027\u5316\u7684\u89e3\u91ca\uff0c\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002\u91cd\u65b0\u5b9a\u4e49\u53ef\u89e3\u91ca\u6027\u4e3a\u52a8\u6001\u6c9f\u901a\u8fc7\u7a0b\uff0c\u9488\u5bf9\u7528\u6237\u89d2\u8272\u548c\u5b66\u4e60\u76ee\u6807\u5b9a\u5236\u89e3\u91ca\u3002\u6587\u7ae0\u8ba8\u8bba\u4e86\u6559\u80b2\u9886\u57df\u4e2d\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u65e8\u5728\u63a8\u52a8\u66f4\u900f\u660e\u3001\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u3002", "motivation": "\u8bb8\u591a\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u6b63\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\u4f53\u9a8c\u8c03\u6574\u6539\u53d8\u6559\u80b2\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u8bb8\u591a\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u65e0\u6cd5\u8be6\u7ec6\u89e3\u91ca\u51b3\u7b56\u8fc7\u7a0b\u3002\u76ee\u524d\u5927\u591a\u6570\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4fa7\u91cd\u4e8e\u6280\u672f\u8f93\u51fa\uff0c\u800c\u5ffd\u89c6\u4e86\u7528\u6237\u89d2\u8272\u548c\u7406\u89e3\u80fd\u529b\u3002\u57fa\u4e8e\u8fd9\u4e00\u80cc\u666f\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u878d\u5408\u4f20\u7edf\u548c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u4f9b\u9488\u5bf9\u7528\u6237\u9700\u6c42\u7684\u591a\u6a21\u6001\u3001\u4e2a\u6027\u5316\u89e3\u91ca\uff0c\u91cd\u65b0\u5b9a\u4e49\u53ef\u89e3\u91ca\u6027\u5e76\u5f3a\u8c03\u4e0e\u7528\u6237\u89d2\u8272\u548c\u5b66\u4e60\u76ee\u6807\u76f8\u5173\u7684\u52a8\u6001\u6c9f\u901a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u878d\u5408\u4e86\u4f20\u7edf\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u3001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u548c\u7528\u6237\u4e2a\u6027\u5316\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4ee5\u751f\u6210\u591a\u6a21\u6001\u3001\u4e2a\u6027\u5316\u7684\u89e3\u91ca\u3002\u91cd\u65b0\u5b9a\u4e49\u4e86\u53ef\u89e3\u91ca\u6027\u7684\u6982\u5ff5\uff0c\u5f3a\u8c03\u4e86\u52a8\u6001\u6c9f\u901a\u8fc7\u7a0b\u5e76\u9488\u5bf9\u7528\u6237\u89d2\u8272\u548c\u5b66\u4e60\u76ee\u6807\u8fdb\u884c\u5b9a\u5236\u3002\u6587\u7ae0\u8fd8\u6982\u8ff0\u4e86\u6846\u67b6\u8bbe\u8ba1\uff0c\u8ba8\u8bba\u4e86\u6559\u80b2\u9886\u57df\u4e2d\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u51c6\u786e\u6027\u3001\u516c\u5e73\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u65b0\u7684\u6df7\u5408\u6846\u67b6\uff0c\u672c\u6587\u4e3a\u6559\u80b2\u9886\u57df\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u63d0\u4f9b\u4e86\u521b\u65b0\u601d\u8def\u548c\u89e3\u51b3\u65b9\u6848\u3002\u4e0d\u4ec5\u91cd\u65b0\u5b9a\u4e49\u4e86\u53ef\u89e3\u91ca\u6027\u7684\u6982\u5ff5\uff0c\u8fd8\u5f3a\u8c03\u4e86\u89e3\u91ca\u8fc7\u7a0b\u5e94\u5f53\u662f\u52a8\u6001\u7684\u6c9f\u901a\u8fc7\u7a0b\uff0c\u5e76\u5e94\u6839\u636e\u7528\u6237\u7684\u89d2\u8272\u548c\u5b66\u4e60\u76ee\u6807\u8fdb\u884c\u4e2a\u6027\u5316\u5b9a\u5236\u3002\u6587\u7ae0\u8fd8\u6307\u51fa\u4e86\u6559\u80b2\u9886\u57df\u4e2d\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5b58\u5728\u7684\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u51c6\u786e\u6027\u3001\u516c\u5e73\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u878d\u5408\u4e86\u4f20\u7edf\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u3001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u548c\u7528\u6237\u4e2a\u6027\u5316\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4ee5\u751f\u6210\u591a\u6a21\u6001\u3001\u4e2a\u6027\u5316\u7684\u89e3\u91ca\uff0c\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002\u91cd\u65b0\u5b9a\u4e49\u53ef\u89e3\u91ca\u6027\u4e3a\u9488\u5bf9\u7528\u6237\u89d2\u8272\u548c\u5b66\u4e60\u76ee\u6807\u91cf\u8eab\u5b9a\u5236\u7684\u52a8\u6001\u6c9f\u901a\u8fc7\u7a0b\u3002\u6587\u7ae0\u6982\u8ff0\u4e86\u6846\u67b6\u7684\u8bbe\u8ba1\u3001\u6559\u80b2\u9886\u57df\u4e2d\u5173\u952e\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u9650\u5236\uff0c\u4ee5\u53ca\u5728\u51c6\u786e\u6027\u3001\u516c\u5e73\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u7684\u7814\u7a76\u65b9\u5411\u3002\u65e8\u5728\u671d\u7740\u589e\u5f3a\u900f\u660e\u5ea6\u3001\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u4f53\u9a8c\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u3002"}}
{"id": "2508.00674", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00674", "abs": "https://arxiv.org/abs/2508.00674", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "comment": null, "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "AI": {"tldr": "\u5728\u672c\u8bba\u6587\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u5206\u5272\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u91ca\u5c42\uff0c\u901a\u8fc7\u4e00\u4e2a\u591a\u6837\u5316\u7684\u89e3\u91ca\u65b9\u6cd5\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u7cfb\u7edf\u6765\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e2dAI\u63a8\u8350\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u9996\u6b21\u5728\u5355\u4e00\u6d41\u7a0b\u4e2d\u5171\u540c\u8c03\u6574\u89e3\u91ca\u65b9\u5f0f\u548c\u7c92\u5ea6\uff0c\u5e76\u8ba1\u5212\u901a\u8fc7\u516c\u5f00\u8bd5\u70b9\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u5176\u5f71\u54cd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684AI\u63a8\u8350\u7f3a\u4e4f\u7528\u6237\u7406\u89e3\u63a8\u8350\u80cc\u540e\u539f\u56e0\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4f7f\u89e3\u91ca\u4e0e\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u4fdd\u6301\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89c6\u5316\u89e3\u91ca\u7cfb\u7edf\uff0c\u5305\u62ec\u8be6\u7ec6\u7248\u672c\u548c\u7b80\u5316\u7248\u672c\uff0c\u901a\u8fc7\u516c\u5f00\u8bd5\u70b9\u6d4b\u8bd5\u7cfb\u7edf\u5bf9\u51b3\u7b56\u5236\u5b9a\u548c\u4fe1\u4efb\u5efa\u7acb\u7684\u5f71\u54cd\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e0e\u89e3\u91ca\u7cfb\u7edf\u65e8\u5728\u63d0\u9ad8\u51b3\u7b56\u5236\u5b9a\u548c\u4fe1\u4efb\u5efa\u7acb\u7684\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u8bd5\u70b9\u6d4b\u8bd5\u7cfb\u7edf\u6765\u9a8c\u8bc1\u5176\u5f71\u54cd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7528\u6237\u5206\u5272\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u91ca\u5c42\uff0c\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u5177\u6709\u591a\u79cd\u89e3\u91ca\u65b9\u6cd5\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e2dAI\u63a8\u8350\u63a8\u8350\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002\u8be5\u6846\u67b6\u662f\u9996\u4e2a\u5728\u5355\u4e00\u6d41\u7a0b\u4e2d\u5171\u540c\u8c03\u6574\u89e3\u91ca\u65b9\u5f0f\uff08\u53ef\u89c6\u5316 vs. \u6570\u5b57\u5316\uff09\u548c\u7c92\u5ea6\uff08\u4e13\u5bb6 vs. \u666e\u901a\u7528\u6237\uff09\u7684\u7cfb\u7edf\u3002"}}
{"id": "2508.00784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00784", "abs": "https://arxiv.org/abs/2508.00784", "authors": ["Tom Or", "Omri Azencot"], "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics", "comment": null, "summary": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ebf\u6027\u5206\u7c7b\u5668\u8bad\u7ec3\u7279\u5f81\u5728\u4e0d\u540c\u6a21\u6001\u4e0b\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5feb\u901f\u8bad\u7ec3\uff0c\u5728\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u4e5f\u6709\u6548\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u97f3\u9891\u548c\u56fe\u50cf\u9886\u57df\u7684\u4f2a\u9020\u68c0\u6d4b\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u6216\u5339\u914d\u4e86\u5f3a\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u9274\u4e8e\u6076\u610f\u7528\u6237\u5229\u7528\u5408\u6210\u5a92\u4f53\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u548c\u6563\u5e03\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u7684\u60c5\u51b5\u65e5\u76ca\u4e25\u91cd\uff0c\u5bf9\u4e8e\u7a33\u5065\u548c\u7a33\u5b9a\u7684\u5047\u68c0\u6d4b\u5668\u7684\u9700\u6c42\u8feb\u5728\u7709\u776b\u3002\u73b0\u6709\u5927\u591a\u6570\u5de5\u4f5c\u8bad\u7ec3\u5206\u7c7b\u5668\u533a\u5206\u771f\u5047\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u5de5\u5177\u901a\u5e38\u4ec5\u5728\u76f8\u540c\u751f\u6210\u5668\u5bb6\u65cf\u548c\u6570\u636e\u6a21\u5f0f\u5185\u6cdb\u5316\uff0c\u5bf9\u5176\u4ed6\u751f\u6210\u7c7b\u522b\u548c\u6570\u636e\u9886\u57df\u7684\u7ed3\u679c\u8f83\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u901a\u7528\u5206\u7c7b\u5668\u3002", "method": "\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u6765\u68c0\u6d4b\u751f\u6210\u5185\u5bb9\uff0c\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u6f5c\u5728\u7f16\u7801\u81ea\u7136\u5730\u533a\u5206\u771f\u5047\u4fe1\u606f\u3002\u901a\u8fc7\u8fd9\u4e00\u53d1\u73b0\uff0c\u5c55\u793a\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u8bad\u7ec3\u53ef\u4ee5\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u3001\u5feb\u901f\u548c\u5728\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u4e5f\u6709\u6548\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u97f3\u9891\u548c\u56fe\u50cf\u9886\u57df\u7684\u4f2a\u9020\u68c0\u6d4b\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u6216\u5339\u914d\u4e86\u5f3a\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u6765\u68c0\u6d4b\u751f\u6210\u5185\u5bb9\uff0c\u901a\u8fc7\u7ebf\u6027\u5206\u7c7b\u5668\u8bad\u7ec3\u8fd9\u4e9b\u7279\u5f81\u53ef\u4ee5\u5728\u591a\u79cd\u6a21\u6001\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5728\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u3002\u4e3b\u8981\u96c6\u4e2d\u5728\u97f3\u9891\u548c\u56fe\u50cf\u9886\u57df\u7684\u4f2a\u9020\u68c0\u6d4b\uff0c\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u6216\u5339\u914d\u5f3a\u57fa\u51c6\u65b9\u6cd5\u3002"}}
