<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 该论文提出了统一的后续解释方法、通用框架以平衡有效性和简洁性，并改进了评估协议以提高可重现性和影响力，强调解释的最终用户意义。


<details>
  <summary>Details</summary>
Motivation: 作者指出在知识图谱完成中，后续解释的形式化和一致的评估不足，阻碍了其可重现性和跨研究比较，因此需要统一方法来解决这一问题。

Method: 该论文提出了一个通用框架，通过多目标优化来表征后续解释，平衡解释的有效性和简洁性。还提出了改进的评估协议，并采用像Mean Reciprocal Rank和Hits@k等流行指标来支持这一观点。

Result: 该研究通过统一后续解释算法和解释产生的方法，提出改进的评估协议，并强调解释能力对最终用户有意义的重要性。旨在提高KGC可解释性研究的可重复性和影响力。

Conclusion: 该论文旨在提出统一的后续解释方法，以促进知识图谱完成中可解释性方法的可重现性和跨研究比较。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [2] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文研究了如何应用数据准备对于AI的原则到用于训练基础模型的大规模科学数据集中。通过分析不同领域的工作流程和预处理模式，提出了数据准备级别和数据处理阶段构成的两维框架，并描述了在转换科学数据以进行可扩展AI训练中的关键挑战。引入的概念成熟度矩阵描述了科学数据准备情况，并指导基础设施发展朝着为可扩展和可重现的科学AI提供跨领域支持的标准化方向。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决科学数据准备与AI训练中的关键挑战，为实现可扩展和可重现的科学AI提供支持。通过分析不同领域的工作流程和数据处理模式，揭示了共同的模式和特定的约束，为构建跨领域标准化基础设施提供指导。

Method: 通过分析代表性领域的工作流程和预处理模式，引入了两个维度的数据准备框架，突出了转换器式生成模型的重要性，强调了在高性能计算环境中的关键挑战。利用这些维度构建了概念成熟度矩阵，以指导基础设施的发展。

Result: 提出了两维度的准备框架，揭示了科学数据在可扩展AI训练中的关键挑战，并为基础设施发展提供了指导。同时，构建了概念成熟度矩阵，描述了科学数据准备状态，并引领基础设施朝着支持可扩展和可重现的科学AI的方向发展。

Conclusion: 本文研究了数据准备对于用于训练基础模型的大规模科学数据集的适用性，提出了两个维度的准备框架，分别为数据准备级别和数据处理阶段，针对高性能计算环境进行定制。通过对四个代表性领域的典型工作流程进行分析，识别了常见的预处理模式和领域特定的约束，强调了基于转换器的生成模型，指出了在可扩展的AI训练中转换科学数据的关键挑战。提出的概念成熟度矩阵描绘了科学数据准备情况，并指导基础设施的发展，实现跨领域支持可扩展和可重现的科学AI。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [3] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 研究探讨了多模态大语言模型在推理能力和社会偏见之间的权衡问题。实验结果显示，在强化学习训练中采用一定比例的去偏见样本可以降低社会偏见，同时保持模型的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明，提高多模态大语言模型的推理能力可能会增加社会偏见。因此，平衡推理能力和社会偏见之间的权衡成为一个重要研究问题。

Method: 研究以比较三种偏见缓解策略的基准优劣势为起点，然后调整去偏见和推理样本比例，探索推理与偏见之间的权衡。结果显示强化学习训练的一定比例能够在减少刻板印象得分的情况下，保留模型原始推理准确性的一部分。

Result: 通过实验比较不同偏见缓解策略在去偏见和推理准确性之间的权衡情况，发现强化学习训练中的特定比例可以在一定程度上降低社会偏见的同时保持模型的推理准确性。

Conclusion: 在处理多模态大语言模型（MLLMs）时，平衡推理能力和社会偏见之间的权衡是一个重要的研究问题。研究表明，在采用先进提示方案和后训练微调等技术来推动推理能力的同时，模型的输出常常存在明显的社会偏见。本研究通过对三种偏见缓解策略（有监督微调、知识蒸馏和基于规则的强化学习）进行基准测试，建立它们的基准优势和劣势，并在此基础上改变每种范式中关注去偏见和推理中心样本的比例，以探讨推理与偏见之间的权衡。研究结果显示，在使用强化学习训练的约1:4混合比例下，刻板印象得分下降10％，同时保留了88％的原始推理准确性，为在MLLMs中平衡公平性和能力提供了具体指导。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [4] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 研究通过引入音频图灵测试，评估最先进的音频模型，发现它们在处理听觉任务时存在灾难性失败，失败率超过93%。最佳模型仅准确率为6.9%，远低于人类成功率。结果揭示了AI系统在处理复杂听觉场景时的问题，并呼吁整合选择性注意力、基于物理的音频理解和上下文感知到多模态AI系统中。


<details>
  <summary>Details</summary>
Motivation: 受Moravec悖论启发，研究旨在揭示当前AI系统在处理听觉任务中的失败，并解释这些失败的原因。通过引入图灵测试并评估最先进的音频模型，旨在量化人机听觉差距，并为实现人类级别的机器听力提供见解。

Method: 引入了音频图灵测试，评估了多种最先进的音频模型，展示了它们在处理听觉任务时的失败率。通过对比人类的成功率，揭示了当前AI系统在处理复杂听觉场景中存在的问题。建立了衡量人机听觉差距的基准，并指出当前体系结构缺乏类似人类的听觉场景分析机制。

Result: 评估了多种最先进的音频模型，结果显示其在处理听觉任务上失败率超过93%，最佳模型准确率仅为6.9%，远低于人类的成功率。结果揭示了AI系统在处理复杂听觉场景时的关注力、噪声鲁棒性和上下文适应性等方面存在问题。

Conclusion: 当前AI系统在处理听觉任务时存在灾难性失败，对比人类轻松完成的任务。研究引入了音频图灵测试，包括七类917个挑战，评估了当前最先进的音频模型，结果显示失败率超过93%，最佳模型仅准确率为6.9%，而人类解决的成功率为52%。结果揭示了AI系统处理复杂听觉场景时的关注力、噪声鲁棒性和环境适应性等方面的失败。提出了衡量机器听力进展的诊断框架，强调了将选择性注意力、基于物理的音频理解和上下文感知整合到多模态AI系统中的必要性。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [5] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出并定义了论证一致性属性，要求预测者的推理与其预测相一致。研究发现过滤不一致的预测可以提高准确性，在人类和基于LLM的预测中具有实际价值。然而，用户通常不遵循这一属性，表明需要整合机制来过滤不一致观点。


<details>
  <summary>Details</summary>
Motivation: 通过研究预测中论证结构的属性，提出论证一致性对预测的重要性。研究人工和LLM预测者在一致性要求下的表现，以及用户对论证一致性属性的认可程度。

Method: 提出和正式定义了论证一致性的属性，并进行了三项评估。首先评估强制一致性对人类预测者和基于LLM的预测者的影响，结果表明过滤不一致的预测可以提高预测准确性。然后通过众包用户实验表明，用户通常不符合一致性属性。

Result: 研究发现过滤不一致的预测可以改善预测准确性，在人类和基于LLM的预测中具有实际价值。然而，用户通常不遵循论证一致性属性，因此在基于论证的判断性预测中需要整合机制来过滤不一致观点。

Conclusion: 这篇论文提出并正式定义了一种论证一致性的属性，要求预测者的推理与其预测相一致。研究表明，过滤出不一致的预测可以提高人类和基于大型语言模型（LLM）的预测准确性，支持一致性在人类和基于LLM的预测中的实际价值。然而，通过众包用户实验发现，尽管论证一致性属性直觉上有用，用户通常不遵循这一属性，这表明在基于论证的判断性预测中需要在获得团体预测之前整合机制来过滤不一致观点。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [6] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 本研究探讨了在本体中介查询应答设置中计算责任得分的复杂性。研究发现对于一阶重写的本体中介查询，WSMS计算具有多项式数据复杂性，但当涉及到具有连词支持的原子查询和‘良好行为’的合取查询时，WSMS计算变为“shP”难。而在DL-Lite方言中，确定了一些结构受限的合取查询类别，其WSMS计算是可处理的。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在理解在本体中介查询应答设置中计算责任得分的复杂性。通过对不同类型的查询以及本体语言的特性进行分析，为了更好地理解WSMS计算的可处理性边界，探讨了不同查询类别下的计算复杂性。

Method: 通过利用数据库设置的结果，探讨了Shapley值责任度量的复杂性及其计算方法。研究了WSMS的计算，证明在某些查询类别下为多项式数据复杂性，而在具有可达性查询的情况下则为“shP”难。探讨了联合计算复杂性，对于一些原子查询和‘良好行为’的合取查询是不可处理的，但对DL-Lite方言中的特定结构受限的合取查询类别具有积极结果。

Result: 通过研究可得：对于一阶重写的本体中介查询，WSMS计算具有多项式数据复杂性；当涉及到具有连词支持的原子查询和‘良好行为’的合取查询时，WSMS计算变为“shP”难；在DL-Lite方言中，确定了一些结构受限的合取查询类别，其WSMS计算是可处理的。

Conclusion: 研究了在本体中介查询应答设置中计算责任得分的复杂性。当本体语言能够编码可达性查询时，通过利用数据库设置的结果，可证明此类责任得分具有多项式数据复杂性，适用于能够进行一阶重写的本体中介查询类别。但当问题涉及到具有连词支持的原子查询和‘良好行为’的合取查询时，计算WSMS的复杂性变为“shP”难。此外，对于常见的DL-Lite方言，通过细致分析，确定了一些结构受限的合取查询类别，使得WSMS的计算具有可处理性。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [7] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 本文重新审视分治方法，使用多个部分MILP调用代替少量复杂的调用，提出一种新的解决方案感知ReLU评分（SAS）方法。通过选择关键ReLU变量并减少二进制变量数量，提高了效率并保持了准确性水平。在大型CNN中实施的新方法将未决实例数量减少高达40％，同时保持合理的运行时间。


<details>
  <summary>Details</summary>
Motivation: 先前的尝试在选择重要ReLU变量方面并不理想，为了提高效率，需要改进方法。通过减少复杂性，选择关键的ReLU变量并采用新的解决方案感知ReLU评分（SAS）方法，可以提高效率并减少二进制变量的数量。

Method: 重新审视分治方法，使用多个部分MILP调用代替少量复杂的调用。通过选择少量但非常重要的ReLU变量并使用昂贵的二进制变量处理这些变量，提出了一种新的解决方案感知ReLU评分（SAS）方法。将BaB-SR和BaB-FSB分支函数调整为全局ReLU评分（GS）函数。实现了在Hybrid MILP中调用首次α，β-CROWN解决简单实例，然后使用部分MILP的方法。

Result: SAS方法在选择要打开的变量集方面比先前的尝试更有效，将二进制变量数量减少约6倍，同时保持准确性水平。在大型CNN中实施的新方法将未决实例数量减少高达40％，同时保持合理的运行时间。

Conclusion: 通过改进的分治方法，我们提出了一种解决复杂实例的新方法，使用多个部分MILP调用代替少量复杂的调用。通过选择少量但非常重要的ReLU变量并使用昂贵的二进制变量处理这些变量，我们提高了效率。我们引入了一种新的解决方案感知ReLU评分（SAS）方法来选择这些重要的ReLU变量，并将BaB-SR和BaB-FSB分支函数调整为全局ReLU评分（GS）函数。通过理论和实验证明，SAS在使用二进制变量选择要打开的变量集方面更有效。相较于先前的尝试，SAS将二进制变量数量减少约6倍，同时保持相同的准确性水平。在Hybrid MILP中实现了这一方法，首先使用α，β-CROWN解决简单实例，然后使用部分MILP，可以产生非常准确且高效的验证器，将未决实例数量降低高达40％，并保持合理的运行时间（每个实例平均46秒至417秒），即使是具有200万参数的相当大的CNN也是如此。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [8] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: 该论文调查了人工智能科学家系统在科学研究领域的进展，着重评估了其当前成就和未来发展前景，旨在为科学代理人的出现以及科学人工智能的最终目标提供见解。


<details>
  <summary>Details</summary>
Motivation: 研究挖掘了基于大型语言模型的人工智能科学家系统的进展，探讨了人工智能科学家改变世界和重塑科学研究范式的可能性。

Method: 通过展望驱动的综述，全面分析了人工智能科学家系统的当前成就，识别了关键瓶颈和必要的要素，为科学代理人的出现提供了思路。

Result: 研究评估了人工智能科学家系统的当前成就，识别了关键瓶颈和必要的要素，希望为产生具有突破性发现能力的科学代理人的出现做出贡献。

Conclusion: 该论文调查了基于大型语言模型的人工智能科学家系统在科学研究领域的进展，指出人工智能科学家可能很快能够揭示人类之前未知的现象。研究重点在于评估人工智能科学家系统的当前成就，识别关键瓶颈和必要的要素，以期为产生具有突破性发现能力的科学代理人的出现作出贡献。希望该调查能够更清晰地了解当前人工智能科学家系统的局限性，展示我们目前的状态，以及科学人工智能的最终目标。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [9] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: The paper argues against fully autonomous AI, citing risks such as artificial superintelligence. It presents theories, 12 arguments, 6 counterarguments with rebuttals, and 15 evidence pieces to emphasize the need for human oversight in AI development.


<details>
  <summary>Details</summary>
Motivation: To address the risks of fully autonomous AI, particularly with the potential emergence of artificial superintelligence in the near future. Highlighting the necessity of human oversight in mitigating these risks.

Method: Discussing theories of autonomy, AI, and agents to support the argument against fully autonomous AI. Presenting 12 arguments, 6 counterarguments with rebuttals, and 15 recent evidence pieces in the appendix.

Result: Supporting the claim that AI should not be fully autonomous and advocating for the crucial role of responsible human oversight in AI development and deployment.

Conclusion: AI should not be fully autonomous due to the risks involved, emphasizing the importance of responsible human oversight. The paper provides arguments, counterarguments, and evidence to support this position.

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [10] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 本文介绍了一个专门设计的基准测试用于评估数据科学代理性能，评估了三种LLM模型在不同方法下的表现，探讨了模型对常见提示问题和温度参数的敏感性。研究发现了性能差异，强调了实际部署的关键因素，为未来数据科学代理研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的快速采用，但对这些代理的效力和局限性缺乏系统性基准评估。因此，本文的动机在于引入一个全面的基准测试，以反映数据科学代理与商业应用的真实用户交互情况，并为未来研究提供更健壮、更有效的数据科学代理奠定基础。

Method: 介绍了一个专门设计的基准测试来评估数据科学代理的性能，并评估了三种LLM模型的表现。涵盖了不同的方法和任务类别，以及模型对常见提示问题和温度参数的敏感性。

Result: 通过评估不同LLM模型和方法，研究发现了性能差异，并强调了影响实际部署的关键因素。提出的基准数据集和评估框架旨在促进更深入的数据科学代理研究。

Conclusion: 本文介绍了一个专门为反映数据科学代理用户与商业应用的交互而设计的广泛基准测试。评估了三种LLM模型：Claude-4.0-Sonnet、Gemini-2.5-Flash和OpenAI-o4-Mini，涵盖了三种方法：零-shot结合上下文工程，多步结合上下文工程，以及SmolAgent。研究评估了这些模型对八种数据科学任务类别的性能，并探讨了模型对常见提示问题（例如数据泄露和略微模糊的指令）的敏感性。进一步研究了温度参数对每个模型和方法的整体和任务特定结果的影响。研究发现揭示了评估模型和方法论之间的性能差异，突显影响实际部署的关键因素。本文介绍的基准数据集和评估框架旨在为未来更健壮、更有效的数据科学代理研究奠定基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [11] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail is a novel railway service platform enhanced with large language models, offering personalized services such as ticketing, dining recommendations, weather information, and chitchat. The platform integrates the QTAO prompting framework for accurate responses and introduces the CRFD-25 dataset for personalized dining recommendations, along with an LLM-based recommender system.


<details>
  <summary>Details</summary>
Motivation: To meet the increasing demands for individualized railway service, the paper develops LLM4Rail to leverage large language models for providing custom modules and personalized services within the railway industry. The focus is on enhancing the onboard dining experience and addressing the need for tailored recommendations in railway services.

Method: The paper proposes the iterative "Question-Thought-Action-Observation (QTAO)" prompting framework that integrates verbal reasoning with task-oriented actions. It also introduces the Chinese Railway Food and Drink (CRFD-25) dataset for personalized onboard dining services and an LLM-based zero-shot conversational recommender for railway catering, with a feature similarity-based post-processing step to improve recommendations.

Result: The study successfully implements LLM4Rail, showcasing its ability to provide personalized railway services through the integration of large language models. The introduction of CRFD-25 dataset and the LLM-based recommender system demonstrate the effectiveness of the platform in offering tailored dining recommendations for railway passengers.

Conclusion: LLM4Rail is a novel LLM-augmented railway service consulting platform that provides personalized services such as ticketing, dining recommendations, weather information, and chitchat. The platform incorporates the QTAO prompting framework for verbal reasoning and task-oriented actions to generate accurate responses.

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [12] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 该论文介绍了设计、实施和评估LLM代理与工业级生产ERP系统交互的过程，成功实现了自然语言查询转化为SQL语句的功能，并提出了双代理架构以提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 通过与工业级生产ERP系统交互，提高自然语言查询转化为SQL语句的可靠性。

Method: 设计、实施和评估LLM代理，提出了双代理架构。

Result: 成功实现了能够对自然语言查询进行解释和转化为SQL语句的LLM代理，并提出了双代理架构以提高查询生成的可靠性。

Conclusion: 该论文介绍了一个与工业级生产ERP系统交互的大型语言模型（LLM）代理的设计、实施和评估。该代理能够解释自然语言查询，并将其转化为可执行的SQL语句，利用开放权重LLM。提出了一种结合推理和批判阶段的新颖双代理架构，以提高查询生成的可靠性。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [13] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: 该论文提出了Self-Foveate方法，通过引入多级注视方法来指导大型语言模型合成指导文本，从而提高了指导文本的多样性和难度。实验证实了该方法的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的自动合成范式在确保合成指导的多样性和难度方面存在显著限制，本文旨在解决这些挑战。

Method: Self-Foveate方法引入了“微散-宏”多级注视方法，通过指导大型语言模型深入挖掘细节信息，提高指导文本的多样性和难度。

Result: 通过全面实验验证了Self-Foveate方法的有效性和优越性。

Conclusion: 该论文提出了一种名为Self-Foveate的创新大型语言模型（LLM）驱动的指导合成方法，通过引入“微散-宏”多级注视方法有效引导LLM深入挖掘无监督文本中嵌入的细粒度信息，从而增强合成指导的多样性和难度。实验表明，该方法在多个无监督语料库和不同模型架构下都验证了其有效性和优越性。作者公开发布了他们的数据和代码链接：https://github.com/Mubuky/Self-Foveate

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [14] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 最新研究发现，通过合理的上下文框架，推理模型在因果发现任务中取得了重大进展。研究使用了Emergent OpenAI的o系列和DeepSeek-R模型家族，在Corr2Cause基准上进行研究，并引入了模块化上下文流程，取得了近三倍的改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对因果推断的挑战是一个根本性问题，先进的内部推理方法引起了人们对最先进的推理模型在因果发现中的性能兴趣。

Method: 使用Emergent OpenAI的o系列和DeepSeek-R模型家族，在Corr2Cause基准上进行因果发现研究，引入了受Tree-of-Thoughts和Chain-of-Thoughts方法启发的模块化上下文流程，对比传统方法。

Result: 研究发现，理性优先的架构比先前方法实现了显著更大的收益，而受启发的上下文流程导致了比传统基线几乎三倍的改进。

Conclusion: 研究表明，通过结构合理的上下文框架，先进的推理模型在因果发现任务中取得了显著的进展，对于优化它们的能力至关重要，并为跨领域的因果发现提供了可推广的蓝图。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [15] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 本文提出了对图像分类器的因果解释方法，形式属性与逻辑解释相同，且适用于黑盒算法。他们引入了对比性因果解释和完整的因果解释，将解释定义与置信度感知相结合。实验结果表明，算法计算效率高，且完全是黑盒算法，不需要了解模型内部或任何属性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分类器输出解释算法缺乏形式严谨性，基于逻辑的解释虽然形式严谨，但在图像分类器上的可计算性受到严格假设的限制。因此，本文旨在提出形式严谨且适用于图像分类器的因果解释方法。

Method: 介绍了因果解释与逻辑解释的对比，证明了因果解释的形式属性，并提出了对比性因果解释的概念。他们还将解释的定义与置信度感知相结合，并引入了完整的因果解释。实现了他们的定义，并通过实验结果展示了算法的效果和性能。

Result: 他们的因果解释方法在形式属性上与逻辑解释相同，适用于黑盒算法和图像分类器。通过实验结果展示了算法的有效性和计算效率。

Conclusion: 本文提出了对图像分类器的因果解释方法，与基于逻辑的解释具有相同的形式属性，同时适用于黑盒算法和图像分类器。他们证明了因果解释的形式属性，并引入了对比性因果解释用于图像分类器。此外，他们将解释的定义与置信度感知相结合，并引入了完整的因果解释。他们的实验结果表明，不同模型具有不同的充分性、对比性和完整性模式。他们的算法计算效率高，仅需平均6秒即可计算出各种解释类型，完全是黑盒算法，不需要了解模型、访问模型内部、梯度或模型的任何属性。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [16] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: 本文介绍了一种名为DICE的框架，用于动态上下文示例选择，通过分解示例知识为可转移和不可转移组件，并提出了改进代理性能的步骤选择标准。该方法通用、框架独立，可无需额外训练成本地集成到现有代理框架中。实验证实了该方法的有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有作品表明，ICL的有效性对演示选择非常敏感，而亚优化示例通常会导致不稳定或降级的性能。缺乏一个通用的、理论上基础的标准来确定跨推理步骤什么构成有效演示，因此开发一个有原则、通用的选择演示的方法是具有挑战性的。因此，为了开发出能够持续提升代理性能的有原则的通用方法是非平凡的。

Method: 作者提出了一种名为DICE的动态上下文示例选择框架，通过因果视角将示例知识分解为可转移和不可转移的组件，进一步提出了一个具有形式保证的改进代理性能的步骤选择标准。DICE是一个通用的、框架独立的解决方案，可以无需额外训练成本地集成到现有的代理框架中。

Result: 通过实验验证，DICE方法在不同领域展示了其有效性和通用性，为强大和高效的LLM代理带来了重要的贡献。

Conclusion: 本文提出了一种名为DICE的动态上下文示例选择框架，用于代理任务，可以在推理的每个步骤中选择最相关的演示。该方法通过因果视角将示例知识分解为可转移和不可转移的组件，展示后者如何引入损害泛化的伪依赖。作者进一步提出了一个具有改进代理性能的形式保证的分步选择标准。DICE是一个通用的、框架独立的解决方案，可以作为插件模块集成到现有的代理框架中，而无需额外的训练成本。作者通过各种领域的广泛实验验证了本方法的有效性和通用性，突显了基于原则的上下文感知演示选择对强大且高效的LLM代理的重要性。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [17] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 本文提出了一种基于语义信任链的自主信任编排方法，通过智能代理AI和超图技术实现了设备之间的信任关系建立和维护，从而高效利用分布式资源，支持任务特定信任评估和多跳协作，实现了资源高效的信任评估。


<details>
  <summary>Details</summary>
Motivation: 本文针对在协作系统中任务特定信任评估的复杂性和资源消耗增加的问题，提出了一种自主信任编排方法。通过解决任务的空间和时间动态性、分布设备资源的复杂性和信任评估开销等挑战，以实现高效的协作任务执行。

Method: 本文方法利用智能代理AI和超图技术，通过在设备空闲期间仅基于历史性能数据执行协作者的信任评估，从而实现分布式资源的有效利用。此外，通过分析资源能力和任务需求之间的对齐性，实现对协作者资源的任务特定信任评估。同时，通过维护嵌入信任语义的信任超图，实现协作者的层次管理和减少评估开销。最后，本文还支持多跳协作，实现大规模系统的高效协调。

Result: 实验结果表明，本文提出的方法实现了资源高效的信任评估，通过智能代理AI和超图技术实现了信任关系的建立和维护，支持任务特定信任评估和多跳协作，为分布式协作系统的高效管理提供了解决方案。

Conclusion: 本文提出了一种基于语义信任链的自主信任编排方法，利用智能代理AI和超图在设备之间建立和维护信任关系，实现了高效的分布式资源利用。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>


### [18] [MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying](https://arxiv.org/abs/2507.23633)
*Qian Zhao,Zhuo Sun,Bin Guo,Zhiwen Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖的基于策略引导的代理辅助记忆召回方法，使用Recall Router框架来解决选择适当的记忆策略和生成高质量响应的挑战。实验结果表明，MemoCue在记忆灵感召回方面的表现超过了基于LLM的方法17.74%，人类评估进一步突出了其在记忆召回应用中的优势。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于传统的代理辅助记忆召回方法受到内存模块大小限制，难以获取完整的记忆，影响实际记忆召回性能。受记忆理论启发，提出了策略引导的代理辅助记忆召回方法，旨在通过设计有效线索，帮助个人主动激活相关记忆。

Method: 论文使用了Recall Router框架，设计了5W Recall Map对记忆查询进行分类，并定义了跨越五种场景的十五种记忆策略模式。采用了层次化回忆树与蒙特卡洛树搜索算法优化策略选择和响应生成。通过构建指导调整数据集和微调多个开源的大型语言模型（LLM），开发出了MemoCue代理。

Result: 实验结果显示，MemoCue在记忆灵感召回方面表现优异，超过了基于LLM的方法17.74%。进一步的人类评估也突出了它在记忆召回应用中的优势。

Conclusion: 该论文提出了一种新颖的基于策略引导的代理辅助记忆召回方法，使用Recall Router框架来解决选择适当的记忆策略和生成高质量响应的挑战。通过设计Recall Map和记忆策略模式，以及利用Monte Carlo Tree Search算法优化策略选择和响应生成，成功开发了MemoCue代理。实验结果表明，MemoCue在记忆灵感召回方面的表现超过了基于LLM的方法17.74%，人类评估进一步突出了其在记忆召回应用中的优势。

Abstract: Agent-assisted memory recall is one critical research problem in the field of
human-computer interaction. In conventional methods, the agent can retrieve
information from its equipped memory module to help the person recall
incomplete or vague memories. The limited size of memory module hinders the
acquisition of complete memories and impacts the memory recall performance in
practice. Memory theories suggest that the person's relevant memory can be
proactively activated through some effective cues. Inspired by this, we propose
a novel strategy-guided agent-assisted memory recall method, allowing the agent
to transform an original query into a cue-rich one via the judiciously designed
strategy to help the person recall memories. To this end, there are two key
challenges. (1) How to choose the appropriate recall strategy for diverse
forgetting scenarios with distinct memory-recall characteristics? (2) How to
obtain the high-quality responses leveraging recall strategies, given only
abstract and sparsely annotated strategy patterns? To address the challenges,
we propose a Recall Router framework. Specifically, we design a 5W Recall Map
to classify memory queries into five typical scenarios and define fifteen
recall strategy patterns across the corresponding scenarios. We then propose a
hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to
optimize the selection of strategy and the generation of strategy responses. We
construct an instruction tuning dataset and fine-tune multiple open-source
large language models (LLMs) to develop MemoCue, an agent that excels in
providing memory-inspired responses. Experiments on three representative
datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall
inspiration. Further human evaluation highlights its advantages in
memory-recall applications.

</details>


### [19] [Personalized Education with Ranking Alignment Recommendation](https://arxiv.org/abs/2507.23664)
*Haipeng Liu,Yuxuan Liu,Ting Long*

Main category: cs.AI

TL;DR: Personalized question recommendation is addressed using Ranking Alignment Recommendation (RAR) to improve recommendation performance efficiently. Collaborative ideas are incorporated into the exploration mechanism. The framework is applicable to any RL-based question recommender.


<details>
  <summary>Details</summary>
Motivation: Previous methods struggle with efficient exploration in personalized question recommendation. They fail to identify the best questions for each student during training.

Method: Proposed Ranking Alignment Recommendation (RAR) which incorporates collaborative ideas into the exploration mechanism for more efficient exploration within limited training episodes.

Result: Experiments demonstrate that RAR enhances recommendation performance effectively.

Conclusion: RAR effectively improves recommendation performance in personalized question recommendation. The framework can be applied to any RL-based question recommender.

Abstract: Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.

</details>


### [20] [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
*Long Phan,Mantas Mazeika,Andy Zou,Dan Hendrycks*

Main category: cs.AI

TL;DR: 介绍了TextQuests基准测试，用于评估AI代理在探索性环境中长期推理能力，重点关注自主问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代理基准测试虽然有效评估了工具使用技能或在结构化任务上的表现，但往往无法完全捕捉代理在需要持续自主推理的探索性环境中操作的能力。为促进开发能够在长期环境中进行更强大内在推理的代理，引入了TextQuests基准测试。

Method: 引入了TextQuests基准测试，设计用于评估AI代理在交互式虚构游戏中的自主问题解决能力。基准测试特别注重LLM代理的内在长期推理能力，禁止使用外部工具，并侧重于探索性环境中的试错学习和持续问题解决。

Result: 发布了TextQuests基准测试，旨在评估AI代理在需要长期上下文推理的探索性环境中的自主问题解决能力。

Conclusion: 介绍了一种基于Infocom套件的互动小说游戏的基准测试TextQuests，用于评估AI代理在长期环境中自主操作的能力。该基准测试旨在评估LLM代理的自主问题解决能力，重点关注长期上下文推理能力。

Abstract: Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.

</details>


### [21] [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
*Luoxin Chen,Jinming Gu,Liankai Huang,Wenhao Huang,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Kaijing Ma,Cheng Ren,Jiawei Shen,Wenlei Shi,Tong Sun,He Sun,Jiahui Wang,Siran Wang,Zhihong Wang,Chenrui Wei,Shufa Wei,Yonghui Wu,Yuchen Wu,Yihang Xia,Huajian Xin,Fan Yang,Huaiyuan Ying,Hongyi Yuan,Zheng Yuan,Tianyang Zhan,Chi Zhang,Yue Zhang,Ge Zhang,Tianyun Zhao,Jianqiu Zhao,Yichi Zhou,Thomas Hanwen Zhu*

Main category: cs.AI

TL;DR: 论文引入Seed-Prover和Seed-Geometry系统，通过强化学习和形式验证提高数学推理能力，在IMO竞赛中取得成功。Seed-Prover成功率达78.1%，在PutnamBench上超过50%，Seed-Geometry优于以往几何引擎。参与IMO 2025并证明5道问题，标志着自动数学推理领域的重大进展。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在数学推理中缺乏清晰的监督信号，本文旨在通过引入形式验证和领域特定语言来提高推理能力。同时，为了解决Lean中几何支持的不足，引入了Seed-Geometry系统来提升几何推理。参与IMO竞赛并完整解决大部分问题是论文的动机之一。

Method: 使用了强化学习和形式验证，设计了Seed-Prover系统和Seed-Geometry系统来提高数学推理能力。通过迭代改进证明、Lean反馈、已证引理和自我总结等方式，实现了整个证明的推理。同时还设计了三种测试推理策略来解决IMO水平竞赛问题。

Result: Seed-Prover在过去IMO问题中成功率达到78.1%，并在PutnamBench上表现出色。Seed-Geometry优于以往的形式几何引擎。两个系统共同参与了IMO 2025并证明了5道问题，展示出自动数学推理的有效性。

Conclusion: 该论文提出了Seed-Prover和Seed-Geometry两个系统，通过结合强化学习、领域特定语言和形式验证来提高数学推理能力。Seed-Prover在解决IMO水平的竞赛问题中表现出色，达到了过去的顶尖水平，并且在PutnamBench上实现了超过50%的成功率。Seed-Geometry引入了几何推理引擎，优于以往的形式几何引擎。这两个系统共同参与了IMO 2025，并完整证明了6道问题中的5道，这标志着自动数学推理领域的重大进展。

Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.

</details>


### [22] [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
*Ping Yu,Jack Lanchantin,Tianlu Wang,Weizhe Yuan,Olga Golovneva,Ilia Kulikov,Sainbayar Sukhbaatar,Jason Weston,Jing Xu*

Main category: cs.AI

TL;DR: CoT-Self-Instruct 提出了新的合成数据生成方法，通过CoT推理和自我指导提高了LLMs的训练效果。在验证推理和非验证指令遵循任务中，合成数据的性能明显优于现有数据集和人类/标准提示。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于提出一种新的合成数据生成方法，以提高LLMs的训练效果。通过引入自我指导和CoT推理，旨在生成高质量和复杂度的合成数据，以帮助LLMs更好地理解和学习任务。同时，通过自动度量来筛选高质量数据，进一步提升训练效果。

Method: 该方法通过CoT-Self-Instruct方法生成合成数据用于LLMs的训练。首先，LLMs通过Chain-of-Thought（CoT）对给定的种子任务进行推理和规划，然后生成类似质量和复杂度的合成提示。接着，使用自动指标对生成的数据进行高质量数据的筛选。在可验证推理和非可验证的指令遵循任务中，该方法的性能优于现有数据集和人类或标准自我指导提示。

Result: 在可验证推理和非可验证的指令遵循任务中，CoT-Self-Instruct方法的合成数据性能均优于现有数据集和人类或标准自我指导提示。具体在MATH500、AMC23、AIME24和GPQA-Diamond等数据集上表现显著优越，并在AlpacaEval 2.0和Arena-Hard上取得较好表现。

Conclusion: CoT-Self-Instruct 提出了一种合成数据生成方法，通过Chain-of-Thought（CoT）指导LLMs首先通过给定的种子任务进行推理和规划，然后生成一个类似质量和复杂度的新合成提示，用于LLMs训练，然后通过自动指标进行高质量数据的过滤。在可验证推理方面，我们的合成数据在MATH500、AMC23、AIME24和GPQA-Diamond等数据集上显著优于现有的训练数据集，如s1k和OpenMathReasoning。对于不可验证的指令遵循任务，我们的方法在AlpacaEval 2.0和Arena-Hard上均优于人类或标准自我指导提示的表现。

Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.

</details>


### [23] [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
*Mingkai Deng,Jinyu Hou,Yilin Shen,Hongxia Jin,Graham Neubig,Zhiting Hu,Eric Xing*

Main category: cs.AI

TL;DR: SimuRA introduces a goal-oriented architecture for agentic reasoning, utilizing a world model for planning via simulation to overcome autoregressive limitations. It demonstrated enhanced success in web browsing tasks and showed the advantage of world model simulation in reasoning. The approach has the potential to train a single, general agent model for superintelligent performance across diverse environments.


<details>
  <summary>Details</summary>
Motivation: The current one-task-one-agent approach using autoregressive LLMs lacks scalability and generality. Humans' ability to reason by mentally simulating outcomes inspired the development of a more general and powerful AI agent, SimuRA.

Method: Introducing SimuRA, a goal-oriented architecture for generalized agentic reasoning. It is based on a principled formulation of optimal agent in any environment, implementing a generalized world model using LLM for planning via simulation.

Result: SimuRA showed improved success rates in challenging web browsing tasks, with up to 124% advantage over autoregressive planning. It presents the potential of training a single, general agent model based on LLMs for superintelligent behavior in all environments.

Conclusion: SimuRA, a goal-oriented architecture for generalized agentic reasoning, overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. It demonstrated improved success in difficult web browsing tasks and the advantage of world model simulation as a reasoning paradigm.

Abstract: AI agents built on large language models (LLMs) hold enormous promise, but
current practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental
limitations of autoregressive LLMs. On the other hand, humans are general
agents who reason by mentally simulating the outcomes of their actions and
plans. Moving towards a more general and powerful AI agent, we introduce
SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based
on a principled formulation of optimal agent in any environment, \modelname
overcomes the limitations of autoregressive reasoning by introducing a world
model for planning via simulation. The generalized world model is implemented
using LLM, which can flexibly plan in a wide range of environments using the
concept-rich latent space of natural language. Experiments on difficult web
browsing tasks show that \modelname improves the success of flight search from
0\% to 32.2\%. World-model-based planning, in particular, shows consistent
advantage of up to 124\% over autoregressive planning, demonstrating the
advantage of world model simulation as a reasoning paradigm. We are excited
about the possibility for training a single, general agent model based on LLMs
that can act superintelligently in all environments. To start, we make SimuRA,
a web-browsing agent built on \modelname with pretrained LLMs, available as a
research demo for public testing.

</details>
