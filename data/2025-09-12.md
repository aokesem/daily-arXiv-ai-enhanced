<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文介绍了如何将贝叶斯定理扩展到区间二型（IT2）版本。首先开发了一个IT2版本的贝叶斯定理，避免了潜在的不一致性，并提出了一种算法，将SME提供的区间编码成IT2模糊隶属函数，以指定贝叶斯定理中的输入概率。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，通常最好的信息来源是领域内专家（SMEs）提供的输入概率的区间范围估计。现有应用中的贝叶斯推断通常假设精确的输入值来产生精确的输出值，这在实际应用中是不现实的。因此，本文的动机是将贝叶斯定理扩展到一种区间型-2（IT2）版本，以更好地适用于实际情况。

Method: 本文开发了一个IT2版本的贝叶斯定理，并提出了一种灵活的算法，将SME提供的区间编码成IT2模糊隶属函数（MFs），用于指定贝叶斯定理中的输入概率。算法避免输入IT2 MFs中的潜在不一致性，以产生有效的输出结果。

Result: 本文提出了一种扩展贝叶斯定理到IT2版本的方法，并描述了一种用于编码SME提供的区间的新颖算法。该算法避免了输入不一致性，并能够产生有效结果。

Conclusion: 本文提供了两个关键贡献，扩展贝叶斯定理到一个区间二型（IT2）版本。首先，我们开发了一个IT2版本的贝叶斯定理，使用一种新颖而保守的方法，以避免可能导致无效输出结果的输入IT2 MFs中的潜在不一致性。然后，我们描述了一种新颖灵活的算法，将SME提供的区间编码成IT2模糊隶属函数（MFs），我们可以用来指定贝叶斯定理中的输入概率。我们的算法推广和扩展了先前解决这个问题的主要是将区间编码成用于计算与词语应用的单词MFs的方法。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [2] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 该论文介绍了一种利用NLP和LLMs将GDDs转化为Unity游戏原型的自动化框架，通过细调LLaMA-3模型和自定义Unity集成包实现生成代码。评估结果显示方法优于基准模型，在多个指标上取得优异表现，生成的模板符合GDD规范，填补了AI辅助游戏开发中的关键空白。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于解决AI辅助游戏开发中的关键挑战，将游戏设计文档转化为功能性Unity游戏原型的过程。通过引入NLP和多模态LLMs，旨在提高生成模板的质量和符合度，为游戏开发提供更高效的工具。

Method: 该论文的方法是建立一个端到端系统，解析GDDs，提取结构化游戏规范，并合成Unity兼容的C#代码来实现设计文档中定义的核心机制、系统和架构。方法结合了专为Unity代码生成而精调的LLaMA-3模型和一个自定义的Unity集成包，简化了实现过程。

Result: 评估结果显示，该方法相较于基准模型取得显著改进，且在多个指标上表现优异。生成的模板在多个游戏类型上高度符合GDD规范，为游戏设计到实现的过渡提供了高质量的支持。

Conclusion: 该论文提出了一种新颖的自动化游戏模板生成框架，利用自然语言处理（NLP）和多模态大型语言模型（LLMs）将游戏设计文档（GDDs）转化为功能性Unity游戏原型。通过细粒度调整的LLaMA-3模型和自定义的Unity集成包相结合，该方法在编译成功率、GDD遵从性、最佳实践采纳和代码模块化度等指标上取得了显著改进，生成的模板在多个游戏类型上高度符合GDD规范。该系统有效地填补了AI辅助游戏开发中的关键空白，使LLMs成为简化从游戏设计到实现过渡的有价值工具。

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [3] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本研究介绍了一个框架，利用多个大型语言模型代理将自然语言描述的优化或满足问题转换为MiniZinc模型。该框架通过分解问题并简化推理过程，降低复杂性。作者进行了初步实验，结果显示比基准方法性能更好，并提出了未来工作的改进方向。


<details>
  <summary>Details</summary>
Motivation: 将自然语言描述的问题转换为MiniZinc模型需要逻辑推理和约束编程专业知识，这是一个具有挑战性的过程。作者的动机在于解决这一挑战，通过引入一个主动性方法的框架，以多个专业化的大型语言模型代理来分解建模任务，从而简化推理过程，降低复杂性。

Method: 提出了一个框架，利用多个专业化的大型语言模型代理来解决将自然语言描述的优化或满足问题转换为正确的MiniZinc模型的挑战。每个代理专注于一个特定类别的全局约束，最终的组装代理将约束代码整合成完整的MiniZinc模型。通过分解问题并让每个大型语言模型处理更简单的推理挑战，以降低总体复杂性。进行了初步实验并展示了更好的性能。

Result: 在初步实验中，通过使用多个大型语言模型代理，作者展示了比一次性提示和思维链提示等基准方法更好的性能。未来工作的路线图提出了潜在的增强和改进方向。

Conclusion: 介绍了一个以主动性方法为特点的框架，利用多个专业化的大型语言模型代理来解决将自然语言描述的优化或满足问题转换为正确的MiniZinc模型的挑战。该框架通过按全局约束类型分解建模任务，每个代理专门用于检测和生成特定类别的全局约束代码，同时一个最终的组装代理将这些约束代码片段集成到一个完整的MiniZinc模型中。通过将问题分解为更小、定义明确的子任务，每个大型语言模型处理一个更简单的推理挑战，从而可能降低总体复杂性。通过利用几个大型语言模型进行初步实验，并展示了与一次性提示和思维链提示等对照组相比更好的性能。最后，概述了未来工作的全面路线图，重点突出潜在的增强和改进方向。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [4] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 这篇论文研究了模型训练中合成数据的关键挑战：模型崩溃。提出了一种基于置信度的损失函数 Truncated Cross Entropy（TCE），用于降低高置信度预测的权重，显著延迟了模型崩溃。通过理论和实证验证表明，这种方法可以将模型的保真度间隔延长至多 2.3 倍，并在不同模态间有效泛化。损失函数设计成为保留生成模型质量的重要工具。


<details>
  <summary>Details</summary>
Motivation: 随着对生成式人工智能模型的依赖增加，合成数据的生成速率也在加快，一些预测表明到 2030 年，训练中可用的新数据大多数可能是由机器生成的。这种从主要合成内容转变的关键挑战在于，对合成数据的重复训练会导致模型崩溃现象，即模型性能随训练世代的增加而下降，最终使模型失效。尽管先前的研究探讨了模型崩溃的原因和检测方法，但现有的缓解策略仍然有限。

Method: 我们研究了模型对自动生成数据的过度自信是导致模型崩溃的关键驱动因素。基于这一观察，我们提出了一种基于置信度的损失函数，命名为 Truncated Cross Entropy（TCE）。我们展示了 TCE 在延迟模型崩溃方面的显著效果。同时，我们提供了一个与特定模型无关的框架，将损失函数设计与模型崩溃缓解联系起来。我们通过理论和实证的验证表明，我们的方法可以显著延长模型在崩溃之前的保真度间隔。

Result: 我们的研究提出了一种基于置信度的损失函数，可以显著延迟模型崩溃，通过理论和实证的验证，我们的方法在延长模型保真度间隔方面取得了显著进展。此外，我们展示了我们的方法在不同模态间的泛化性。

Conclusion: 我们的研究发现模型对自动生成数据的过度自信是模型崩溃的关键驱动因素。我们提出了一种基于置信度的损失函数，该损失函数在训练过程中减小了高置信度预测的权重。我们介绍了一种名为Truncated Cross Entropy（TCE）的新型损失函数。我们展示了TCE在递归训练中显著延迟了模型的崩溃。我们提供了一个与模型无关的框架，将损失函数设计与模型崩溃缓解联系起来，并在理论和实证上验证了我们的方法，表明它可以将模型在崩溃之前的保真度间隔延长至多 2.3 倍。最后，我们展示了我们的方法在不同模态间的泛化性。这些发现表明，在增加合成数据的时代，损失函数的设计为保留生成模型质量提供了一个简单而强大的工具。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [5] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 该论文研究解释性人工智能中的不确定性解释和全局解释，探讨选取涵盖多个概念的算法对用户信任度的影响，结果表明直观性提高了用户满意度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 许多研究者致力于构建解释性人工智能（XAI）方案的普适指导方针，该研究选择关注不确定性解释和全局解释，这些方面经常被忽略。

Method: 选取一种涵盖多个概念的算法进行研究，探究其能否提高用户对解释性人工智能的信任度。

Result: 通过测试算法对信任度的校准能力，并检验算法是否能够提供更直观的视觉理解，尽管难以理解，但提高了用户满意度和人类可解释性。

Conclusion: 该论文关注解释性人工智能（XAI）中的不确定性解释和全局解释，通过选择覆盖不确定性、稳健性和全局XAI等概念的算法，探讨其对信任校准的能力。研究发现，尽管某些算法难以理解，但提供更直观的视觉理解，可能会提高用户满意度和人类可解释性。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [6] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本研究旨在解决冷启动用户问题对推荐系统的影响，通过优化指导提示流程，提出了一种基于LLM的方法。研究表明在低数据环境下，通过优化示例注入和指导结构，可以显著提高模型性能，指导式自适应可能成为解决冷启动推荐问题的途径之一。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决冷启动用户问题对推荐系统有效性的影响，并优化在推荐任务中使用LLM时的指导提示流程。

Method: 研究使用了基于transformer的自回归LLM（如BioGPT、LLaMA-2、GPT-4）进行系统实验，通过优化示例注入和指导结构，采用token级别的对齐和嵌入空间正则化来改善模型在低数据环境中的性能。

Result: 通过实验结果表明，研究的方法能显著提高模型的精度和NDCG分数，提示式自适应可能是解决冷启动推荐问题的一种有效途径。

Conclusion: 本研究表明，使用特定的指令提示形式方法可以显著提高在低数据环境中基于LLM的模型的精度和NDCG分数，通过实证研究发现在指定支持集合和最佳示例注入时可取得最佳效果。此外，提示式的自适应可能是解决基于LLM的管道中所遇到的冷启动推荐问题的一种途径。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [7] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 研究比较了人类、LLMs和贝叶斯代理在动态协商环境中的表现。贝叶斯代理通过积极优化获得最高盈余，但频繁拒绝交易；人类和LLMs通过不同行为方式达到类似盈余；LLMs倾向于保守交易，人类更具战略性和公平意识。性能平衡可能掩盖了各代理之间重要的过程和多样性差异。


<details>
  <summary>Details</summary>
Motivation: 随着自主代理越来越多地执行人类传统执行的协调任务，评估代理的性能和协商过程变得至关重要。不同的代理展现出不同的优势：传统统计代理，如贝叶斯模型，在确定条件下表现卓越，而大型语言模型(LLMs)可以在不同环境中进行泛化。

Method: 比较人类（N=216）、LLMs（GPT-4o、Gemini 1.5 Pro）和贝叶斯代理在动态协商环境中的表现，捕捉结果和行为动态，以便进行直接的、相同条件的跨人群比较。

Result: 研究发现，具有表现平衡的贝叶斯代理、LLMs和人类在实际部署中可能存在关键的过程和多样性差异，这对真实世界协调任务的实际部署至关重要。

Conclusion: 研究发现，在动态协商环境中，贝叶斯代理通过积极优化获得最高的盈余，但频繁拒绝交易。人类和大型语言模型（LLMs）可以通过不同的行为方式实现类似的总体盈余，LLMs倾向于保守、让步型的交易，并几乎没有拒绝，而人类采用更具战略性、冒险性和注重公平的行为方式。

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [8] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 该论文提出了一个机器学习管道方法，用于识别高风险银行客户，在竞赛中获得第二名，AUROC达到0.961。研究方法包括16步设计和统计分析，基于SQL的特征工程算法，预先训练模型与数据库连接并提供了可解释的人工智能模块。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在利用机器学习管道识别高风险银行客户，解决反洗钱行动和衡量对金融机构的重要性。研究的动机在于利用机器学习技术提高AML行动的效率和准确性。

Method: 研究采用了16步设计和统计分析的方法，将数据存储在SQLite数据库中，开发了基于SQL的特征工程算法，并构建了可连接预先训练模型的管道。研究还提供了可解释的人工智能模块，用于推导特征的重要性。

Result: 研究提出的机器学习管道在竞赛中取得了第二名，具有很高的成功率，并在验证中达到了0.961的平均接收者操作特征曲线下面积（AUROC）。

Conclusion: 该论文提出了一个系统化的机器学习管道方法，用于识别高风险银行客户。他们在一项大数据和人工智能竞赛中，利用包括195,789个客户ID的数据集，经过设计和统计分析确保了管道的稳健性。该管道在竞赛中获得第二名，并取得了显著的成功。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [9] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文探讨了当前agentic AI系统在空间推理能力方面的局限性，提出了基于神经科学原理的计算框架，建立了六个关键计算模块，以推动Agentic Spatial Intelligence在物理3D世界中的发展。作者从框架视角分析了最新方法，评估了它们与每个模块的相关性，识别了关键缺口，探究了新兴领域，提出了潜在研究方向，为研究社区提供了一个神经科学视角和结构化路径。


<details>
  <summary>Details</summary>
Motivation: 作者的动机在于当前agentic AI系统虽然在任务执行和基于语言的推理方面取得了进展，但其空间推理能力仍然有限且未被充分探索，需要借鉴人类空间智能的多感知感知、空间记忆和认知地图等特点，以提高AI系统在物理3D世界中的交互能力。因此，作者希望通过引入基于神经科学原理的计算框架，弥合这一差距，推动Agentic Spatial Intelligence的发展。

Method: 作者通过审查空间神经模型和引入基于神经科学原理的计算框架，提出了六个关键计算模块，从而为agentic spatial reasoning的发展奠定基础。作者还使用框架指导分析了最近的方法，评估其与每个模块的相关性，并发现了阻碍更多基于神经科学的空间推理模块发展的关键缺口。进一步审查了新兴基准和数据集，探讨了各种潜在应用领域，最后提出了潜在的研究方向。

Result: 通过提出基于神经科学原理的计算框架，作者建立了六个关键计算模块，为agentic spatial reasoning的发展提供了新的途径。作者还从框架视角对最近的方法进行了分析，评估了它们与各模块的相关性，并识别了阻碍更多基于神经科学的空间推理模块发展的关键缺口。研究还探究了新兴基准和数据集，探索了各种潜在应用领域，提出了潜在的研究方向。

Conclusion: 这篇论文旨在弥合当前涉及自主空间智能的AI系统与人类空间智能之间的鸿沟，提出了一个基于神经科学原理的计算框架，以促进Agentic Spatial Intelligence在物理3D世界中的交互。通过审查空间神经模型，并引入一个新颖的计算框架，将核心生物功能映射到六个重要的计算模块，为agentic空间推理能力提供了一个全景视野，并评估最近方法的相关性，揭示阻碍更多基于神经科学的空间推理模块发展的关键缺口。研究还探讨了新兴基准和数据集，探索从虚拟到具体系统（如机器人）等潜在应用领域。最后，提出了潜在的研究方向，强调了可推广空间推理到动态或非结构化环境的有前途的路线图。希望这项工作能为研究社区提供一个基于神经科学的视角和结构化的路径，项目页面可在Github上找到。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [10] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 研究针对自主车辆安全规划中准确预测周围智能体的运动，提出了ProD策略，利用动态异质图场景建模渐进捕获未来场景中的社交互动演变，以提高智能体未来运动预测的准确性，取得了领先的性能成果。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法忽视未来场景中互动的动态演变这一限制，提出了新颖的ProD策略，旨在更全面地捕获未来场景中不断变化的社交互动，避免其内在不确定性，以实现更准确的智能体未来运动预测。

Method: 采用动态异质图场景建模设计渐进场景建模，利用因式化架构处理未来场景中的时空依赖关系并逐步消除多智能体未来运动的不确定性。同时结合多尺度解码过程用于改进未来场景建模和一致性智能体未来运动的预测。

Result: 在INTERACTION多智能体预测基准和Argoverse 2多世界预测基准上取得了最先进的性能表现，排名第一。

Conclusion: 提出了一种新的渐进多尺度解码策略ProD，结合动态异质图场景建模，用于捕获未来场景中不断演变的社交互动，提升未来场景建模和一致性预测。在INTERACTION多智能体预测基准和Argoverse 2多世界预测基准上取得了最先进的性能。

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [11] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 本文提出了基于区块链的分层架构，用于监管代理协作，在大规模代理生态系统中建立可信、弹性和可扩展的监管机制。设计了代理行为追踪和仲裁、动态声誉评估、恶意行为预测等关键模块。该方法为未来区块链启用的监管框架研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型-赋能的自主代理正在改变数字和物理环境，但其不可预测的行为和异构能力带来了重大治理和问责挑战。因此，本文旨在解决这些挑战，为大规模代理生态系统构建可信、弹性、可扩展的监管机制。

Method: 提出了基于区块链的分层架构，包括关键模块的设计，用于监管代理协作，以确保可信、弹性和可扩展的监管机制。

Result: 通过设计基于区块链的分层架构和关键监管模块，确立了可信、弹性、可扩展的监管机制基础，并为未来研究方向提出了讨论。

Conclusion: 本文提出了一种基于区块链的分层架构，用于监管代理协作，包括代理层、区块链数据层和监管应用层。设计了三个关键模块：(i)代理行为追踪和仲裁模块，用于自动问责；(ii)动态声誉评估模块，用于信任评估；(iii)恶意行为预测模块，用于早期检测对抗性活动。这种方法为大规模代理生态系统中可信、弹性和可扩展的监管机制建立了系统的基础。最后，讨论了区块链启用的监管框架在多代理系统中的未来研究方向。

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [12] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出了一个流水线，从Jupyter笔记本中提取高质量工具任务和可执行的多步解决方案，建立了NbQA数据集和Jupiter框架来增强多步推理能力。实验结果表明模型在NbQA数据集上表现优异，解决了大部分任务，并在多步推理任务中展示了改进的泛化能力和工具使用推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在复杂数据分析任务中仍然存在多步推理和工具使用方面的不足，限制了它们在复杂数据分析任务中的有效性。为了解决这一问题，提出了提取高质量基于工具的数据分析任务和解决方案的流水线，并引入了NbQA数据集和Jupiter框架以增强多步推理能力。

Method: 提出了可扩展的流水线从Jupyter笔记本和相关数据文件中提取任务和解决方案。介绍了NbQA数据集和Jupiter框架，将数据分析问题建模为搜索问题，并应用MCTS生成解决方案轨迹。在推理阶段，使用价值模型和节点访问计数来高效收集可执行的多步计划。进行了实验评估，并在NbQA数据集上展示了模型的性能。

Result: 实验证明，在NbQA数据集上，Qwen2.5-7B和14B-Instruct模型分别解决了77.82%和86.38%的InfiAgent-DABench任务，表现与或超过了GPT-4o和先进的代理框架。进一步评估显示在各种多步推理任务上有改进的泛化能力和更强的工具使用推理能力。

Conclusion: 介绍了一个可扩展的流水线，从Jupyter笔记本和相关数据文件中提取高质量的基于工具的数据分析任务及其可执行的多步解决方案。引入了NbQA，这是一个包含标准化任务-解决方案对的大规模数据集，反映了实际数据科学场景中的真实工具使用模式。通过提出的Jupiter框架，将数据分析表述为一个搜索问题，并应用蒙特卡洛树搜索(MCTS)生成不同的解决方案轨迹，以进行价值模型学习。在推理过程中，Jupiter结合价值模型和节点访问计数，以最小的搜索步骤高效地收集可执行的多步计划。实验结果表明，在NbQA上，Qwen2.5-7B模型和14B-Instruct模型分别解决了77.82%和86.38%的InfiAgent-DABench任务，与GPT-4o和先进的代理框架相匹配或超越。进一步的评估表明，在多样的多步推理任务中，有改进的泛化能力和更强的工具使用推理能力。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [13] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 该论文对构建知识图三元组的三种方法进行技术比较研究，结论显示OpenIE拥有最全面的三元组覆盖，而GraphRAG表现出最优越的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法在从简明文本中提取基于事实和局部上下文的信息方面表现出色，但在处理复杂、广泛文本的主题和整体理解时存在局限性，需要对文本和上下文进行更深入的分析。

Method: 该论文提出了三种不同方法论的综合技术比较研究，用于构建知识图三元组并将它们与大型语言模型（LLMs）集成用于问答系统。

Result: 实验结果表明，OpenIE提供了最全面的三元组覆盖，GraphRAG在三种方法中展示了最优越的推理能力。

Conclusion: 该论文通过实验结果表明，OpenIE提供了最全面的三元组覆盖，GraphRAG在三种方法中展示了最优越的推理能力。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [14] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 本文探讨了如何利用MCTS衍生的轨迹来提高偏好强化学习中的策略优化。通过研究结构化的优势估计，发现其能够稳定更新并更好地反映组合理由的质量。然而，仍然存在挑战，提出了一些启发式和统计解决方案。


<details>
  <summary>Details</summary>
Motivation: 受到在大语言模型中使用MCTS生成高质量中间轨迹的启发，本文探索了如何将MCTS衍生的轨迹用于改善偏好强化学习中的策略优化。特别关注了GRPO算法，介绍了一种不需要价值网络的偏好一致性策略学习方法。

Method: 本文采用了分阶段的GRPO训练范式，通过部分展示的MCTS模拟完成项来引入新颖的树形设置，用于优势估计。在理论和实证分析中，研究了一类丰富的前缀条件奖励信号。提出了启发式和统计解决方案以应对优势饱和和奖励信号崩溃等挑战。

Result: 通过研究结构化的优势估计，证明了其能够稳定更新并更好地反映组合理由的质量。然而，仍然存在挑战，如优势饱和和奖励信号崩溃。提出了启发式和统计解决方案以帮助应对这些问题。

Conclusion: 本文探讨了如何利用MCTS衍生的轨迹来改善基于偏好的强化学习中的策略优化。提出了分阶段的GRPO训练范式，引入了从部分展示的MCTS模拟中得出的完成项，为优势估计引入了一种新颖的树形设置。研究表明，结构化的优势估计可以稳定更新，并更好地反映组合理由的质量。然而，挑战仍然存在，例如优势饱和和奖励信号崩溃。提出了启发式和统计解决方案以缓解这些问题，并讨论了在分阶段或类似树状奖励结构下学习的开放挑战。

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [15] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: 本文提出了LightAgent框架，一个轻量级但功能强大的智能框架，解决了现有框架中灵活性和简洁性之间的权衡问题，使开发者能够轻松构建自学习代理人。框架集成了记忆、工具和思维树等核心功能，保持了极其轻量级的结构，并与主流聊天平台无缝集成。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，多Agent系统在各种应用场景取得重大进展，但在设计多功能、稳健和高效的代理部署平台方面仍存在挑战。

Method: 提出了LightAgent框架，集成了记忆、工具和思维树等核心功能，保持了极其轻量级的结构。

Result: 发布了完全开源的LightAgent框架，并在GitHub上提供了下载链接。

Conclusion: LightAgent是一个轻量级而强大的智能框架，解决了现有框架中灵活性和简洁性之间的权衡问题，能够与主流聊天平台无缝集成，帮助开发者快速构建自学习代理人。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [16] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 研究了在各种锦标赛规则下提供获胜者出现的认证解释的问题，确定了最小支持的大小，并展示了计算它们的多项式时间算法。最终表明最小支持可以用于产生简洁、认证和直观的解释。


<details>
  <summary>Details</summary>
Motivation: 这项研究旨在探讨为何获胜者在锦标赛中胜出的认证解释问题，这是形式可解释人工智能中的核心概念。

Method: 识别了候选人在子锦标赛中赢得胜利的最小支持，研究了常见的锦标赛解决方案，并确定了除加权未覆盖集外的所有规则的最小支持的大小。还展示了计算这些最小支持的多项式时间算法。

Result: 确定了最小支持的大小，并提出了计算它们的多项式时间算法，展示了如何利用最小支持产生简洁、认证和直观的解释。

Conclusion: 研究了在各种锦标赛规则下提供获胜者出现的认证解释的问题，确定了最小支持的大小，并展示了计算它们的多项式时间算法。最终表明最小支持可以用于产生简洁、认证和直观的解释。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [17] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 这篇论文研究了三个空间协调维度如何影响团队绩效，发现空间专业化对团队表现有积极影响，适应性空间接近具有边缘倒U形关系。高绩效团队和低绩效团队在这些指标的时间动态上有所区别，为隐性空间协调提供了见解，突出了平衡自适应策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 在没有机会进行明确沟通的快节奏决策环境中，团队协作至关重要。已有文献广泛探讨了隐性协调，但大部分关注于同地点、同步的团队合作（如体育队）或在分布式团队中主要关注知识工作的协调。但是，许多团队（消防队员、军队、执法部门、应急响应）必须在物理空间中协调移动，没有视觉线索或广泛明确沟通的好处。

Method: 本文通过分析34个四人团队（136名参与者）在一项搜索和救援任务中担任专业角色的数据，研究了三个空间协调维度（即探索多样性、移动专业化和适应性空间接近）如何影响团队绩效。

Result: 研究结果显示，空间专业化积极地预测了团队表现，而适应性空间接近表现出一种边缘倒U形的关系，表明适度的适应水平是最佳的。此外，这些指标的时间动态在区分高绩效团队和低绩效团队方面起到了作用。

Conclusion: 研究结果表明，空间专业化积极地预测了团队表现，而适应性空间接近表现出一种边缘倒U形的关系，表明适度的适应水平是最佳的。此外，这些指标的时间动态在区分高绩效团队和低绩效团队方面起到了作用。这些发现提供了关于角色为基础的团队工作中的隐性空间协调的见解，并强调平衡自适应策略的重要性，对培训和基于人工智能的团队支持系统具有重要意义。

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [18] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench is a new benchmark for evaluating large language model-based agents in end-to-end machine learning tasks. It includes innovative features like task acquisition automation, difficulty modeling, and multi-dimensional evaluation. The benchmark offers three subsets of different sizes, with the Lite version providing a practical testbed for daily benchmarking.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating large language model-based agents lack task coverage, domain diversity, and evaluation rigor. TAM Bench aims to address these limitations by providing a diverse, realistic, and structured benchmark for evaluating LLM-based agents on end-to-end ML tasks.

Method: The paper presents TAM Bench, a benchmark that collects and structures ML challenges automatically from platforms like Kaggle, AIcrowd, and Biendata. It includes a difficulty modeling mechanism based on participant counts and score dispersion, along with a multi-dimensional evaluation framework.

Result: The paper constructs three benchmark subsets of different sizes (Lite, Medium, and Full) based on 150 curated AutoML tasks. The Lite version with 18 tasks offers a balanced coverage across modalities and difficulty levels, serving as a practical testbed for benchmarking and comparative studies.

Conclusion: TAM Bench is introduced as a diverse and structured benchmark for evaluating large language model-based agents on end-to-end machine learning tasks. It includes innovative features such as task acquisition system, difficulty modeling, and multi-dimensional evaluation framework.

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [19] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 本文提出了一种专门设计用于资源高效的语义探索的深度强化学习架构，通过视觉语言模型(VLM)和分层奖励函数的集成解决了传统强化学习方法在平衡探索效率和语义理解方面的挑战。实验结果表明，该代理实现了显着提高的物体发现率、有效导航到语义丰富区域的能力，并具有策略性地选择何时请求外部环境信息的能力。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于解决传统强化学习方法在平衡探索效率和语义理解方面遇到的挑战。由于代理的小策略中嵌入的认知能力有限，传统RL方法往往在处理语义探索时难以平衡有效探索和语义理解。因此，本研究致力于提供一种特定设计的DRL架构，以实现资源高效的语义探索。

Method: 本文采用了一种深度强化学习架构，通过集成视觉语言模型和分层奖励函数的方法，同时结合课程学习策略，指导不同复杂级别的学习，以确保稳健和稳定的学习过程。关键方法论贡献是将VLM查询建模为专用动作，使代理能够在必要时策略性地查询VLM以获得外部指导，从而节约资源。

Result: 实验评估结果表明，代理实现了显着提高的物体发现率，并有效地导航到语义丰富区域的能力。此外，代理还表现出了在何时提示外部环境信息的策略性掌握。

Conclusion: 本文提出了一种专门设计用于资源高效的语义探索的深度强化学习(DRL)架构，通过视觉语言模型(VLM)和分层奖励函数的集成解决了传统强化学习方法在平衡探索效率和语义理解方面的挑战。实验结果表明，该代理实现了显着提高的物体发现率，并有效地导航到具有语义丰富区域的能力。本研究为在机器人领域实现全智能和自主导航提供了一种实用和可扩展的方法。

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [20] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: 本研究介绍了Template-Oriented Reasoning (TORSO)方法，通过引导大型语言模型利用内在推理能力生成回应，避免依赖提供的少样本示例。实验证明TORSO在多样的LLMs基准任务中表现出色且具有合理的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用少样本提示生成回应，但依赖提供的示例，限制了模型固有推理能力的利用，且构建任务特定的少样本提示成本高且可能导致不一致性。因此，提出TORSO方法引导模型充分利用内在推理能力，实现跨任务生成适当回应。

Method: 提出了Template-Oriented Reasoning (TORSO)方法，引导模型利用内在推理能力生成回应，避免依赖提供的少样本示例并避免构建任务特定的少样本示例。

Result: 实验证明TORSO在各种LLMs基准任务中取得强大表现，具有合理的推理过程。

Conclusion: 介绍了一种名为Template-Oriented Reasoning (TORSO)的方法，通过引导大型语言模型( Large Language Models，LLMs)利用内在推理能力，在各种任务中生成适当的回应，而无需手动创建少样本示例。实验证明TORSO在多样的LLMs基准任务中表现出色且具有合理的原理。

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [21] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 该技术报告分析了在法律领域中LLMs中“幻觉”（虚假信息）的挑战，探讨了其原因、表现形式以及RAG减轻策略的有效性，提出优化建议。强调人类监督不可替代的重要性，主张采用“咨询式”AI范式，以保证真实性和可追溯性，作为增强而非取代专业判断的工具。


<details>
  <summary>Details</summary>
Motivation: 研究法律领域中LLMs中“幻觉”的挑战，探索其对法律应用的影响，寻找减轻策略并提出优化方案。关注伦理和监管问题，强调人类监督的不可或缺性。

Method: 分析幻觉在LLMs中的挑战，探查诱因、表现和RAG减轻策略的有效性，提出优化建议，并探讨伦理和监管方面的影响。强调人类监督的重要性，主张采用“咨询式”AI范式，以保证真实性和可追溯性，并作为专业判断的增强工具。

Result: 提出了在法律领域中解决LLMs中“幻觉”挑战的综合优化方案，强调采用“咨询式”AI范式以加强专业判断不被替代的重要性。

Conclusion: 该技术报告分析了法律领域中LLMs中“幻觉”（虚假信息）的挑战，探讨了其原因、表现形式以及RAG减轻策略的有效性，突出其局限性并提出全面的优化建议。论文探讨了伦理和监管方面的影响，强调人类监督作为不可替代的角色。结论是解决方法不在于逐步改进生成模型，而在于采用一种“咨询式”人工智能范式，优先考虑真实性和可追溯性，作为增强而非取代专业判断的工具。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [22] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM is a framework for efficient memory management in multi-agent systems. It actively optimizes memory through verifiable write admission, self-scheduling, and knowledge diffusion. It improves reasoning accuracy, reduces overhead, and supports knowledge transfer across tasks.


<details>
  <summary>Details</summary>
Motivation: Efficient memory management is crucial for performance and scalability in long-term multi-agent systems. Existing methods have limitations such as noise accumulation, memory expansion, and domain-specific generalization.

Method: SEDM integrates verifiable write admission, self-scheduling memory controller, and cross-domain knowledge diffusion. It transforms memory into an active, self-optimizing component.

Result: SEDM outperforms strong memory baselines in reasoning accuracy and token overhead reduction. It facilitates knowledge transfer from fact verification to enhance multi-hop reasoning.

Conclusion: SEDM is a scalable and sustainable memory mechanism for open-ended multi-agent collaboration. It improves reasoning accuracy, reduces token overhead, and enables knowledge transfer across heterogeneous tasks.

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [23] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 研究旨在解决当前AI工具中缺乏的符合性泛化问题。提出使用量子模型训练变分量子电路来学习张量模型表征，取得了良好概念验证结果。使用MHE编码获得较好性能，CLIP图像向量表现优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 由于当前人工智能工具如视觉-语言模型缺乏符合性泛化能力，之前的工作尝试使用张量句子语义来克服这一挑战，但结果不理想。本研究猜测量子模型的提升训练效率将提高这些任务的性能。

Method: 该研究解释了符合性张量模型在希尔伯特空间中的表示，并通过训练变分量子电路学习这些表示。在图像字幕任务中使用了两种图像编码技术：二进制图像向量的多热编码（MHE）和从视觉-语言模型CLIP获取的图像向量的角度/振幅编码。

Result: 使用量子模型训练变分量子电路学习符合性张量模型的表征，在图像字幕任务上取得了良好的概念验证结果。MHE编码取得了较好性能，而CLIP图像向量的表现虽不稳定，但仍优于经典模型。

Conclusion: 此文献提出使用量子模型训练变分量子电路来学习符合性张量模型在图像字幕任务中的表征，取得了良好的概念验证结果。在使用含噪声的MHE编码时，取得了较好的性能。而在使用CLIP图像向量时表现不稳定，但仍优于经典符合性模型。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [24] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras algorithm optimizes embodied AI agent inference frequency by separating perception and generation modules and implementing controlled pipeline parallelism. It enhances throughput by 2.54x on average while preserving 102.7% of original accuracy, addressing data staleness and demonstrating efficacy in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Traditional sequential computation patterns have limitations in achieving the required 'thinking' frequency for real-world applications in dynamic environments. There is a need for seamless integration of perception and generation modules for high-frequency input and output demands in embodied AI systems.

Method: Auras algorithm-system co-designed inference framework optimizes inference frequency by disaggregating perception and generation modules and providing controlled pipeline parallelism. It addresses the data staleness problem through a public context for sharing between perception and generation.

Result: Experimental results show that Auras significantly improves throughput while maintaining high accuracy, indicating its effectiveness in overcoming the constraints of sequential computation.

Conclusion: Auras improves throughput by 2.54x on average while maintaining 102.7% of the original accuracy, demonstrating its efficacy in optimizing inference frequency of embodied AI agents.

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [25] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 这项研究探讨了大型语言模型（LLMs）持续扩展是否会带来递减收益的问题。作者发现单步准确性的边际增益可以使模型成功完成任务的长度呈指数级改善。通过提供解决长期任务所需的知识和计划来孤立执行能力，大型模型能够在单个转换中执行更多转以及更长的任务。研究结果还观察到模型的每步准确性随着步数增加而降低，部分原因是自我调节效应。最后，作者评估了前沿思考模型在单个转换中执行任务长度的水平。


<details>
  <summary>Details</summary>
Motivation: 研究动机来自于一种简单却反直觉的观察，即单步准确性的边际增益如何影响模型成功完成长期任务的长度。作者希望通过研究解决LLMs在简单任务变长时出现失败的原因，提出解决方案来强调执行能力的重要性。通过提供必要的知识和计划来帮助模型解决长期任务，作者希望能够解决关于LLMs如何解决复杂问题但在简单任务上失败的问题，并强调扩展模型大小和顺序测试时间计算对长期任务的重要性。

Method: 作者通过观察单步准确性的边际增益如何影响模型成功完成长期任务的长度，提出了孤立执行能力的方法。他们提出通过明确提供解决长期任务所需的知识和计划来分离执行能力。研究结果显示，即使小型模型具有100%单步准确性，大型模型也能正确执行更多转换。作者还观察到模型的每步准确性随着步数增加而降低，部分原因是自我调节效应，即模型在包含先前转换中错误的上下文中更容易出错。最后，作者评估了前沿思考模型在单个转换中执行任务长度的能力。

Result: 研究发现大型语言模型（LLMs）的持续扩展并不一定会带来增长的收益，而是可能出现递减收益。通过孤立执行能力以及提供解决长期任务所需的知识和计划，大型模型能够在单个转换中执行更多转以及更长的任务。此外，作者观察到模型的每步准确性会随着步数增加而降低，部分原因是自我调节效应。最后，研究还评估了前沿思考模型在单个转换中执行任务长度的水平。

Conclusion: 大型语言模型（LLM）的持续扩展会带来递减收益。执行能力对模型成功完成长期任务至关重要。作者发现单步准确性的边际增益可以使模型成功完成任务的长度呈指数级改善。通过提供解决长期任务所需的知识和计划来孤立执行能力，大型模型即使在小型模型具有100%单步准确性的情况下，也可以正确执行更多转换。随着步数的增加，模型的每步准确性会降低，这不仅仅是由于长上下文的限制，还观察到自我调节效应--当上下文包含先前转换中的错误时，模型更有可能出现错误。自我调节并不仅通过扩展模型尺寸来减少。与此相反，最近的思考模型不会自我调节，并且也可以在单个转换中执行更长的任务。最后，作者评估了前沿思考模型在单个转换中执行任务长度的水平。通过着眼于执行能力，希望能够弥合在LLMs如何解决复杂推理问题但面对试图延长的简单任务时失败的争论，并突出扩展模型大小和顺序测试时间计算对长期任务的巨大益处。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>
