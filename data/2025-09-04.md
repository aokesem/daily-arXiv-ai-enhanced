<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis](https://arxiv.org/abs/2509.02650)
*Henrique Correia da Fonseca,António Fernandes,Zhao Song,Theodor Cimpeanu,Nataliya Balabanova,Adeela Bashir,Paolo Bova,Alessio Buscemi,Alessandro Di Stefano,Manh Hong Duong,Elias Fernandez Domingos,Ndidi Bianca Ogbo,Simon T. Powers,Daniele Proverbio,Zia Ush Shamszaman,Fernando P. Santos,The Anh Han,Marcus Krellner*

Main category: cs.AI

TL;DR: 研究通过进化博弈论探讨媒体报道对促进AI创作者和用户合作的影响，发现媒体报道在引导AI安全方面具有潜在作用，但依赖于信息质量和成本因素。


<details>
  <summary>Details</summary>
Motivation: 研究AI创作者在利润与用户安全之间的选择，探讨媒体报道可能对其决策产生的影响和如何推动AI技术的广泛应用。

Method: 通过进化博弈论研究人工智能创作者和用户之间合作的影响，模拟了以自利为基础的人工群体。

Result: 研究发现，媒体报道能促进AI创作者和用户之间的合作，但存在信息质量和成本限制。

Conclusion: 媒体报道对AI创作者和用户之间合作具有推动作用，但存在依赖信息质量和成本高低的限制。

Abstract: When developers of artificial intelligence (AI) products need to decide
between profit and safety for the users, they likely choose profit.
Untrustworthy AI technology must come packaged with tangible negative
consequences. Here, we envisage those consequences as the loss of reputation
caused by media coverage of their misdeeds, disseminated to the public. We
explore whether media coverage has the potential to push AI creators into the
production of safe products, enabling widespread adoption of AI technology. We
created artificial populations of self-interested creators and users and
studied them through the lens of evolutionary game theory. Our results reveal
that media is indeed able to foster cooperation between creators and users, but
not always. Cooperation does not evolve if the quality of the information
provided by the media is not reliable enough, or if the costs of either
accessing media or ensuring safety are too high. By shaping public perception
and holding developers accountable, media emerges as a powerful soft regulator
-- guiding AI safety even in the absence of formal government oversight.

</details>


### [2] [The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)](https://arxiv.org/abs/2509.02661)
*Andrew Ferguson,Marisa LaFleur,Lars Ruthotto,Jesse Thaler,Yuan-Sen Ting,Pratyush Tiwary,Soledad Villar,E. Paulo Alves,Jeremy Avigad,Simon Billinge,Camille Bilodeau,Keith Brown,Emmanuel Candes,Arghya Chattopadhyay,Bingqing Cheng,Jonathan Clausen,Connor Coley,Andrew Connolly,Fred Daum,Sijia Dong,Chrisy Xiyu Du,Cora Dvorkin,Cristiano Fanelli,Eric B. Ford,Luis Manuel Frutos,Nicolás García Trillos,Cecilia Garraffo,Robert Ghrist,Rafael Gomez-Bombarelli,Gianluca Guadagni,Sreelekha Guggilam,Sergei Gukov,Juan B. Gutiérrez,Salman Habib,Johannes Hachmann,Boris Hanin,Philip Harris,Murray Holland,Elizabeth Holm,Hsin-Yuan Huang,Shih-Chieh Hsu,Nick Jackson,Olexandr Isayev,Heng Ji,Aggelos Katsaggelos,Jeremy Kepner,Yannis Kevrekidis,Michelle Kuchera,J. Nathan Kutz,Branislava Lalic,Ann Lee,Matt LeBlanc,Josiah Lim,Rebecca Lindsey,Yongmin Liu,Peter Y. Lu,Sudhir Malik,Vuk Mandic,Vidya Manian,Emeka P. Mazi,Pankaj Mehta,Peter Melchior,Brice Ménard,Jennifer Ngadiuba,Stella Offner,Elsa Olivetti,Shyue Ping Ong,Christopher Rackauckas,Philippe Rigollet,Chad Risko,Philip Romero,Grant Rotskoff,Brett Savoie,Uros Seljak,David Shih,Gary Shiu,Dima Shlyakhtenko,Eva Silverstein,Taylor Sparks,Thomas Strohmer,Christopher Stubbs,Stephen Thomas,Suriyanarayanan Vaikuntanathan,Rene Vidal,Francisco Villaescusa-Navarro,Gregory Voth,Benjamin Wandelt,Rachel Ward,Melanie Weber,Risa Wechsler,Stephen Whitelam,Olaf Wiest,Mike Williams,Zhuoran Yang,Yaroslava G. Yingling,Bin Yu,Shuwen Yue,Ann Zabludoff,Huimin Zhao,Tong Zhang*

Main category: cs.AI

TL;DR: 该论文总结了关于人工智能（AI）和数学物理科学（MPS）之间关系的观点，强调了加强AI与科学联系的重要性，提出了促进AI+MPS研究、建立跨学科研究社区和推动AI在MPS研究和教育中的发展的活动和战略重点。最终为资助机构、教育机构和个人研究者提供了指导，以使MPS社区充分利用AI+MPS的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了理解MPS领域如何最好地利用和贡献于未来的人工智能，探讨AI和MPS之间联系的紧密性，并提出行动和战略以加强AI与科学之间的联系。

Method: 通过召开NSF关于AI和MPS未来的研讨会，汇总了MPS社区对AI和科学未来关系的见解，强调了加强AI与科学联系的关键性。提出了促进AI+MPS研究的活动和战略重点，包括促进跨学科社区建设和加强AI在MPS研究和教育方面的发展。

Result: 总结了MPS社区的观点，并提出了活动和战略重点，以促进AI+MPS研究、建立跨学科研究社区以及推动AI在MPS研究和教育中的发展。最后，为资助机构、教育机构和个人研究者提供了建议，以帮助MPS社区充分利用AI+MPS的潜力。

Conclusion: 该论文总结了关于人工智能（AI）和数学物理科学（MPS）之间关系的观点，强调了在当前迅速发展的领域中加强AI与科学之间联系的重要性。提出了一些建议活动和战略重点，包括促进AI+MPS研究、建立跨学科研究社区以及加强AI在MPS研究和教育方面的发展。最后，总结了对资助机构、教育机构和个人研究者的建议，以帮助MPS社区在AI+MPS的转变潜力方面处于领先地位。

Abstract: This community paper developed out of the NSF Workshop on the Future of
Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),
which was held in March 2025 with the goal of understanding how the MPS domains
(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)
can best capitalize on, and contribute to, the future of AI. We present here a
summary and snapshot of the MPS community's perspective, as of Spring/Summer
2025, in a rapidly developing field. The link between AI and MPS is becoming
increasingly inextricable; now is a crucial moment to strengthen the link
between AI and Science by pursuing a strategy that proactively and thoughtfully
leverages the potential of AI for scientific discovery and optimizes
opportunities to impact the development of AI by applying concepts from
fundamental science. To achieve this, we propose activities and strategic
priorities that: (1) enable AI+MPS research in both directions; (2) build up an
interdisciplinary community of AI+MPS researchers; and (3) foster education and
workforce development in AI for MPS researchers and students. We conclude with
a summary of suggested priorities for funding agencies, educational
institutions, and individual researchers to help position the MPS community to
be a leader in, and take full advantage of, the transformative potential of
AI+MPS.

</details>


### [3] [Planning with Reasoning using Vision Language World Model](https://arxiv.org/abs/2509.02722)
*Delong Chen,Theo Moutakanni,Willy Chung,Yejin Bang,Ziwei Ji,Allen Bolourchi,Pascale Fung*

Main category: cs.AI

TL;DR: 论文介绍了Vision Language World Model（VLWM），该模型在语言世界建模方面取得了显著成就，实现了最先进的视觉规划辅助（VPA）性能，在不同基准测试上都表现出色，超越了强大的VLM基线模型。


<details>
  <summary>Details</summary>
Motivation: 有效的规划需要强大的世界模型，高层次的世界模型对具有语义和时间抽象的行动理解和推理仍然不够发达。

Method: 介绍了Vision Language World Model (VLWM)，该模型学习行为策略和动力学模型，通过成本最小化促进反应性System-1计划解码和反思性System-2规划。VLWM利用Tree of Captions表示的压缩未来观察结果，通过迭代LLM Self-Refine提取目标，并使用自监督训练的评论模型衡量动作对目标的语义距离。

Result: VLWM在视觉规划辅助（VPA）性能方面取得了最先进的成绩，在benchmark评估和提出的PlannerArena人类评估中表现出色。System-2在Elo分数上比System-1提高了+27%。

Conclusion: 引入了Vision Language World Model (VLWM)，这是一个为基于自然视频的语言世界建模而训练的基础模型。VLWM实现了最先进的视觉规划辅助(VPA)性能，超越了强大的VLM基准线，在RoboVQA和WorldPrediction基准评估上表现优异。

Abstract: Effective planning requires strong world models, but high-level world models
that can understand and reason about actions with semantic and temporal
abstraction remain largely underdeveloped. We introduce the Vision Language
World Model (VLWM), a foundation model trained for language-based world
modeling on natural videos. Given visual observations, the VLWM first infers
the overall goal achievements then predicts a trajectory composed of
interleaved actions and world state changes. Those targets are extracted by
iterative LLM Self-Refine conditioned on compressed future observations
represented by Tree of Captions. The VLWM learns both an action policy and a
dynamics model, which respectively facilitates reactive system-1 plan decoding
and reflective system-2 planning via cost minimization. The cost evaluates the
semantic distance between the hypothetical future states given by VLWM
roll-outs and the expected goal state, and is measured by a critic model that
we trained in a self-supervised manner. The VLWM achieves state-of-the-art
Visual Planning for Assistance (VPA) performance on both benchmark evaluations
and our proposed PlannerArena human evaluations, where system-2 improves the
Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM
baselines on RoboVQA and WorldPrediction benchmark.

</details>


### [4] [Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics](https://arxiv.org/abs/2509.02751)
*Matthew Russo,Tim Kraska*

Main category: cs.AI

TL;DR: 研究者建立了一个原型，使深度研究代理能够编写和执行优化的语义操作程序，并展示了在两个基本查询上的优越性能。相较于标准深度研究代理，原型的F1分数提高了最高达1.95倍，并实现了高达76.8%和72.7%的成本和运行时间节省。


<details>
  <summary>Details</summary>
Motivation: AI驱动的分析需要将语义操作的优化执行与深度研究系统更灵活和动态执行的特性相结合。

Method: 搭建了一个允许深度研究代理编写和执行优化语义操作程序的原型，并对其进行评估。

Result: 建立的原型能够优于手工制作的语义操作程序和开放式深度研究系统，且在两个基本查询上展示出更好的性能。

Conclusion: 研究者建立了一种原型，使深度研究代理能够编写和执行优化的语义操作程序，并在两个基本查询上展示其技术的优越性。相较于标准的开放式深度研究代理，该原型的F1分数提高了最高达1.95倍。此外，即使给予代理对语义操作的访问权，该原型仍可通过其优化执行实现高达76.8%和72.7%的成本和运行时间节省。

Abstract: With advances in large language models (LLMs), researchers are creating new
systems that can perform AI-driven analytics over large unstructured datasets.
Recent work has explored executing such analytics queries using semantic
operators -- a declarative set of AI-powered data transformations with natural
language specifications. However, even when optimized, these operators can be
expensive to execute on millions of records and their iterator execution
semantics make them ill-suited for interactive data analytics tasks. In another
line of work, Deep Research systems have demonstrated an ability to answer
natural language question(s) over large datasets. These systems use one or more
LLM agent(s) to plan their execution, process the dataset(s), and iteratively
refine their answer. However, these systems do not explicitly optimize their
query plans which can lead to poor plan execution. In order for AI-driven
analytics to excel, we need a runtime which combines the optimized execution of
semantic operators with the flexibility and more dynamic execution of Deep
Research systems. As a first step towards this vision, we build a prototype
which enables Deep Research agents to write and execute optimized semantic
operator programs. We evaluate our prototype and demonstrate that it can
outperform a handcrafted semantic operator program and open Deep Research
systems on two basic queries. Compared to a standard open Deep Research agent,
our prototype achieves up to 1.95x better F1-score. Furthermore, even if we
give the agent access to semantic operators as tools, our prototype still
achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its
optimized execution.

</details>


### [5] [Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving](https://arxiv.org/abs/2509.02754)
*Mingyi Wang,Jingke Wang,Tengju Ye,Junbo Chen,Kaicheng Yu*

Main category: cs.AI

TL;DR: 本研究通过评估五个关键的大型语言模型模块，在Waymo Sim Agents基准测试上展示了这些模块对自动驾驶行为生成性能的显著提升。适当调整后的模块可显著改善性能，为将大型语言模型应用于自动驾驶领域提供重要参考。


<details>
  <summary>Details</summary>
Motivation: 尽管在自动驾驶行为生成领域已经有一些有前景的尝试，但对于哪些大型语言模型模块是可以真正转移的系统化理解还不足。因此，本研究旨在填补这一空白，探讨适用于自动驾驶的大型语言模型模块，以提高自动驾驶行为生成的性能。

Method: 通过对五个关键的大型语言模型模块进行综合评估，在Waymo Sim Agents基准测试上进行了大量实验，展示了这些模块在适当情况下对自动驾驶行为生成性能的显著提升。

Result: 在Waymo Sim Agents基准测试上，研究展示了适当调整后的大型语言模型模块可以显著改善自动驾驶行为生成性能，并确定了哪些技术可以有效转移，分析了其他技术失败的潜在原因，并讨论了自动驾驶场景中所需的具体适应性。在Sim Agents任务上评估了该方法，并取得了竞争性的结果。

Conclusion: 研究表明，在自动驾驶行为生成方面，合适的大型语言模型模块的运用可以显著提高性能，这为将大型语言模型应用于自动驾驶领域提供了重要参考。

Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced
natural language processing but also inspired their application in domains with
structurally similar problems--most notably, autonomous driving motion
generation. Both domains involve autoregressive sequence modeling, token-based
representations, and context-aware decision making, making the transfer of LLM
components a natural and increasingly common practice. However, despite
promising early attempts, a systematic understanding of which LLM modules are
truly transferable remains lacking. In this paper, we present a comprehensive
evaluation of five key LLM modules--tokenizer design, positional embedding,
pre-training paradigms, post-training strategies, and test-time
computation--within the context of motion generation for autonomous driving.
Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate
that, when appropriately adapted, these modules can significantly improve
performance for autonomous driving motion generation. In addition, we identify
which techniques can be effectively transferred, analyze the potential reasons
for the failure of others, and discuss the specific adaptations needed for
autonomous driving scenarios. We evaluate our method on the Sim Agents task and
achieve competitive results.

</details>


### [6] [Plan Verification for LLM-Based Embodied Task Completion Agents](https://arxiv.org/abs/2509.02761)
*Ananth Hariharan,Vardhan Dongre,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: 本研究提出了一个基于大型语言模型（LLM）的任务规划迭代验证框架，通过判定LLM对动作序列进行批判和修订，得到更清晰和空间连贯的轨迹。在TEACh数据集上实验，四种LLM模型达到高召回率和精度。改进循环快速，提高效率和组织性，保留了人类错误恢复模式，为未来研究提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的任务规划存在不必要的动作、冗余导航和逻辑错误等问题，降低了策略质量。因此，本研究旨在提出一个迭代验证框架，通过判定LLM对动作序列进行批判，计划者LLM应用修订，希望得到更加清洁和空间连贯的轨迹。此方法与基于规则的方法不同，依赖自然语言提示，可广泛泛化，并解决包括不必要的动作、矛盾和缺失步骤等错误类型。

Method: 提出了一个基于大型语言模型（LLM）的任务规划和人类演示的迭代验证框架，由判定LLM对动作序列进行批判，计划者LLM应用修订。该框架利用自然语言提示，实现了对错误类型的广泛泛化，包括不必要的动作、冗余导航、逻辑错误等。在TEACh体验智能数据集上进行实验，评估了四种最先进的LLM模型（GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout）的性能表现。改进循环快速收敛，提高了时间效率和空间动作组织。方法保留了人类错误恢复模式，为未来研究提供了支持。

Result: 在TEACh体验智能数据集上，框架实现了高水平的召回率和精度，四种LLM模型的性能表现显示出潜力。改进循环迅速收敛，提高了时间效率和空间动作组织。方法保留了人类错误恢复模式，为未来研究提供了支持。

Conclusion: 提出了一个基于大型语言模型（LLM）的任务规划和人类演示的迭代验证框架，通过判定LLM对动作序列进行批判，计划者LLM应用修订，产生逐渐更加清洁和空间连贯的轨迹。相比基于规则的方法，我们的方法依赖自然语言提示，实现了对错误类型的广泛泛化，包括不必要的动作、冗余导航、逻辑错误等。在TEACh体验智能（embodied AI）数据集的一组人工标记动作上，我们的框架在四种最先进的LLM模型（GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout）中实现了高达90%的召回率和100%的精度。改进循环迅速收敛，96.5%的序列最多需要三次迭代，同时提高了时间效率和空间动作组织。这一方法保留了人类错误恢复模式而非将其合并，支持未来关于鲁棒纠错行为的研究。通过确立计划验证作为空间规划和动作改进的可靠LLM能力，为提高体验智能中模仿学习的高质量训练数据提供了可扩展的途径。

Abstract: Large language model (LLM) based task plans and corresponding human
demonstrations for embodied AI may be noisy, with unnecessary actions,
redundant navigation, and logical errors that reduce policy quality. We propose
an iterative verification framework in which a Judge LLM critiques action
sequences and a Planner LLM applies the revisions, yielding progressively
cleaner and more spatially coherent trajectories. Unlike rule-based approaches,
our method relies on natural language prompting, enabling broad generalization
across error types including irrelevant actions, contradictions, and missing
steps. On a set of manually annotated actions from the TEACh embodied AI
dataset, our framework achieves up to 90% recall and 100% precision across four
state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).
The refinement loop converges quickly, with 96.5% of sequences requiring at
most three iterations, while improving both temporal efficiency and spatial
action organization. Crucially, the method preserves human error-recovery
patterns rather than collapsing them, supporting future work on robust
corrective behavior. By establishing plan verification as a reliable LLM
capability for spatial planning and action refinement, we provide a scalable
path to higher-quality training data for imitation learning in embodied AI.

</details>


### [7] [Key Principles in Cross-Domain Hyper-Heuristic Performance](https://arxiv.org/abs/2509.02782)
*Václav Sobotka,Lucas Kletzander,Nysret Musliu,Hana Rudová*

Main category: cs.AI

TL;DR: 本研究关注选择超启发式方法的集合组成和战略转换，通过转换，使简单的随机选择机制在挑战性领域中超越现有超启发式方法并发现新的最佳解。


<details>
  <summary>Details</summary>
Motivation: 现有的选择超启发式主要集中在从预定义集合中自适应选择低级启发式（LLHs），我们专注于这个集合的组成和战略转换。希望能够在不牺牲性能的前提下简化方法设计。

Method: 系统分析基于三个关键原则的转换：解决方案接受、LLH重复和扰动强度。通过适当构建的转换，让无偏随机选择机制胜过所有现有的超启发式方法，并发现了11个新的最佳解。

Result: 基于策略性转换的方法在挑战性真实领域中表现出色，发现新的最佳解并超越了当前最先进的方法，同时简化设计。

Conclusion: 通过对选择超启发式方法的策略性转换，我们的方法在三个具有挑战性的真实领域中表现优异，发现了11个新的最佳解，并与CHeSC竞赛的获胜者竞争。我们的方法在CHeSC基准和真实领域上胜过当前最先进的方法，同时经常简化设计。

Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on
problem-specific heuristic search algorithms into adaptable general-purpose
search strategies. In this respect, existing selection hyper-heuristics
primarily focus on an adaptive selection of low-level heuristics (LLHs) from a
predefined set. In contrast, we concentrate on the composition of this set and
its strategic transformations. We systematically analyze transformations based
on three key principles: solution acceptance, LLH repetitions, and perturbation
intensity, i.e., the proportion of a solution affected by a perturbative LLH.
We demonstrate the raw effects of our transformations on a trivial unbiased
random selection mechanism. With an appropriately constructed transformation,
this trivial method outperforms all available state-of-the-art hyper-heuristics
on three challenging real-world domains and finds 11 new best-known solutions.
The same method is competitive with the winner of the CHeSC competition,
commonly used as the standard cross-domain benchmark. Moreover, we accompany
several recent hyper-heuristics with such strategic transformations. Using this
approach, we outperform the current state-of-the-art methods on both the CHeSC
benchmark and real-world domains while often simplifying their designs.

</details>


### [8] [Learning General Policies From Examples](https://arxiv.org/abs/2509.02794)
*Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 本文提出了一种符号学习方法，基于采样计划的泛化，可处理数百万状态和数十万特征的问题。该方法确保结构终止和非循环性，不依赖于SAT/ASP方法，而是采用击中集算法。实验验证了该方法的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 近年来，人们开发了一种学习解决大量计划问题的通用策略的组合方法。相对于深度学习方法，其中一大优势在于产生的策略可被理解并证明是正确的。然而，存在的一个弱点是这些方法无法扩展，并且仅能从包含少量状态和特征信息的小型训练实例和特征集中学习。因此，我们提出了使用泛化采样计划的新符号学习方法。

Method: 本文提出了一种符号学习方法，基于采样计划的泛化，确保结构终止和非循环性。该方法采用了一个能够有效处理数百万状态和数十万特征信息的击中集算法。

Result: 研究了该方法的形式属性，并对其可扩展性进行了测试。实验表明，该方法能够处理拥有数百万状态和数十万特征的问题。

Conclusion: 本文提出了一种基于采样计划泛化的符号学习方法，以确保结构终止和非循环性。该方法不依赖于之前的符号方法中的SAT/ASP，而是基于一个能够有效处理拥有数百万状态和数十万特征的问题的击中集算法。研究了该方法的形式属性，并在多个基准测试中测试了其可扩展性。

Abstract: Combinatorial methods for learning general policies that solve large
collections of planning problems have been recently developed. One of their
strengths, in relation to deep learning approaches, is that the resulting
policies can be understood and shown to be correct. A weakness is that the
methods do not scale up and learn only from small training instances and
feature pools that contain a few hundreds of states and features at most. In
this work, we propose a new symbolic method for learning policies based on the
generalization of sampled plans that ensures structural termination and hence
acyclicity. The proposed learning approach is not based on SAT/ASP, as previous
symbolic methods, but on a hitting set algorithm that can effectively handle
problems with millions of states, and pools with hundreds of thousands of
features. The formal properties of the approach are analyzed, and its
scalability is tested on a number of benchmarks.

</details>


### [9] [Uncertainty-driven Adaptive Exploration](https://arxiv.org/abs/2509.03219)
*Leonidas Bakopoulos,Georgios Chalkiadakis*

Main category: cs.AI

TL;DR: 本文提出了一种通用的自适应探索框架，利用不确定性解决了在探索和利用之间切换时机的重要问题。实验结果表明，该框架的自适应探索策略在多个MuJoCo环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 对于需要学习长且复杂动作序列的领域，确定何时在探索和利用之间切换是至关重要的。为了解决这一问题，本文提出了一种框架，利用不确定性机制来处理这一重要问题。

Method: 本文提出了一种自适应探索框架，利用不确定性来确定在探索和利用之间切换的时机。框架可包含各种不确定性测量机制，如内在动机或认知不确定性。作者进行了实验来验证框架的有效性。

Result: 实验结果表明，本框架生成的自适应探索策略在多个MuJoCo环境中表现优异，优于标准方法。

Conclusion: 本文提出了一种通用的自适应探索框架，利用不确定性以原则性的方式解决了在探索和利用之间切换的重要问题。实验表明，我们的框架产生的自适应探索策略在多个MuJoCo环境中优于标准方法。

Abstract: Adaptive exploration methods propose ways to learn complex policies via
alternating between exploration and exploitation. An important question for
such methods is to determine the appropriate moment to switch between
exploration and exploitation and vice versa. This is critical in domains that
require the learning of long and complex sequences of actions. In this work, we
present a generic adaptive exploration framework that employs uncertainty to
address this important issue in a principled manner. Our framework includes
previous adaptive exploration approaches as special cases. Moreover, we can
incorporate in our framework any uncertainty-measuring mechanism of choice, for
instance mechanisms used in intrinsic motivation or epistemic uncertainty-based
exploration methods. We experimentally demonstrate that our framework gives
rise to adaptive exploration strategies that outperform standard ones across
several MuJoCo environments.

</details>


### [10] [Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making](https://arxiv.org/abs/2509.03286)
*Prachi Bagave,Marcus Westberg,Marijn Janssen,Aaron Yi Ding*

Main category: cs.AI

TL;DR: 该论文探讨了AI在医疗领域中的责任问题，提出了一个新的责任框架和三层结构，强调了责任的重要性以及促进协作的必要性。


<details>
  <summary>Details</summary>
Motivation: 由于对AI责任问题的担忧不断增加以及对该术语的模糊性，作者希望填补现有指导中缺乏的具体实施细节，就AI在医疗领域中的责任问题提出新观点和引导性框架。

Method: 通过广泛的分析，对责任概念进行了界定，制定了责任框架，并提出了适用于处理各种责任机制的三层结构。

Result: 提出了一个建立在医疗AI系统监管和参与者机制之上的一致性责任体制，通过三层结构指导医疗AI系统的参与者管理各种责任机制。同时，强调了卫生保健AI决策中的共同依赖性和责任共担，以促进合作。

Conclusion: 该论文通过对AI在医疗领域的作用进行深入分析，提出了应对AI决策中的责任问题的新观点和框架，强调了卫生保健AI系统中责任的重要性，以及如何建立一致的责任体制。

Abstract: AI is transforming the healthcare domain and is increasingly helping
practitioners to make health-related decisions. Therefore, accountability
becomes a crucial concern for critical AI-driven decisions. Although regulatory
bodies, such as the EU commission, provide guidelines, they are highlevel and
focus on the ''what'' that should be done and less on the ''how'', creating a
knowledge gap for actors. Through an extensive analysis, we found that the term
accountability is perceived and dealt with in many different ways, depending on
the actor's expertise and domain of work. With increasing concerns about AI
accountability issues and the ambiguity around this term, this paper bridges
the gap between the ''what'' and ''how'' of AI accountability, specifically for
AI systems in healthcare. We do this by analysing the concept of
accountability, formulating an accountability framework, and providing a
three-tier structure for handling various accountability mechanisms. Our
accountability framework positions the regulations of healthcare AI systems and
the mechanisms adopted by the actors under a consistent accountability regime.
Moreover, the three-tier structure guides the actors of the healthcare AI
system to categorise the mechanisms based on their conduct. Through our
framework, we advocate that decision-making in healthcare AI holds shared
dependencies, where accountability should be dealt with jointly and should
foster collaborations. We highlight the role of explainability in instigating
communication and information sharing between the actors to further facilitate
the collaborative process.

</details>


### [11] [app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding](https://arxiv.org/abs/2509.03310)
*Evgenii Kniazev,Arseny Kravchenko,Igor Rekun,James Broadhead,Nikita Shamgunov,Pranav Sah,Pratik Nichite,Ivan Yamshchikov*

Main category: cs.AI

TL;DR: 本研究介绍了一个开源框架app.build，通过多层次验证管道、特定堆栈协调和模型无关的体系结构，提高了LLM基础应用生成的可靠性。评估结果显示全面验证的有效性，同时指出开放权重模型在结构化环境中表现良好。开源框架已被社区广泛应用，生成应用程序数量超过3,000个。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于改进LLM基础应用生成过程，通过系统验证和结构化环境提高可靠性。此外，作者试图展示在可靠AI代理系统中，扩展环境对于提高性能至关重要的观点。

Method: 本研究采用多层次验证管道、特定堆栈协调和与模型无关的体系结构相结合的方法，在三个参考堆栈上实施。通过在30个生成任务上的评估，展示了综合验证的成果，并指出开放权重模型在提供结构化环境的情况下可以达到80.8%的封闭模型性能。

Result: 评估结果显示，全面验证可以实现73.3%的可行性率，30%达到完美质量分数，同时在提供结构化环境时，开放权重模型可以达到封闭模型性能的80.8%。开源框架已被广泛采纳，已生成超过3,000个应用程序。

Conclusion: 本研究介绍了app.build，一个开源框架，通过系统验证和结构化环境改进LLM基础应用生成。评估结果显示，全面验证可以实现73.3%的可行性率，30%达到完美质量分数，同时在提供结构化环境时，开放权重模型可以达到封闭模型性能的80.8%。该开源框架已被社区采纳，至今已生成超过3,000个应用程序。研究表明，扩展可靠的AI代理需要扩展环境，而不仅仅是模型，为生产导向的代理系统提供了经验性见解和完整的参考实现。

Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source
framework that improves LLM-based application generation through systematic
validation and structured environments. Our approach combines multi-layered
validation pipelines, stack-specific orchestration, and model-agnostic
architecture, implemented across three reference stacks. Through evaluation on
30 generation tasks, we demonstrate that comprehensive validation achieves
73.3% viability rate with 30% reaching perfect quality scores, while
open-weights models achieve 80.8% of closed-model performance when provided
structured environments. The open-source framework has been adopted by the
community, with over 3,000 applications generated to date. This work
demonstrates that scaling reliable AI agents requires scaling environments, not
just models -- providing empirical insights and complete reference
implementations for production-oriented agent systems.

</details>


### [12] [Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning](https://arxiv.org/abs/2509.03345)
*Yunxin Sun,Abulhair Saparov*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型在归纳和诱导推理方面的能力，提出了新的数据集InAbHyD和评估度量基于奥卡姆剃刀的质量评估方法。发现LLMs在简单情景下表现良好，但在复杂模型和高质量假设方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数工作集中在推理方面，但其他类型的推理在解决实际问题中也是至关重要且不太被探索。因此，本研究专注于评估LLMs的归纳和诱导推理能力。

Method: 介绍了一个可编程的合成数据集InAbHyD，通过每个推理示例包含的不完整世界模型和一组观察来评估LLMs的归纳和诱导推理能力。提出了一个基于奥卡姆剃刀的新度量来评估假设的质量。评估和分析了一些最先进的LLMs。

Result: 评估表明LLMs在简单情景中可以执行归纳和诱导推理，但在复杂世界模型和产生高质量假设方面遇到困难。

Conclusion: 这项工作评估了大型语言模型（LLMs）的归纳和诱导推理能力，发现LLMs在简单情景中能够执行归纳和诱导推理，但在复杂世界模型下以及产生高质量假设方面存在困难。

Abstract: Reasoning is a core capability in artificial intelligence systems, for which
large language models (LLMs) have recently shown remarkable progress. However,
most work focuses exclusively on deductive reasoning, which is problematic
since other types of reasoning are also essential in solving real-world
problems, and they are less explored. This work focuses on evaluating LLMs'
inductive and abductive reasoning capabilities. We introduce a programmable and
synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example
consists of an incomplete world model and a set of observations. The task for
the intelligent agent is to produce hypotheses to explain observations under
the incomplete world model to solve each reasoning example. We propose a new
metric to evaluate the quality of hypotheses based on Occam's Razor. We
evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs
can perform inductive and abductive reasoning in simple scenarios, but struggle
with complex world models and producing high-quality hypotheses, even with
popular reasoning-enhancing techniques such as in-context learning and RLVR.

</details>


### [13] [Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems](https://arxiv.org/abs/2509.03380)
*Peter J. Bentley,Soo Ling Lim,Fuyuki Ishikawa*

Main category: cs.AI

TL;DR: 介绍了一个底层框架，将AI代理与环境联系起来，实现信息的清晰控制。相较于传统架构，这种AI代理能够实现零信息泄漏，预计可以提高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决目前AI代理实际上是自主聊天机器人，按照脚本行动，并经常受到不可靠指导者控制的问题。引入新的框架以实现更高效地在环境中运作，并提高信息控制。

Method: 介绍了一个底层框架，将AI代理与环境联系起来，并通过环境变化触发所有行为。引入了“方面”概念，以实现更清晰的信息控制。提供了一个实例实现并展示，相较于典型架构，能够实现零信息泄漏。

Result: 通过实现示例展示，与传统架构相比，这种思考型AI实现了零信息泄漏。预计这种专门代理在信息领域能够提高安全性和效率。

Conclusion: 该工作提出了一个从底层开始的框架，将AI代理置于其环境中，通过环境变化触发所有行为。引入了“方面”概念，类似于乌姆维格，其中一组代理以不同方式感知其环境，从而更清晰地控制信息。与典型架构相比，此类思考型AI实现了零信息泄漏。预计专门代理在其信息领域高效工作的概念可以提高安全性和效率。

Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors
following scripts, often controlled by an unreliable director. This work
introduces a bottom-up framework that situates AI agents in their environment,
with all behaviors triggered by changes in their environments. It introduces
the notion of aspects, similar to the idea of umwelt, where sets of agents
perceive their environment differently to each other, enabling clearer control
of information. We provide an illustrative implementation and show that
compared to a typical architecture, which leaks up to 83% of the time,
aspective agentic AI enables zero information leakage. We anticipate that this
concept of specialist agents working efficiently in their own information
niches can provide improvements to both security and efficiency.

</details>


### [14] [ANNIE: Be Careful of Your Robots](https://arxiv.org/abs/2509.03383)
*Yiyang Huang,Zixuan Wang,Zishen Wan,Yapeng Tian,Haobo Xu,Yinhe Han,Yiming Gan*

Main category: cs.AI

TL;DR: 本研究首次系统研究了对具身AI系统的对抗安全攻击，提出了一种系统性的安全攻击方法，评估结果显示攻击成功率超过50%，强调了对物理AI时代安全驱动防御的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言-动作（VLA）模型融入具身AI机器人，它们在人类中心环境中执行复杂、长程任务的能力正在快速发展。然而，具身AI系统引入了关键的安全风险：受损的VLA模型可以直接将对抗性扰动转化为不安全的物理动作。

Method: 提出了一种系统性的安全攻击方法，基于对物理约束如距离、速度和碰撞边界等的安全违规的原则性分类，并引入了ANNIEBench用于评估具体身安全的基准测试，并介绍了ANNIE-Attack，一种具有任务感知的对抗性框架。

Result: 攻击成功率超过50%的结果显示出具身AI系统中以前未探索但影响重大的攻击面，强调了物理AI时代对安全驱动防御的迫切需求。

Conclusion: 本研究首次系统研究了对具身AI系统的对抗安全攻击，在人机交互的ISO标准基础上建立了基础，展示了攻击成功率超过50%的恶意攻击，强调了物理AI时代对安全驱动防御的紧迫需求。

Abstract: The integration of vision-language-action (VLA) models into embodied AI (EAI)
robots is rapidly advancing their ability to perform complex, long-horizon
tasks in humancentric environments. However, EAI systems introduce critical
security risks: a compromised VLA model can directly translate adversarial
perturbations on sensory input into unsafe physical actions. Traditional safety
definitions and methodologies from the machine learning community are no longer
sufficient. EAI systems raise new questions, such as what constitutes safety,
how to measure it, and how to design effective attack and defense mechanisms in
physically grounded, interactive settings. In this work, we present the first
systematic study of adversarial safety attacks on embodied AI systems, grounded
in ISO standards for human-robot interactions. We (1) formalize a principled
taxonomy of safety violations (critical, dangerous, risky) based on physical
constraints such as separation distance, velocity, and collision boundaries;
(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with
2,400 video-action sequences for evaluating embodied safety; and (3)
ANNIE-Attack, a task-aware adversarial framework with an attack leader model
that decomposes long-horizon goals into frame-level perturbations. Our
evaluation across representative EAI models shows attack success rates
exceeding 50% across all safety categories. We further demonstrate sparse and
adaptive attack strategies and validate the real-world impact through physical
robot experiments. These results expose a previously underexplored but highly
consequential attack surface in embodied AI systems, highlighting the urgent
need for security-driven defenses in the physical AI era. Code is available at
https://github.com/RLCLab/Annie.

</details>


### [15] [sam-llm: interpretable lane change trajectoryprediction via parametric finetuning](https://arxiv.org/abs/2509.03462)
*Zhuo Cao,Yunxiao Shi,Min Xu*

Main category: cs.AI

TL;DR: SAM-LLM is a hybrid architecture that enhances interpretability and computational efficiency in lane change trajectory prediction, achieving high prediction accuracy and reducing output size significantly.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between contextual reasoning and physical precision for autonomous driving by creating a complete, continuous, and physically plausible trajectory model that is easily interpretable and computationally efficient.

Method: Introduces SAM-LLM, a hybrid architecture that combines the contextual reasoning of Large Language Models (LLMs) with the precision of kinematic lane change models. It finetunes an LLM to output core physical parameters for interpretable lane change trajectory prediction.

Result: SAM-LLM achieves an 80% reduction in output size compared to coordinate-based methods, providing core physical parameters for enhanced Sinusoidal Acceleration Model (SAM) in lane change maneuvers.

Conclusion: SAM-LLM achieves state-of-the-art overall intention prediction accuracy of 98.73% with significant advantages in explainability and resource efficiency compared to traditional LLM predictors.

Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the
gap between the contextual reasoning of Large Language Models (LLMs) and the
physical precision of kinematic lane change models for autonomous driving. The
system is designed for interpretable lane change trajectory prediction by
finetuning an LLM to output the core physical parameters of a trajectory model
instead of raw coordinates. For lane-keeping scenarios, the model predicts
discrete coordinates, but for lane change maneuvers, it generates the
parameters for an enhanced Sinusoidal Acceleration Model (SAM), including
lateral displacement, maneuver duration, initial lateral velocity, and
longitudinal velocity change. This parametric approach yields a complete,
continuous, and physically plausible trajectory model that is inherently
interpretable and computationally efficient, achieving an 80% reduction in
output size compared to coordinate-based methods. The SAM-LLM achieves a
state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating
performance equivalent to traditional LLM predictors while offering significant
advantages in explainability and resource efficiency.

</details>
