<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种统一优化框架，用于多线地铁人员计划和调度，解决了跨线协调和快速重新规划的问题。通过实验验证，方法在成本降低和任务完成方面优于基准算法，并通过整合跨线操作获得显著的效率提升。论文突出了全局优化和跨线协调在多线地铁系统运营中的重要性。


<details>
  <summary>Details</summary>
Motivation: 鉴于目前研究主要集中在单个地铁线路上，对于跨线协调和快速重新规划的关注不足，这项工作的动机在于解决多线地铁人员计划和调度的问题。随着地铁网络的快速扩张，多线调度和紧急管理已成为大规模无缝运营的必要条件。

Method: 该论文提出了一种统一的优化框架，用于多线地铁人员计划和调度，包括异构资格和偏好的计算高效约束和公式。进一步开发了基于列生成和最短路径调整的解决算法，并利用提出的网络模型。实验使用上海和北京地铁的真实数据，验证了该方法在成本降低和任务完成方面的优越性，并通过跨线操作实现了显著的效率提升，特别是在紧急任务期间。

Result: 通过实验验证，论文提出的方法在多线地铁系统运营中表现出优越性，特别是在成本降低和任务完成方面。整合跨线操作后，方法实现了显著的效率提升，尤其是在紧急情况下。

Conclusion: 该论文提出了一种统一的优化框架，用于多线地铁人员计划和调度，具有异构劳动力。通过基于分级时间-空间网络模型的统一人员行动空间表示，以及针对异构资格和偏好的计算高效约束和公式，派生了解决算法。实验表明，该方法在成本降低和任务完成方面优于基准启发式算法，并通过整合跨线操作实现了显著的效率提升，特别是在紧急任务期间。本工作突出了全局优化和跨线协调在多线地铁系统运营中的作用，为智能城市中公共交通的高效可靠运行提供了见解。

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [2] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 本文全面评估了基于大型语言模型的代理在渗透测试中的效果和可靠性，发现有针对性的增强可以显著改善模块化代理的性能，特别是在复杂、多步骤和实时任务中。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探究大型语言模型在渗透测试中的效果和可靠性，着重于多个核心功能能力对代理性能的影响，并通过增强方法来提高模块化代理的性能。

Method: 本文通过对多种基于大型语言模型的代理进行全面评估，分析了不同核心功能能力的影响，并进行了有针对性的增强。

Result: 研究结果表明，通过有针对性的增强可以显著提高模块化代理的性能，尤其在复杂、多步骤和实时渗透测试任务中。

Conclusion: 本文提出了一种全面评估多个基于大型语言模型（LLMs）的代理的方法，从单代理到模块化设计，跨真实渗透测试场景进行评估，测量实证性能和重复失败模式。研究结果显示，通过有针对性的增强可以显著提高模块化代理的性能，特别是在复杂、多步骤和实时渗透测试任务中。

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [3] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 该论文分析了现有基于大型语言模型的web代理评估存在的问题，提出了模块化评估框架，展示了如何通过细分代理流水线进行详细的错误分析，并通过案例研究揭示了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对基于大型语言模型的web代理进行评估时，主要关注整体成功而忽视中间错误，导致对失败模式的洞察有限，阻碍了系统性改进。

Method: 提出了模块化评估框架，将代理流水线分解为可解释的阶段进行详细错误分析。使用SeeAct框架和Mind2Web数据集作为案例研究来展示该方法的有效性。

Result: 通过提出的模块化评估框架和实际案例研究，成功展示了如何发现标准指标遗漏的可行性弱点，为构建更健壮和通用的web代理提供了可能的路径。

Conclusion: 该论文分析了现有基于大型语言模型的web代理的评估方面存在的问题，提出了一个模块化评估框架来细分代理流水线，以便进行详细的错误分析。通过使用SeeAct框架和Mind2Web数据集作为案例研究，展示了该方法如何揭示标准指标所忽略的可行性弱点，为构建更健壮和通用的web代理铺平道路。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [4] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench is introduced as the first benchmark for predicting founder success in venture capital. It provides anonymized founder profiles, evaluates large language models, and aims to establish a community-driven standard for evaluating AGI in early-stage venture forecasting with a focus on reproducibility and privacy preservation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges in the venture capital domain where signals are sparse, outcomes are uncertain, and top investors perform modestly. By introducing VCBench, the authors aim to accelerate progress in predicting founder success and establish a benchmark for AGI evaluation that is reproducible and privacy-preserving.

Method: VCBench provides 9,000 anonymized founder profiles standardized to preserve predictive features while resisting identity leakage. The benchmark evaluates nine state-of-the-art large language models and includes adversarial tests to reduce re-identification risk by over 90%.

Result: At inception, the market index achieves a precision of 1.9%. Y Combinator outperforms the index by 1.7x, while tier-1 firms are 2.9x better. State-of-the-art large language models such as DeepSeek-V3 and GPT-4o deliver improvements in precision and surpass human benchmarks.

Conclusion: VCBench introduces a benchmark for predicting founder success in venture capital, providing anonymized founder profiles and evaluating state-of-the-art large language models. The benchmark aims to establish a community-driven standard for reproducible and privacy-preserving evaluation of AGI in early-stage venture forecasting.

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [5] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 本论文提出了一个新的认知架构范式，将人工智能研究重点从外部模仿转向基础认知架构的发展，定义了真正智能的六大核心组件，并提出了基于可衡量组件数量的五级AGI分类系统。论文认为实现五级AGI等同于真正智能，为研究社区提供了清晰可行的研究路径。


<details>
  <summary>Details</summary>
Motivation: 认为当前以性能为基础的定义不足以提供清晰的机制重点研究路径，并未能恰当地定义真正智能的定性特性，因此提出新的观点并框架。

Method: 从人类大脑中汲取灵感，提出了新的认知架构范式，定义了真正智能的六大核心组件，并提出了基于可衡量组件数量的五级人工智能分类系统。

Result: 提供了一个新的定义AGI的观点与框架，为研究社区提供了清晰可行的研究方向。

Conclusion: 提供了一个新的观点，将人工智能的研究重点从外部模仿转向基础认知架构的发展，定义了真正智能的六大核心组件，提出了基于可衡量组件数量的五级人工智能分类系统，并认为实现五级人工智能等同于真正智能，从而为研究社区提供了清晰可行的研究方向。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [6] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 本文利用Measurements Layouts评估AI系统的合作能力，在Melting Pot竞赛中发现亲社能力不一定与更好表现相关。推荐改进合作需求的注释，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 社交AI代理的发展和评估需要复杂的环境，其中竞争性和合作性行为会自然产生。本文旨在评估AI系统的合作能力。

Method: 本文应用称为测量布局的贝叶斯方法来推断“Melting Pot”竞赛中多智能体系统的能力概况。

Result: 研究发现高度亲社能力有时与更好的性能相关，但这并非普遍趋势。某些得分较低的代理表现出更强的合作能力。最好的竞赛提交更有可能在不需要亲社能力的情况下取得高分。表明至少一个高效的团队可能已优化为在不需要合作的条件下工作。

Conclusion: 研究表明测量布局提供了强大的预测准确性和可行见解，有助于更透明、更可推广地评估复杂社交环境中的AI系统。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [7] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: 本文提出DeKeyNLU数据集，用于改进任务分解和提高RAG管道的关键字提取准确性。通过DeKeyNLU的微调，提出了基于RAG的NL2SQL管道DeKeySQL，以提高SQL生成准确性。实验证明，使用DeKeyNLU进行微调显著提高了SQL生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集的局限性包括任务过分细化和缺乏领域特定关键字标注。为改善这些问题，提出了DeKeyNLU数据集。

Method: 提出DeKeyNLU数据集，提出了基于RAG的NL2SQL管道DeKeySQL，包括多个模块用于任务分解和SQL生成。

Result: 通过DeKeyNLU的微调，提高了SQL生成的准确性，BIRD数据集的准确性从62.31%提高到69.10%，Spider数据集的准确性从84.2%提高到88.7%。

Conclusion: 提出了DeKeyNLU数据集，用于改进任务分解和提高RAG管道的关键字提取准确性。通过DeKeyNLU的微调，提出了基于RAG的NL2SQL管道DeKeySQL，包括用户问题理解、实体检索和生成等模块，以提高SQL生成准确性。实验证明，使用DeKeyNLU进行微调显著提高了SQL生成的准确性。

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [8] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 本研究提出了评估LLMs整体理性的首个基准，涵盖广泛领域和LLMs。该基准包括易于使用的工具包，并揭示了LLMs与理想化人类理性的汇合与分歧。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs已被应用于模拟人类和作为人工智能助手，人们对LLMs是否会像真实人类代理那样思考和行为产生了极大关注。理性是评估人类行为中最重要的概念之一，在思考(理论理性)和采取行动(实践理性)方面均起着重要作用。

Method: 该研究提出了评估LLMs整体理性的基准，包括易于使用的工具包、广泛的实验结果和分析。

Result: 研究结果包括一个用于评估LLMs整体理性的基准，覆盖了广泛的领域和LLMs，并提供了深入的实验结果和分析。

Conclusion: 该研究提出了评估大型语言模型(Large Language Models, LLMs)整体理性的第一个基准，为开发者和用户提供了一个重要工具。研究结果揭示了LLMs在理性行为方面与理想化的人类理性存在的汇合与分歧之处。

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [9] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 近期研究表明，精心设计的工作流能显著提升大型语言模型(LLMs)的任务解决能力。现有方法大多仅依赖于历史经验，限制了效率和适应性。为此，提出了一种先验动态框架，通过优化决策空间并根据任务特征灵活选择工作流结构。实验证明该方法相较于现有方法平均改进了4.05%，同时降低了工作流构建和推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多仅依赖于历史经验，限制了效率和适应性。研究认为除了历史经验外，工作流构建应灵活响应每个任务的独特特征。因此，提出了一种先验动态框架，旨在更好地利用历史经验并根据不同任务选择合适的工作流结构。

Method: 研究首先利用Q表学习优化决策空间，引导智能体决策并有效利用历史经验。智能体评估当前任务进展并对下一个执行智能体做出先验决策，从而主动选择适合每个任务的工作流结构。另外，还引入了冷启动初始化、提前停止和修剪等机制以进一步提高系统效率。

Result: 通过在四个基准数据集上的实验评估，该方法的可行性和有效性得到了证明。与现有基准相比，该方法平均改进了4.05%，同时将工作流构建和推理成本降低到现有方法的30.68%-48.31%。

Conclusion: 该研究提出了一种自动工作流构建的先验动态框架，通过优化决策空间，评估当前任务进展并进行先验决策，有效利用历史经验，实现了更有效的工作流结构选择。实验证明该方法相较于现有方法平均改进了4.05%，同时将工作流构建和推理成本降低到现有方法的30.68%-48.31%。

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [10] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 本研究通过评估差分隐私文本生成方法和大语言模型在领域特定数据生成中的性能，提出了严格的隐私审计的紧迫需求，强调了开放领域和专业领域评估之间的差距，为隐私敏感、高风险环境中生成式人工智能的负责任部署提供了指导。


<details>
  <summary>Details</summary>
Motivation: 最近的生成式人工智能模型在开放领域任务中表现出色，但在敏感环境中的采纳受到了限制。现有的匿名化方法通常不足够，尤其是对于非结构化文本。因此，本研究旨在解决数据共享障碍和隐私问题，探索差分隐私作为一种合成数据的隐私保障替代方案。

Method: 本研究通过引入全面的评估框架和大规模实证研究，基于九个筛选数据集，针对技术术语、长期依赖关系和专业文档结构等领域特定复杂性，评估了最先进的差分隐私文本生成方法和不同尺寸的大语言模型（LLM）的性能。通过开发针对合成文本的成员推理攻击方法，提供了首次实证证据，表明公共数据集的使用可能会使声称的隐私保证失效。

Result: 通过研究发现，高质量领域特定的差分隐私合成数据生成仍然是一个未解决的挑战，性能随领域复杂性增加而降低。同时，合成文本的成员推理攻击方法揭示了公共数据集的潜在风险。

Conclusion: 本文通过三个关键贡献解决了数据驱动决策支持在医疗保健和金融领域中面临的数据共享障碍，并在不确定行为和隐私保护数据不足等方面的限制下提出了利用差分隐私生成合成数据的方案。研究结果强调了对隐私审计的紧迫需求，并揭示了开放域与专业领域评估之间持久的差距，为在隐私敏感、高风险环境中负责任地部署生成式人工智能提供指导。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [11] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass is an innovative evaluation framework for monitoring and debugging agentic workflows post-deployment. It models expert reasoning through a multi-stage analytical pipeline and dual memory system, demonstrating superior performance on key metrics and uncovering critical issues missed by human annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind AgentCompass is to address the mounting risks organizations face from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture in the era of Large Language Models (LLMs) in automating complex, multi-agent workflows.

Method: AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline including error identification, thematic clustering, quantitative scoring, and strategic summarization. It is enhanced with a dual memory system for continual learning across executions.

Result: AgentCompass has proven practical utility on real-world deployments and efficacy against the TRAIL benchmark, highlighting its role as a robust, developer-centric tool for monitoring and improving agentic systems in production.

Conclusion: AgentCompass is a novel evaluation framework for post-deployment monitoring and debugging of agentic workflows, demonstrating state-of-the-art results on key metrics and uncovering critical issues missed in human annotations.

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [12] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: 这篇论文介绍了将Schoenfeld的Episode Theory应用于LRMs推理痕迹分析的新方法，通过注释模型生成的数学问题解决方案，并揭示了LRM推理中的不同模式。研究建立了机器推理细粒度分析的公开基准，为理解LRM认知和开发更可控透明推理系统提供了理论方法论。


<details>
  <summary>Details</summary>
Motivation: LRMs生成的推理具有广泛的思维链条，但缺乏理解这些思维结构的原则性框架。因此，本研究的动机在于引入一种新方法，以识别LRMs推理过程中的模式和认知状态转换等特征。

Method: 应用Schoenfeld的Episode Theory对LRMs生成的推理痕迹进行分析，使用七个认知标签对模型生成的数学问题解决方案进行注释，创建了一个大型注释语料库和详细的注释指南。

Result: 建立了机器推理细粒度分析的首个公开可用基准，初步分析揭示了LRM推理中的不同模式，为理解LRM认知提供了基于理论的方法论，并为未来开发更可控透明的推理系统铺平了道路。

Conclusion: 这篇论文介绍了将Schoenfeld的Episode Theory应用于分析LRMs推理痕迹的新方法。通过对数千个由模型生成的解决数学问题的句子和段落进行注释，使用七个认知标签（如Plan、Implement、Verify），建立了机器推理细粒度分析的首个公开可用基准。初步分析揭示了LRM推理中的不同模式，例如认知状态之间的过渡动态。该框架为解释LRM认知提供了基于理论的方法论，并为更可控透明的推理系统的未来工作奠定了基础。

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [13] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: 提出了RationAnomaly框架，利用Chain-of-Thought（CoT）微调和强化学习增强日志异常检测，解决了现有方法的问题，实验表现优秀，超过了现有技术基线，并提供了透明的、逐步的分析输出。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化日志异常检测方法存在问题，如传统深度学习模型缺乏解释性和泛化性，利用大型语言模型的方法往往受到不可靠性和事实错误的限制。因此，为了解决这些问题，提出了RationAnomaly框架。

Method: 提出了RationAnomaly框架，结合了Chain-of-Thought（CoT）微调和强化学习，通过专家驱动过程校正的高质量数据集进行监督微调，然后使用多方面奖励函数的强化学习阶段优化准确性和逻辑一致性。

Result: 在实验中，RationAnomaly在关键基准上表现优异，优于现有技术基线，提供透明的、逐步的分析输出，并发布了相应的资源，包括代码和数据集。

Conclusion: 提出了一种名为RationAnomaly的新框架，通过将Chain-of-Thought（CoT）微调与强化学习相结合，增强了日志异常检测。该方法首先使用CoT引导的监督微调注入类似专家的推理模式，依赖于经过严格专家驱动过程纠正的高质量数据集。随后，强化学习阶段采用多方面的奖励函数，优化准确性和逻辑一致性，有效缓解了幻觉。实验表明，RationAnomaly在关键基准上优于现有技术基线，实现了更高的F1分数，并提供透明的、逐步的分析输出。已发布相应的资源，包括代码和数据集。

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [14] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo, a benchmark built from Japanese children's riddles, outperforms models except for GPT-5. It emphasizes the significance of insight-based reasoning in evaluation. Reasoning models excel over non-reasoning ones regardless of size. Verification failures in thought logs reveal a meta-cognitive weakness. Nazonazo provides a cost-effective benchmark addressing evaluation issues and suggesting improvement areas.


<details>
  <summary>Details</summary>
Motivation: Address benchmark saturation and contamination problems in LLM evaluation. Develop a cost-effective and extensible benchmark that enables rapid refreshing of blind sets to prevent leakage suspicion. Compare model performance on insight-based reasoning using a diverse set of riddles. Identify and illustrate verification failures among models to understand meta-cognitive weaknesses.

Method: Built Nazonazo benchmark using Japanese children's riddles to test insight-based reasoning. Evaluated 38 frontier models and 126 adults on 120 riddles, comparing their performance to human accuracy. Extended evaluation on 201 items showed superiority of reasoning models over non-reasoning ones. Conducted candidate-tracking analysis on thought logs to identify verification failures among models.

Result: Nazonazo benchmark outperforms most models, highlighting the importance of insight-based reasoning. Reasoning models perform significantly better than non-reasoning ones, regardless of model size. Verification failures among models indicate a meta-cognitive weakness that needs to be addressed in future research and development. Nazonazo offers a scalable and easily renewable benchmark format for improved model evaluation.

Conclusion: Nazonazo, built from Japanese children's riddles, outperforms other models except for GPT-5. It highlights the importance of insight-based reasoning in model evaluation. Model comparison shows reasoning models perform better than non-reasoning models, regardless of model size. The analysis of thought logs reveals verification failures among models, indicating a meta-cognitive weakness. Nazonazo provides a cost-effective and scalable benchmark for addressing evaluation issues and suggesting areas for future improvements.

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [15] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: Retrieval-augmented Generation（RAG）经常受到“Retrieval Hallucinations”困扰，未能应对质量低下的检索文档，影响性能。为解决此问题，提出了Adversarial Collaboration RAG（AC-RAG）框架，通过对抗协作在探测器和解算器之间创建动态过程，显著提高了检索准确性，并在各个领域优于最先进的RAG方法。


<details>
  <summary>Details</summary>
Motivation: 针对Retrieval Hallucinations现象提出解决方案，因为Retrieval-augmented Generation（RAG）常常受到其困扰，即精调模型未能识别和利用质量低劣的检索文档，从而影响性能。

Method: 提出了对抗协作RAG（AC-RAG）框架，使用两个异质代理：一般性探测器和专注于领域的解算器。通过主持人的引导，这些代理进行对抗协作，探测器的持续质疑挑战解算器的专业知识，允许迭代问题分析和精细知识检索。

Result: 通过大量实验证明AC-RAG显著提高了检索准确性，并在各个垂直领域优于最先进的RAG方法。

Conclusion: AD-RAG架构显著提高检索准确性，优于各个领域的最先进RAG方法。

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [16] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: 介绍了一种全自动框架OpenLens AI，专为健康信息学定制。该框架整合了代理技术用于文献审阅、数据分析、代码生成和手稿准备，通过图像-语言反馈提升医学可视化能力，并引入质量控制确保研究结果可重复。该框架能生成 publication-ready 的 LaTeX 手稿，为促进健康信息学研究提供了定制解决方案。


<details>
  <summary>Details</summary>
Motivation: 健康信息学研究面临多样化的数据模态、知识迅速扩展以及需要整合生物医学科学、数据分析和临床实践洞见的挑战。现有系统在医学可视化解释能力和领域特定质量要求方面存在局限。因此，为了解决这些问题，引入了OpenLens AI全自动框架，旨在提供一种定制化的解决方案以推进健康信息学研究。

Method: 引入了一种全自动框架OpenLens AI，采用代理技术整合了文献审阅、数据分析、代码生成和手稿准备的功能，通过图像-语言反馈提升了医学可视化方面的能力，并引入质量控制以确保研究结果的可重复性。该框架生成 publication-ready 的 LaTeX 手稿，透明且可追溯的工作流程，为健康信息学研究提供了定制化的解决方案。

Result: OpenLens AI框架实现了代理技术在健康信息学研究中的应用，提高了医学可视化方面的能力，并引入了质量控制确保研究结果的可重复性。该框架能够生成 publication-ready 的 LaTeX 手稿，为健康信息学研究提供了有效的自动化解决方案。

Conclusion: 介绍了一种针对健康信息学的全自动框架OpenLens AI，该框架整合了专门用于文献审阅、数据分析、代码生成和手稿准备的代理，通过图像-语言反馈增强了医学可视化方面的能力，并通过质量控制确保可重复性。该框架自动化了整个研究流程，生成了适合发布的 LaTeX 手稿，提供了一个为推进健康信息学研究而量身定制的解决方案。

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [17] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: 本研究使用可解释的AI建模框架分析爱尔兰一家医院的电子病历数据，发现Transformer模型的表现优异，尤其是TabTransformer在多个临床预测任务中表现出色。感染相关特征和网络变量对预测患者结果和CPE获取风险具有重要影响。研究强调了Transformer模型的优越性能和临床、网络特征的重要性。


<details>
  <summary>Details</summary>
Motivation: Carbapenemase-Producing Enterobacteriace对医院感染防控构成重大关注，但先前关注的CPE相关风险的预测建模仍未充分探讨，尤其是在现代深度学习方法下。本研究的动机是引入一个可解释的AI建模框架，分析复杂的电子病历数据，以识别关键风险因素并预测CPE相关结果。

Method: 本研究使用了一个可解释的AI建模框架，分析了爱尔兰一家急诊医院的患者数据，包括诊断代码、病房转换、患者人口统计学、感染相关变量和接触网络特征。使用了多种基于Transformer的架构和传统机器学习模型进行了比较。通过预测临床结果，并应用XAI技术解释模型决策，成功展示了Transformer模型的效用，特别是TabTransformer在多个临床预测任务中的表现优于基准模型。

Result: 框架成功展示了Transformer模型的效用，TabTransformer在多个临床预测任务中表现优异，特别是在CPE获取方面。感染相关特征和网络变量对预测患者结果和CPE获取风险具有高度影响力。解释性分析揭示了一些关键风险因素，如“居住区域”、“入院病房”和先前的入院情况。网络变量如“病房PageRank”也排名很高，反映了结构性暴露信息的潜在价值。

Conclusion: 本研究引入了一个可解释的AI建模框架，通过分析爱尔兰一家医院的电子病历数据来研究CPE对患者结果的影响。Transformer模型表现出色，TabTransformer在多个临床预测任务中始终优于基准模型，特别是对于CPE感染的预测。感染相关特征和网络变量对预测患者结果和CPE获取风险具有高度影响力。研究揭示了住院历史暴露、入院背景和网络中心性等特征对预测患者结果和CPE风险的重要性，强调了Transformer模型的卓越性能和临床和网络特征的重要性。

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [18] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 该论文提出了一种新的多Agent系统安全框架，使用Sentinel Agents和协调Agent的双层安全方法以增强安全性和可靠性。作者进行了仿真研究验证了这一方法的有效性，展示了框架对多Agent系统安全防御的重要性和实用性。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于解决多Agent系统中的安全和可靠性问题。作者认识到现有安全机制可能无法全面应对多Agent系统中复杂的威胁，因此提出了Sentinel Agents和协调Agent的框架，旨在提供动态、适应性的防御机制，确保MAS生态系统的完整性和安全性。

Method: 该论文采用了Sentinel Agents和协调Agent相结合的双层安全方法，其中Sentinel Agents负责持续监控Agent间通信和识别潜在威胁，而协调Agent则负责政策实施、管理代理参与和处理Sentinel Agents的警报。作者还进行了仿真研究，注入不同家族的合成攻击，并验证了Sentinel Agents成功检测这些攻击的能力。

Result: 通过提出的Sentinel Agents和协调Agent框架，论文成功实现了对多Agent系统中安全性和可靠性的增强。仿真研究表明Sentinel Agents能够有效检测不同类型的攻击，证明了这种监控方法的实际可行性。此外，框架还提供了系统可观察性、支持法规合规，并支持政策演进。

Conclusion: 该论文提出了一种新颖的架构框架，旨在增强多Agent系统（MAS）中的安全性和可靠性。框架的关键组成部分是一组Sentinel Agents，作为一个分布式安全层，集成了诸如通过大型语言模型（LLMs）进行语义分析、行为分析、检索增强验证和跨Agent异常检测等技术。这些代理可以监督Agent间的通信，识别潜在威胁，执行隐私和访问控制，并保持全面的审核记录。与Sentinel Agents的概念相辅相成的是协调Agent的使用。协调Agent监督政策实施，并管理代理的参与。此外，协调Agent还从Sentinel Agents接收警报。基于这些警报，它可以调整政策，隔离或隔离行为不端的代理，并遏制威胁以维护MAS生态系统的完整性。这种双层安全方法，将Sentinel Agents的持续监控与协调Agent的治理功能相结合，支持针对各种威胁的动态和适应性防御机制，包括即时注入、共谋代理行为、由LLMs生成的幻觉、隐私泄漏和协同多Agent攻击。除了架构设计，我们还提出了一项仿真研究，在该研究中向一个多Agent对话环境注入了162种不同家族（即时注入、幻觉和数据外泄）的合成攻击。Sentinel Agents成功检测到攻击尝试，确认了所提出的监控方法的实际可行性。该框架还提供了增强的系统可观察性，支持法规合规，并能够随时间演进的政策。

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [19] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文引入了一组集合贡献函数，用于量化一组参数对主题的贡献。通过概括现有函数并提出新原则，更全面地分析了参数贡献。结果表明在推荐系统中应用这些原则可以提高推荐精度。


<details>
  <summary>Details</summary>
Motivation: 通过引入集合贡献函数，可以更全面地理解一组参数对主题的贡献。现有贡献函数主要关注单个参数对主题的影响，而集合贡献函数扩展了这一概念，考虑了参数间的相互影响。针对推荐系统应用场景，这种集合贡献函数的引入可以提高对参数贡献的理解，有助于系统提供更精准的推荐结果。

Method: 提出了一组集合贡献函数，概括了现有的量化单个贡献参数对主题贡献的函数。推广了现有的贡献函数原则，为集合贡献函数提供了对应的基于原则的分析。引入了新的针对基于集合函数的原则，关注集合内参数交互的特性。勾勒了这些原则如何在推荐系统应用场景中展现。

Result: 成功提出了一组集合贡献函数，并通过基于原则的分析来说明其在感兴趣的主题中的作用。还展示了在推荐系统应用中这些原则的具体运用方式。

Conclusion: 引入了量化一组参数在定量双极论证图中对感兴趣的论点（所谓的主题）的贡献的函数。提出了一组贡献函数，这些函数是对现有函数的概括，用于量化单个贡献参数对主题的贡献。通过为集合贡献函数概括现有的贡献函数原则，并提供相应的基于原则的分析。引入了针对基于集合函数的新原则，重点关注集合内参数之间的交互特性。最后，勾勒了这些原则在给定推荐系统应用场景中不同集合贡献函数之间的展示方式。

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [20] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC is a framework that allows LLM agents to dynamically form expert teams in medical decision-making, outperforming other methods in complex clinical scenarios like cancer prognosis.


<details>
  <summary>Details</summary>
Motivation: Inspired by the collaborative nature of multidisciplinary teams in medical decision-making, the paper aims to improve reasoning and adaptability in multi-agent collaboration frameworks using large language models.

Method: The paper proposes the KAMAC framework, which starts with expert agents and dynamically expands teams through knowledge-driven discussions to fill knowledge gaps. It emphasizes adaptability and dynamic knowledge integration in medical decision-making.

Result: Experiments on two real-world medical benchmarks show that KAMAC significantly outperforms other methods, particularly in complex clinical scenarios. The decisions are finalized through reviewing updated agent comments.

Conclusion: KAMAC is a Knowledge-driven Adaptive Multi-Agent Collaboration framework that outperforms single-agent and advanced multi-agent methods, especially in complex clinical scenarios like cancer prognosis. It allows LLM agents to dynamically form expert teams based on evolving diagnostic contexts, enabling flexible and scalable collaboration.

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [21] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: 本研究使用生成AI支持形成性评估，分析了120个元评论，发现生成AI能够模拟有效人类反馈的关键特征，并促进学生对同行评审的参与。AI元反馈平衡了赞扬和建设性批评，符合评分标准，并突出学生代理。


<details>
  <summary>Details</summary>
Motivation: 通过研究使用生成AI支持形成性评估的潜力，探索AI反馈与人类反馈之间的异同，以及生成AI对于学生参与同行评审的影响。

Method: 本研究运用系统功能语言学和评价理论，分析了120个元评论，探讨了生成AI反馈如何跨观念、人际和文本维度构建意义。

Result: 生成AI在支持形成性评估方面表现出了潜在优势，能够模拟有效人类反馈的关键特征，同时提供明确且支持性的指导。研究发现AI元反馈呈现出平衡的评价和建设性批评，符合评分标准，并突出学生的主动性。

Conclusion: 研究发现，生成AI可以在形成性评估中近似有效人类反馈的关键修辞和关系特征，提供指导性清晰度同时保持支持性立场。AI元反馈模拟了平衡的赞扬和建设性批评，符合评分标准的期望，并呈现出突出学生代理的结构化分阶。AI元反馈有潜力建构反馈素养并增强学生参与同行评审的能力。

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [22] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: 本文强调了XAI在海事领域中的重要性，提出了一种领域特定的调查方法以捕捉海事专业人士对XAI系统的感知，旨在引导用户中心XAI系统的发展。


<details>
  <summary>Details</summary>
Motivation: 随着自主技术对海事操作产生越来越大的影响，理解人工智能系统为何做出决策与决策本身一样关键。在复杂和动态的海事环境中，对人工智能的信任不仅取决于性能，还取决于透明度和可解释性。

Method: 提出了一种领域特定的调查方法，旨在捕捉海事专业人士对信任、可用性和可解释性的感知，以引导用户中心的XAI系统的开发。

Result: 旨在增进人们对海事专业人士信任、可用性和可解释性感知的意识，并引导量身定制给海员和海事团队需求的用户中心XAI系统的发展。

Conclusion: 本文强调了可解释人工智能（XAI）在海事领域中作为有效人机协作基础的重要性，以支持用户中心的XAI系统的整合和发展。

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [23] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: 本研究提出了MACA框架，通过多智能体辩论中的多数/少数意见形成，训练模型偏好与内部共识一致的推理轨迹。MACA显著提高了模型性能，在自一致性、单一智能推理、基于抽样的推断和多智能体集成决策等方面表现突出，并在未见基准测试中具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: LWs生成的响应常常存在矛盾，推理路径选择不可靠，对一致性的处理需要改进。现有方法虽然可以缓解这些问题，但未能解决核心问题。本研究旨在提高语言模型的自一致性，引入MACA框架来改善模型在多智能体环境中的表现，使模型能更加可靠地选择推理路径并产生一致的结果。

Method: 提出了Multi-Agent Consensus Alignment（MACA）这一强化学习框架，通过多智能体辩论中的多数/少数意见形成，训练模型偏好与其内部共识一致的推理轨迹。MACA使模型能够自我教导，使其在多智能体环境中更具决策性和简洁性，在自一致性、单一智能推理、基于抽样的推断和多智能体集成决策等方面取得了突出表现。

Result: MACA框架能够显著提高模型在自一致性、单一智能推理、基于抽样的推断和多智能体集成决策等方面的性能。实验结果显示，在多种基准测试上，MACA框架都取得了很好的表现，表明其能够更可靠地发掘语言模型的潜在推理能力。

Conclusion: 提出了一种名为Multi-Agent Consensus Alignment（MACA）的强化学习框架，用于训练模型偏向于与内部共识一致的推理轨迹，从而提高自一致性、单一智能推理、基于抽样的推断和多智能体集成决策等方面的性能。结果表明，MACA能够在没有外部监督的情况下使智能体更加果断和简明，并更好地利用多智能体环境中的同行见解，从而显著提高模型在各方面的表现，并且在未见过的基准测试中具有强大的泛化能力。

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [24] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: 本文通过引入Reinforcement Learning with Verifiable Rewards（RLVR）方法，成功改善了多模态大语言模型在推理能力方面的表现，特别是在解决几何问题中。生成的数据集提高了模型在数学和工程任务中的准确性，对任务泛化能力有显著改善。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型在解决复杂几何问题时仍存在困难，主要挑战在于缺乏高质量的图像-文本配对数据集。此外，现有基于模板的数据合成方法通常无法泛化到预定义模板之外的问题。因此，本研究的动机是填补这一空白，提出一种新的数据生成流程。

Method: 本文采用了Reinforcement Learning with Verifiable Rewards（RLVR）方法，通过50个基本几何关系合成图像，并从数学问题求解任务中获取奖励信号，成功捕获了几何问题解决的关键特征。

Result: 采用RLVR用于改进几何图像标题的生成，能够提高任务泛化能力，并在几何图像以外的场景中改善多模态大语言模型的推理能力。统计、算术、代数、数字任务中的准确性提高了2.8%-4.8%，在MathVista和MathVerse中非几何输入图像的艺术、设计、技术和工程任务中获得了2.4%-3.9%的改进。

Conclusion: 本文介绍了一种将强化学习与可验证奖励相结合的方法，用于生成几何图像的标题，并通过该方法改善了多模态大语言模型的推理能力。生成的数据集提高了多模态大语言模型在统计、算术、代数、数字任务中的准确性，并在数学、艺术、设计、技术和工程任务中获得改进。

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>
