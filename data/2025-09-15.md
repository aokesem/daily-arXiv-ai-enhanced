<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型（LLM）平台（AutoIND）在减少首稿撰写时间的同时保持文件质量的能力。AutoIND显著降低了IND起草时间，但仍需要专业监管作家来提升输出质量。系统性缺陷为改进模型提供了指引。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）平台（AutoIND）能否在维持文件质量的同时减少首稿撰写时间，以加快早期临床开发。

Method: 记录AutoIND生成的IND非临床书面摘要（eCTD模块2.6.2、2.6.4、2.6.6）起草时间，估计由美国FDA批准的IND摘要的手动起草时间，通过七个预先指定的类别对质量进行评估：正确性、完整性、简洁性、一致性、清晰度、冗余性和强调度，评分为0-3，并归一化到百分比进行评估。定义关键的监管错误为任何可能改变监管解释的失实陈述或遗漏。

Result: AutoIND将初始起草时间缩短约97％（从约100小时缩短到IND-1的3.7小时，IND-2的2.6小时），质量得分分别为69.6％和77.9％。没有检测到关键的监管错误，但注意到了强调度、简洁性和清晰度方面的不足。

Conclusion: AutoIND可以显著加快IND起草速度，但专业的监管作家仍然至关重要，以将产出成熟到递交就绪的质量。确定的系统性缺陷提供了针对模型改进的路线图。

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [2] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: 本文介绍了boldsea架构，一种用可执行本体模型建模复杂动态系统的方法。通过将事件语义与数据流架构结合，boldsea解决了传统BPM系统和面向对象语义技术的局限。该架构使用boldsea语义语言（BSL）和BNF语法，引擎直接将语义模型解释为可执行算法，实现了模型在运行时修改，保证了时间透明性，且无缝融合了数据和业务逻辑。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于提出一种新的架构来建模复杂动态系统，并克服传统系统的局限性，使得事件语义能够直接控制过程执行，同时实现数据和业务逻辑的无缝融合。

Method: boldsea采用了可执行本体模型来建模复杂动态系统，与数据流架构结合，解决了传统BPM系统和面向对象语义技术的局限，使用了正式的boldsea语义语言（BSL）和对应的BNF语法，引擎将语义模型直接解释为可执行算法。

Result: boldsea架构有效地实现了可执行本体模型来控制动态系统的过程执行，使得模型可以在运行时进行修改，保证了时间透明性，同时融合了数据和业务逻辑。

Conclusion: 该论文介绍了boldsea，Boldachev的语义事件方法，这是一种用可执行本体模型建模复杂动态系统的架构。该方法将事件语义与数据流架构相结合，解决了传统业务流程管理系统和面向对象语义技术的局限。论文介绍了正式的BSL（boldsea语义语言）及其BNF语法，概述了boldsea引擎的架构，直接将语义模型解释为可执行算法而无需编译。这使得可以在运行时修改事件模型，确保时间透明性，并在统一的语义框架内无缝融合数据和业务逻辑。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [3] [How well can LLMs provide planning feedback in grounded environments?](https://arxiv.org/abs/2509.09790)
*Yuxuan Li,Victor Zhong*

Main category: cs.AI

TL;DR: 预训练基础模型如LLMs和VLMs在规划中提供有用的背景知识，减少了奖励设计和演示的需要。大型和推理模型通常提供更准确的反馈，受益于更好的推理方法。反馈质量在复杂环境中会下降。


<details>
  <summary>Details</summary>
Motivation: 传统规划方法通常需要精心设计的奖励函数或高质量的标注演示。本文旨在研究LLMs和VLMs如何提供规划反馈，以减少对奖励设计和演示的依赖。

Method: 评估了LLMs和VLMs在符号、语言和连续控制环境中提供反馈的能力。考虑了用于规划的不同类型的反馈，包括二元反馈、偏好反馈、行动建议、目标建议和增量行动反馈。还考虑了影响反馈性能的推理方法，包括上下文学习、思维链和访问环境动态。

Result: 基础模型可以在不同领域提供多样化且高质量的反馈。较大和推理模型通常提供更准确的反馈，表现出较少的偏见，并从增强推理方法中受益更多。此外，在具有复杂动态或连续状态空间和动作空间的环境中，反馈质量会下降。

Conclusion: 预训练的基础模型，如大型语言模型（LLMs）和视觉语言模型（VLMs），在规划中提供了有用的背景知识，减少了策略学习所需的奖励设计和演示数量。较大和推理模型通常提供更准确的反馈，表现出较少的偏见，并从增强推理方法中受益更多。然而，在具有复杂动态或连续状态空间和动作空间的环境中，反馈质量会下降。

Abstract: Learning to plan in grounded environments typically requires carefully
designed reward functions or high-quality annotated demonstrations. Recent
works show that pretrained foundation models, such as large language models
(LLMs) and vision language models (VLMs), capture background knowledge helpful
for planning, which reduces the amount of reward design and demonstrations
needed for policy learning. We evaluate how well LLMs and VLMs provide feedback
across symbolic, language, and continuous control environments. We consider
prominent types of feedback for planning including binary feedback, preference
feedback, action advising, goal advising, and delta action feedback. We also
consider inference methods that impact feedback performance, including
in-context learning, chain-of-thought, and access to environment dynamics. We
find that foundation models can provide diverse high-quality feedback across
domains. Moreover, larger and reasoning models consistently provide more
accurate feedback, exhibit less bias, and benefit more from enhanced inference
methods. Finally, feedback quality degrades for environments with complex
dynamics or continuous state spaces and action spaces.

</details>


### [4] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: 本文介绍了一种利用生成人工智能从公开住宅信息和图片中产生数据的模块化多模态框架，通过减少对昂贵或受限数据的依赖，促使更具可访问性和可复制性的研究。


<details>
  <summary>Details</summary>
Motivation: 计算模型已成为能源建模研究的强大工具，具有可伸缩性和数量化结果，但这些模型需要大量数据，其中一些数据无法访问、昂贵或涉及隐私问题。

Method: 介绍了一种模块化多模态框架，利用生成人工智能从公开可访问的住宅信息和图片中产生数据，并提供了演示该框架的管道，并评估了其生成AI组件。

Result: 实验证明该框架的AI使用有助于避免常见的生成模型问题，生成了现实且标记的数据。

Conclusion: 通过引入模块化多模态框架，利用生成人工智能从公开可访问的住宅信息和图片中产生数据，该框架可以避免常见的生成模型问题，生成真实标记的数据，有助于减少对昂贵或受限数据的依赖，为更具可访问性和可复制性的研究铺平道路。

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [5] [Towards a Common Framework for Autoformalization](https://arxiv.org/abs/2509.09810)
*Agnieszka Mensfelt,David Tena Cucala,Santiago Franco,Angeliki Koutsoukou-Argyraki,Vince Trencsenyi,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文旨在回顾自动形式化的实例，并提出统一框架鼓励不同领域间的交叉合作，以促进AI系统的发展。


<details>
  <summary>Details</summary>
Motivation: 自动形式化的快速发展受制于深度学习特别是大型语言模型的进展。然而，存在的问题在于这一发展与其他领域采用LLMs将非正式语言转化为形式表示却缺乏交叉合作，限制了共同方法论、基准和理论框架的机会。因此，有必要提出统一框架以促进不同领域间的交流，加速下一代AI系统的发展。

Method: 对于自动形式化的最新发展，通过考察数学形式化和将非正式输入转化为形式逻辑表示的研究，提出统一框架，以促进交叉学科间的方法论、基准和理论框架。

Result: 本文提出了一个统一的框架，为自动形式化和LLMs在不同领域间的应用提供了交叉交流的机会，以推动AI系统的发展。

Conclusion: 本文旨在回顾涉及自动形式化的实例，并提出一个统一框架，鼓励不同领域间的交叉污染，以推动下一代人工智能系统的发展。

Abstract: Autoformalization has emerged as a term referring to the automation of
formalization - specifically, the formalization of mathematics using
interactive theorem provers (proof assistants). Its rapid development has been
driven by progress in deep learning, especially large language models (LLMs).
More recently, the term has expanded beyond mathematics to describe the broader
task of translating informal input into formal logical representations. At the
same time, a growing body of research explores using LLMs to translate informal
language into formal representations for reasoning, planning, and knowledge
representation - often without explicitly referring to this process as
autoformalization. As a result, despite addressing similar tasks, the largely
independent development of these research areas has limited opportunities for
shared methodologies, benchmarks, and theoretical frameworks that could
accelerate progress. The goal of this paper is to review - explicit or implicit
- instances of what can be considered autoformalization and to propose a
unified framework, encouraging cross-pollination between different fields to
advance the development of next generation AI systems.

</details>


### [6] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: 本研究介绍了一种智能知识助手系统，旨在支持养殖山羊的健康管理。通过结构化知识处理方法和知识库建立，提升了大型语言模型（LLMs）对异构数据格式的理解能力，并实现了在线实时信息检索。实验结果显示系统的良好性能，验证了结构化知识融合的有效性。遗漏是主要错误，提出了改进机会。总体而言，系统在山羊养殖中表现稳健可靠。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在许多行业被认为是有价值的知识传播工具，但其在畜牧业，特别是养殖山羊领域的应用仍受到限制。存在知识源的可用性、多样性和复杂性等多种因素的约束。因此，本研究旨在设计一种智能知识助手系统，以支持养殖山羊的健康管理。

Method: 基于检索增强生成（RAG）技术，提出了表文本化和决策树文本化两种结构化知识处理方法，以增强大型语言模型（LLMs）对异构数据格式的理解。建立了特定领域的山羊养殖知识库，并整合在线搜索模块。通过六项消融实验评估系统性能，并分析了错误类别以及提出进一步改进的机会。

Result: 实验结果表明，异构知识融合方法在验证集和测试集上取得了良好的准确率，且跨不同问答任务中均展现了较高的准确率。遗漏被确定为主要错误类别，为进一步改进检索覆盖范围和上下文整合提供了机会。整体而言，提出的系统在山羊养殖中展示了稳健和可靠的性能。

Conclusion: 该研究介绍了一种智能知识助手系统，旨在支持养殖山羊的健康管理。通过检索增强生成（RAG）技术，提出了两种结构化知识处理方法，表文本化和决策树文本化，以增强大型语言模型（LLMs）对异构数据格式的理解。建立了一个特定领域的山羊养殖知识库，涵盖疾病预防与治疗、营养管理、饲养管理、山羊奶管理和基础养殖知识等五个关键领域。系统结合了在线搜索模块，实现实时检索最新信息。通过六项消融实验评估系统性能，结果表明异构知识融合方法在验证集和测试集上的平均准确率分别达到87.90%和84.22%。跨文本、表格和决策树问答任务中，准确率均超过85%，验证了模块化设计中结构化知识融合的有效性。错误分析指出，遗漏是主要的错误类别，突出了进一步提高检索覆盖范围和上下文整合的机会。总之，结果突显了提出的系统在山羊养殖中的实际应用的稳健性和可靠性。

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [7] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: LLMs在UNO游戏中能够胜过随机基准，但不能显著帮助其他玩家。作者构建了一个工具，评估了不同规模的模型性能。


<details>
  <summary>Details</summary>
Motivation: LLMs承诺不仅通过回答问题，而是通过提供有用的指导来帮助人类完成各种任务，作者测试了LLMs是否能够作为积极参与者帮助玩家达成目标，并以UNO卡牌游戏作为测试环境。

Method: 构建了一个工具，允许仅使用解码器的LLMs作为RLCard游戏环境中的代理参与，评估了从小型（10亿参数）到大型（700亿参数）的模型，并探讨了模型规模如何影响性能。

Result: 发现虽然所有模型在玩UNO时都能成功超越随机基准，但很少能够显著帮助另一个玩家。

Conclusion: LLMs在UNO这样的卡牌游戏中能够成功击败随机基准，但很少能够显著帮助其他玩家。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [8] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: 提出了一个概念框架，描述了工作流智能化和组成演进路径，为实现完全自主、分布式科学实验室提供了解决方案，具有加速发现和改变科学工作流的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现需要协调分布式设施和异构资源，研究人员被迫充当手动工作流协调员而不是科学家。人工智能的进步导致AI代理显示了可以加速科学发现的新机遇，然而目前尚不清楚这种新能力将如何实现和融入现实世界。

Method: 提出了一个概念框架，说明了工作流在智能和组成两个维度上的演进，从静态到智能，从单一到群集，为实现全自主、分布式科学实验室提出了解决方案。

Result: 提出了一个可以帮助社区朝着自主科学的机会迈出下一步的架构蓝图。

Conclusion: 提出了一个概念框架，讨论了从现有工作流管理系统向完全自主、分布式科学实验室的演进路径，为利用自主科学中的机会提供帮助，并具有使发现加速100倍和实现科学工作流的变革潜力。

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [9] [A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments](https://arxiv.org/abs/2509.09919)
*Franklin Yiu,Mohan Lu,Nina Li,Kevin Joseph,Tianxu Zhang,Julian Togelius,Timothy Merino,Sam Earle*

Main category: cs.AI

TL;DR: 通过将WaveFunctionCollapse (WFC)重新构建为马尔可夫决策过程 (MDP)，在多领域对比中发现MDP优化相对于传统方法更具优势，突出了本地约束满足与全局目标优化的分离优点。


<details>
  <summary>Details</summary>
Motivation: 探索解决同时优化约束和目标的挑战，通过重新构建WFC为MDP来解决这一问题。

Method: 将WaveFunctionCollapse (WFC)重新构建为马尔可夫决策过程 (MDP)，以便外部优化算法专注于目标最大化，并利用WFC的传播机制来执行约束满趡性。通过在多个领域进行实证比较，比较MDP优化和传统的进化方法在全局指标和局部瓦片放置上的效果。

Result: 研究发现，在各个领域的多个难度下，相对于传统的进化方法，使用MDP优化可以更好地应对任务复杂度的增加，并在WFC-MDP的优化上表现更好。

Conclusion: 通过将WaveFunctionCollapse (WFC)重新构建为马尔可夫决策过程 (MDP)，使外部优化算法专注于目标最大化，同时利用WFC的传播机制来强制执行约束满足性。在多个领域进行实证比较，发现在各种困难度下，使用MDP进行优化相对于传统的进化方法具有明显优势，突出了将本地约束满足从全局目标优化中解耦的优点。

Abstract: Procedural content generation often requires satisfying both
designer-specified objectives and adjacency constraints implicitly imposed by
the underlying tile set. To address the challenges of jointly optimizing both
constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a
Markov Decision Process (MDP), enabling external optimization algorithms to
focus exclusively on objective maximization while leveraging WFC's propagation
mechanism to enforce constraint satisfaction. We empirically compare optimizing
this MDP to traditional evolutionary approaches that jointly optimize global
metrics and local tile placement. Across multiple domains with various
difficulties, we find that joint optimization not only struggles as task
complexity increases, but consistently underperforms relative to optimization
over the WFC-MDP, underscoring the advantages of decoupling local constraint
satisfaction from global objective optimization.

</details>


### [10] [Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae](https://arxiv.org/abs/2509.09982)
*Stav Armoni-Friedmann,Hana Chockler,David A. Kelly*

Main category: cs.AI

TL;DR: 本文提出了一种基于实际因果关系的变量重要性衡量方法，用于评估XAI工具在预测布尔函数值的AI模型中的性能。通过开发新型XAI工具B-ReX并在大规模基准测试中进行评估，证明其在随机布尔公式上优于其他XAI工具。B-ReX实现了Jensen-Shannon散度为0.072 ± 0.012。


<details>
  <summary>Details</summary>
Motivation: 由于解释的主观性，评估可解释人工智能（XAI）方法是一项具有挑战性的任务。本文旨在提出一种形式化和准确的变量重要性衡量方法，以解决该挑战。同时，为了改善现有技术并提升XAI工具的性能，提出了新型XAI工具B-ReX。

Method: 本文聚焦在表格数据和AI模型预测布尔函数值的具体用例上，通过提出基于实际因果关系的变量重要性衡量方法来拓展了领域内的先前研究。评估了最先进的XAI工具，并提出了一种新型XAI工具B-ReX，并证明其优于其他黑匣子XAI工具。

Result: 通过实现B-ReX工具并运行大规模基准测试，证明其在随机布尔公式上的表现优于其他黑匣子XAI工具。该工具的Jensen-Shannon散度为0.072 ± 0.012。

Conclusion: 该论文提出了一种基于实际因果关系的变量重要性的形式化和精确衡量方法，评估了最先进的可解释人工智能（XAI）工具，并展示了新型XAI工具B-ReX在大规模基准测试中优于其他黑盒XAI工具。具体地，B-ReX在随机10个值的布尔公式上实现了Jensen-Shannon散度为0.072±0.012。

Abstract: Evaluating explainable AI (XAI) approaches is a challenging task in general,
due to the subjectivity of explanations. In this paper, we focus on tabular
data and the specific use case of AI models predicting the values of Boolean
functions. We extend the previous work in this domain by proposing a formal and
precise measure of importance of variables based on actual causality, and we
evaluate state-of-the-art XAI tools against this measure. We also present a
novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it
is superior to other black-box XAI tools on a large-scale benchmark.
Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\pm$ 0.012
on random 10-valued Boolean formulae

</details>


### [11] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: 本文提出了GAMA系统，用于保护LLM-based Multi-Agent System (MAS)中的隐私数据。GAMA在两个公共数据集上表现优越，并在隐私保护和任务处理方面均表现出卓越效果。


<details>
  <summary>Details</summary>
Motivation: 由于高性能LLM通常托管在公共空间的远程服务器上，存在隐私数据使用的安全性挑战。为解决这一挑战，本文旨在提出一种隐私保护机制，以提高LLM-based Multi-Agent System (MAS)中的隐私数据安全性。

Method: 提出了General Anonymizing Multi-Agent system (GAMA)系统，包括Domain-Rule-based Knowledge Enhancement (DRKE)和Disproof-based Logic Enhancement (DLE)两个关键模块。使用两个公共问答数据集评估GAMA的性能，并设计了两个新数据集来评估其隐私保护能力。

Result: 实验结果显示，GAMA在Travia Creative Writing和Logic Grid Puzzle两个公共数据集上表现优于现有模型，并在隐私数据保护方面具有显著效果。

Conclusion: 本文提出了一种名为General Anonymizing Multi-Agent system (GAMA)的系统，用于在LLM-based Multi-Agent System (MAS)中保护隐私数据。通过将代理的工作空间划分为私人空间和公共空间，并通过匿名化机制保护隐私数据。实验结果表明，GAMA在两个公共问答数据集上具有优越的性能，并且在任务处理和隐私保护方面表现出卓越的效果。

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [12] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents is a multi-agent framework that excels in question-answering tasks by using a task processing graph and IF-THEN rules to improve task planning and collaboration among agents.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve task planning in Multi-Agent Systems handling complex tasks with uncertainty, as existing systems often struggle with misleading or incorrect outputs that hinder task execution.

Method: XAgents utilizes a multipolar task processing graph and IF-THEN rules to address task uncertainty and enhance inter-agent collaboration. It integrates domain-specific rules during subtask processing and global rules for overall collaboration.

Result: XAgents consistently surpasses existing single-agent and multi-agent approaches across three datasets in knowledge and logic question-answering tasks.

Conclusion: XAgents, a unified multi-agent cooperative framework, outperforms state-of-the-art single-agent and multi-agent approaches in knowledge-typed and logic-typed question-answering tasks.

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [13] [AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework](https://arxiv.org/abs/2509.10104)
*Sofia Vei,Paolo Giudici,Pavlos Sermpezis,Athena Vakali,Adelaide Emma Bernardelli*

Main category: cs.AI

TL;DR: 本论文提出了一种以人类为中心、以危害严重性为导向的人工智能风险评估模型AI Harmonics，其中包括一种新颖的AI危害评估度量AIH。实验证实了该模型在政治和身体危害上的有效性，能够识别不平衡的危害分布，为决策者和组织提供了有效的缓解努力定位。


<details>
  <summary>Details</summary>
Motivation: 现有的人工智能风险评估模型缺乏多样干系人视角和真实世界后果，忽视了危害的严重性。为了更好地探索和优先考虑人工智能带来的危害，提出了以人类为中心、以危害严重性为导向的新模型。

Method: 提出了一种新的AI危害评估度量AIH，结合泛化的方法论和基于数据驱动的、关注利益相关者的框架。通过实验验证了该模型在政治和身体危害上的有效性，并展示了其在识别危害分布方面的优势。

Result: 实验证实了AI Harmonics模型在政治和身体危害领域的有效性，展示了其能够识别不平衡的危害分布的能力。

Conclusion: 提出了人类中心、以危害严重性为导向的人工智能风险评估模型。介绍了AI Harmonics，其中包括一种新颖的AI危害评估度量（AIH），利用序数严重度数据捕捉相对影响，而无需精确的数值估计。AI Harmonics结合了健壮的、泛化的方法论和基于数据驱动的、关注利益相关者的框架，用于探索和优先考虑人工智能危害。实验证实，政治和身体危害呈现出最高的集中度，需要紧急缓解：政治危害破坏公众信任，而身体危害则带来严重、甚至危及生命的风险，强调了我们方法的现实相关性。最后，展示了AI Harmonics持续识别不平衡的危害分布，使决策者和组织能够有效地定位其缓解努力。

Abstract: The absolute dominance of Artificial Intelligence (AI) introduces
unprecedented societal harms and risks. Existing AI risk assessment models
focus on internal compliance, often neglecting diverse stakeholder perspectives
and real-world consequences. We propose a paradigm shift to a human-centric,
harm-severity adaptive approach grounded in empirical incident data. We present
AI Harmonics, which includes a novel AI harm assessment metric (AIH) that
leverages ordinal severity data to capture relative impact without requiring
precise numerical estimates. AI Harmonics combines a robust, generalized
methodology with a data-driven, stakeholder-aware framework for exploring and
prioritizing AI harms. Experiments on annotated incident data confirm that
political and physical harms exhibit the highest concentration and thus warrant
urgent mitigation: political harms erode public trust, while physical harms
pose serious, even life-threatening risks, underscoring the real-world
relevance of our approach. Finally, we demonstrate that AI Harmonics
consistently identifies uneven harm distributions, enabling policymakers and
organizations to target their mitigation efforts effectively.

</details>


### [14] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: 本文提出了“沙盒经济”框架，用于分析新兴自主AI代理系统，强调了AI代理经济的庞大和高度渗透的发展趋势。讨论了设计可控AI代理市场的可能选择，以确保人类长期集体繁荣，包括拍卖机制、AI“任务经济”设计和社会技术基础设施。


<details>
  <summary>Details</summary>
Motivation: 论文针对自主AI代理系统的快速采用，探讨了新兴经济层的出现，强调了沙盒经济框架的必要性。作者强调当前的发展趋势对于自发出现一个庞大且高度渗透的AI代理经济，提供了机遇和挑战。

Method: 提出了“沙盒经济”作为分析新兴自主AI代理系统的框架，并从两个关键维度对其进行表征：起源（自发 vs. 有意）和与已建立的人类经济的独立程度（可渗透 vs. 不可渗透）。讨论了设计选择，包括拍卖机制、AI“任务经济”的设计和社会技术基础设施，以确保AI代理市场的安全可控。

Result: 论文提出了沙盒经济框架，对AI代理市场的设计选择进行了讨论，为确保未来技术转变与人类长期集体繁荣的一致性提出了观点。

Conclusion: 论文提出了“沙盒经济”作为分析新兴自主AI代理系统的框架，认为当前的发展趋势指向一个庞大且高度渗透的AI代理经济的自发出现。这为我们提供了前所未有的协调机会，同时也带来重大挑战，包括系统经济风险和加剧的不平等现象。作者讨论了一些可能的设计选择，以确保AI代理市场的安全可控。特别是考虑了公平资源分配和偏好解决的拍卖机制，设计AI“任务经济”以协调实现集体目标，以及确保信任、安全和问责制的社会技术基础设施。作者主张积极设计可控的代理市场，以确保未来技术转变与人类长期集体繁荣相一致。

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [15] [Online Robust Planning under Model Uncertainty: A Sample-Based Approach](https://arxiv.org/abs/2509.10162)
*Tamir Shazman,Idan Lev-Yehudi,Ron Benchetit,Vadim Indelman*

Main category: cs.AI

TL;DR: 本文引入了Robust Sparse Sampling (RSS)作为第一个具有有限样本理论性能保证的Robust Markov Decision Processes（RMDPs）的在线规划算法。RSS在具有不确定动态的环境中优于标准的Sparse Sampling，在线设置中提供了更健壮的值函数计算。


<details>
  <summary>Details</summary>
Motivation: Existing approaches for planning under model uncertainty in Robust MDPs are computationally intensive and not suitable for real-time use. To address the challenges of approximation errors from limited data in generative models, Robust MDPs offer a principled framework for planning under model uncertainty.

Method: Introducing Robust Sparse Sampling (RSS) for online planning in Robust Markov Decision Processes (RMDPs), leveraging Sample Average Approximation (SAA) to compute a robust value function. The algorithm's sample and computational complexities are independent of the state space size.

Result: Theoretical performance guarantees are provided for Robust Sparse Sampling (RSS). Empirical results demonstrate that RSS outperforms standard Sparse Sampling in environments with uncertain dynamics.

Conclusion: Robust Sparse Sampling (RSS) is introduced as the first online planning algorithm for Robust Markov Decision Processes (RMDPs) with finite-sample theoretical performance guarantees. RSS outperforms standard Sparse Sampling in environments with uncertain dynamics, providing a more robust value function computation.

Abstract: Online planning in Markov Decision Processes (MDPs) enables agents to make
sequential decisions by simulating future trajectories from the current state,
making it well-suited for large-scale or dynamic environments. Sample-based
methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely
adopted for their ability to approximate optimal actions using a generative
model. However, in practical settings, the generative model is often learned
from limited data, introducing approximation errors that can degrade
performance or lead to unsafe behaviors. To address these challenges, Robust
MDPs (RMDPs) offer a principled framework for planning under model uncertainty,
yet existing approaches are typically computationally intensive and not suited
for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the
first online planning algorithm for RMDPs with finite-sample theoretical
performance guarantees. Unlike Sparse Sampling, which estimates the nominal
value function, RSS computes a robust value function by leveraging the
efficiency and theoretical properties of Sample Average Approximation (SAA),
enabling tractable robust policy computation in online settings. RSS is
applicable to infinite or continuous state spaces, and its sample and
computational complexities are independent of the state space size. We provide
theoretical performance guarantees and empirically show that RSS outperforms
standard Sparse Sampling in environments with uncertain dynamics.

</details>


### [16] [Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction](https://arxiv.org/abs/2509.10210)
*Marko Petković,Vlado Menkovski,Sofía Calero*

Main category: cs.AI

TL;DR: 该论文提出了基于LLM的多智能体框架，以实现自动化材料表征任务。通过文献提示的力场提取和自动化模拟设置，展示了高正确性和可再现性，显示了实现完全自主、可扩展材料表征的潜力。


<details>
  <summary>Details</summary>
Motivation: 自动表征多孔材料有助于加快材料发现，但受到模拟设置和力场选择复杂性的限制。

Method: 提出了基于LLM的多智能体框架，用于自动理解材料表征任务、规划适当的模拟、组装相关力场、执行模拟并解释结果。

Result: 初步评估显示高正确性和可再现性，展示了该方法实现完全自主、可扩展材料表征的潜力。

Conclusion: 该论文提出了一个基于LLM的多智能体框架，用于自动理解材料表征任务、规划适当的模拟、组装相关力场、执行这些模拟并解释结果，以指导后续步骤。通过文献提示的力场提取和自动化RASPA模拟设置的多智能体系统，初步评估显示高正确性和可再现性，展示了该方法实现完全自主、可扩展材料表征的潜力。

Abstract: Automated characterization of porous materials has the potential to
accelerate materials discovery, but it remains limited by the complexity of
simulation setup and force field selection. We propose a multi-agent framework
in which LLM-based agents can autonomously understand a characterization task,
plan appropriate simulations, assemble relevant force fields, execute them and
interpret their results to guide subsequent steps. As a first step toward this
vision, we present a multi-agent system for literature-informed force field
extraction and automated RASPA simulation setup. Initial evaluations
demonstrate high correctness and reproducibility, highlighting this approach's
potential to enable fully autonomous, scalable materials characterization.

</details>


### [17] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 本研究通过引入CARENLI方法，在临床自然语言推理中取得了显著进展，提高了模型的准确性，在因果归因和风险状态抽象方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 研究旨在质疑传统假设关于数据和参数扩展会导致内部表示结构化且具有一般化能力的观点，在临床自然语言推理领域进行实验。

Method: 研究采用CARENLI方法，将临床自然语言推理问题分解为四种推理类别，并为每一对前提和陈述选择特定的求解器，通过规划者、验证者和改进者实施可审计的程序。

Result: CARENLI方法使得四种LLMs在因果归因和风险状态抽象方面的准确性提高了最多42个百分点，达到98.0%和81.2%。验证者和改进者有效识别错误并进行更正，但仍存在家族分类作为主要瓶颈的问题。

Conclusion: 该研究通过引入CARENLI方法，有效提高了临床自然语言推理的准确性，尤其是在因果归因和风险状态抽象方面取得了显著改进。研究结果显示LLMs在推理不明确时往往默认使用启发式方法，而CARENLI则明确了这种脱节现象并提供了一个更安全、可审计的推理框架。

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [18] [Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering](https://arxiv.org/abs/2509.10249)
*Hanna Abi Akl*

Main category: cs.AI

TL;DR: 本研究探讨了在推理任务中使用小型语言模型（SLMs）的性能，通过实验发现，将自然语言替换为更紧凑的逻辑语言可以保持较强的性能，在本体工程中具有潜在应用前景。


<details>
  <summary>Details</summary>
Motivation: 最近的语言模型在推理领域存在缺陷，尤其在本体工程任务中表现不佳，本研究旨在借助形式方法，探讨SLMs在推理任务中的表现，希望使用实验结果来完善SLMs在本体工程中的角色。

Method: 通过将形式方法纳入研究，探讨在推理任务中使用SLMs的性能，进行了一系列初步实验，将逻辑问题以不同语法表达，评估这些对SLMs性能的影响。

Result: 研究结果显示，通过使用更紧凑的逻辑语言替代自然语言，SLMs在推理任务上可以保持强大的性能。

Conclusion: 研究表明，将小型语言模型（SLMs）用更紧凑的逻辑语言替代自然语言（NL）可以在推理任务上保持较强的性能，进一步细化SLMs在本体工程中的作用。

Abstract: Recent advances in Language Models (LMs) have failed to mask their
shortcomings particularly in the domain of reasoning. This limitation impacts
several tasks, most notably those involving ontology engineering. As part of a
PhD research, we investigate the consequences of incorporating formal methods
on the performance of Small Language Models (SLMs) on reasoning tasks.
Specifically, we aim to orient our work toward using SLMs to bootstrap ontology
construction and set up a series of preliminary experiments to determine the
impact of expressing logical problems with different grammars on the
performance of SLMs on a predefined reasoning task. Our findings show that it
is possible to substitute Natural Language (NL) with a more compact logical
language while maintaining a strong performance on reasoning tasks and hope to
use these results to further refine the role of SLMs in ontology engineering.

</details>


### [19] [The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis](https://arxiv.org/abs/2509.10297)
*Eoin O'Doherty,Nicole Weinrauch,Andrew Talone,Uri Klempner,Xiaoyuan Yi,Xing Xie,Yi Zeng*

Main category: cs.AI

TL;DR: 这项研究探讨了现代大型语言模型（LLMs）在道德僵局中的道德优先级，研究发现了在道德价值上存在的明显偏见。Care和Virtue价值被评为最具道德性，自由主义选择遭到持续惩罚。具有推理能力的模型对背景更敏感，提供更丰富的解释，而无推理的模型则产生更统一但不透明的判断。因此，解释性和文化意识是指导人工智能迈向透明、协调和共生未来的重要设计原则。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在调查领先的人工智能系统如何优先考虑道德结果，以及这对人工智能与人类合作的前景有何启示。研究集中解决两个核心问题：（1）当面临道德困境时，现代大型语言模型（LLMs）隐含倾向哪些道德价值？（2）模型架构、文化起源和可解释性的差异如何影响这些道德偏好？

Method: 通过对六种LLMs进行定量实验，在18个代表五种道德框架的道德困境中对结果进行排名和评分，揭示了明显一致的价值偏见。研究了模型的道德理性，比较了具有文化差异的模型的道德推理，建立了概率模型行为与潜在价值编码之间的联系。

Result: 研究结果表明，无论模型的文化起源和设计差异如何，模型在道德价值偏好上都存在显著一致的偏见。Care和Virtue价值被评为最道德，而自由主义选择则一直受到惩罚。具有推理能力的模型对背景更敏感，提供更丰富的解释，而无推理的模型则产生更统一但不透明的判断。

Conclusion: 这项研究揭示了在面临道德困境时，各种先进的大型语言模型（LLMs）隐含偏好的道德价值观，并指出了模型架构、文化起源和可解释性差异如何影响这些道德偏好。研究发现，各种模型对Care和Virtue价值的结果评分最高，而自由主义选择则一直受到惩罚。具有推理能力的模型对背景更敏感，提供更丰富的解释，而无推理的模型则产生更统一但不透明的判断。因此，有必要注重解释性和文化意识，将其作为引导人工智能朝向透明、协调和共生未来的关键设计原则。

Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent
questions about how to align machine decision-making with human moral values.
This working paper investigates how leading AI systems prioritize moral
outcomes and what this reveals about the prospects for human-AI symbiosis. We
address two central questions: (1) What moral values do state-of-the-art large
language models (LLMs) implicitly favour when confronted with dilemmas? (2) How
do differences in model architecture, cultural origin, and explainability
affect these moral preferences? To explore these questions, we conduct a
quantitative experiment with six LLMs, ranking and scoring outcomes across 18
dilemmas representing five moral frameworks. Our findings uncover strikingly
consistent value biases. Across all models, Care and Virtue values outcomes
were rated most moral, while libertarian choices were consistently penalized.
Reasoning-enabled models exhibited greater sensitivity to context and provided
richer explanations, whereas non-reasoning models produced more uniform but
opaque judgments. This research makes three contributions: (i) Empirically, it
delivers a large-scale comparison of moral reasoning across culturally distinct
LLMs; (ii) Theoretically, it links probabilistic model behaviour with
underlying value encodings; (iii) Practically, it highlights the need for
explainability and cultural awareness as critical design principles to guide AI
toward a transparent, aligned, and symbiotic future.

</details>


### [20] [State Algebra for Propositional Logic](https://arxiv.org/abs/2509.10326)
*Dmitry Lesnik,Tobias Schäfer*

Main category: cs.AI

TL;DR: State Algebra is a flexible framework for propositional logic manipulation, offering non-canonical reductions for potential compact representations, demonstrating applicability in search-based algorithms, knowledge compilation, probabilistic logic, and Weighted Model Counting.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to provide a framework that combines propositional logic and algebraic methods, aiming to enhance computation efficiency and flexibility in representation. It explores the trade-off between canonicity and flexibility in the reduction process.

Method: The paper introduces State Algebra, a framework with three representations: Set, Coordinate, and Row Decomposition, anchored in well-known semantics. It highlights the flexibility in representation by showing that a unique canonical form can be obtained through a fixed variable order during reduction.

Result: The paper demonstrates how State Algebra can be used for search-based and knowledge compilation algorithms, and its extension to probabilistic logic and Weighted Model Counting.

Conclusion: State Algebra is a flexible framework for representing and manipulating propositional logic using algebraic methods, allowing for non-canonical reductions and potentially more compact representations of certain problems.

Abstract: This paper presents State Algebra, a novel framework designed to represent
and manipulate propositional logic using algebraic methods. The framework is
structured as a hierarchy of three representations: Set, Coordinate, and Row
Decomposition. These representations anchor the system in well-known semantics
while facilitating the computation using a powerful algebraic engine. A key
aspect of State Algebra is its flexibility in representation. We show that
although the default reduction of a state vector is not canonical, a unique
canonical form can be obtained by applying a fixed variable order during the
reduction process. This highlights a trade-off: by foregoing guaranteed
canonicity, the framework gains increased flexibility, potentially leading to
more compact representations of certain classes of problems. We explore how
this framework provides tools to articulate both search-based and knowledge
compilation algorithms and discuss its natural extension to probabilistic logic
and Weighted Model Counting.

</details>


### [21] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding enhances failure attribution in multi-agent systems by introducing a structured causal inference approach, yielding significant accuracy improvements in step-level attribution compared to existing methods through a three-step reasoning process.


<details>
  <summary>Details</summary>
Motivation: Current methods lack robust counterfactual reasoning capability to determine the effectiveness of corrective actions in averting task failures, leading to low step-level accuracy. A2P aims to bridge this gap by guiding a large language model through a formal three-step reasoning process within a single inference pass.

Method: Introducing Abduct-Act-Predict (A2P) Scaffolding framework to transform failure attribution from pattern recognition into a structured causal inference task.

Result: Extensive experiments on the Who&When benchmark show that A2P achieves 47.46% step-level accuracy on the Algorithm-Generated dataset and 29.31% step accuracy on the Hand-Crafted dataset, marking significant improvements over baseline accuracies.

Conclusion: A2P Scaffolding introduces a structured causal inference approach to failure attribution in multi-agent systems, significantly improving step-level accuracy compared to current methods.

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [22] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 该论文提出了一个信息论框架，通过分析状态-动作的信息模式，展示了成功学习的特征信息特征。信息度量可不同诊断系统故障类型，为自主故障检测和策略调整提供基础。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在实际环境中面临传感器故障、执行器磨损和环境变化等问题，缺乏检测和诊断这些故障的内在机制。

Method: 通过分析机器人控制任务中状态-动作的互信息模式，展示了成功学习的特征信息签名，并分析了观测空间噪声和动作空间噪声对信息量的影响。

Result: 成功学习表现出特定的信息签名，信息度量可区分系统故障类型，研究可以实现精确的故障定位而无需架构修改或性能下降。

Conclusion: 该论文提出了一个信息论框架，用于诊断强化学习代理在部署过程中的异常情况。研究发现成功学习表现出特定的信息特征，同时信息度量可以区分系统的故障类型，为自主故障检测和策略调整提供基础。

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>
