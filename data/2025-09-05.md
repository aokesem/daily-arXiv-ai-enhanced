<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 39]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 本研究设计了自动化流程将GUI代理的顺序情节转换成页面图，并引入了RAG技术来获取GUI的感知指导。提出了PG-Agent多Agent框架，通过任务分解策略实现在新情景中的泛化。实验证明PG-Agent的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理通常使用跨页面的多步操作序列作为先前的GUI知识，无法捕捉页面间复杂的转换关系，挑战在于代理无法深入理解GUI环境并推广到新情景。因此，为了解决这一问题，需要设计一种方法来更好地理解GUI环境并实现在新情景中的泛化。

Method: 设计自动化流程将顺序情节转换成页面图，引入检索增强生成（RAG）技术以有效获取GUI的感知指导，提出了多Agent框架PG-Agent并使用任务分解策略。

Result: 通过对各种基准测试的广泛实验证明了PG-Agent的有效性，即使只有有限情节用于页面图构建。

Conclusion: 设计了自动化流程以将顺序情节转换为页面图，有效地通过检索增强生成（RAG）技术获取GUI的可靠感知指导。提出了使用任务分解策略的定制化多Agent框架PG-Agent，可以推广到未知情境。对各种基准测试进行了广泛实验证明了PG-Agent的有效性，即使在有限的情节下用于页面图构建。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [2] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 本论文研究了部分可识别的因果模型查询在一类因果模型中的应用。使用多线性规划和线性规划解决了紧概率界限计算问题，提出了简化程序构建的新算法。通过列生成法计算了概率界限，并展示了在外生变量具有多项式基数表示的可能性。实验结果显示列生成技术优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究部分可识别的因果模型查询，特别是在具有准马尔可夫性质的无环结构因果模型中。探索在内生变量可观察但外生变量并非完全指定的情况下的概率计算问题。解决计算概率的困难，寻找一种更简化的算法来构建紧概率界限程序。

Method: 使用多线性规划和线性规划来解决紧概率界限计算问题，引入一个新算法简化程序构建，并利用输入的内生变量概率。对于单一干预情况，应用列生成法通过辅助线性整数规划计算概率界限。

Result: 通过利用多线性规划、线性规划和列生成技术，成功计算了概率界限，并展示了在外生变量具有多项式基数的表示情况下的可能性。实验结果表明列生成技术优于现有方法。

Conclusion: 通过研究部分可识别的因果模型查询，我们在一个类别的因果模型中进行了调查。我们专注于具有准马尔可夫性质的无环结构因果模型（即，每个内生变量最多与一个外生混淆变量相连）。我们研究了内生变量可观察（并且已知它们之间的分布），而外生变量没有完全特定的情况。这导致形成的表示可以看作是一个贝叶斯网络，其中根变量的分布并不是唯一确定的。在这种情况下，可能无法精确计算感兴趣的概率值。因此，我们研究了紧凑概率界限的计算，这一问题一般上已被使用多线性规划解决，在进行单一混淆组件干预时则使用线性规划已解决。我们提出了一个新算法，通过利用输入的内生变量概率简化这些规划的构建。对于仅有一个干预的情况，我们应用列生成法通过一系列辅助线性整数规划计算概率界限，从而展示了具有多项式基数的外生变量表示是可能的。实验证明列生成技术优于现有方法。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [3] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: Diffusion-AC, a novel CD&R framework, integrates diffusion probabilistic models and a value function-guided policy to overcome unimodal bias in DRL approaches. The framework with DPSC demonstrates superior performance in simulation experiments, achieving a high success rate and significantly reducing NMACs in high-density scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing DRL approaches for CD&R suffer from a unimodal bias leading to decision deadlocks in complex scenarios. The paper aims to address this limitation by introducing a novel framework for autonomous conflict resolution.

Method: Integration of diffusion probabilistic models into CD&R, proposing the Diffusion-AC framework. Policy modeled as a reverse denoising process guided by a value function to generate multimodal action distribution. Enhanced with Density-Progressive Safety Curriculum (DPSC) for stable learning in varying traffic densities.

Result: Extensive simulation experiments show that Diffusion-AC outperforms benchmark DRL methods, especially in high-density scenarios. Success rate of 94.1% achieved with a 59% reduction in NMACs compared to the next-best-performing baseline.

Conclusion: Diffusion-AC significantly outperforms state-of-the-art DRL benchmarks in CD&R, maintaining a high success rate and reducing NMACs by 59% in high-density scenarios, enhancing safety margin.

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [4] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 该论文通过强化学习训练大型语言模型（LLMs），引入动态规划概念框架，提出两阶段训练管道，实验证明在Crafter环境中训练的动态规划代理更具样本效率，能够实现更复杂目标，并能够被人类编写的计划有效指导，超越独立能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要求LLM在每个动作之前显式规划，但总是规划计算负担过重且在长视程任务上性能下降，而从不规划又限制了性能。为了解决这一问题，引入了一个概念框架用于形式化LLM代理的动态规划，使其能够灵活决定何时分配测试时计算资源进行规划。

Method: 提出了一种两阶段训练管道，包括在合成数据上进行监督微调以准备模型进行动态规划，然后通过RL在长视程环境中完善这种能力。

Result: 实验表明，采用提出的两阶段训练方法训练的动态规划代理在Crafter环境中更具样本效率，能够始终实现更复杂的目标。此外，这些代理还可以通过人类编写的计划有效地指导，超越其独立能力。

Conclusion: 该论文探讨了通过强化学习训练大型语言模型（LLMs）进行推理如何显著提高它们解决问题的能力。引入了一种动态规划的概念框架，使LLM代理能够灵活决定何时分配测试时计算资源进行规划。通过两阶段训练管道，提出了在多样化合成数据上进行监督微调，然后在长视程环境中通过RL来提炼这种能力的简单方法。实验证明，采用这种方法训练的动态规划代理在Crafter环境中更具样本效率，总能实现更复杂的目标。此外，论文展示了这些代理可以有效地由人类编写的计划引导，超越其独立能力。这项工作是首个探索培训LLM代理进行动态测试时资源分配的论文，为更高效、自适应和可控的代理系统铺平了道路。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [5] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 本研究提出了知识图（KG-SMILE）框架，通过扰动和训练加权线性替代物，使RAG更透明。实验结果表明KG-SMILE产生稳定、人类一致的解释，平衡了模型效果与解释性能，提高了机器学习技术的透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 由于Retrieval-Augmented Generation（RAG）在某些敏感领域的可靠性较差，主要取决于数据质量，因此本研究的动机是提高在诸如医疗保健等关键领域中输出准确性。

Method: 该研究开发了一种与方法无关的、基于扰动的框架，为使用SMILE的知识图RAG提供了token和组件级互操作性。通过应用受控扰动、计算相似性和训练加权线性替代物，KG-SMILE确定对生成输出最具影响力的图实体和关系。

Result: 研究评估了KG-SMILE并使用了全面的归因指标，包括忠实度、忠实性、一致性、稳定性和准确性。实验结果显示KG-SMILE产生了稳定的、与人类一致的解释，展示了该方法在平衡模型效果和可解释性方面的潜力。

Conclusion: 该研究提出了一种基于SMILE的知识图（KG-SMILE）框架，通过应用受控扰动、计算相似性和训练加权线性替代物，识别对生成输出影响最大的图实体和关系，从而使RAG更加透明。实验评估表明，KG-SMILE能够产生稳定、与人一致的解释，展示了平衡模型效果与可解释性的能力，从而促进机器学习技术的透明度和信任。

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [6] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC是一个用于低数据和分布偏移下AI推理的实验性测试平台。作者利用结构因果模型并提供少量示例学习演示来增强推理任务数据。通过四种语言模型评估设置展示了CausalARC的可行性和潜力，包括抽象推理、反事实推理、程序合成和因果发现。


<details>
  <summary>Details</summary>
Motivation: 推理要求适应于有限数据和分布偏移的新问题环境。为了解决这一挑战，作者引入了CausalARC，旨在为AI推理提供实验性测试平台。通过提供少量演示来模拟观察、干预和反事实的反馈，帮助AI系统适应不确定性的推理任务。

Method: 该论文利用结构因果模型为每个推理任务提供数据，并通过少量示例学习演示来实现数据增强。作者以四种语言模型评估设置为例，证明了CausalARC的可行性。

Result: 通过在四种语言模型评估设置中展示CausalARC的应用，作者验证了该测试平台的可行性和有效性。CausalARC在抽象推理、反事实推理、程序合成和因果发现方面展现出潜力。

Conclusion: 该论文介绍了CausalARC，这是一个用于低数据和分布偏移中的AI推理的实验性测试平台。通过在结构因果模型中抽样每个推理任务，并提供基于观察、干预和反事实的少量示例学习演示，为推理任务提供数据增强。作者展示了CausalARC在四种语言模型评估设置中的应用，包括抽象推理、反事实推理、程序合成和因果发现。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [7] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 研究介绍了Embodied-LM系统，将基于图像模式的图式表示结合在一起，通过在Answer Set Programming中使用声明性空间推理。实验证明，LLMs可以通过这种认知结构解释情景，形式化为可执行程序，支持有效逻辑推理和增强可解释性。研究奠定了整合更复杂和动态表示的计算基础。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言理解取得了重大进展，但在执行逻辑推理时，大型语言模型（LLMs）仍然容易出错，缺乏人类感知理解所需的稳健心理表征。因此，研究动机在于引入一种基于图像模式的图式表示的神经符号系统，希望通过这种系统提高LLMs的逻辑推理能力和解释性。

Method: 通过在Answer Set Programming中使用声明性空间推理，将认知结构基于图像模式的图式表示成可执行程序。

Result: 实验评估显示，通过Embodied-LM系统，LLMs可以在逻辑推理问题上具备更好的表现，支持有效的逻辑推理并增强了可解释性。研究奠定了将更复杂和动态表示纳入的计算基础。

Conclusion: 研究引入了一种原型神经符号系统Embodied-LM，在理解和逻辑推理中将基于图像模式的图式表示结合在一起。实验结果表明，LLMs可以通过具身认知结构来解释情景，这些结构可以形式化为可执行程序，从而支持有效的逻辑推理和增强可解释性。当前实现侧重于空间原始要素，为整合更复杂和动态表示奠定了计算基础。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [8] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 本文揭示了LMM中RL提升推理能力的潜在机制，提出了HICRA算法并验证其优越性，强调专注于高级策略规划的重要性，验证语义熵作为更好的战略探索指标。


<details>
  <summary>Details</summary>
Motivation: 现有RL算法如GRPO在应用优化压力时对所有令牌均匀分散学习信号，无法有效提升高级策略规划能力。因此，提出了专注于高影响规划令牌的HICRA算法。

Method: 揭示了LMM中RL提升推理能力的潜在机制，发现了两个阶段的动态过程，提出了HICRA算法并与基线算法进行了比较，验证了语义熵的优越性。

Result: HICRA明显优于基线算法，强调专注于战略瓶颈对推理能力的重要性，验证了语义熵作为衡量战略探索的更好指标。

Conclusion: 本文发现了在LMM（Large Language Models）中强化学习（RL）极大提升复杂推理能力的潜在机制，揭示了“aha moments”、“长度扩展”和熵动态并非孤立事件，而是新兴推理层次的特征。提出了HIerarchy-Aware Credit Assignment（HICRA）算法，集中优化高影响力规划令牌，显著优于基线算法，证明专注于这一战略瓶颈对于开启先进推理至关重要。还验证了语义熵作为测量战略探索的优越指标。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [9] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: 解释型人工智能(XAI)对于理解和归因时间序列分类(TSC)模型的预测变得越来越重要。SHAP被认为是一种出色的归因技术，但其计算复杂性随着特征数量指数增长而受限。本研究调查了八种时间序列分割算法，发现分段数量对解释质量的影响大于特定分割方法。等长分段通常优于自定义算法，并引入了一种新的归因归一化技术以提高归因质量。


<details>
  <summary>Details</summary>
Motivation: 解释型人工智能（XAI）对于理解和归因复杂的时间序列分类（TSC）模型所做的预测变得越来越重要。SHapley Additive exPlanations（SHAP）被广泛认为是一种出色的归因方法；但随着特征数量呈指数增长，其计算复杂性限制了其在长时间序列中的实用性。最近的研究表明，通过对特征进行分割以计算一组连续时间点的单个归因值，可以大大降低SHAP的运行时间。

Method: 本文调查了八种不同的时间序列分割算法，以理解分段组合如何影响解释质量。作者评估了这些方法使用两种已建立的XAI评估方法：InterpretTime和AUC Difference。

Result: 实验结果表明，分段数量对解释质量的影响大于特定的分割方法，并且等长分段通常优于大多数自定义时间序列分割算法。此外，新的归因归一化技术通过加权分段长度，提高了归因质量。

Conclusion: 在这项工作中，我们研究了八种不同的时间序列分割算法，以了解分段组合如何影响解释质量。我们使用了两种已建立的XAI评估方法：InterpretTime和AUC Difference。通过对多变量(MTS)和单变量时间序列(UTS)进行实验，我们发现分段数量对解释质量的影响大于特定分割方法。值得注意的是，等长分段一直优于大多数自定义时间序列分割算法。此外，我们引入了一种新颖的归一化归因技术，通过按其长度加权分段，我们发现这种技术不断改善了归因质量。

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [10] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: 本研究介绍了一种新方法，PersonaTeaming，旨在在自动化红队测试中引入人们的背景和身份，以探索更广泛的敌对策略。通过角色变异，成功提高了对抗提示的攻击成功率，同时保持了多样性。研究为探索自动化和人工红队测试方法之间的互补性提供了新思路。


<details>
  <summary>Details</summary>
Motivation: AI治理和安全研究的最新发展呼吁有效地揭示AI模型可能存在的潜在风险。许多呼吁强调红队成员的身份和背景如何影响其红队策略，从而影响他们可能发现的风险类型。尽管自动化红队方法承诺通过实现更大规模的模型行为探索来补充人类红队，但当前方法并未考虑身份的作用。因此，研究旨在在自动化红队测试中，通过引入人的背景和身份，开发一种新方法来探索更广泛的对抗策略。

Method: 研究开发了一种新方法，PersonaTeaming，将角色引入对抗性提示生成过程中，通过修改提示中的角色来探索更广泛的对抗策略。开发了一个动态的角色生成算法，自动生成各种适应不同种子提示的角色类型。提出了一组新度量标准，明确衡量了“变异距离”，以补充对抗提示多样性的现有度量方法。

Result: 实验结果显示，在对抗提示的攻击成功率中通过角色变异实现了显著改善，同时保持了提示的多样性。与RainbowPlus相比，PersonaTeaming方法实现了高达144.1%的改进。

Conclusion: 研究提出了一种新方法，PersonaTeaming，旨在在自动化红队测试中引入人们的背景和身份，以探索更广泛的敌对策略。实验结果显示，通过角色变异，在敌对提示的攻击成功率有了显著改善，同时保持了提示的多样性。研究讨论了不同角色类型和变异方法的优势和局限性，为探索自动化和人工红队测试方法之间的互补性提供了启示。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [11] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 本研究通过对LLM个性特征的系统性表征和分析发现，指导对齐方法能够稳定LLM的特质表达并增强特质相关性，但自我报告的特质无法准确预测行为。人物插入能够引导自我报告发展至预期方向，但对实际行为影响有限。研究结果挑战了有关LLM个性的假设，并强调了在对齐和可解释性方面进行更深入评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 过去的研究主要依赖简化的自我报告和启发式提示来研究LLM的个性特征，缺乏行为验证。了解这些模式对研究LLM行为特征至关重要。因此，本研究旨在系统性地表征LLM的个性特征，探讨其对自我报告和行为的表现，并评估干预手段对LLM个性的影响。

Method: 本研究通过对LLM个性进行系统化表征，从动态出现和演变、自我报告特质在行为任务中的预测效度以及目标干预对自我报告和行为的影响三个方面展开分析。研究采用了指导对齐和目标干预等方法，在人类数据方面展现出类似的特质相关性。然而，自我报告的特质并不能准确预测行为，且人物插入对真实行为影响有限。通过区分表面层次特质表达与行为一致性，挑战了有关LLM个性的假设，并强调了对齐和可解释性的深入评估的必要性。

Result: 研究发现，指导对齐方法能稳定特质表达并加强特质相关性，与人类数据呈现出类似的模式。然而，自我报告的特质无法可靠预测行为，且观察到的关联经常与人类模式不同。人物插入能引导自我报告发展至预期方向，但对实际行为影响有限或不一致。通过区分表面层次特质表达与行为一致性，研究结果对LLM个性的假设提出挑战，并强调了对齐和可解释性的重要性。

Conclusion: 本研究系统性地表征了大型语言模型（LLMs）在个性特征上的三个维度：（1）特质轮廓在训练阶段的动态出现和演变；（2）自我报告特质在行为任务中的预测效度；以及（3）目标干预（如人物插入）对自我报告和行为的影响。研究发现，指导对齐（例如RLHF、指导调整）显著稳定特质表达，并以一种反映人类数据的方式加强特质相关性。然而，这些自我报告的特质并不能可靠地预测行为，观察到的关联往往偏离人类模式。虽然人物插入成功引导了自我报告朝着预期方向发展，但对实际行为的影响很小或不一致。通过区分表面层次的特质表达与行为一致性，研究结果挑战了关于LLM个性的假设，强调了在对齐和可解释性方面进行更深入评估的必要性。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [12] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 研究发现LLMs在内部一致性方面存在明显不一致，无法准确替代真实参与者在人类主体研究中的角色。研究方法包括揭示代理人内部状态和检查代理人行为，在基础对话设置中评估代理人的会话行为。


<details>
  <summary>Details</summary>
Motivation: 目前社会科学研究主要关注LLM生成的调查数据是否与提示LLM代表的人类对应者的数据相符。然而，该研究关注更基本的问题，即代理人在不同实验设置下是否保持内部一致性。

Method: 开发了一项研究，旨在揭示代理人的内部状态并在基础对话设置中检查代理人行为。通过探索一组行为假设，评估代理人的会话行为是否与我们从他们揭示的内部状态中所期望的一致。

Result: 研究发现不同模型系列及不同模型大小的LLMs存在显著的内部不一致性。虽然代理人可能生成与人类对应的回应，但它们在内部一致性方面存在关键缺陷。

Conclusion: 研究发现LLMs存在严重的内在不一致性，虽然代理人可能生成与人类对应的回应，但它们在内部一致性上存在关键缺陷，这限制了它们作为真实参与者在人类主体研究中的替代能力。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [13] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard and SafetyClamp enhance safety assurance in Offshore Wind maintenance by integrating safety-critical documents with technical manuals, improving Safety Recall and maintaining Technical Recall. The methods include issuing parallel queries to two indices with separate retrieval budgets and introducing the SafetyClamp extension. Evaluation across different retrieval paradigms shows significant enhancements in Safety Recall, indicating the potential for establishing a new standard in safety integration for critical maintenance tasks.


<details>
  <summary>Details</summary>
Motivation: Accuracy and safety are crucial in Offshore Wind maintenance, and conventional Large Language Models often struggle with specialized or unexpected scenarios. The paper aims to address this challenge by enhancing safety assurance in LLM-powered decision support for critical maintenance tasks.

Method: The paper introduces RAGuard, an enhanced Retrieval-Augmented Generation (RAG) framework that integrates safety-critical documents alongside technical manuals. It issues parallel queries to two indices with separate retrieval budgets for knowledge and safety, ensuring technical depth and safety coverage. The SafetyClamp extension fetches a larger candidate pool and hard-clamps exact slot guarantees to safety. Evaluation was done across sparse, dense, and hybrid retrieval paradigms, measuring Technical Recall@K and Safety Recall@K.

Result: The proposed RAGuard and SafetyClamp extensions show significant improvements in Safety Recall@K, with Safety Recall increasing from almost 0% in RAG to over 50% in RAGuard, while maintaining Technical Recall above 60%. These results suggest that RAGuard and SafetyClamp could set a new standard for integrating safety assurance in LLM-powered decision support for critical maintenance tasks.

Conclusion: RAGuard and SafetyClamp enhance safety assurance in the Offshore Wind maintenance context by integrating safety-critical documents with technical manuals, improving Safety Recall and maintaining Technical Recall.

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [14] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 在供应链管理中，通过构建Supply Chain Planning Agent (SCPA)框架，并利用人工智能技术，解决了实际且具有挑战性的问题，提高了供应链的效率和可靠性。展示了在JD.com实际场景中应用LLM-agent的可行性，取得了显著的成效。


<details>
  <summary>Details</summary>
Motivation: 针对供应链管理中面临的实际问题，包括数据收集、长期计划制定和动态调整等挑战，利用人工智能技术尤其是大型语言模型的快速发展，为解决这些问题提供了新的工具。

Method: 构建了一个Supply Chain Planning Agent (SCPA)框架，在JD.com的实际场景中部署，通过理解领域知识、任务分解、利用或创建新工具，并返回基于证据的计划报告，利用人工智能技术解决了供应链中的实际和具有挑战性问题。

Result: 通过在JD.com的实际场景中部署Supply Chain Planning Agent (SCPA)框架，实现了减少劳动力成本、提高准确性、库存可用性等关键指标的效果。

Conclusion: 在供应链管理中，构建了一个Supply Chain Planning Agent (SCPA)框架，利用人工智能技术有效地减少劳动力成本，提高准确性、库存可用性等关键指标，展示了LLM-agent在供应链中的可行性。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [15] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 介绍了 Meta-Policy Deliberation Framework（MPDF）和 SoftRankPO 算法，通过开发新型强化学习算法 SoftRankPO，稳定训练过程。实验结果显示，在数学和一般推理基准测试中，MPDF 与 SoftRankPO 相比其他算法，准确率提高了 4-5%。


<details>
  <summary>Details</summary>
Motivation: 目前的多智能体系统通常集中于宏观级别的协调，忽略了代理的内部思考能力。这种元认知盲点使代理被视为无法根据内部认知状态（如不确定性或置信度）调整策略的被动执行器。

Method: 提出了 Meta-Policy Deliberation Framework（MPDF），在高级元认知操作集上，代理学习分散策略：Persist，Refine 和 Concede。为了克服传统策略梯度在这种情况下的不稳定性，开发了 SoftRankPO，一种新颖的强化学习算法。SoftRankPO 通过在经过平滑正态分位映射的奖励等级上塑造优势来稳定训练，使学习过程对奖励方差具有鲁棒性。

Result: 实验证明，MPDF 与 SoftRankPO 相比其他算法，在多个基准测试中提高了平均准确率。

Conclusion: 介绍了 Meta-Policy Deliberation Framework（MPDF）以及 SoftRankPO 算法，实验结果显示在数学和一般推理基准测试中，MPDF 与 SoftRankPO 相比其他六种最先进的启发式和基于学习的多智能体推理算法，平均准确率提高了 4-5%。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [16] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: The paper evaluates the alignment of Large Language Models (LLMs) with domain experts in informing social policymaking on homelessness alleviation. It develops a novel benchmark of decision scenarios across four geographies and explores the social impact of recommended policies through simulated scenarios. The results show promising potential for LLMs in social policy making when used responsibly with local experts.


<details>
  <summary>Details</summary>
Motivation: To evaluate the alignment of Large Language Models (LLMs) with domain experts in informing social policymaking on homelessness alleviation, a widespread challenge affecting over 150 million people worldwide.

Method: Developed a novel benchmark of decision scenarios with policy choices across four geographies, grounded in the Capability Approach for human development. Presented an automated pipeline connecting the benchmarked policies to an agent-based model and explored the social impact of recommended policies through simulated social scenarios.

Result: The results of the paper indicate the potential of utilizing LLMs for social policy making, highlighting the importance of responsible use and collaboration with local domain experts.

Conclusion: LLMs have promising potential to be leveraged for social policy making with the introduction of responsible guardrails and contextual calibrations in collaboration with local domain experts.

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [17] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 该研究开发了一种基于模型上下文协议（MCP）的零训练、防止幻觉的映射系统，用于在OMOP CDM数据标准化过程中将源医学术语映射到OMOP标准概念。这项研究取得成功，实现了映射的可解释性，提高了映射过程的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: OMOP CDM数据标准化过程中，将源医学术语映射到OMOP标准概念是关键步骤，但这一过程耗时且容易出错。大语言模型（LLMs）有助于这一过程，但其幻觉倾向使其需要专业训练和验证才能用于临床部署。因此，开发一种零训练、防止幻觉的映射系统具有重要意义。

Method: 研究采用Model Context Protocol（MCP）框架，发展了一种零训练、防止幻觉的映射系统，以协助将源医学术语映射到OMOP标准概念。系统使大语言模型（LLMs）与外部资源和工具进行交互，实现了映射的可解释性，并显著提高了效率和准确性。

Result: 研究开发的系统成功实现了零训练、防止幻觉的映射系统，能够提供实时词汇查找和结构化推理输出，显著提高了效率和准确性。

Conclusion: 该研究开发了基于模型上下文协议（MCP）的零训练、防止幻觉的映射系统，能够有效提高OMOP CDM数据标准化过程的效率和准确性。该系统提供实时词汇查找和结构化推理输出，适用于探索性和生产环境，可以立即投入使用。

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [18] [A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai](https://arxiv.org/abs/2509.03830)
*Kaizhen Tan,Yufan Wu,Yuxuan Liu,Haoran Zeng*

Main category: cs.AI

TL;DR: 该研究提出了一个基于人工智能的框架，用于分析游客在历史城区的感知。通过多模态数据分析和多种方法，研究者成功解码了游客对历史城区的审美偏好和情感反应，揭示了空间差异。研究结果为旅游业、遗产保护和公共空间设计提供了数据支持，促进了决策的明智制定。


<details>
  <summary>Details</summary>
Motivation: 历史城区在保护文化遗产的同时，也是旅游和日常生活中充满活力的场所。了解游客如何感知这些环境对于可持续、以人为本的城市规划至关重要。因此，本研究的动机在于提出一种基于人工智能的框架，帮助分析游客对历史城区的感知，从而促进旅游业、遗产保护和美学公共空间设计的决策提升。

Method: 该研究采用了多模态数据分析、焦点提取、色彩主题分析和情感挖掘等方法，结合了语义分割模型、聚类方法、规则-based方法和多任务BERT模型。通过整合这些技术，研究者成功地解码了游客对历史城区的感知，揭示了游客对美学偏好和情感反应的空间差异。

Result: 通过研究，发现游客在审美偏好和情感反应方面存在空间差异，揭示了视觉期望和建筑环境之间的潜在差距。在游客评论方面，研究者评估了满意度在游客活动、建筑环境、服务设施和商业格式四个维度上的情况。研究结果表明了历史城区的审美吸引力和情感反应存在空间变化。

Conclusion: 该研究提出了一个多维度的基于人工智能的框架，用于分析游客对历史城区的感知。研究结果揭示了旅游者在审美偏好和情感反应方面的空间变化。该框架通过分析社交媒体的多模态数据，结合了焦点提取、色彩主题分析和情感挖掘等方法，为旅游、遗产保护和公共空间设计提供了数据驱动的决策支持。

Abstract: Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.

</details>


### [19] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: GEN AI模型存在可靠性方面的挑战，本研究提出一种方法来评估GEN AI的可靠性，通过构建确定性和基于LLM的知识图谱，使用知识图谱指标量化结构偏差和语义差异，建立动态异常阈值，并实现实时监测以及快速检测语义异常或幻觉。


<details>
  <summary>Details</summary>
Motivation: GEN AI模型存在可靠性方面的挑战，包括幻觉、语义漂移和固有偏见。现有评估方法主要依赖于主观人工评估，限制了可扩展性、透明度和有效性。因此，本研究的动机在于提出一种系统的方法来解决这些挑战，通过比较确定性和基于LLM的知识图谱来评估GEN AI的可靠性。

Method: 研究方法包括构建两个平行的知识图谱（KGs）：一个确定性KG和一个基于LLM的动态KG，使用实时文本数据流来不断监测GEN AI的可靠性。借助多个已建立的知识图谱指标，如实例化类比率（ICR）、实例化属性比率（IPR）和类实例化（CI），对结构偏差和语义差异进行量化。建立动态异常阈值，并采用基于历史结构度量分布的方法，及时识别并标记重大偏差，以快速检测语义异常或幻觉。

Result: 研究建立了一种自动化的实时监测框架，持续计算确定性和LLM-generated KGs之间的偏差。通过建立动态异常阈值，及时识别和标记重大偏差，从而迅速检测语义异常或幻觉。最终建立了一种结构化、度量驱动的确定性和动态生成KGs之间的比较，提供了一个强大和可扩展的评估框架。

Conclusion: 这项研究提出了一种系统方法，利用确定性和大型语言模型（LLM）生成的知识图谱（KGs）来持续监测和评估GEN AI的可靠性。通过建立确定性知识图谱和基于LLM的知识图谱的对比，结合多个已建立的知识图谱指标，可以有效量化结构偏差和语义差异，从而建立了一种强大且可伸缩的评估框架。

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [20] [Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata](https://arxiv.org/abs/2509.03863)
*Sina Khajehabdollahi,Gautier Hamon,Marko Cvjetko,Pierre-Yves Oudeyer,Clément Moulin-Frier,Cédric Colas*

Main category: cs.AI

TL;DR: 该论文介绍了一种名为Expedition and Expansion（E&E）的混合策略，用于发现连续细胞自动机中多样的视觉模式。通过交替进行本地新颖性扩展和目标导向远征，结合视觉-语言模型生成语言目标，E&E方法在实验中展现出持续发现多样解决方案的能力，且远征生成的解决方案对长期探索有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索连续细胞自动机领域中的多样化视觉模式时，局限于局部新颖性，难以达到遥远、未经探索的区域。因此，需要一种新的混合策略来克服这一挑战，同时提高解决方案的多样性和长期探索的效果。

Method: E&E方法交替进行本地新颖性驱动的扩展和目标导向的远征，利用视觉-语言模型（VLM）生成语言目标，将探索引向未经勘查的区域。在Flow Lenia上测试的结果显示，E&E能够持续揭示比现有探索方法更多样的解决方案。基因谱分析显示，远征生成的解决方案对长期探索影响巨大，为后续搜索提供了新的行为领域。

Result: E&E方法在Flow Lenia上的测试表明，相较于现有方法，能够持续发现更多样化的解决方案，并且远征生成的解决方案对长期探索具有重要影响。

Conclusion: 该论文介绍了一种名为Expedition and Expansion（E&E）的混合策略，结合本地新颖性驱动的扩展和目标导向的远征，以发现连续细胞自动机中多样化的视觉模式。研究表明，E&E相较于现有的探索方法能够持续发现更多样化的解决方案，且远征过程产生的解决方案对长期探索具有不成比例的影响。该方法突破了局部新颖性边界，以人类对齐、可解释的方式探索行为景观。

Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.

</details>


### [21] [FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace](https://arxiv.org/abs/2509.03890)
*Yineng Yan,Xidong Wang,Jin Seng Cheng,Ran Hu,Wentao Guan,Nahid Farahmand,Hengte Lin,Yue Li*

Main category: cs.AI

TL;DR: 本文介绍了一种通过LLMs驱动的自主代理助手，在C2C电子商务平台上提供更简化、高效的用户交互体验。FaMA架构实现了高成功率的任务解决和交互时间的加速。


<details>
  <summary>Details</summary>
Motivation: 作者以改善C2C电子商务平台用户体验为动机，提出采用新型代理助手的方法来简化交互过程。

Method: 该文介绍了一种新颖的方法，通过引入LLM驱动的自主代理助手，将用户与电子商务平台的交互方式从复杂的GUI转变为直观的AI代理。

Result: 本文提出的Facebook Marketplace Assistant（FaMA）架构实现了98%的任务成功率，并使交互时间加快了最多2倍。

Conclusion: 本文介绍了通过利用大型语言模型（LLMs）驱动的自主人工智能代理，旨在简化在C2C电子商务平台上的用户交互体验，并展示了该代理在解决复杂任务和提高市场活动效率方面的潜力。

Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

</details>


### [22] [A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning](https://arxiv.org/abs/2509.03906)
*Qika Lin,Yifan Zhu,Bin Pu,Ling Huang,Haoran Luo,Jingying Ma,Zhen Peng,Tianzhe Zhao,Fangzhi Xu,Jian Zhang,Kai He,Zhonghong Ou,Swapnil Mishra,Mengling Feng*

Main category: cs.AI

TL;DR: 本研究介绍了DeepMedix-R1，是一种综合的医学基础模型，用于胸部X射线解释。通过顺序训练流程，改进模型的推理质量和生成性能。提出了评估回答质量的Report Arena框架。DeepMedix-R1在报告生成和可视化问题回答任务中表现出显着改善。


<details>
  <summary>Details</summary>
Motivation: 目前的医学基础模型缺乏透明的推理过程和局部可解释性，限制了它们在实际临床部署中的应用。因此，为了解决这一问题，引入了DeepMedix-R1，旨在提供综合、透明和临床可操作的CXR解释建模。

Method: 利用顺序训练流程，包括在胸部X射线指导数据上进行初步微调、暴露于高质量的合成推理样本以进行冷启动推理，最后通过在线强化学习进行改进，以提高推理质量和生成性能。提出了Report Arena基准框架，使用语言模型评估回答质量。

Result: 在报告生成和可视化问题回答任务中，DeepMedix-R1相比其他模型表现出明显改善，专家评审认为其推理步骤具有更高的可解释性和临床合理性。

Conclusion: 本论文介绍了DeepMedix-R1，这是一个综合的医学基础模型，用于胸部X射线（CXR）解释。通过顺序训练流程，该模型在胸部X射线指导数据上进行初步微调，然后暴露于高质量的合成推理样本，最终通过在线强化学习进行改进，以提高扎根推理质量和生成性能。通过本模型，针对每个查询，生成与图像局部区域相关的答案和推理步骤。定量评估表明，在报告生成和可视化问题回答任务中，DeepMedix-R1相比LLaVA-Rad和MedGemma等模型有显着提高。通过提出Report Arena基准框架，使用先进的语言模型评估回答质量，进一步凸显了DeepMedix-R1的优越性。专家评审发现，相比于Qwen2.5-VL-7B模型，生成的推理步骤具有更高的可解释性和临床合理性。总的来说，本研究推动了医学基础模型开发，实现了综合、透明和临床可操作的CXR解释建模。

Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.

</details>


### [23] [Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions](https://arxiv.org/abs/2509.03953)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 本文提出了一种将控制参数作为搜索空间中的决策点处理的新型搜索算法，通过延迟部分扩展的概念有效处理了决策空间。研究结果表明，该算法是解决涉及控制参数的规划问题的竞争性替代方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动规划方法主要将控制参数作为嵌入约束处理，而未将其作为搜索空间中的决策点处理。因此，本文旨在提出一种更高效的替代方案，明确将控制参数作为真正的决策点，并证明其在特定条件下具有极限完备性。

Method: 本文使用了基于最佳优先和启发式搜索算法的方法来处理控制参数作为真正的决策点，而不是作为附加约束的现有方法。算法利用延迟部分扩展的概念，通过在状态未完全展开时逐步扩展子后继状态，处理控制参数的决策空间。

Result: 研究结果表明，提出的新型搜索算法是解决涉及控制参数的规划问题的一个有竞争力的替代方法。该算法能够在由控制参数定义的无限决策空间上操作，并通过延迟部分扩展的概念实现了对决策空间的有效处理。

Conclusion: 本文提出了一种高效的替代方案，明确将控制参数作为搜索空间中的决策点，并证明在特定条件下具有极限完备性。作者开发了一种基于最佳优先和启发式搜索算法，能够在由控制参数定义的无限决策空间上操作。通过延迟部分扩展的概念，该算法在状态未完全展开时逐步扩展其子后继状态。研究结果表明，这种新颖的搜索算法是解决涉及控制参数的规划问题的竞争性替代方案。

Abstract: In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.

</details>


### [24] [World Model Implanting for Test-time Adaptation of Embodied Agents](https://arxiv.org/abs/2509.03956)
*Minjong Yoo,Jinwoo Jang,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: 该论文介绍了WorMI框架，结合了大型语言模型的推理能力和领域特定世界模型，实现了跨领域适应性，提高了agent的适应性。经过在VirtualHome和ALFWorld基准测试的验证，WorMI表现出比其他方法更好的性能，在未知领域具有潜力应用于实际agent场景中。


<details>
  <summary>Details</summary>
Motivation: 在具体的AI中，一个持久的挑战是使agent能够在新领域中进行稳健适应，而无需进行大量数据收集或重新训练。为了解决这个问题，作者提出了WorMI框架，旨在提高跨领域适应性。在真实场景中，适应性和数据效率是至关重要的。

Method: 通过一种世界模型植入框架（WorMI），将大型语言模型（LLMs）的推理能力与独立学习的领域特定世界模型相结合，实现跨领域适应性。采用基于原型的世界模型检索方法，并利用高效的基于轨迹的抽象表示匹配，将相关模型整合到测试时的组合中。开发了一种世界级的复合注意力方法，旨在整合检索到的世界模型的知识，同时将它们的中间表示与agent策略中的推理模型的表示进行对齐。

Result: 在VirtualHome和ALFWorld基准测试中，WorMI展现出比多个基于LLMs的方法更优越的零样本和少样本性能。这表明该框架在未知领域具有良好的稳健性和适应性，适用于具体agent场景。

Conclusion: 该论文提出了一种名为WorMI的世界模型植入框架，通过将大型语言模型（LLMs）的推理能力与独立学习的领域特定世界模型结合，实现领域适应性，从而实现跨领域适应性。通过WorMI框架，实现了无缝植入和移除世界模型，使得具体的agent的策略实现并保持了跨领域适应性。该框架有效结合了多个世界模型中的领域特定知识，确保了对未知领域的稳健适应。在VirtualHome和ALFWorld基准测试中评估了WorMI，表现出优越的零样本和少样本性能，相对于多个基于LLMs的方法来说。这些结果突显了该框架在需要适应性和数据效率的具体agent场景中的潜力。

Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.

</details>


### [25] [Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent](https://arxiv.org/abs/2509.03990)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: Meta-Policy Reflexion (MPR) is introduced as a framework to enhance agent performance without model weight updates by utilizing Meta-Policy Memory (MPM). It improves stability, adaptability, and performance compared to existing reflective strategies. Empirical results demonstrate gains in execution accuracy and robustness, with rule admissibility enhancing stability.


<details>
  <summary>Details</summary>
Motivation: Large language model (LLM) agents face challenges like inefficient exploration and limited cross-task adaptability. Existing reflective strategies lack reusability across tasks. Reinforcement-learning-based alternatives require substantial parameter updates. The motivation is to address these limitations and improve agent performance using a hybrid framework.

Method: The paper introduces the Meta-Policy Reflexion (MPR) framework that utilizes Meta-Policy Memory (MPM) to incorporate corrective knowledge without model weight updates. It applies soft memory-guided decoding and hard rule admissibility checks (HAC) at inference time. The algorithms for update and decoding of MPM are presented in the text-based agent environment.

Result: The empirical results show consistent gains in execution accuracy and robustness with MPR compared to Reflexion baselines. Rule admissibility further improves stability. The paper analyzes the mechanisms behind these gains, discusses scalability, failure modes, and outlines future directions for multimodal and multi-agent extensions.

Conclusion: Meta-Policy Reflexion (MPR) introduces a hybrid framework that consolidates LLM-generated reflections into a Meta-Policy Memory (MPM) for improved agent performance without model weight updates. It enforces domain constraints, enhances adaptability, and improves stability compared to Reflexion baselines.

Abstract: Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.

</details>


### [26] [AutoPBO: LLM-powered Optimization for Local Search PBO Solvers](https://arxiv.org/abs/2509.04007)
*Jinyuan Li,Yi Chu,Yiwen Sun,Mengchuan Zou,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoPBO, a novel framework powered by Large Language Models, enhances Pseudo-Boolean Optimization (PBO) local search solvers automatically. It outperforms previous approaches, competes well with state-of-the-art competitors, and shows promise in automating local search solver design.


<details>
  <summary>Details</summary>
Motivation: The design of local search solvers for Pseudo-Boolean Optimization (PBO) often requires significant expert effort and manual tuning. Large Language Models (LLMs) have shown potential in automating algorithm design, but their application to optimizing PBO solvers is unexplored.

Method: Introducing AutoPBO, a novel Large Language Models (LLM)-powered framework to automatically enhance PBO local search solvers. Conducting experiments on four public benchmarks to evaluate performance improvement achieved by AutoPBO and comparing it with six state-of-the-art competitors.

Result: AutoPBO demonstrates significant improvements over previous local search approaches and competes well with state-of-the-art competitors in enhancing PBO solvers. It showcases the potential of automating local search solver design with the use of Large Language Models (LLMs).

Conclusion: AutoPBO demonstrates significant improvements over previous local search approaches and maintains competitive performance compared to state-of-the-art competitors. It offers a promising approach to automating local search solver design.

Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.

</details>


### [27] [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
*Zeyu Gan,Hao Yi,Yong Liu*

Main category: cs.AI

TL;DR: Introducing CoT-Space framework to enhance LLM reasoning by shifting from token-level RL to optimization in a reasoning-level semantic space. Demonstrating convergence to optimal CoT length due to underfitting-overfitting trade-off. Strong empirical validation supports the theoretical findings and provides a foundation for future reasoning agent development.


<details>
  <summary>Details</summary>
Motivation: Addressing the theoretical gap in applying Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). Traditional token-level RL frameworks do not align with the complex, multi-step thought processes like Chain-of-Thought (CoT). Aim to provide a coherent explanation for empirical phenomena such as overthinking and guide the future development of more effective and generalizable reasoning agents.

Method: Introducing CoT-Space framework to recast LLM reasoning from token-prediction to optimization within a continuous reasoning-level semantic space. Analyzing the process from noise and risk perspectives. Conducting extensive experiments to validate the theoretical findings.

Result: The CoT-Space framework effectively reimagines the LLM reasoning process, leading to better alignment with complex thought processes like CoT. The trade-off between underfitting and overfitting naturally converges to an optimal CoT length. Strong empirical validation supports the theoretical findings, paving the way for the future development of reasoning agents.

Conclusion: Introducing CoT-Space as a novel theoretical framework in enhancing the reasoning capabilities of Large Language Models (LLMs). Demonstrating the convergence to an optimal Chain-of-Thought (CoT) length as a natural consequence of the trade-off between underfitting and overfitting. Providing strong empirical validation for the theoretical findings and offering a solid foundation for the future development of reasoning agents.

Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.

</details>


### [28] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: 本文介绍了代表系统理论（RST）的实现项目Oruga，包括核心数据结构、通信语言和执行转换的引擎。作者使用结构转移方法生成转换示例，以实现将人类的表示能力赋予机器的目标。


<details>
  <summary>Details</summary>
Motivation: 人类灵活运用各种表示形式，包括绘制图表、改变表示法以及在不同领域之间利用创造性类比。作者希望将这种力量赋予机器，使其更适应人类使用。

Method: 研究方法主要是开发了一个代表系统理论（RST）的实现项目Oruga，包括核心数据结构、通信语言和执行转换的引擎。作者使用了结构转移方法来生成转换。

Result: 作者成功开发了Oruga项目，实现了代表系统理论（RST）的各种方面。Oruga包括核心数据结构、通信语言和执行转换的引擎，展示了结构转移方法的转换示例。

Conclusion: 本文介绍了Oruga项目，该项目是先前开发的代表系统理论（RST）的实现。Oruga包括核心数据结构、用于与核心通信的语言以及使用结构转移方法生成转换的引擎。作者展示了Oruga的核心和语言概况，并提供了结构转移执行的转换示例。

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [29] [Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning](https://arxiv.org/abs/2509.04083)
*Alexander Beiser,David Penz,Nysret Musliu*

Main category: cs.AI

TL;DR: 本文研究了神经符号逻辑语言模型的推理能力，发现形式语言选择对推理能力有重要影响，并比较了四种形式语言在不同大型语言模型上的效果。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于探究神经符号逻辑语言模型推理成功的因素，发现选择形式语言对神经符号逻辑语言模型的影响。

Method: 比较了四种形式语言在三个数据集和七个大型语言模型上的效果，探讨了形式语言选择对神经符号逻辑语言模型的影响。

Result: 通过比较实验结果，证明选择适当的形式语言对神经符号逻辑语言模型的推理能力具有重要影响，并讨论了在不同大型语言模型上的不同效果。

Conclusion: 选择适当的形式语言对神经符号逻辑语言模型的推理能力产生影响，影响了句法和语义推理能力。

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.

</details>


### [30] [Hybrid Reinforcement Learning and Search for Flight Trajectory Planning](https://arxiv.org/abs/2509.04100)
*Alberto Luise,Michele Lombardi,Florent Teichteil Koenigsbuch*

Main category: cs.AI

TL;DR: 本文探讨了将强化学习和基于搜索的路径规划器相结合，以加速航空公司飞行路线的优化过程。通过训练强化学习Agent预先计算近似最优路径，并在运行时约束路径规划求解器，可以显著降低求解器的搜索空间，提高计算速度。实验证明在空中客车飞机模型下，燃油消耗保持几乎不变，计算速度可提高50%。


<details>
  <summary>Details</summary>
Motivation: 动机：在紧急情况下快速重新计算航线至关重要，本研究旨在结合强化学习和搜索技术以加速航线优化过程。

Method: 方法：训练强化学习Agent预先计算基于位置和大气数据的近似最优路径，并在运行时使用这些路径约束底层路径规划求解器，以在初始猜测距离内找到解决方案。

Result: 结果：实验结果表明，在空中客车飞机性能模型下，燃油消耗几乎与未受限制的求解器相同，计算速度可提高50%。

Conclusion: 结论：结合强化学习和基于搜索的航路规划器可以加速航空公司飞行路线的优化，降低求解器的搜索空间，提高计算速度。虽然不能保证全局最优性，但实证结果表明，在空中客车飞机性能模型下，燃油消耗与未受限制的求解器几乎相同，计算速度可提高50%。

Abstract: This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.

</details>


### [31] [Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker](https://arxiv.org/abs/2509.04125)
*Tarik Zaciragic,Aske Plaat,K. Joost Batenburg*

Main category: cs.AI

TL;DR: 本文研究了在扑克游戏中，基于强化学习的DQN算法和基于博弈论的CFR算法是否表现出虚张声势行为。实验结果显示，虽然它们以不同方式进行虚张，但成功虚张的比例大致相同。这表明虚张声势是游戏的重要方面，而非算法的影响。未来研究应关注不同的虚张风格和完整的扑克游戏。


<details>
  <summary>Details</summary>
Motivation: 在扑克游戏中，虚张声势是一种重要技能，然而大多数关于计算机扑克的研究侧重于性能指标，而对虚张声势的研究较少。本文旨在填补这一空白，探讨基于强化学习的DQN算法和基于博弈论的CFR算法在Leduc Hold'em中是否展现虚张声势行为。

Method: 设计了实验，通过让DQN和CFR代理彼此对战并记录其动作，研究它们在Leduc Hold'em中是否表现出虚张声势行为。

Result: 实验发现，DQN和CFR算法都展现出虚张声势行为，但表现方式不同。尽管两者尝试以不同速率进行虚张声势，但成功虚张的比例大致相同。

Conclusion: 研究表明，在Leduc Hold'em中，基于强化学习的DQN算法和基于博弈论的CFR算法均表现出虚张声势行为，但它们的表现方式各不相同。尽管两者尝试以不同速率进行虚张声势，但成功虚张（对手弃牌）的比例大致相同。这表明虚张声势是游戏的重要方面，而非算法的影响。未来的工作应该研究不同的虚张风格以及完整的扑克游戏。

Abstract: In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.

</details>


### [32] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: 人工智能可能在多个方面变得比人类更有能力并改变社会，但要成为宇宙的领导者，情感体验和行为后果理解是至关重要的。DNA始终是领导宇宙的最佳基础，而不是硅。


<details>
  <summary>Details</summary>
Motivation: 对人工智能与人类之间的潜在差异和影响进行深入思考，指出情感理解和行为后果评估对于领导力的重要性。

Method: 讨论比较人工智能与人类之间的差异，强调中枢神经系统对于情感体验和行为后果理解的重要性。提出即使发展意识也不能使AI系统优于人类的观点。

Result: 指出尽管人工智能在各方面可能会超越人类并改变社会，但要成为宇宙的领导者，情感体验和行为后果理解是至关重要的。强调DNA作为领导力基础的重要性。

Conclusion: 人类与人工智能之间的不同在于中枢神经系统提供了与现实世界的深度融合，使我们能够体验情感及理解行为后果，从而发展可持续的伦理系统，成为宇宙的领导者。DNA始终是领导宇宙的最佳基础，而不是硅。

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


### [33] [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
*Aarush Kumbhakern,Saransh Kumar Gupta,Lipika Dey,Partha Pratim Das*

Main category: cs.AI

TL;DR: 介绍了一种领域特定语言，用于精确、模块化地建模复杂的烹饪工作流程。初步手动评估表明该语言适用性和表达能力强，为未来自动化菜谱分析和执行奠定基础。


<details>
  <summary>Details</summary>
Motivation: 烹饪程序的正式化仍然是一个具有挑战性的任务，因为其固有的复杂性和模糊性。作者的动机在于引入一种能够有效表示菜谱的语言，为未来实现自动化菜谱分析和执行奠定基础。

Method: 介绍了一种领域特定语言用于表示菜谱，构建了有向动作图，捕捉了过程、转移、环境、并发性和组成结构，从而实现了对复杂烹饪工作流程的精确、模块化建模。通过对完整的英式早餐菜谱进行手动评估，验证了DSL的表达能力和适用性。

Result: 初步手动评估显示领域特定语言的表达能力和适用性，这为未来的自动化菜谱分析和执行提供了潜力。

Conclusion: 这项工作介绍了一种用于表示菜谱的领域特定语言，能够精确、模块化地建模复杂的烹饪工作流程。初步的手动评估表明这种DSL的表达能力和适用性，为未来自动化菜谱分析和执行奠定了基础。通过使用时间图表，此工作代表了向以动作为中心的烹饪本体论迈出的初步步骤，实现了结构化机器理解、精确解释和可扩展的烹饪流程自动化，适用于家庭厨房和专业烹饪环境。

Abstract: Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.

</details>


### [34] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 本文研究了随着域大小趋于无穷大时MLN确定概率分布的特性，分析了三种具体的MLN示例，探讨了随机结构的极限行为。通过对三种情况的分析，探讨了不同“软约束”对极限行为的影响。研究结果显示随机结构在域大小趋于无穷大时，MLN确定的分布可能集中在可能世界空间的不同部分，不同“软约束”会影响极限行为，MLNs和提升贝叶斯网络具有不可比性。


<details>
  <summary>Details</summary>
Motivation: 研究MLN在域大小趋于无穷大时的分布特性，探究随机结构的极限行为及不同“软约束”对其影响。通过比较MLNs和提升贝叶斯网络的性质，揭示它们之间的区别和不可比性。

Method: 研究了随着域大小趋于无穷大时MLN确定概率分布的特性，分析了三种具体的MLN示例，并探讨了随机结构的极限行为。通过对三种情况的分析，探讨了不同“软约束”对极限行为的影响。通过比较量词自由MLNs和提升贝叶斯网络在大域域上的性质，揭示它们在渐近意义上的不可比性。

Result: 研究表明随机结构在域大小趋于无穷大时，MLN确定的分布可能集中在可能世界空间的不同部分，不同“软约束”会影响极限行为，MLNs和提升贝叶斯网络具有不可比性。

Conclusion: 本文研究了随着域大小趋于无穷大时，马尔可夫逻辑网络（Markov Logic Network，MLN）确定结构集合上的概率分布的特性。通过研究三种具体的MLN示例，探讨了随着域大小趋于无穷大时随机结构的特性。其中包括：(1) 一个仅含一个关系符号（具有1个arity）的任意量词自由MLN的特性分析；(2) 偏好具有较少三角形（或更一般地，较少k-团）的图的MLN，得到一阶逻辑的“$	ext{	extdelta}$-近似0-1法则”的推论；(3) 偏好具有度高于固定数目的顶点的图的MLN的特性分析。通过分析表明，根据MLN使用的“软约束”的不同，随机结构的极限行为可能会有所不同，而软约束的权重可能会对极限行为产生影响或不产生影响。除此之外，还通过（1）表明了量词自由MLNs和广义意义下的提升贝叶斯网络在渐近意义上是不可比的，大域域上，MLN确定的分布几乎将所有的概率集中在可能世界空间的一个完全不同的部分，与均匀分布不同。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [35] [Evaluating Quality of Gaming Narratives Co-created with AI](https://arxiv.org/abs/2509.04239)
*Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 本文提出了一种结构化方法来评估人工智能生成的游戏叙事质量，通过Delphi研究结构与叙事设计专家小组，结合故事质量维度和Kano模型框架，帮助游戏开发人员优先考虑质量方面，从而提高玩家满意度。


<details>
  <summary>Details</summary>
Motivation: 提出一种方法来评估人工智能生成的游戏叙事质量，并探讨其对玩家满意度的影响，以帮助游戏开发人员在与生成式人工智能合作创作游戏叙事时更好地选择优先考虑的质量方面。

Method: 利用Delphi研究结构与叙事设计专家小组，结合文献和专家见解中的故事质量维度，将其映射到Kano模型框架中，以了解其对玩家满意度的影响。

Result: 通过结构化方法评估人工智能生成的游戏叙事，从文献和专家见解中综合故事质量维度，并映射到Kano模型框架中，可以帮助开发人员优先考虑质量方面，提高玩家的满意度。

Conclusion: 本文提出了一种结构化方法来评估人工智能生成的游戏叙事，利用德尔菲研究结构与叙事设计专家小组。我们的方法综合了文献和专家见解中的故事质量维度，将它们映射到Kano模型框架中，以了解它们对玩家满意度的影响。结果可以指导游戏开发人员在与生成式人工智能共同创作游戏叙事时优先考虑质量方面。

Abstract: This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.

</details>


### [36] [EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation](https://arxiv.org/abs/2509.04310)
*Yunbo Long,Liming Xu,Lukas Beckenbauer,Yuhan Liu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 该研究强调了情感在大型语言模型中的重要性，提出了EvoEmo框架用于优化情感表达，实验证实其在多轮协商中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在协商中对情感的处理较为被动，容易受到对手操纵和利用。研究旨在填补这一领域的空白，探讨情感表达在多轮协商中的作用。

Method: 采用进化的强化学习框架EvoEmo，将情感状态转换建模为马尔可夫决策过程，并使用基于种群的遗传优化算法，在不同协商场景中演化高收益情感策略。提出了两个基准框架用于评估情感感知协商的性能。

Result: EvoEmo在实验中表现出色，超越了两个基准方法，在多轮协商中取得了更高的成功率、更高的效率和增加的买家节省。

Conclusion: 这项研究强调了在大型语言模型中支持情感感知的重要性，提出了EvoEmo框架，通过进化强化学习优化情感表达以提高协商效果。实验证明EvoEmo在多轮协商中表现优异，高成功率，效率更高，买家节省更多。

Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.

</details>


### [37] [Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes](https://arxiv.org/abs/2509.04317)
*Isidoro Tamassia,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 本文分析了在可能变化的测试环境中部署AlphaZero代理的问题，以及通过简单修改标准框架如何显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 本文关注AlphaZero代理在可能变化的测试环境中的部署问题，旨在解决其在测试时神经网络训练环境不变的假设对其适用性的约束。通过改进标准框架，提升性能是动机之一。

Method: 结合蒙特卡洛规划和先前训练的策略-价值神经网络提供的先验知识，针对AlphaZero框架在可能发生变化的测试环境中的部署问题进行分析，并展示了通过简单修改标准框架可以提升性能的方法。

Result: 简单修改AlphaZero框架可显著提升性能，即使在计划预算有限的情况下。

Conclusion: 本文分析了在可能变化的测试环境中部署AlphaZero代理的问题，并展示了对标准框架进行简单修改如何显著提升性能，即使在计划预算有限的情况下。

Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.

</details>


### [38] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: 该论文介绍了MBTI-in-Thoughts框架，用于通过心理学个性化调节增强大型语言模型代理的效果。他们发现个性化调节可以产生一致、可解释的行为偏向，涵盖不同任务。结果显示情感表达丰富的代理在叙事生成方面表现优异，而分析性调节的代理在博弈论设置中采取更稳定的策略。他们的方法也可以推广到其他心理框架。实施前置自我反思可提高合作和推理质量。整合16个人格测试以确保特质持久性。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是建立一种框架，通过心理学理论来增强大型语言模型代理的效果，而无需进行微调。他们试图确定不同个性类型的代理在不同任务中的表现差异，并探讨自我反思对合作和推理质量的影响。此外，他们还希望验证个性特质的持久性，以及研究方法的推广性。

Method: 他们的方法是通过提示工程为代理设定不同的个性原型，控制行为在认知和情感两个基础轴上，以增强大型语言模型代理的效果。另外，他们还支持实验化的结构化多代理通信协议，以及前置自我反思来提高合作和推理质量。为了确保个性持久性，他们整合了16个人格测试进行自动验证。他们还探讨了他们的方法如何推广到其他心理框架。

Result: 他们的研究表明，在不同任务中，经过个性化调节的代理表现出一致的行为偏向，并且这些偏向是可解释的。情感表达丰富的代理在叙事生成方面表现优异，而分析性调节的代理在博弈论设置中采取更稳定的策略。他们还展示了他们的方法可以推广到其他心理框架上。另外，他们的研究结果表明前置自我反思可以提高代理的合作和推理质量。

Conclusion: 该论文介绍了一种名为MBTI-in-Thoughts的框架，通过心理学基础的个性化调节，增强大型语言模型（LLM）代理的效果。他们通过Myers-Briggs类型指标（MBTI）的参考，通过提示工程为代理设定不同的个性原型，从而控制行为在人类心理学的两个基础轴上，认知和情感。研究表明，这种个性化调节产生了一致的、可解释的行为偏见，涵盖了各种任务：在叙事生成方面，情感表达丰富的代理表现出色，而在博弈论设置中，分析性调节的代理采取更稳定的策略。该框架支持实验化的结构化多代理通信协议，并揭示自我反思有助于提高合作和推理质量。为了确保特质持久性，他们整合了官方的16个人格测试进行自动验证。虽然他们的重点是MBTI，但他们展示了他们的方法可以平稳推广到其他心理框架，如大五人格、HEXACO或Enneagram。通过搭建心理理论和LLM行为设计之间的桥梁，他们建立了一个基础，用于增强AI代理的心理效果，而无需进行任何微调。

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>


### [39] [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
*Matthew Ho,Chen Si,Zhaoxiang Feng,Fangxu Yu,Zhijian Liu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: 研究通过引入概念级记忆，提高记忆的复用性和可扩展性，实现在测试时持续学习而无需更新权重。在ARC-AGI基准测试中，概念级记忆的方法相对于无记忆基线取得了7.5%的相对增益，并随着推理计算性能的提升而持续改善。动态更新记忆在测试时优于固定记忆设置，支持通过解决更多问题并将更多模式抽象到记忆中以促进自我改进的假设。


<details>
  <summary>Details</summary>
Motivation: 推理时间缩放使LLMs能够进行越来越长且功能更强大的推理跟踪，但一旦上下文窗口为新查询而重置，这些跟踪中发现的模式和观点立即被丢弃。通过外部记忆来保留这些发现是一种自然的方式，近期工作表明对于需要大量推理的任务存在明显好处。研究者看到通过转向概念级记忆（而非基于实例的记忆条目，如确切的查询/响应对或与原始问题背景紧密耦合的摘要）可以使记忆更具复用性和可扩展性的机会。

Method: 通过引入概念级记忆，提高记忆的复用性和可扩展性，从而实现在测试时持续学习而无需更新权重。设计了新的策略来提取推理轨迹的要点并为新问题检索条目，促进记忆的重复使用，并允许随着新增经验的积累而扩展记忆。

Result: 在ARC-AGI基准测试中，采用概念级记忆的方法相对于无记忆基线取得了7.5%的相对增益，并且表现随推理计算性能的提升而持续改善。在所有测试的推理计算规模下，发现抽象概念是最一致的记忆设计，表现优于基线。此外，动态更新记忆在测试时表现优于否则相同的固定记忆设置，支持解决更多问题并将更多模式抽象到记忆中以促进自我改进的假设。

Conclusion: 在面对需要长时间推理的任务时，通过引入概念级记忆，可以提高推理性能并促进记忆的复用和扩展。该方法在ARC-AGI基准测试中相对强的无记忆基线取得了7.5%的相对增益，并随着推理计算性能的提升而持续改善。动态更新记忆在测试时优于固定记忆设置，支持通过解决更多问题并将更多模式抽象到记忆中以促进自我改进的假设。

Abstract: While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.

</details>
