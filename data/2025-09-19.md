<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 该研究提出了统一的优化框架，用于多线地铁人员计划和重规划，通过层次化时间-空间网络模型和解决算法，提高了效率并优于基准方法，在跨线协调和紧急任务方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在单个地铁线路上，对于跨线协调和紧急情况下的快速重规划关注不足。鉴于地铁网络的迅速扩张，多线调度和紧急管理对于大规模无缝运营至关重要。

Method: 提出了层次化的时间-空间网络模型来表示统一的人员操作空间，针对人员的异构资质和偏好推导了计算效率高的约束条件和公式，发展了基于列生成和最短路径调整的解决算法，利用提出的网络模型。

Result: 实验证明，方法在成本节约、任务完成和效率上优于基准启发式方法，尤其是在跨线操作和紧急任务期间表现出色。

Conclusion: 该研究提出了统一的优化框架，用于多线地铁人员计划和重规划，有效利用异构劳动力，实验证明方法在成本节约、任务完成和效率上均优于基准启发式方法，尤其在跨线运营和紧急任务期间表现出色。强调了全局优化和跨线协调在多线地铁系统运营中的重要性，为智慧城市公共交通的高效可靠运行提供了见解。

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [2] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 本研究评估了多个基于LLM的代理在实际渗透测试场景中的性能，并通过对代理进行有针对性的增强，发现增强可以显著提高模块化代理的性能，特别是在复杂、多步骤和实时任务中。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在自动化或增强渗透测试方面的应用日益增多，但它们在攻击阶段的有效性和可靠性仍不清楚。因此，研究的动机是评估LLM代理在渗透测试中的性能和失败模式，并探讨核心功能能力对代理性能的影响。

Method: 通过对多个基于LLM的代理进行全面评估，在实际渗透测试场景中衡量其经验性能和重复的失败模式。并通过针对性增强的方式分离了五个核心功能能力的影响。

Result: 针对性增强显著提高了模块化代理的性能，特别是在复杂、多步骤和实时渗透测试任务中。

Conclusion: 研究展示了对多个基于LLM的代理进行全面评估，并针对核心功能能力进行有针对性的增强，结果表明针对性增强显著提高了模块化代理的性能，尤其在复杂、多步骤和实时渗透测试任务中。

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [3] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 该研究分析了当前基准测试忽略中间错误的问题，提出了 modulization evaluation framework 框架，通过详细错误分析揭示了可行弱点，为构建更强大、更具一般性的网络代理铺平道路。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要集中在总体成功上，忽略了中间错误，限制了对失败模式的洞察力，且阻碍了系统性改进。

Method: 提出了 modulization evaluation framework 框架，将代理管道分解成可解释的阶段，进行详细的错误分析。使用 SeeAct 框架和 Mind2Web 数据集作为案例研究，展示了这种方法如何揭示被标准指标忽略的可行弱点。

Result: 通过提出的 modulization evaluation framework 框架，发现了标准度量未能捕捉到的代理弱点，从而为构建更强大、更具一般性的网络代理铺平道路。

Conclusion: 该研究分析了现有基准测试的局限性，提出了 modulization evaluation framework 框架以分解代理管道，并展示了这种方法如何揭示标准指标所忽略的可行弱点，为构建更强大、更具一般性的网络代理铺平道路。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [4] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench是第一个用于风险投资中创始人成功预测的基准测试，提供9000个匿名创始人数据集，并评估了九种大型语言模型。结果显示其中一些模型的性能超过了人类基准，深度Seek-V3的精度超过了基线6倍。


<details>
  <summary>Details</summary>
Motivation: 在风险投资领域，信号稀疏，结果不确定，即使顶级投资者表现也一般，并且现有基准测试主要关注人工智能的发展。因此，为VC领域引入VCBench基准测试，旨在提供一个公共的标准来评估AGI在早期风险投资预测中的性能。

Method: 介绍了VCBench作为预测风险投资中创始人成功的第一个基准测试，并提供了9000个匿名创始人数据集。评估了九种先进的大型语言模型，并展示了各种模型的性能。

Result: 在起始阶段，市场指数的精度为1.9％。Y Combinator的表现比指数高出1.7倍，而一流公司高出2.9倍。VCBench提供了9000个匿名创始人资料，并通过敌对测试显示重识别风险降低超过90％。

Conclusion: VCBench是第一个用于预测风险投资（VC）中创始人成功的基准测试，提供了9000个匿名创始人数据集，并评估了九种最先进的大型语言模型。结论表明，DeepSeek-V3在精度上超过基线6倍，GPT-4o达到最高的F0.5，大多数模型超过人类基准。VCBench建立了一个公共和不断发展的资源，为早期风险投资预测中的AGI提供可重复和保护隐私的评估标准。

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [5] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 论文围绕人工通用智能（AGI）的辩论，提出基于认知基础架构的新范式True Intelligence（TI）系统，并定义了TI系统的六个核心组件。提出了五级AGI的分类体系，认为第五级AGI在实践上等同于TI。通过综合多领域见解，提供了机制为基础的AGI定义和研究路径。


<details>
  <summary>Details</summary>
Motivation: 认为目前以性能为基础的AGI定义不足够，未提供清晰的机制路线图和对智能质量的准确定义；希望通过提出新的范式，从外部模仿转向基础认知架构的发展；通过综合不同领域的观点，为研究社区提供清晰可行的AGI定义和研究路径。

Method: 从人类大脑获得灵感，提出了基于认知基础架构发展的新范式；定义了True Intelligence（TI）系统的六个核心组件，并基于这些组件提出了五级AGI的分类体系；认为第五级AGI在实践上等同于TI。

Result: 提出了True Intelligence （TI）系统的六个核心组件；提出了五级AGI的分类体系；主张第五级AGI在实践上等同于TI；综合了多个领域的见解，提供了第一个机制为基础的AGI定义和研究路径。

Conclusion: 提出了新的True Intelligence (TI)系统范式，包括六个核心组件，为构建真正智能系统提供了清晰的路径；认为第五级AGI在实践上等同于TI；综合分析心理学、模式理论、元认知、现代大脑架构和人工智能领域的最新成果，提供了第一个机制为基础的AGI定义，并为研究界提供了清晰可行的路径。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [6] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 本文介绍了Melting Pot竞赛，旨在评估人工智能系统的合作能力。研究采用了称为测量布局的贝叶斯方法，推断了多代理系统的性能概况，并发现高亲社会能力有时与更好的表现相关，但并非普遍规律。


<details>
  <summary>Details</summary>
Motivation: 研究为评估人工智能系统在复杂社会环境中的表现提供了新的评估框架，探讨了合作能力的预测以及代理人的亲社会能力。

Method: 在Melting Pot竞赛中应用称为测量布局的贝叶斯方法来推断多代理系统的性能概况。

Result: 研究发现高亲社会能力有时与更好的表现相关，但并非普遍规律；一些得分较低的代理人表现出更强的合作能力。此外，在Melting Pot竞赛中表现最佳的作品更可能在不需要亲社会能力的情景中取得高分。

Conclusion: 研究表明，高社会能力有时与更好的表现相关，但这并不是普遍趋势；部分得分较低的代理人表现出更强的合作能力。此外，比赛中表现最佳的提交作品更有可能在不需要社会能力的情景中取得高分。研究结果还指出，至少有一个表现最佳的团队可能已经为不需要合作的条件进行了优化，潜在地利用了评估框架中的限制。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [7] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: 提出了DeKeyNLU数据集，用于改善自然语言转SQL的性能。通过DeKeyNLU的优化，提出了DeKeySQL管道，提高了SQL生成的准确性，实验结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的NL2SQL模型在任务分解和关键词提取方面存在挑战，导致SQL生成错误，因此需要解决这些问题。

Method: 提出了DeKeyNLU数据集，以优化任务分解和提升RAG管道的关键词提取准确性，提出了基于RAG的NL2SQL管道DeKeySQL，并通过对比实验评估了多种模型配置。

Result: 实验结果显示，使用DeKeyNLU明显提高了SQL生成的准确性，在BIRD和Spider数据集上均取得了显著的性能提升。

Conclusion: 提出了一个新的数据集DeKeyNLU，旨在改善自然语言转SQL的性能，实验结果表明使用DeKeyNLU显著提高了SQL生成的准确性。

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [8] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 本论文提出了评估大型语言模型（LLMs）整体理性的首个基准，包括工具包、实验结果和分析，旨在揭示LLMs的理性行为与理想化的人类理性间的一致性和差异性。


<details>
  <summary>Details</summary>
Motivation: LLMs近来在人工智能领域展现出惊人的能力，成为模拟人类和作为人工智能助手的重要工具。作者认为评估LLMs理性行为的基准至关重要，可以揭示LLMs与真实人类代理的相似性和差异性。

Method: 提出了评估LLMs整体理性的基准，包括设计易于使用的工具包、进行广泛实验以及分析LLMs在理性领域的表现。

Result: 论文提出了首个评估LLMs整体理性的基准，并展示了实验结果和分析，为LLMs的开发者和用户提供了有用的工具。

Conclusion: 该论文提出了评估大型语言模型（LLMs）整体理性的首个基准，并介绍了易于使用的工具包、广泛的实验结果和分析，以揭示LLMs在理性行为方面与理想化的人类理性相一致和相背离的地方。作者相信这一基准可以成为LLMs的开发者和用户的基础工具。

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [9] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 研究提出了一种先验动态框架用于自动化工作流程构建，提高了工作效率和适应性。实验结果显示方法相较现有技术基准在提升任务解决能力方面有显著提升，并降低了工作成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖于历史经验，限制了效率和适应性。作者认为工作流程构建除了需要历史经验外，还应灵活应对每个任务的独特特征。因此，提出了一种先验动态框架来解决这一问题。

Method: 提出了一种先验动态框架，利用Q表学习优化决策空间，引导代理决策并有效利用历史经验。代理评估当前任务进展，并在下一个执行代理方面做出先验决策，使系统能够积极选择更适合每个给定任务的工作流结构。此外，还加入了冷启动初始化、提前停止和修剪等机制来进一步提高系统效率。

Result: 实验证明了该方法的可行性和有效性，相较于现有技术基准有较大提升，并且降低了工作成本。

Conclusion: 该研究提出了一种先验动态框架用于自动化工作流程构建，通过优化决策空间和灵活响应任务特征提高了工作效率和适应性。在四个基准数据集上的实验评估表明，该方法相较于现有技术基准，在提升任务解决能力方面平均提升了4.05%，同时将工作流程构建和推断成本降低到现有方法所需成本的30.68%-48.31%。

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [10] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 本研究探索了在医疗保健和金融等高风险领域中数据驱动决策支持面临的挑战，提出了差分隐私作为生成合成数据的有效方法。研究发现当前仍存在领域特定合成数据生成质量不高、隐私保障挑战和公共数据集可能破坏隐私保障等问题。建议加强隐私审核，推动负责任部署生成式AI技术。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于解决现有的数据共享障碍和隐私问题，探索差分隐私作为生成合成数据的可行方法，在高风险领域中推动负责任的AI使用。

Method: 研究通过引入综合评估框架、大规模实证研究以及成员推理攻击方法来应对数据共享和隐私问题。

Result: 研究表明在差分隐私约束下生成高质量领域特定合成数据仍然是一个未解决的挑战，公共数据集的使用可能会破坏隐私保障。同时突出了对负责任部署生成式AI的紧迫性审核需求。

Conclusion: 本研究强调了在高风险领域如医疗保健和金融中进行数据驱动的决策支持所面临的数据共享障碍，提出了通过不同隐私保护方法解决这些挑战的必要性。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [11] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass是为后部署监控和调试代理工作流设计的评估框架，通过模拟专家调试人员的推理过程，实现错误识别分类、主题聚类、定量评分和战略总结。采用双重记忆系统促进持续学习，在TRAIL基准测试中取得最先进结果，显示出其作为可靠开发者中心工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的评估方法未能捕捉到大型语言模型在自动化复杂、多代理工作流中带来的错误、新兴行为和系统性失败所带来的风险。因此，本研究旨在为后部署监控和调试代理工作流提供一种评估框架。

Method: AgentCompass通过结构化的多阶段分析流程模拟专家调试人员的推理过程，包括错误识别和分类、主题聚类、定量评分和战略总结。它还采用双重记忆系统，即情节性和语义性，促进执行之间的持续学习。

Result: AgentCompass在TRAIL基准测试上取得了最先进的结果，展示了在真实部署中的实际效用，并发现了人工标注中忽略的关键问题。

Conclusion: AgentCompass是第一个专为后部署监控和调试代理工作流而设计的评估框架，通过结构化的多阶段分析流程模拟专家调试人员的推理过程，实现错误识别和分类、主题聚类、定量评分和战略总结。AgentCompass采用双重记忆系统——情节性和语义性，促进执行之间的持续学习。经过与设计合作伙伴的合作，我们展示了该框架在真实部署中的实际效用，并在公开可用的TRAIL基准测试上证明了其有效性。AgentCompass在关键指标上取得了最先进的结果，同时发现了人工标注中忽略的关键问题，强调了其作为可靠的开发者中心工具，用于生产中代理系统的可靠监控和改进。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [12] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: 该论文介绍了一种新颖方法，利用Schoenfeld的Episode理论分析LRM的推理过程，提供了公开可用的具有详细标注数据和指南的细粒度分析基准，揭示了LRM推理中具有不同模式和认知状态转变动态，为解释LRM认知提供了理论基础，为未来推理系统研究提供可能。


<details>
  <summary>Details</summary>
Motivation: 目前LRM生成的推理内容丰富，但缺乏理解这些推理结构的原则性框架，为了分析和理解LRM的推理过程，引入Schoenfeld的Episode理论对LRM的推理进行分析，并创建了大量标注数据和详细指南，为进一步研究提供基础。

Method: 在数学问题解决过程中，使用Schoenfeld的Episode理论和七种认知标签对LRM生成的解决方案的句子和段落进行标注，进行细粒度的认知分析，揭示LRM推理中的模式和认知状态之间的转变动态。

Result: 论文提供了机器推理的精细分析基准，揭示了LRM推理中的模式和认知状态之间的转变动态，为解释LRM认知提供了理论基础，促进了未来可控透明的推理系统的研究。

Conclusion: 该论文介绍了一种新颖方法，将Schoenfeld的Episode理论应用于分析LRM的推理痕迹，为机器推理的精细分析提供了公开可用的基准，揭示了LRM推理中的不同模式并提供了理论基础的解释方法，为未来更可控透明的推理系统研究提供了可能。

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [13] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: The paper introduces RationAnomaly, a framework combining Chain-of-Thought fine-tuning and reinforcement learning for log anomaly detection. It surpasses existing methods, achieves high F1-scores, and offers transparent analytical results.


<details>
  <summary>Details</summary>
Motivation: Existing log anomaly detection approaches have limitations such as lack of interpretability, generalization issues, unreliability, and factual inaccuracies. The motivation is to address these issues by developing a framework that overcomes these limitations and enhances the reliability of log anomaly detection in software systems.

Method: The paper proposes RationAnomaly, which first utilizes Chain-of-Thought guided supervised fine-tuning to instill expert-like reasoning patterns, followed by a reinforcement learning phase with a multi-faceted reward function. The framework corrects a high-quality dataset through expert-driven processes to optimize for accuracy and logical consistency.

Result: Experimental results show that RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks. The framework also provides transparent, step-by-step analytical outputs.

Conclusion: RationAnomaly is a novel framework that enhances log anomaly detection by combining Chain-of-Thought fine-tuning with reinforcement learning, outperforming existing approaches and providing transparent analytical outputs.

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [14] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo introduces a reliable benchmark for insight-based reasoning evaluation, highlighting GPT-5 as comparable to human performance. Reasoning models excel over non-reasoning ones, and model size does not affect accuracy. The paper identifies meta-cognitive weaknesses through verification failures in models' final answers.


<details>
  <summary>Details</summary>
Motivation: Benchmark saturation and contamination in LLM evaluation undermine confidence. Nazonazo aims to provide a cost-effective, scalable benchmark that addresses the evaluation crisis and identifies meta-cognitive weaknesses in models.

Method: The paper introduces Nazonazo, a benchmark created from Japanese children's riddles to evaluate insight-based reasoning. 38 frontier models and 126 adults were tested on 120 riddles. An extended evaluation with 201 items compared reasoning and non-reasoning models, highlighting the superior performance of reasoning models.

Result: Nazonazo demonstrates GPT-5 as the only model comparable to human performance, reasoning models outperforming non-reasoning peers, and model size showing no significant impact on accuracy. Verification failure in models' final answers reveals a meta-cognitive weakness.

Conclusion: Nazonazo presents a reliable benchmark for testing insight-based reasoning, with GPT-5 being the only model comparable to human performance. Reasoning models outperform non-reasoning peers, and model size does not significantly impact accuracy. Verification failure in models' final answers indicates a meta-cognitive weakness.

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [15] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: The paper introduces the AC-RAG framework to overcome Retrieval Hallucinations in RAG. AC-RAG involves a Detector and a Resolver in an adversarial collaboration, leading to improved retrieval accuracy in various domains.


<details>
  <summary>Details</summary>
Motivation: Address the issue of Retrieval Hallucinations in the Retrieval-augmented Generation (RAG) approach by leveraging adversarial collaboration.

Method: Proposed the Adversarial Collaboration RAG (AC-RAG) framework with two agents: a Detector and a Resolver for dynamic and iterative problem dissection and refined knowledge retrieval.

Result: Extensive experiments demonstrate the effectiveness of AC-RAG in enhancing retrieval accuracy in domain-specific LLMs.

Conclusion: AC-RAG framework significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [16] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种针对健康信息学定制的全自动框架OpenLens AI，通过专门的代理完成文献综述、数据分析、代码生成和手稿准备，提高医学可视化和质量控制的能力。该框架自动化整个研究流程，生成出版准备的LaTeX手稿，为推进健康信息学研究提供了有益的解决方案。


<details>
  <summary>Details</summary>
Motivation: 健康信息学研究具有多样的数据模态、快速的知识扩展以及需要整合生物医学科学、数据分析和临床实践见解的需求。这些特征使其特别适合于基于代理的方法，可以自动化知识探索、管理复杂工作流程，并生成具有临床意义的输出。现有系统在健康信息学方面存在局限性，因缺乏解释医学可视化的机制并经常忽视领域特定的质量要求。因此，为了解决这些差距，引入了OpenLens AI框架。

Method: 介绍了一种全自动框架OpenLens AI，用于健康信息学研究中的文献综述、数据分析、代码生成和手稿准备，结合视觉-语言反馈和质量控制。

Result: OpenLens AI框架成功解决了健康信息学研究中的自动化需求，能够生成出版准备的LaTeX手稿，并提供透明和可追溯的工作流程。通过专门的代理进行文献综述、数据分析、代码生成和手稿准备，并结合视觉-语言反馈和质量控制，增强了医学可视化和可复现性的质量要求。

Conclusion: 介绍了一种针对健康信息学定制的全自动框架OpenLens AI，通过专门的代理进行文献综述、数据分析、代码生成和手稿准备，结合视觉-语言反馈和质量控制以提高医学可视化和可复现性的质量要求。该框架自动化整个研究流程，生成出版准备的LaTeX手稿，并提供透明和可追溯的工作流程，为推进健康信息学研究提供了领域适应的解决方案。

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [17] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: 这项研究介绍了一个可解释的人工智能建模框架，用于分析住院数据以确定关键风险因素和预测与CPE相关的结果。研究结果表明，Transformer模型的表现优越，并强调了多样化临床和网络特征的重要性。


<details>
  <summary>Details</summary>
Motivation: 碳青霉烯酶产生肠杆菌对医院感染预防和控制构成重要威胁，但先前对CPE相关风险的预测建模仍未充分探索，尤其是使用现代深度学习方法。因此，这项研究的动机在于引入一个可解释的人工智能建模框架，从爱尔兰医院的电子病历数据中探讨CPE对患者结果的影响。

Method: 该研究使用了一个可解释的人工智能建模框架，分析了爱尔兰一家急诊医院的住院数据，包括诊断代码、病区转换、患者人口统计学、感染相关变量和接触网络特征。与传统机器学习模型相比，对多种Transformer架构进行了基准测试。预测了临床结果，并应用了XAI技术来解释模型的决策。

Result: 研究发现，传染相关特征，包括历史性医院暴露、入院背景和网络中心性度量，在预测患者结果和CPE获取风险方面具有很高的影响力。解释性分析揭示了“居住区域”、“入院病区”和先前入院等特征是关键风险因素。网络变量如“病区PageRank”也排名靠前，反映了结构性暴露信息的潜在价值。

Conclusion: 该研究介绍了一个可解释的人工智能建模框架，用于研究碳青霉烯酶产生肠杆菌对患者结果的影响。模型成功地展示了基于Transformer的模型的效用，TabTransformer在多个临床预测任务中始终优于基线，特别是对于CPE获取（AUROC和敏感性）的预测。

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [18] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 该论文提出了一种新型的架构框架，通过Sentinel Agents和Coordinator Agent的双重安全层次方法，实现了对多代理系统的持续监控和动态自适应防御机制。仿真研究验证了该方法的有效性，能够检测各种合成攻击，支持系统可观察性和政策演变。


<details>
  <summary>Details</summary>
Motivation: 本论文的动机在于提高多代理系统的安全性和可靠性，特别是面对不断增长的网络安全威胁和风险。通过引入Sentinel Agents和Coordinator Agent，构建了一种新的架构框架，旨在应对各种攻击类型和维护MAS生态系统的完整性。

Method: 论文的方法是通过建立Sentinel Agents网络和Coordinator Agent的双重安全层次方法，结合大型语言模型（LLMs）的语义分析、行为分析、检索增强验证和跨代理异常检测等技术，实现对多代理系统的安全和可靠性增强。同时，通过仿真研究验证了该方法的有效性。

Result: 通过仿真研究验证了Sentinel Agents网络的有效性，能够检测各种合成攻击，如提示注入、幻觉和数据外泄，证实了该监控方法的实际可行性。另外，该框架还具有增强的系统可观察性、支持监管合规性及政策演变的特点。

Conclusion: 该论文提出了一种新颖的架构框架，旨在增强多代理系统（MAS）的安全性和可靠性。通过Sentinel Agents网络和Coordinator Agent的双重安全层次方法，实现了对多代理系统的持续监控和动态自适应防御机制。在仿真研究中，Sentinel Agents成功检测到不同类型的合成攻击，验证了监控方法的实际可行性。该框架还提供了增强的系统可观察性，支持监管合规性，并实现随时间变化的政策演变。

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [19] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: 研究引入了集合贡献函数来量化一组参数对话题的贡献，泛化了现有函数，提出了新原则，讨论了在推荐系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 通过量化一组参数对话题的贡献，引入了一种更加细致的分析方法。

Method: 泛化了现有的单个贡献参数到话题的函数，并提出了集合贡献函数的新原则。

Result: 提出了集合贡献函数及相应的原则分析，并探讨了在推荐系统应用中的实际效果。

Conclusion: 引入一种新的函数来量化一组参数对一个特定话题的贡献，该函数是现有函数的泛化。提出了适用于集合贡献函数的新原则，并给出了基于原则的分析。讨论了这些原则在推荐系统应用场景中的作用。

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [20] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: The paper introduces KAMAC, a framework that allows LLM agents to form expert teams dynamically in response to the diagnostic context, outperforming other methods in complex medical scenarios like cancer prognosis.


<details>
  <summary>Details</summary>
Motivation: Inspired by the collaborative nature of multidisciplinary teams in medical decision-making and the limitations of existing multi-agent frameworks with static roles, the authors aimed to develop a framework that enables dynamic team formation and knowledge integration in complex clinical scenarios.

Method: The paper proposes KAMAC, a framework that starts with expert agents and allows them to recruit additional specialists through knowledge-driven discussions to address knowledge gaps. The final decisions are made after reviewing updated agent comments. Experiments on two real-world medical benchmarks were conducted to demonstrate the superiority of KAMAC over other methods.

Result: The experiments show that KAMAC significantly outperforms other methods, especially in complex clinical scenarios such as cancer prognosis, showcasing its ability to adapt and integrate dynamic, cross-specialty expertise.

Conclusion: KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework, outperforms both single-agent and advanced multi-agent methods in complex clinical scenarios, particularly in cancer prognosis, by enabling LLM agents to dynamically form expert teams based on the evolving diagnostic context.

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [21] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: 本研究探讨了生成式人工智能在支持在线研究生课程中的应用，分析了其在反馈方面的表现。研究发现生成式人工智能可以模拟有效人类反馈并提高学生的反馈素养。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨生成式人工智能在支持在线研究生课程中的形成性评估方面的应用，分析了其反馈在各个维度上的构建情况。

Method: 研究采用系统功能语言学和评价理论，分析了120个元评审，探讨了生成式人工智能反馈在概念、人际关系和文本维度上构建意义的情况。

Result: 研究结果表明生成式人工智能可以模拟有效人类反馈的关键特征，并提供支持性的反馈，有助于学生参与同行评审并提高反馈素养。

Conclusion: 研究发现生成式人工智能可以在研究生在线课程中支持审查评估，其反馈构建了意念、人际关系和文本维度的意义。生成式人工智能可以模拟有效人类反馈的关键修辞和关系特征，同时保持支持性的立场。分析的评审表明，生成式人工智能反馈平衡了表扬和建设性批评，符合评分标准的期待，并通过结构化的分阶段突显学生的主体性。通过模拟这些特征，人工智能元反馈有潜力培养反馈素养，并增强学习者参与同行评审的参与度。

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [22] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: 本文强调了在海事领域中可解释人工智能（XAI）的重要性，提出了一个用于捕获海事专业人员看法的领域特定调查，旨在支持以用户为中心的XAI集成。


<details>
  <summary>Details</summary>
Motivation: 在复杂和动态的海事环境中，对AI系统的信任不仅依赖于性能，还取决于透明性和可解释性。为了有效的人机协作，理解AI系统决策背后的原因变得至关重要。

Method: 本文提出了一个专为捕获海事专业人员对信任、可用性和可解释性看法而设计的领域特定调查，以支持用户为中心的XAI集成。

Result: 通过引入可解释人工智能（XAI）作为有效人机协作基础的概念，并提出领域特定调查以捕获海事专业人员的看法，以支持用户为中心的XAI集成。

Conclusion: 本文强调了可解释人工智能（XAI）在海事领域中作为有效人机协作基础的重要性。通过提出一个旨在捕获海事专业人员对信任、可用性和可解释性看法的领域特定调查，旨在支持以用户为中心的XAI集成，指导满足海员和海事团队需求的定制XAI系统的发展。

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [23] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: 本文提出了MACA框架，通过强化学习方法训练模型更倾向于选择内部共识的推理路径，显著提升了模型的自一致性和推理性能，并在泛化能力上表现出色。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在一致性推理的问题，现有的推理方法未能有效解决模型在选择推理路径上的困难。为了提高模型的自一致性和推理性能，有必要引入新的方法来指导模型选择更加合理的推理轨迹。

Method: 通过Multi-Agent Consensus Alignment (MACA)框架进行模型后训练，使模型更倾向于选择与其内部共识一致的推理路径。利用多代理辩论中的多数/少数结果来引导模型的训练过程。通过代理之间的辩论交流，模型根据对等论据进行推理，而非仅仅汇总独立尝试的结果。

Result: MACA框架显著改善了模型的自一致性、单一代理推理、采样推理以及多代理集成决策制定等方面的表现。在未见基准测试集上也表现出良好的泛化能力，为语言模型潜在推理能力的更可靠释放提供了有效途径。

Conclusion: 提出了一种名为Multi-Agent Consensus Alignment (MACA)的强化学习框架，用于训练模型倾向于与其内部共识一致的推理轨迹，从而改善自一致性、单一代理推理、基于采样的推理和多代理集成决策制定等方面的性能。结果表明，MACA显著提升了模型的自一致性，并在多个基准测试中展现出强大的泛化能力。

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [24] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: 本文將RLVR引入數據生成管道中，成功提高了多模態大語言模型在解決幾何問題時的表現。通過從數學問題解決任務中獲得的獎勵信號，使得模型在統計學、算術、代數和數值任務中的精度提升了2.8%	ext{-}4.8%，同時在藝術、設計、技術和工程任務中的精度提升了2.4%	ext{-}3.9%。


<details>
  <summary>Details</summary>
Motivation: 多模態大語言模型在解決幾何問題時仍然存在挑戰，缺乏高質量的圖像-文本配對數據集。模板化的數據合成管道通常無法泛化到預定模板以外的問題。因此，本文旨在通過引入RLVR方法來改善數據生成過程，提高幾何問題解決的表現。

Method: 將強化學習與可驗證回饋（RLVR）應用於改進生成幾何圖像標註的數據生成管道。通過從數學問題解決任務中獲得的獎勵信號，成功捕獲了幾何問題解決的關鍵特徵，提高任務泛化能力並取得了實質改善。

Result: 成功利用RLVR方法改進了數據生成管道，提高了多模態大語言模型的任務泛化能力，並在多個任務領域中取得了精度改善。

Conclusion: 本文引入了強化學習與可驗證回饋（RLVR）到數據生成管道中，成功捕獲幾何問題解決的關鍵特徵，提高了多模態大語言模型的任務泛化能力。在幾何圖像以外的情況下，生成的數據集提升了多模態大語言模型的推理能力，導致在統計學、算術、代數和數值任務中的精度提升了2.8%	ext{-}4.8%，同時在藝術、設計、技術和工程任務中的精度提升了2.4%	ext{-}3.9%。

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>
