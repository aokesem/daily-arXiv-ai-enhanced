<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: 本研究通过引入多模态人工智能代理重新构想传统生命周期评估方法，有效减少时间成本并填补数据缺失问题，提供准确的碳排放估算；研究提出了直接评估环境影响的方法以及数据驱动的排放因子生成方法，取得了良好的效果。结果显示改进的方法与传统方法相比，MAPE有显著提高。


<details>
  <summary>Details</summary>
Motivation: 近年来可持续性信息越来越受到关注，但进行生命周期评估所需的数据通常难以获取，本研究旨在填补数据缺失问题，提出更高效的方法计算产品的碳排放量，同时降低专家时间成本。

Method: 引入多模态人工智能代理，利用自定义数据抽象和软件工具从在线文本和图像中提取信息，计算电子设备的生产阶段碳排放量；开发了直接评估环境影响的方法，比较输入和类似描述和已知碳足迹的产品集群；提出了数据驱动的方法生成排放因子，将未知材料属性表示为排放因子的加权求和。

Result: 通过多模态人工智能代理重新构想传统的生命周期评估方法，缩短了时间成本，填补了数据缺失问题，并提供了准确的碳排放估算，有效提高了MAPE；提出直接评估环境影响的方法，并用数据驱动的方式生成排放因子，取得了较好的效果。

Conclusion: 通过引入多模态人工智能代理，重新构想传统的生命周期评估方法，可以准确计算电子设备的生产阶段碳排放量。该方法不仅缩短了专家花费的时间，还填补了数据缺失的问题，并在不需要专有数据的情况下，给出了与专家评估相符合的碳足迹估算。此外，研究者开发了一种直接评估环境影响的方法，通过比较输入和具有相似描述和已知碳足迹的产品集群来实现。他们还开发了一种数据驱动的方法，用于生成排放因子，通过将未知材料的属性表示为类似材料排放因子的加权求和，相较于人工专家选择最接近的LCA数据库条目，改善了MAPE达到了120.26%。最后，研究者对这一方法的扩展性进行了分析，并讨论了对未来生命周期评估工作流程的影响。

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [2] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: MAPF问题的一种新方法：Conflict-Based Flex Distribution、Delay-Based Flex Distribution和Mixed-Strategy Flex Distribution的引入，提高了解决效率和性能，实验证明优于原始方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在解决MAPF问题时存在效率低下的问题，需要提高算法的灵活性以提高解决效率。作者对此进行了研究，提出了新的灵活分配机制，旨在改善现有算法的性能和效率。

Method: 通过引入Conflict-Based Flex Distribution、Delay-Based Flex Distribution和Mixed-Strategy Flex Distribution等机制，改进现有的算法，提高解决MAPF问题的效率和性能。

Result: 作者的新方法在实验中表现优秀，优于原始的灵活分配方法，证明了完备性和有界次优解性能。

Conclusion: 作者提出了一种解决Multi-Agent Path Finding (MAPF)问题的新方法，通过引入Conflict-Based Flex Distribution、Delay-Based Flex Distribution和Mixed-Strategy Flex Distribution等机制来改进现有的算法，实现了完备性和有界次优解性能。实验证明新方法优于原始的灵活分配方法。

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [3] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: 本研究使用LoRA进行安全对齐微调，有效保护LLMs的安全性，不损害其推理能力。通过在四个基准测试上的实验证明，这种方法产生了高度安全的LLMs，其安全水平可与完整模型微调相媲美，同时不损害其推理能力。LoRA引起的权重更新与初始权重的重叠较小，作者还探索了进一步减少重叠的方法，并在某些任务中观察到一定的改进。结果表明，设计更加一致的推理-安全性权衡方法具有积极意义。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于解决在后训练阶段进行安全对齐微调时可能出现的“安全税”问题，即安全对齐微调可能显著降低推理能力。通过使用LoRA进行SFT微调，作者展示了一种有效的方法，旨在在保护LLMs的安全性的同时，不损害其推理能力。作者还希望这一结果能够激发设计更加一致的推理-安全性权衡方法的动力。

Method: 使用LoRA进行SFT微调拒绝数据集，限制安全权重更新到低秩空间，以最小化对推理权重的干扰。另外，通过正则化或在权重合并过程中进一步减少权重更新的重叠来探索方法，并在某些任务中观察到一定改进。

Result: 作者展示了使用LoRA进行SFT微调可以有效保护LLM的安全性，而不损害其推理能力。在多个基准测试中，这种方法产生了安全水平高且不影响推理能力的LLMs。LoRA引起的权重更新与初始权重的重叠较小，还探讨了通过正则化或权重合并等方法进一步减少这种重叠的方式，并在某些任务上实现了一定的改进。

Conclusion: 使用LoRA进行安全对齐微调可有效保护语言模型的安全性，同时不损害其推理能力。在四个基准测试中，这种方法产生了高度安全的LLMs，其安全水平可与完整模型微调相媲美，同时不损害其推理能力。LoRA对初始权重的更新重叠较小，且可以通过正则化或权重合并进一步减少这种重叠。最终结果表明，设计更加一致改进推理-安全性权衡的方法具有积极意义。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [4] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: 该论文讨论了最近自主系统架构的发展趋势，介绍了如何改进常见的安全分析技术以适应复杂的基础模型。提出了HySAFE-AI混合框架，用于评估人工智能系统的安全性，并提出了未来工作方向和建议。


<details>
  <summary>Details</summary>
Motivation: 人工智能已经成为自主驾驶系统和机器人等安全关键领域不可或缺的一部分。最近自主系统的架构趋于采用端到端单体架构，如大型语言模型和视觉语言模型。研究人员希望改进现有的安全分析技术，以适应基础模型的复杂性。

Method: 通过回顾不同的架构解决方案并评估常见的安全分析技术（如FMEA和FTA），展示这些技术如何可以针对基础模型的复杂性进行改进。介绍了HySAFE-AI，一个混合框架，将传统方法应用于评估人工智能系统的安全性。

Result: 论文展示了如何改进常见的安全分析技术以适应最近自主系统的架构趋势。提出了HySAFE-AI混合框架，并给出了未来工作方向和建议。

Conclusion: 该论文提出了HySAFE-AI，这是一个混合框架，用于评估人工智能系统的安全性。论文评估了常见的安全分析技术在最近自主系统架构中的有效性，展示了这些技术如何可以针对基础模型的复杂性进行改进。最后，论文提出了未来工作的方向和建议，以引导未来人工智能安全标准的发展。

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [5] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 本研究利用图问题推理（GPR）任务和GraphPile数据集，训练了GraphMind模型来增强大型语言模型（LLMs）的推理能力，取得了在数学和非数学推理任务上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 之前的领域特定继续预训练（CPT）方法虽然在数学推理方面表现出色，但在更广泛的推理任务上缺乏可转移性。因此，本研究旨在利用GPR任务教授各种推理模式，弥补现有方法的不足。

Method: 本研究采用图问题推理（GPR）任务，包括寻径、网络分析、数字计算和拓扑推理等，设计了GraphPile数据集，并引入GraphMind模型对LLMs进行训练。

Result: 通过GraphPile数据集和GraphMind模型，本研究在数学推理和非数学推理任务上实现了显着提升，分别达到了4.9%和21.2%的准确性提升。

Conclusion: 本研究引入了图问题推理（GPR）来增强大型语言模型（LLMs）的一般推理能力，通过引入GraphPile数据集和训练GraphMind模型，实现了在数学推理和非数学推理任务上的显着提升。研究填补了领域特定预训练和通用推理能力之间的差距，提升了LLMs的适应性和鲁棒性。

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [6] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: The paper advocates for integrating AI copilots in vehicles to shift maintenance strategies from reactive to proactive, emphasizing the importance of AI communication with both machines and drivers. It aims to inspire interdisciplinary conversations and shape the future of intelligent vehicle systems, predictive maintenance, and AI-driven user interaction.


<details>
  <summary>Details</summary>
Motivation: The need to bring AI to vehicles as sensing platforms to revolutionize maintenance processes from reactive to proactive.

Method: Conceptual and technical perspective to spark interdisciplinary dialogue and guide future research and development in intelligent vehicle systems, predictive maintenance, and AI-powered user interaction.

Result: The article aims to stimulate interdisciplinary discussions and steer advancement in intelligent vehicle systems, predictive maintenance, and AI-enhanced user interaction.

Conclusion: AI integration in vehicles can transform maintenance from reactive to proactive by incorporating AI copilots that communicate with both machines and drivers.

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [7] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 本文介绍了对语言模型代理的代理能力和可信度的重要性，以及LMAs系统在时间上维护其代理身份的困难。为了解决挑战，引入了代理身份评估（AIE）作为一种衡量LMAs系统代理身份的框架，包括一系列新颖的指标，可以与其他性能、功能和代理稳健性的测量相集成。


<details>
  <summary>Details</summary>
Motivation: LMAs继承了大型语言模型（LLMs）的一些病态，这可能会损害它们的识别性、连续性、持久性和一致性，影响其推理、规划和行动等代理能力。因此，作者提出了AIE框架，旨在解决这些挑战，并帮助设计最佳的LMA基础设施和支撑结构。

Method: 论文介绍了代理身份评估（AIE）作为一种衡量LMAs系统代理身份的框架，包括新颖的指标和方法，并提出可以应用于LMAs生命周期各个阶段的正式定义和方法。

Result: 论文提出了代理身份评估（AIE）作为一种衡量LMAs系统代理身份的框架，包括一系列新颖的指标，可以与其他性能、功能和代理稳健性的测量相集成。这有助于设计最佳的LMA基础设施和支持结构。

Conclusion: 该论文介绍了对语言模型代理(LMAs)的代理能力和可信度至关重要的中心，即它们在时间上保持稳定、可靠、持久的身份的程度。论文提出了代理身份评估（AIE），这是一个严谨、统计驱动的经验框架，用于衡量LMA系统在时间上展示和维护其代理身份的程度。

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [8] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: 研究开发了SCRIPT工具，基于ChatGPT-4o-mini构建，用于支持初学者编程学习。通过实验评估了工具，在大型德国大学的编程课程中，分析了学生与工具的互动方式并关注他们的反馈偏好。结果显示学生的反馈请求似乎具有特定顺序，聊天机器人的回应与学生请求的反馈类型高度一致，并遵循系统提示的约束。这些发现有助于设计基于GenAI的学习支持系统，并突显了AI辅助工具中在指导和灵活性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 在以往关于生成式人工智能（GenAI）和相关的编程教育工具的研究基础上，开发了SCRIPT工具，旨在支持初学者学习者。研究的动机在于探索如何利用ChatGPT-4o-mini构建的聊天机器人来提供学习支持。

Method: 在大型德国大学的初级编程课程中，通过实验评估了SCRIPT工具，分析了学生在解决编程任务时与SCRIPT的互动方式，重点关注他们的反馈偏好。工具基于ChatGPT-4o-mini构建，允许开放式互动和通过预定义提示进行结构化指导。

Result: 研究结果表明学生的反馈请求似乎遵循特定顺序，聊天机器人的回应与学生请求的反馈类型高度一致（75%），并且符合系统提示的约束。

Conclusion: 研究结果揭示学生的反馈请求似乎遵循特定顺序，聊天机器人的回应与学生请求的反馈类型高度一致（75%），并且遵循了系统提示的约束。这些见解为基于GenAI的学习支持系统的设计提供了信息，并突显了在AI辅助工具中平衡指导和灵活性方面存在的挑战。

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [9] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: 本文介绍了Compliance Brain Assistant（CBA），一个设计用于提高企业合规任务效率的对话型AI助手。设计了用户查询路由器，实现了快速跟踪模式和完全代理模式的智能选择。实验结果显示CBA在真实世界的查询中表现优越，比基准模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 为了提高企业环境中人员日常合规任务的效率，设计了Compliance Brain Assistant（CBA）这样一个对话式AI助手。通过通过选择不同的模式来处理简单和复杂的请求，以平衡响应质量和延迟。

Method: 设计了用户查询路由器，可以智能选择快速跟踪模式和完全代理模式来处理简单和复杂的请求。进行了实验评估比较CBA与基准LMM在各种真实世界隐私/合规相关查询中的表现，并对完整的基于路由的设计与“仅快速跟踪”和“完全代理”模式进行了度量比较。

Result: 实验评估显示CBA在各种真实世界的隐私/合规相关查询中表现优越，相比于基准LMM，在关键词匹配率和通过率方面有显著提升。整体路由设计相比于其他模式在匹配率和通过率方面表现更好，并且运行时间相近。

Conclusion: 该论文介绍了Compliance Brain Assistant（CBA），这是一个旨在提高企业环境中人员日常合规任务效率的对话式AI助手。通过设计用户查询路由器，可以智能选择快速跟踪模式和完全代理模式以在响应质量和延迟之间取得平衡。实验评估表明，CBA在各种真实世界的隐私/合规相关查询中，相比于基准LMM，表现出了较高的关键词匹配率和通过率。整体路由设计在平均匹配率和通过率方面表现更好，同时保持了相近的运行时间，验证了路由机制在两个模式之间取得了良好的折衷。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [10] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: 提出了Ctx2TrajGen，一个上下文感知的轨迹生成框架，通过GAIL合成城市驾驶行为，实现生成与真实世界上下文一致的交互感知轨迹，在DRIFT数据集上实验证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 精确建模微观车辆轨迹对交通行为分析和自动驾驶系统至关重要。

Method: 提出了Ctx2TrajGen，一个上下文感知的轨迹生成框架，利用GAIL合成城市驾驶行为，通过PPO和WGAN-GP解决微观环境中的非线性交互和训练不稳定性。

Result: 通过在DRIFT数据集上的实验，验证了Ctx2TrajGen在逼真度、行为多样性和上下文保真度方面的优越性。

Conclusion: Ctx2TrajGen是一个上下文感知的轨迹生成框架，利用GAIL合成城市驾驶行为，通过PPO和WGAN-GP解决微观环境中的非线性交互和训练不稳定性，实现生成与真实世界上下文一致的交互感知轨迹。在DRIFT数据集上的实验证明了Ctx2TrajGen在逼真度、行为多样性和上下文保真度方面优于现有方法，为克服数据稀缺和领域转移提供了坚固的解决方案。

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [11] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: The paper introduces the UDASA framework to enhance Large Language Model alignment without human annotations. By generating multiple responses, quantifying uncertainties, and optimizing progressively, UDASA outperforms existing methods across various tasks, significantly improving model performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of achieving high-quality alignment with human intent and safety norms in Large Language Models (LLMs) without human annotations. Aim to improve LLM alignment fully automated manner.

Method: Proposed an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework that generates multiple responses for each input, quantifies output uncertainty, constructs preference pairs, categorizes training samples into three stages, and optimizes the model progressively. Conducted preliminary studies to validate core design assumptions and provide empirical motivation for the framework.

Result: UDASA framework shows superior performance to existing alignment methods in tasks related to harmlessness, helpfulness, truthfulness, and controlled sentiment generation.

Conclusion: UDASA framework improves LLM alignment in a fully automated manner, outperforming existing alignment methods across multiple tasks and significantly enhancing model performance.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [12] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: 本研究引入了LTLZinc，一个基准框架，用于生成包括时间推理和持续学习任务的数据集。实验表明时间学习和推理的挑战性，并揭示了当前最先进方法的局限性。研究人员发布了LTLZinc生成器和十个任务，以促进对统一时间学习和推理框架的研究。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号人工智能大多应用于静态场景，对需要沿着时间维度推理的挑战性设置鲜有探索。本研究旨在通过引入LTLZinc，为神经符号和持续学习社区带来一个基准框架，以评估在时间和约束驱动维度上的学习方法。通过实验证明当前方法在时间学习和推理方面的局限性，希望促进对统一时间学习和推理框架的研究。

Method: 研究引入了LTLZinc，这是一个基准框架，用于生成包括时间推理和持续学习任务的数据集。从线性时态逻辑规范和MiniZinc约束中生成具有表达力的任务，并在任意图像分类数据集上进行评估。通过精细的注释，在相同生成的数据集上进行多种神经和神经符号训练设置。

Result: 实验证明了时间学习和推理的挑战性，并突显了当前最先进方法的限制。研究人员发布了LTLZinc生成器和十个任务，以鼓励研究人员在统一时间学习和推理框架方面进行进一步探索。

Conclusion: 该研究介绍了LTLZinc，一个基准框架，用于生成涵盖不同问题的数据集，以评估神经符号和持续学习方法在时间和约束驱动维度上的表现。实验证明了时间学习和推理的挑战性，突显了当前最先进方法的局限性。研究人员释放了LTLZinc生成器和十个可供使用的任务，希望促进朝着统一的时间学习和推理框架发展的研究。

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [13] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 研究了在本体上受认识依赖 (EDs) 管制的 Controlled Query Evaluation (CQE)，结合最优 GA 传感器的概念。该方法确保信息披露受到限制，并且在安全性和计算行为方面表现良好。首先，特征化基于交集的方法的安全性，并针对一类 EDs（全 EDs）展示了其安全性。对于一类 EDs 和 DL-Lite_R 本体，通过提出适当的详细一阶重写算法，证明了在数据复杂度中的 AC^0 复杂度。实验结果表明了提出的重写函数的实际可行性。


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the security and computational efficiency of CQE over ontologies by introducing epistemic dependencies and optimal GA censors. The aim is to provide strong security guarantees while ensuring practical feasibility.

Method: The paper combines epistemic dependencies with optimal GA censors to regulate Controlled Query Evaluation (CQE) over ontologies. It focuses on answering Boolean unions of conjunctive queries (BUCQs) with respect to the intersection of all optimal GA censors. It characterizes the security of the intersection-based approach, identifies safe class of EDs (full EDs), and presents a first-order rewriting algorithm for AC^0 complexity in data complexity for a subclass of EDs and DL-Lite_R ontologies.

Result: The paper achieves strong security guarantees and favorable computational behavior by integrating epistemic dependencies and optimal GA censors in CQE over ontologies. It identifies safe EDs, demonstrates AC^0 complexity for a subclass of EDs and DL-Lite_R ontologies, and shows the practical feasibility of the proposed rewriting function in experimental evaluations.

Conclusion: CQE over ontologies regulated by epistemic dependencies combined with optimal GA censors provides strong security guarantees and favorable computational behavior. The approach remains safe for full EDs and is in AC^0 complexity for a subclass of EDs and DL-Lite_R ontologies. Experimental results demonstrate the practical feasibility of the proposed rewriting function.

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [14] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 这篇论文提出了自动化混合grounding方法，解决了Answer Set Programming中的grounding bottleneck问题，实验表现良好，尤其在难以ground和难以解决的情况下取得了改进。


<details>
  <summary>Details</summary>
Motivation: 这篇论文的动机在于解决Answer Set Programming中的grounding bottleneck挑战，为了促进该技术在工业界的广泛应用。

Method: 通过开发自动化混合grounding，引入了基于数据结构启发的分割算法，检测何时使用body-decoupled grounding和何时使用标准的bottom-up grounding。方法基于规则结构和估算程序，结合实例数据。

Result: 实验结果显示，作者的原型实现在难以ground的场景中取得了改进，在难以解决的实例上接近了最先进性能。

Conclusion: 这篇论文解决了Answer Set Programming中的grounding bottleneck问题，提出了自动化的混合grounding方法，在硬难以ground的情况下取得了改进，在难以解决的实例上接近了最先进性能。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [15] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 本研究通过系统性调查在RLVR框架内的多领域推理，重点关注数学推理、代码生成和逻辑谜题解决。研究使用GRPO算法和Qwen-2.5-7B模型家族，分析了模型的领域内改进和跨领域泛化能力。研究结果揭示了影响推理表现的关键因素，为优化RL方法论提供了宝贵的指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在孤立的推理领域，缺乏对多个认知技能综合应用的理解。本研究旨在填补这一空白，从多领域推理的角度探索RLVR框架内的推理能力。

Method: 使用GRPO算法和Qwen-2.5-7B模型系列，从单领域数据集训练模型，评估模型在领域内的改进和跨领域泛化能力。研究了跨领域训练中产生的相互增强和冲突等复杂交互作用。分析SFT对RL的影响，比较基础模型与指导模型在相同RL配置下性能差异。探讨了RL训练的关键细节，系统性地探索了课程学习策略、奖励设计的变化以及语言特定因素的影响。

Result: 通过全面的研究，研究结果为优化RL方法论以培养LLMs全面、多领域推理能力提供了重要见解。

Conclusion: 本研究系统性地研究了RLVR框架内的多领域推理，着重于数学推理、代码生成和逻辑谜题解决。通过广泛实验，揭示了领域交互的动态以及影响专业化和通用化推理表现的关键因素。这些发现为优化RL方法论以促进LLMs的全面、多领域推理能力提供了宝贵的指导。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [16] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: 该论文介绍了一种基于RAG的TAI自我评估工具，支持法律TAI评估，强调便捷的合规性。通过定性评估验证了工具的准确性和有效性，展示了工具的推理基于与高风险系统设置的比较。工具在预测风险级别和检索相关条款方面表现良好，为用户提供准确的信息以支持合规决策。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一种便捷的自我评估工具来支持法律TAI评估并促进合规性，同时提供有关风险级别和条款的洞察。通过简化输入、输出相关信息和准确预测风险级别，为用户提供实用的工具以应对高风险AI系统的部署。

Method: 论文使用了定性评估来验证TAI Scan Tool的有效性和准确性，展示了工具的推理基于与高风险系统设置的比较。通过使用案例场景来评估工具在预测风险级别和检索相关条款方面的表现，证明了工具的可靠性和实用性。

Result: 研究结果表明，TAI Scan Tool在评估风险级别和检索相关条款方面表现良好，为用户提供准确的信息以支持合规决策。工具的推理基于与高风险系统的比较，这有助于用户更好地理解其义务和合规要求。

Conclusion: 该论文介绍了一种基于RAG的TAI自我评估工具，支持法律TAI评估，强调便捷的合规性。评估包括预筛选和评估两个步骤，系统输出包括AI系统的风险级别，同时检索相关条款以帮助合规并提示义务。定性评估结果显示，在使用案例场景下，工具能够准确预测风险级别，并检索出涵盖三个不同语义群体的相关条款。此外，结果的解释显示，该工具的推理依赖于与高风险系统设置的比较，这种行为被归因于这些系统的部署需要仔细考虑，并因此经常在AI法中提及。

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [17] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: 本文介绍了一种眼科专用的多模态大语言模型FundusExpert，以及通过FundusGen构建的数据集，解决了眼科领域中多模态大语言模型面临的挑战。FundusExpert在眼科问题回答和零-shot报告生成任务中表现优异，超过了现有模型的性能。研究揭示了数据质量和模型能力之间的规律，并发展了可扩展的与临床对齐的MLLM。


<details>
  <summary>Details</summary>
Motivation: 本文针对眼科领域中多模态大语言模型的挑战，提出了FundusExpert和FundusGen的解决方案，旨在改善模型的表现和数据利用效率。通过构建与临床对齐的认知链，指导模型生成可解释的推理路径，以及整合区域级定位和诊断推理链，探索在特定MLLM中弥合视觉语言差距的路径。

Method: 本文通过引入FundusExpert和FundusGen，解决了眼科领域中多模态大语言模型面临的挑战，包括注释细粒度分裂、临床推理逻辑不一致等问题。FundusExpert在眼科问题回答任务和零-shot报告生成任务中表现优异，通过调整来自FundusGen的指导数据，实现了出色的模型性能。研究中还揭示了数据质量和模型能力之间的缩放规律。

Result: 通过Fine-tuned FundusExpert使用来自FundusGen的指导数据，在眼科问题回答任务中超过40B MedRegA的平均准确率26.6%，在零-shot报告生成任务中达到77.0%的临床一致性水平，明显优于GPT-4o的47.6%。研究揭示了数据质量和模型能力之间的缩放规律，并开发了一种可扩展、与临床对齐的MLLM。

Conclusion: 本文介绍了一种眼科专用的多模态大语言模型FundusExpert，通过集成定位-诊断推理能力，以及通过智能Fundus-Engine系统构建的数据集FundusGen，实现了在眼科问答任务中的最佳表现。同时，在零-shot报告生成任务中也表现出色，显著优于GPT-4o。通过本文的研究，揭示了数据质量和模型能力之间的缩放规律，证明了FundusGen中的认知对齐注释增强了数据利用效率。最终，本文开发了一种可扩展、与临床对齐的MLLM，探索了在特定MLLM中弥合视觉语言差距的路径。

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [18] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: HoPeS模型框架利用大型语言模型驱动的代理人，让用户体验不同利益相关者的视角。在实验中发现现实世界中研究者和决策者之间的观点不一致。用户反映了感到挫折和失望，但展现了对尝试不同叙事策略的高度动机。系统有潜力在社会-生态仿真中探索不同视角，促进跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 理解社会-生态系统需要不同利益相关者观点的洞察，但通常难以获得。为了使模拟探索不同利益相关者视角成为可能，开发了HoPeS模型框架。

Method: 开发HoPeS模型框架，利用大型语言模型(LLMs)驱动的代理人来代表各利益相关者，用户可以扮演代理人角色体验不同视角。设计模拟协议作为“脚手架”来支持多视角仿真，并开发原型系统在机构动态和土地利用变化等方面展示了HoPeS的应用。

Result: 在实验中展示了用户通过代理人角色体验不同视角，反映了研究者和决策者之间的观点不一致。用户反映了感到挫折和失望，尤其由于在争取政治影响力时维护政治中立性的挑战。用户对尝试不同叙事框架策略表现出了高度动机，系统有潜力探索不同视角。

Conclusion: HoPeS模型框架为探索不同利益相关者观点提供了一种创新的模拟方法，有助于理解社会-生态系统。在实验中展示了用户通过代理人角色体验不同视角，结果显示用户反映了现实世界中研究者和决策者之间的观点不一致。系统潜力在于探索不同视角，并通过新形式的跨学科合作促进社会-生态仿真。

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [19] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: 本文介绍了将大型语言模型（LLM）与实时优化算法相结合的新型代理范式，称为“信任人工智能”。研究结果表明，共生代理在决策错误减少和资源开销节约方面具有优势，并展现了在测试平台上的性能表现。该研究为下一代AGI驱动网络系统提供了保持适应性、高效性和可信赖性的基础。


<details>
  <summary>Details</summary>
Motivation: 作者的动机在于介绍一种新的代理范式，通过融合LLM和实时优化算法，提出信任人工智能的概念。同时，作者希望为下一代AGI驱动网络系统提供基础，使之保持适应性、高效性和可信赖性。

Method: 本文融合了大型语言模型（LLM）与实时优化算法，实现了信任人工智能代理。通过设计和实现两种新型代理类型，在5G测试平台进行评估并展示共生代理的性能。

Result: 通过研究，作者引入了共生代理作为信任人工智能的一种范式，展示其在决策错误减少和资源开销节约方面的优势。在测试平台上的性能评估结果表明，共生代理在减少RAN过度利用方面表现出显著灵活性。

Conclusion: 本文介绍了将大型语言模型（LLM）与实时优化算法相结合的新型代理范式，称为“信任人工智能”。作者设计和实现了两种新型代理类型，包括无线接入网络（RAN）优化器和服务级协议（SLAs）的多代理协商者。研究结果表明，共生代理相对于独立LLM代理减少了决策错误，同时小型语言模型（SLM）在减少GPU资源开销的同时实现了类似准确性。通过在5G测试平台上进行评估，作者展示了共生代理在移动车辆信道波动方面的性能。最终，作者引入了共生范式作为未来的下一代AGI驱动网络系统的基础，以保持适应性、高效性和可信赖性。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [20] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: 研究表明，在处理复杂推理任务时，合适利用Python解释器和草稿本等工具增强LRMs的性能，挑战了推理是虚假的论调，突显了工具增强LRMs解决复杂问题的潜力。


<details>
  <summary>Details</summary>
Motivation: 这项研究重新审视了LRMs的局限性，并研究了引入工具增强后LRMs的表现。旨在回应最近有关推理能力的辩论，验证LRMs在面对复杂推理任务时的可行性。

Method: 将Python解释器和草稿本工具引入研究，评估了三种代表性的大型语言模型(LLMs)及其LRM对应物，发现在合适利用工具的情况下，LRMs在各种任务复杂度下始终表现优于非推理类型的模型。

Result: 结果显示，通过合适的工具使用，LRMs在Apple的基准推理难题中始终优于非推理类型的模型，无论任务复杂度如何。

Conclusion: 工具增强的大型推理模型在处理复杂问题时表现出色，挑战了最近关于推理能力的质疑，并突显了工具增强型LRM在解决复杂问题中的潜力。

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [21] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 研究领域难以追踪最新进展，定期举办竞赛评估算法性能。本文提出在线竞赛系统，自动化提交和评估过程，解决了开发环境兼容性问题，已成功应用于多个竞赛。


<details>
  <summary>Details</summary>
Motivation: 研究领域难以追踪不同会议上出现的最新论文，定期举办竞赛评估算法和技术的性能，但竞赛过程具有重大运营负担。

Method: 开发了一个在线竞赛系统，用于自动化提交和评估过程，通过隔离环境评估提交以解决参与者在不同环境下开发解决方案导致的兼容性问题。

Result: 已成功应用于多个竞赛，提高了评估系统的效率和管理大量提交的能力。

Conclusion: 提出了一个在线竞赛系统，用于自动化提交和评估过程，能够有效地管理大量提交并利用隔离环境进行评估。已成功应用于几项竞赛，包括基于网格的路径规划竞赛和机器人跑步者联赛。

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>
