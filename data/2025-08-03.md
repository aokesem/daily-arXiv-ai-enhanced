<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 本文主张统一后续可解释性方法，提出通用框架平衡有效性和简洁性，改进评估协议，强调解释的可解释性对终端用户查询的重要性，旨在增强知识图谱完成中的可解释性研究的再现性和影响力。


<details>
  <summary>Details</summary>
Motivation: 后续可解释性在知识图谱完成中缺乏正式化和一致的评估，阻碍了再现性和跨研究比较。作者认为需要统一的后续可解释性方法，以增强研究的再现性和影响力。

Method: 提出了通用框架以通过多目标优化平衡后续解释的有效性和简洁性，统一现有的后续可解释性算法。改进了评估协议并支持使用Mean Reciprocal Rank和Hits@k等评估指标。强调解释的可解释性与终端用户的查询相关。

Result: 通过提出通用框架、改进评估协议和强调可解释性的重要性，本研究旨在使知识图谱完成中的可解释性研究更具再现性和影响力。

Conclusion: 本文主张统一后续可解释性方法，提出了一个通用框架来表征后续解释，通过多目标优化平衡其有效性和简洁性，统一了知识图谱完成中现有的后续可解释性算法。同时提出并在实证上支持使用流行评估指标如Mean Reciprocal Rank和Hits@k改进评估协议。强调解释的可解释性对于回答对终端用户有意义的查询的重要性。通过统一方法和完善评估标准，本研究旨在使知识图谱完成中的可解释性研究更具再现性和影响力。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [2] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文研究了数据准备对人工智能（AI）在科学领域的应用，提出了一个二维准备框架，涵盖数据准备级别和数据处理阶段，旨在指导基础设施开发以支持可伸缩和可重复的科学AI。框架强调了基于Transformer的生成模型，并描绘了概念成熟度矩阵，用于描述科学数据准备情况，并推动标准化和跨领域支持。


<details>
  <summary>Details</summary>
Motivation: 动机：了解数据准备对科学数据的影响，特别是在训练基础模型时的应用。通过识别预处理模式和领域约束，构建了一个框架来处理这些挑战并推动AI在科学领域的应用。

Method: 方法：通过分析四个代表性领域的工作流程，构建了一个二维准备框架，包括数据准备级别和数据处理阶段，特别针对高性能计算环境。着重介绍了基于Transformer的生成模型，并描绘了一个概念成熟度矩阵，旨在指导基础设施开发。

Result: 结果：提出了经过定制的数据准备级别和数据处理阶段的二维框架，强调了基于Transformer的生成模型，并描述了一个概念成熟度矩阵。这有助于标准化和跨领域支持可伸缩和可重复的科学AI。

Conclusion: 总结：本文研究了数据准备对人工智能（AI）的应用在用于训练基础模型的领导规模科学数据集中的应用。通过分析代表性领域（气候、核聚变、生物/健康和材料）中的原型工作流程，识别了常见的预处理模式和特定领域的约束。引入了一个二维准备框架，由数据准备级别（从原始到AI-ready）和数据处理阶段（摄取到分片）组成，两者都专门针对高性能计算（HPC）环境。该框架概述了转换科学数据以进行可扩展AI训练的关键挑战，强调基于Transformer的生成模型。这些维度共同形成一个概念成熟度矩阵，描述了科学数据准备并引导基础设施开发，以实现可伸缩和可重复的科学领域AI的标准化跨领域支持。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [3] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 研究探讨了在多模态大型语言模型中推理和偏见之间的权衡问题。通过基准测试三种偏见消除策略，发现强化学习训练的1:4样本混合比例能够在减少偏见的同时保留模型推理能力。研究结果为在MLLMs中平衡公平性和能力提供了具体指导。


<details>
  <summary>Details</summary>
Motivation: 针对多模态大型语言模型在推理能力和社会偏见之间的权衡问题，本研究是一个开放且紧迫的研究方向。

Method: 该研究通过基准测试三种消除偏见的策略，然后在这些结果基础上变化每种范式中偏见与推理样本的比例，以描绘推理与偏见之间的权衡关系。

Result: 研究发现通过强化学习训练的1:4样本混合比例能够在减少刻板印象评分约10%的同时，保留88%的模型原始推理准确性。

Conclusion: 研究指出在多模态大型语言模型的推理能力进一步提升时，需要解决输出中存在的社会偏见问题。作者通过基准测试了三种消除偏见的策略（监督微调、知识蒸馏和基于规则的强化学习），并在相同条件下建立了它们的基准优势和劣势。研究发现通过强化学习训练的1:4样本混合比例能够在减少刻板印象评分约10%的同时，保留88%的模型原始推理准确性。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [4] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 研究引入了一个包含917个挑战的听觉图灵测试，评估了最先进的音频模型在处理听觉任务上的表现。结果显示当前AI系统在复杂听觉任务上存在严重失败，最佳模型的准确率只有6.9%，远低于人类水平。研究强调了对人类水平机器听觉的测量进展的诊断框架和将选择性注意力、基于物理的音频理解和上下文感知整合到多模态AI系统中的必要性。


<details>
  <summary>Details</summary>
Motivation: 受Moravec悖论启发，表明对于人类简单的任务通常对机器而言困难，反之亦然。研究的动机在于揭示当前AI系统在处理听觉任务上的失败，并探讨这些失败的原因。

Method: 引入了一个包含7个类别、917个挑战的听觉图灵测试，评估了最先进的音频模型（包括GPT-4的音频能力和OpenAI的Whisper）在此测试中的表现。

Result: 研究结果显示当前AI系统在复杂听觉任务上表现不佳，甚至最佳模型的准确率只有6.9%，远低于人类水平。

Conclusion: 当前AI系统在处理复杂听觉任务上存在着严重失败，表现出93%以上的错误率，最佳模型的准确率仅为6.9%，远低于人类水平（52%）。研究揭示了AI系统在处理复杂听觉场景时关注选择、噪声鲁棒性和情境适应方面存在的缺陷，指出当前架构缺乏类似人类的听觉场景分析机制。此外，还强调了对人类水平机器听觉的测量进展的诊断框架，需要将选择性注意力、基于物理的音频理解和上下文感知整合到多模态AI系统中。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [5] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一致性論證的性質，研究了其對預測的影響。通過在人類和LLM預測者中實施一致性，過濾不一致的預測可以改善預測的準確性。群眾實驗顯示用戶通常不符合這種一致性屬性，強調需要整合過濾不一致意見的機制。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究論證的一致性對預測的影響。通過過濾不一致的預測，提高預測的準確性，支持在人類和基於LLM的預測中一致性的實際價值。同時，群眾實驗顯示用戶通常沒有這種一致性，強調需要在判斷性預測中整合過濾不一致意見的機制。

Method: 在本文中，提倡並正式定義了一致性論證的性質，要求預測者的推理與其預測一致。進行了三個評估，評估了在人類預測者和基於大型語言模型（LLM）的預測者中強制實施一致性對預測準確性的影響。通過群眾實驗，證明了用戶通常不符合這種一致性屬性。

Result: 通過在人類和LLM預測者中實施一致性，過濾不一致的預測可以穩定提高預測的準確性。然而，用戶通常不符合一致性屬性，這表明在群體預測之前需要整合過濾不一致意見的機制。

Conclusion: 透過本文所提出的一個關於論證的一致性的性質，可以改善人類預測者和基於大型語言模型（LLM）的預測者的預測準確性。即使用戶通常不太符合這種一致性屬性，但在判斷性預測中整合過濾不一致意見的機制仍然是必要的。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [6] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 这篇论文研究了在本体中介查询回答设置中计算责任分数的复杂性，发现在一些情况下具有多项式数据复杂性。同时，证明了当本体语言支持某些查询时，责任分数的计算问题是“shP”难解的。该研究还探讨了WSMS计算的复合复杂性，表明当本体语言支持某些操作时，即使在没有本体的情况下，计算也是无法解决的。然而，在一些DL-Lite方言中可能存在易处理的WSMS计算。


<details>
  <summary>Details</summary>
Motivation: 最近的定量方法解释查询回答的工作使用责任度量为事实分配分数，以量化它们对获取给定答案的贡献。本论文的动机在于研究在本体中介查询回答设置中计算这种责任分数的复杂性。

Method: 通过利用数据库设置中的结果，研究了基于Shapley值的WSMS责任度量的计算复杂性。证明了对于某些本体中介查询类别，这些度量具有多项式数据复杂性。进一步证明了在特定的本体语言支持下，WSMS计算问题是难解的。探讨了WSMS计算的复合复杂性，证明在一些情况下计算是无法解决的，但也发现在某些DL-Lite方言中存在易处理的情况。

Result: 证明了在一些情况下计算责任分数的多项式数据复杂性，并指出了某些情况下计算是难解的。针对WSMS计算的复合复杂性证明了一些情况下难解，但也发现在某些DL-Lite方言中存在易处理的情况。

Conclusion: 研究了在本体中介查询回答设置中计算责任分数的复杂性，发现在一些情况下具有多项式数据复杂性。同时，证明了当本体语言支持某些查询时，责任分数的计算问题是“shP”难解的。该研究还探讨了WSMS计算的复合复杂性，表明当本体语言支持某些操作时，即使在没有本体的情况下，计算也是无法解决的。然而，在一些DL-Lite方言中可能存在易处理的WSMS计算。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [7] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 本文重新审视了分而治之方法来处理复杂实例，提出了一种解感知ReLU得分方法（SAS），并改进了分支函数。通过实验和理论比较，SAS在选择要打开的变量集方面更为有效，减少了二进制变量数量并保持了准确性。在实验中，实现了高效且准确的验证器，显著减少了未决实例的数量，并保持了合理的运行时间。


<details>
  <summary>Details</summary>
Motivation: 以前的尝试在选择重要ReLU变量方面是次优的。为了解决这一问题，我们提出了一种新颖的解感知ReLU得分方法（SAS），并改进了分支函数以更有效选择要打开的变量集。我们旨在减少二进制变量的数量，同时保持准确性。

Method: 重新审视分而治之方法以处理复杂实例，依赖于许多小的部分MILP调用而不是少数复杂的BaB调用。提出了基于解的ReLU得分方法（SAS），并将BaB-SR和BaB-FSB分支函数改进为全局ReLU得分函数（GS）。通过在“混合MILP”中调用α，β-CROWN和部分MILP来实现高效验证器。

Result: SAS在选择要打开的变量集方面比以前的尝试更有效，减少了二进制变量数量，并且保持了相同水平的准确性。在实验中，实现了高效且准确的验证器，将未决实例数量降低了多达40％至较低水平（8-15％），同时保持了合理的运行时间。

Conclusion: 在处理复杂实例时，我们重新审视了一种分而治之的方法，通过对复杂性进行拆分：我们依靠许多小的部分MILP调用，而不是少数复杂的BaB调用。关键步骤是选择极少但非常重要的ReLU变量，并使用（昂贵的）二进制变量进行处理。我们提出了一种新颖的基于解的ReLU得分方法（SAS），并将BaB-SR和BaB-FSB分支函数改进为全局ReLU得分函数（GS）。我们在理论上和实验上对它们进行比较，SAS在使用二进制变量选择要打开的变量集方面更有效。与以前的尝试相比，SAS的二进制变量数量减少约6倍，同时保持相同水平的准确性。在“混合MILP”中实现，首先调用具有较短超时时间的α，β-CROWN解决较容易的实例，然后进行部分MILP，可以产生非常准确且高效的验证器，将未决实例数量减少多达40％至较低水平（8-15％），同时保持合理的运行时间（每个实例平均46秒至417秒），即使对于具有200万参数的相当大的CNN也是如此。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [8] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: AI Scientist systems are advancing scientific research with the potential to achieve human-level capabilities. The survey analyzes current achievements, identifies bottlenecks, and outlines critical components needed for groundbreaking discoveries.


<details>
  <summary>Details</summary>
Motivation: To explore the progress and limitations of AI Scientist systems in reshaping scientific research paradigm and uncovering unknown phenomena, aiming to provide insights on how far AI scientists are from changing the world.

Method: Prospect-driven review to analyze current achievements of AI Scientist systems, identifying key bottlenecks and critical components required for the advancement of scientific AI.

Result: The survey provides a comprehensive analysis of the current achievements of AI Scientist systems and highlights areas that need improvement for the development of a scientific agent capable of making groundbreaking discoveries. It aims to offer a clearer understanding of the limitations of current AI Scientist systems and set goals for the future of scientific AI.

Conclusion: AI Scientist systems have the potential to revolutionize scientific research by producing groundbreaking discoveries and solving grand challenges. However, there are key bottlenecks and critical components that need to be addressed for the emergence of a human-level AI Scientist.

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [9] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: The paper argues against fully autonomous AI, emphasizing the need for human oversight. It identifies levels of autonomy, discusses theories, presents arguments, counterarguments, and evidence to support the stance.


<details>
  <summary>Details</summary>
Motivation: To highlight the importance of human oversight in autonomous AI systems and address the potential risks associated with the development of artificial superintelligence.

Method: Identify the 3 levels of autonomous AI, discuss theories of autonomy, AI, and agents, present 12 arguments and 6 counterarguments with rebuttals, and provide 15 pieces of recent evidence of AI misaligned values and risks in the appendix.

Result: Emphasized the necessity of responsible human oversight in AI development to mitigate risks and provided arguments, counterarguments, and evidence to support the position.

Conclusion: AI must not be fully autonomous due to the risks involved, especially with the potential emergence of artificial superintelligence. Responsible human oversight is essential for mitigating these risks.

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [10] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 本文评估了三种大型语言模型在数据科学代理方面的性能，引入了一个全面的基准测试以反映用户与代理的实际互动，探讨了模型的敏感性和温度参数对结果的影响，发现不同模型和方法存在性能差异，提出了促进更健壮、有效数据科学代理研究的基准数据集和评估框架。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在数据科学工作流程中被广泛采用，但缺乏系统性的基准评估。本文旨在填补这一空白，引入一个专门设计用于反映实际用户与数据科学代理互动的基准测试。

Method: 介绍了一个具体制定的基准测试，评估了三种LLM在不同方法下的性能，探讨了模型对常见提示问题的敏感性，研究了温度参数对结果的影响。

Result: 研究发现评估的模型和方法在性能上存在差异，关键因素影响了实际部署。基准数据集和评估框架的引入旨在促进未来更健壮和有效的数据科学代理的研究。

Conclusion: 研究评估了三种大型语言模型在数据科学代理方面的性能和局限性，发现这些模型和方法之间存在明显的性能差异，突出影响实际部署的关键因素。引入了一个全面的基准测试，旨在反映用户与数据科学代理的实际互动，探讨了模型对常见提示问题的敏感性，进一步研究了温度参数对每个模型和方法的整体和任务特定结果的影响。提出的基准数据集和评估框架旨在为未来研究提供更健壮和有效的数据科学代理奠定基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [11] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail is a new railway service platform leveraging large language models to offer personalized services like ticketing, food recommendations, and chitchat. The paper introduces the QTAO prompting framework, CRFD-25 dataset for railway catering, and an LLM-based recommender to enhance dining experiences for passengers.


<details>
  <summary>Details</summary>
Motivation: To meet the increasing demands for individualized railway service and provide personalized onboard dining experiences, the paper develops LLM4Rail with the aim of leveraging large language models to enhance railway services.

Method: The paper proposes the iterative QTAO prompting framework that integrates verbal reasoning with task-oriented actions to retrieve external observations relevant to railway services. It also introduces the CRFD-25 dataset for railway catering and an LLM-based zero-shot conversational recommender. A feature similarity-based post-processing step is added to align recommended items with the CRFD-25 dataset.

Result: The development of LLM4Rail and the proposed frameworks and datasets contribute to improving the customization and efficiency of railway services, particularly in ticketing, food & drink recommendations, and conversational interactions.

Conclusion: LLM4Rail is a novel LLM-augmented railway service consulting platform that offers custom modules for ticketing, food & drink recommendations, weather information, and chitchat. The QTAO prompting framework integrates verbal reasoning with task-oriented actions effectively. The CRFD-25 dataset and LLM-based zero-shot conversational recommender enhance personalized onboard dining services for railway passengers.

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [12] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 该论文介绍了一个大型语言模型代理与工业生产级ERP系统交谈的设计、实施和评估过程，利用双代理架构来提高查询生成的可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是实现一个能够解释自然语言查询并将其翻译为可执行SQL语句的代理系统，以提高生产系统的智能化水平和效率。

Method: 设计、实施和评估了一个大型语言模型代理与工业生产级ERP系统交流的过程。提出了双代理架构来改进查询生成的可靠性。

Result: 提出了一个大型语言模型代理与工业生产级ERP系统交流的方法，并采用双代理架构来改进查询生成的可靠性。

Conclusion: 该论文介绍了一个大型语言模型（LLM）代理与工业生产级ERP系统交谈的设计、实施和评估。该代理能够解释自然语言查询，并将其翻译为可执行的SQL语句，利用开放权重LLM。提出了一种新颖的双代理架构，结合推理和评论阶段，以提高查询生成的可靠性。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [13] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: 提出了Self-Foveate方法，通过引入多级凝视方法有效地增强了合成指令的多样性和难度，实验证明方法的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动合成范式在确保合成指令的多样性和难度方面仍存在显着限制，需要解决这些挑战。

Method: 提出了Self-Foveate方法，通过引入“微-分散-宏”多级凝视方法，从未监督文本中深入挖掘信息。

Result: 在不同的无监督语料库和多样的模型架构上进行了全面实验，验证了所提方法的有效性和优越性。

Conclusion: 提出了一种名为Self-Foveate的新颖的LLM驱动方法，用于指令合成。通过引入“微-分散-宏”多级凝视方法，有效地引导LLM深入挖掘无监督文本中蕴含的细粒度信息，从而增强了合成指令的多样性和难度。在多个无监督语料库和不同模型架构上进行了全面实验，验证了所提方法的有效性和优越性。已经公开发布了数据和代码：https://github.com/Mubuky/Self-Foveate

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [14] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 研究了利用大型语言模型进行因果发现的任务，发现先进的推理模型在性能上具有优势，但需要精心构建的上下文框架来最大化其性能。提出了一种模块化的上下文管道，实现了近三倍于传统方法的改进，为因果发现提供了可推广的蓝图。


<details>
  <summary>Details</summary>
Motivation: 最近大型语言模型内部推理的进展引发了人们对先进推理模型是否能够稳健地执行因果发现的兴趣，传统模型在数据扰动下往往存在严重过拟合和接近随机表现的问题。

Method: 使用Emergent OpenAI的o-series和DeepSeek-R模型系列在Corr2Cause基准上进行因果发现研究，引入了受到Tree-of-Thoughts和Chain-of-Thoughts方法启发的模块化上下文管道，获得了近三倍于传统基准的改进。

Result: 在Corr2Cause基准上，利用推理优先架构实现了比先前方法显著更大的原生增益，通过分析推理链长度、复杂性以及定性和定量比较，发现了模块化上下文管道的重要性。

Conclusion: 研究表明，先进的推理模型对因果发现具有显著优势，但需要精心构建的上下文框架来最大化它们的性能，为跨领域的因果发现提供了可推广的蓝图。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [15] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 该论文提出了基于因果解释的图像分类器输出解释方法，展示了因果解释与逻辑解释具有相似的形式性质，适用于黑盒算法并且是图像分类器的自然选择。引入对比性因果解释和置信度感知，提出完全因果解释，实验结果表明算法高效且完全黑盒。


<details>
  <summary>Details</summary>
Motivation: 现有用于解释图像分类器输出的算法缺乏形式严谨性，而基于逻辑的解释虽然形式和严谨，但在图像分类器上的可计算性依赖于严格的模型假设。因此，本文的动机是探索能够形式严谨定义的因果解释，同时适用于图像分类器，并能够应用于黑盒算法。

Method: 论文通过引入对比性因果解释和置信度感知，增强了解释的定义，并介绍了完全因果解释。他们证明了因果解释的形式性质，并展示了各种解释的计算效率。

Result: 论文提出了基于因果解释的方法，引入对比性因果解释和置信度感知，实现了完全因果解释。实验结果展示了该方法不同模型的解释模式，算法计算高效，能够以平均每张图像6秒的速度计算所有类型的解释，并且完全是黑盒算法，无需了解模型、模型内部、梯度等特性。

Conclusion: 该论文提出了基于因果解释的图像分类器输出解释方法。通过证明因果解释具有形式上的性质，论文展示了因果解释与逻辑解释具有相似的形式特性，同时适用于黑盒算法，并且是图像分类器的自然选择。实验结果表明，不同模型具有不同的解释模式，而论文提出的算法具有高效的计算性能和完全黑盒的特性。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [16] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: 本文提出了DICE框架，是一种动态上下文示例选择框架，用于代理任务。该方法能够在推理的每一步选择最相关的示例，并通过因果透镜将示例知识分解为可转移和不可转移的组件。作者提出了一个具有改进机器人性能形式保证的逐步选择标准。实验表明DICE方法的有效性和通用性，强调了基于原则的上下文意识示例选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文针对现有的基于大型语言模型的代理，在复杂推理和工具使用任务中展现出的ICL强大能力，但发现ICL的有效性高度依赖示例选择，不恰当的示例会导致性能不稳定或降低。现有方法大多依赖启发式方法或任务特定设计，缺乏通用、理论基础的选择标准。因此，开发一个有原则的通用方法来选择示例以提高代理性能是非常困难的。本文旨在解决这一挑战。

Method: 本文提出了DICE框架，通过因果透镜将示例知识分解为可转移和不可转移的组件，提出了一个具有改进机器人性能形式保证的逐步选择标准。DICE是一个通用的、框架不可知的解决方案，可以作为插件模块集成到现有的代理框架中，而不需要额外的训练成本。作者进行了广泛的实验验证方法的有效性和普适性。

Result: 作者提出的DICE方法展示了在代理任务中选择最相关示例的能力，并通过因果透镜分解示例知识，提出了改进机器人性能的选择标准。实验验证了DICE方法的有效性和通用性，证明了基于原则的上下文意识示例选择对于健壮和高效的LLM代理的重要性。

Conclusion: 本文提出了DICE，一种动态上下文示例选择框架，用于代理任务，可以在推理的每一步选择最相关的示例。该方法通过因果透镜将示例知识分解为可转移和不可转移的组件，展示了后者如何引入损害泛化的伪相关性。作者进一步提出了一个具有改进机器人性能形式保证的逐步选择标准。DICE是一个通用的、框架不可知的解决方案，可以作为插件模块集成到现有的代理框架中，而不需要额外的训练成本。作者通过多领域的广泛实验展示了该方法的有效性和普适性，强调了基于原则的、具有上下文意识的演示选择对于健壮和高效的LLM代理的重要性。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [17] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 本文针对分布式协作系统中任务特定信任评估的复杂性和资源消耗提出了一种自主信任编排方法。该方法利用智能代理人工智能和超图技术建立和维护设备之间的信任关系，实现了资源高效的信任评估，通过分析资源能力和任务需求之间的对齐程度，执行特定任务的信任评估，并维护嵌入信任语义的信任超图实现开销与信任准确性的平衡，支持多设备的协作。


<details>
  <summary>Details</summary>
Motivation: 在协作系统中，任务的有效完成取决于对分布式协作的潜在设备的任务特定信任评估。然而，任务的复杂性、分布式设备资源的时空动态性以及不可避免的评估开销增加了信任评估过程的复杂性和资源消耗。不恰当时机或过于频繁的信任评估可能降低受限资源的利用率，对协作任务执行产生负面影响。因此，本文旨在解决这一挑战，提出了一种基于新概念的自主信任编排方法。

Method: 本文采用了智能代理人工智能和超图技术来建立和维护设备之间的信任关系，实现了在设备空闲期间仅基于历史性能数据自主执行信任评估。同时，通过分析资源能力和任务需求之间的对齐程度，执行特定任务的信任评估。此外，通过维护嵌入信任语义的信任超图，实现了协作者的分层管理并识别需要信任评估的协作者，实现开销与信任准确性的平衡。

Result: 实验结果表明，所提出的方法实现了资源高效的信任评估。

Conclusion: 本文提出了一种基于语义信任链的自主信任编排方法，利用智能代理人人工智能和超图技术建立和维护设备之间的信任关系。实验结果表明，该方法实现了资源高效的信任评估。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>


### [18] [MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying](https://arxiv.org/abs/2507.23633)
*Qian Zhao,Zhuo Sun,Bin Guo,Zhiwen Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖的基于策略引导的代理辅助记忆召回方法，通过Recall Router框架和多种优化手段解决了记忆召回中的挑战，实验证明其在记忆激发方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 在传统的代理辅助记忆召回方法中，受到记忆模块大小的限制，记忆召回受到影响。为了解决这一问题，他们受到记忆理论的启发，提出了基于策略引导的方法。

Method: 他们设计了一个5W Recall Map来分类记忆查询，定义了15种召回策略模式，并使用蒙特卡洛树搜索算法优化了策略选择和生成策略响应。此外，他们构建了一个指令调整数据集，通过微调多个开源大型语言模型，开发了MemoCue代理。

Result: 实验结果表明，MemoCue在记忆激发方面比基于大型语言模型的方法提高了17.74%，人类评估进一步突显了该方法在记忆召回应用中的优势。

Conclusion: 该论文提出了一种基于策略引导的代理辅助记忆召回方法，通过设计合理的策略，帮助人们回忆记忆。他们提出了一个Recall Router框架来解决记忆召回中的关键挑战，并通过实验证明该方法在记忆激发方面优于基于大型语言模型的方法。

Abstract: Agent-assisted memory recall is one critical research problem in the field of
human-computer interaction. In conventional methods, the agent can retrieve
information from its equipped memory module to help the person recall
incomplete or vague memories. The limited size of memory module hinders the
acquisition of complete memories and impacts the memory recall performance in
practice. Memory theories suggest that the person's relevant memory can be
proactively activated through some effective cues. Inspired by this, we propose
a novel strategy-guided agent-assisted memory recall method, allowing the agent
to transform an original query into a cue-rich one via the judiciously designed
strategy to help the person recall memories. To this end, there are two key
challenges. (1) How to choose the appropriate recall strategy for diverse
forgetting scenarios with distinct memory-recall characteristics? (2) How to
obtain the high-quality responses leveraging recall strategies, given only
abstract and sparsely annotated strategy patterns? To address the challenges,
we propose a Recall Router framework. Specifically, we design a 5W Recall Map
to classify memory queries into five typical scenarios and define fifteen
recall strategy patterns across the corresponding scenarios. We then propose a
hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to
optimize the selection of strategy and the generation of strategy responses. We
construct an instruction tuning dataset and fine-tune multiple open-source
large language models (LLMs) to develop MemoCue, an agent that excels in
providing memory-inspired responses. Experiments on three representative
datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall
inspiration. Further human evaluation highlights its advantages in
memory-recall applications.

</details>


### [19] [Personalized Education with Ranking Alignment Recommendation](https://arxiv.org/abs/2507.23664)
*Haipeng Liu,Yuxuan Liu,Ting Long*

Main category: cs.AI

TL;DR: RAR introduces Ranking Alignment Recommendation to enhance recommendation performance in personalized question recommendation. It incorporates collaborative ideas for efficient exploration and can be applied to any RL-based question recommender.


<details>
  <summary>Details</summary>
Motivation: Previous methods struggle with efficient exploration and fail to identify the best questions for each student during training. To address this issue, RAR was developed to improve recommendation performance in personalized question recommendation.

Method: Proposed Ranking Alignment Recommendation (RAR) which incorporates collaborative ideas into the exploration mechanism for efficient exploration within limited training episodes.

Result: Experiments demonstrate that RAR effectively enhances recommendation performance in personalized question recommendation tasks.

Conclusion: RAR effectively improves recommendation performance in personalized question recommendation. The framework can be applied to any RL-based question recommender.

Abstract: Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.

</details>


### [20] [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
*Long Phan,Mantas Mazeika,Andy Zou,Dan Hendrycks*

Main category: cs.AI

TL;DR: TextQuests is introduced as a benchmark for evaluating AI agents on intrinsic long-context reasoning capabilities in exploratory environments. It precludes the use of external tools and focuses on self-contained problem-solving.


<details>
  <summary>Details</summary>
Motivation: Existing agent benchmarks do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over long contexts. Therefore, the need to develop agents with robust intrinsic reasoning skills over long horizons led to the creation of TextQuests.

Method: The paper introduces TextQuests, a benchmark based on the Infocom suite of interactive fiction games, to evaluate AI agents. It precludes the use of external tools to focus on intrinsic long-context reasoning capabilities in an exploratory environment.

Result: TextQuests provides a platform for evaluating AI agents' capacity for self-contained problem-solving through focused, stateful tasks. It serves as an effective proxy for assessing agents on intrinsic long-context reasoning capabilities.

Conclusion: Introducing TextQuests, a benchmark based on interactive fiction games, to evaluate AI agents' intrinsic long-context reasoning capabilities in exploratory environments. The benchmark focuses on self-contained problem-solving and prohibits the use of external tools.

Abstract: Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.

</details>


### [21] [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
*Luoxin Chen,Jinming Gu,Liankai Huang,Wenhao Huang,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Kaijing Ma,Cheng Ren,Jiawei Shen,Wenlei Shi,Tong Sun,He Sun,Jiahui Wang,Siran Wang,Zhihong Wang,Chenrui Wei,Shufa Wei,Yonghui Wu,Yuchen Wu,Yihang Xia,Huajian Xin,Fan Yang,Huaiyuan Ying,Hongyi Yuan,Zheng Yuan,Tianyang Zhan,Chi Zhang,Yue Zhang,Ge Zhang,Tianyun Zhao,Jianqiu Zhao,Yichi Zhou,Thomas Hanwen Zhu*

Main category: cs.AI

TL;DR: 本文提出了Seed-Prover和Seed-Geometry两个系统，通过领域特定语言Lean进行形式验证和强化学习解决IMO-level竞赛问题。Seed-Prover证明了78.1%的IMO问题，在MiniF2F上饱和，在PutnamBench上超过50%，明显优于之前的技术。Seed-Geometry引入了几何推理引擎，优于以往的形式几何引擎，参与IMO 2025证明了5个问题。


<details>
  <summary>Details</summary>
Motivation: 由于长篇自然语言推理缺乏明确的监督信号，本文提出了解决定理证明难题的方法。希望通过形式验证和长篇推理等方法实现自动数学推理的进一步发展。

Method: 使用强化学习、领域特定语言Lean进行形式验证，设计了Seed-Prover和Seed-Geometry系统。通过迭代改进证明、自我总结和Lean反馈来提高证明的精确性。

Result: Seed-Prover在过去的IMO问题中证明了78.1%，在MiniF2F上达到饱和，PutnamBench上超过50%，明显优于之前的最先进技术。Seed-Geometry引入几何推理引擎，优于以往的形式几何引擎。参与IMO 2025，完全证明了6个问题中的5个。

Conclusion: 提出了Seed-Prover和Seed-Geometry两个系统，通过结合领域特定语言Lean进行形式验证，并结合强化学习解决IMO-level竞赛问题。在解决数学推理问题方面取得较大进展，效果明显优于之前的最先进技术。

Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.

</details>


### [22] [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
*Ping Yu,Jack Lanchantin,Tianlu Wang,Weizhe Yuan,Olga Golovneva,Ilia Kulikov,Sainbayar Sukhbaatar,Jason Weston,Jing Xu*

Main category: cs.AI

TL;DR: 本文提出了CoT-Self-Instruct方法，通过CoT链式思考指导LLMs生成新合成提示，并使用度量指标筛选高质量数据。实验结果表明，在可验证推理和不可验证的遵循指令任务中，该方法的表现优于现有数据集和人类标准。


<details>
  <summary>Details</summary>
Motivation: 提供了一种新的合成数据生成方法，可以改善LLMs训练中的数据质量，以及在可验证推理和非验证指令遵循任务中的表现。

Method: 提出了CoT-Self-Instruct方法，通过CoT链式思考指导LLMs生成新合成提示，使用自动度量指标筛选高质量数据。在可验证推理和不可验证的遵循指令任务中进行了实验比较。

Result: 在实验中，提出的方法在可验证推理和不可验证指令遵循任务中均取得了优于现有数据集和人类标准的表现。

Conclusion: 提出了CoT-Self-Instruct方法，通过CoT链式思考指导LLMs首先进行推理和规划，然后生成质量和复杂度相似的新合成提示，用于LLM训练，随后使用自动度量指标对高质量数据进行筛选。在可验证推理中，我们的合成数据显著优于现有训练数据集，如s1k和OpenMathReasoning，在MATH500、AMC23、AIME24和GPQA-Diamond上表现良好。对于不可验证的遵循指令任务，我们的方法在AlpacaEval 2.0和Arena-Hard上均超过了人类或标准自我教导提示的表现。

Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.

</details>


### [23] [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
*Mingkai Deng,Jinyu Hou,Yilin Shen,Hongxia Jin,Graham Neubig,Zhiting Hu,Eric Xing*

Main category: cs.AI

TL;DR: SimuRA introduces a goal-oriented architecture based on world model simulation for agentic reasoning, surpassing autoregressive LLM limitations. It demonstrates significant improvements in web browsing tasks, showing up to 124% advantage over autoregressive planning. The potential for a single, general agent model using LLMs for superintelligent performance in all environments is highlighted.


<details>
  <summary>Details</summary>
Motivation: Current AI practice with LLMs focuses on a one-task-one-agent approach, lacking scalability, generality, and facing limitations of autoregressive LLMs. By mimicking human reasoning through simulating outcomes and plans, SimuRA aims to create a more general and powerful AI agent.

Method: SimuRA is based on a principled formulation of an optimal agent in any environment and utilizes a world model for planning through simulation. The generalized world model is implemented using LLM, enabling flexible planning in various environments utilizing the concept-rich latent space of natural language.

Result: Experiments on web browsing tasks demonstrate that SimuRA significantly improves the success rate of flight search and outperforms autoregressive planning by up to 124%. The availability of a generalized agent model based on LLMs for superintelligent performance in all environments excites the researchers.

Conclusion: SimuRA introduces a goal-oriented architecture for generalized agentic reasoning, overcoming the limitations of autoregressive LLMs and showing significant improvements in web browsing tasks. The approach demonstrates the advantage of world model simulation in reasoning paradigms.

Abstract: AI agents built on large language models (LLMs) hold enormous promise, but
current practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental
limitations of autoregressive LLMs. On the other hand, humans are general
agents who reason by mentally simulating the outcomes of their actions and
plans. Moving towards a more general and powerful AI agent, we introduce
SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based
on a principled formulation of optimal agent in any environment, \modelname
overcomes the limitations of autoregressive reasoning by introducing a world
model for planning via simulation. The generalized world model is implemented
using LLM, which can flexibly plan in a wide range of environments using the
concept-rich latent space of natural language. Experiments on difficult web
browsing tasks show that \modelname improves the success of flight search from
0\% to 32.2\%. World-model-based planning, in particular, shows consistent
advantage of up to 124\% over autoregressive planning, demonstrating the
advantage of world model simulation as a reasoning paradigm. We are excited
about the possibility for training a single, general agent model based on LLMs
that can act superintelligently in all environments. To start, we make SimuRA,
a web-browsing agent built on \modelname with pretrained LLMs, available as a
research demo for public testing.

</details>
