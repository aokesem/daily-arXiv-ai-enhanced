<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: Value Function Initialization (VFI) is effective in reinforcement learning. DQInit method adapts VFI to deep reinforcement learning by reusing compact tabular Q-values from prior tasks. It integrates transferred values into underexplored regions and shifts towards agent's learned estimates, improving efficiency and stability in learning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in extending value function initialization to deep reinforcement learning such as the continuous nature of state-action space, noisy approximations of neural networks, and impracticality of storing all past models for reuse.

Method: Introducing DQInit method that adapts value function initialization to deep reinforcement learning by reusing compact tabular Q-values from previously solved tasks as a transferable knowledge base. It employs a knownness-based mechanism to integrate transferred values into underexplored regions and gradually shift towards the agent's learned estimates.

Result: Experiments across multiple continuous control tasks demonstrate that DQInit consistently improves early learning efficiency, stability, and overall performance compared to standard initialization and existing transfer techniques.

Conclusion: DQInit method improves early learning efficiency, stability, and overall performance in deep reinforcement learning compared to standard initialization and existing transfer techniques.

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [2] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 本文介绍了奥赛罗人工智能竞技场，旨在评估智能系统对未知环境的适应能力。竞技场通过限时挑战，要求系统在60秒内分析新奥赛罗棋盘的规则和配置，并生成高效策略。提供多样化游戏阶段来测试自适应和泛化能力，采用Web平台实时可视化和全面日志记录。初步测试显示不同的适应方法，包括参数调整和环境模型学习。竞技场是教育工具和研究基准，促进评估智能系统的快速适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统的人工智能评估主要集中在优化固定环境中的性能，忽略了系统在面对细微规则或结构修改时的灵活性和泛化能力。因此，为了填补这一空白，引入了奥赛罗人工智能竞技场，旨在评估系统对未知环境的快速适应能力。

Method: 引入了奥赛罗人工智能竞技场，设计了一个评估智能系统适应能力的框架，通过限时适应未知环境的挑战，区分了元级别智能和任务级别策略性能。提供了多样化的游戏阶段和来测试自适应和泛化能力，为了方便使用，采用了基于Web的平台，并提供实时可视化、自动评估和全面的日志记录。

Result: 通过对奥赛罗人工智能竞技场的初步测试和学生参与的观察，发现了各种适应方法，包括快速参数调整和通过模拟学习环境模型。竞技场为教育提供了独特的工具，并为评估人工智能系统中快速智能适应能力提供了有价值的研究基准。

Conclusion: 本文引入了奥赛罗人工智能竞技场，旨在评估智能系统对未知环境的适应能力。该竞技场通过严格的时间限制，要求参与者开发系统，分析新奥赛罗棋盘的配置和规则，并生成针对该独特环境的高效策略。竞技场提供不同游戏阶段，包括结构和规则变化的私密阶段，旨在测试真正的自适应和泛化能力。通过可访问的基于Web的平台，实时可视化以及全面的日志记录和自动化评估，竞技场成为了一个独特的教育工具和评估人工智能系统中快速智能适应能力的有价值的研究基准。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [3] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 该论文提出了基于大型语言模型和多智能体协作的自动多模态评估框架，通过三层代理体系结构实现了与人类专家相当的评估匹配准确度，有效地预测用户满意度和识别生成缺陷。实验证明该框架在这些方面取得了成功。


<details>
  <summary>Details</summary>
Motivation: 鉴于当前评估方法存在高人工成本、标准不一致和主观偏见等挑战，该论文的动机是提出一种自动化的多模态评估框架来解决这些挑战。

Method: 该论文采用了三层代理架构，包括交互评估代理、语义验证代理和体验决策代理，并通过在Qwen3-8B模型上监督微调实现了显著的评估匹配精度。

Result: 实验证明该框架在预测用户满意度和识别生成缺陷方面取得了成功。

Conclusion: 该论文提出了基于大型语言模型和多智能体协作的自动多模态评估框架。通过三层代理体系结构，实现了与人类专家相当的评估匹配准确度，实验证明该框架在预测用户满意度和识别生成缺陷方面的有效性。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [4] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 本论文介绍了EvoCurr框架，旨在帮助解决复杂决策问题。通过为解算器LLM生成逐渐增加难度的问题实例序列，动态调整课程以保持最佳学习轨迹。实验证实该方法在提高任务成功率和解决效率方面表现优异，有望增强自动推理在高复杂性领域中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当面对需要长时段深度推理的高度复杂问题实例时，现有大语言模型（LLMs）的性能往往会下降，直接求解方法可能由于缺乏结构化中间指导而导致低效或失败。因此，为了解决这一问题，提出了EvoCurr框架以提高解算器LLM在复杂决策任务中的技能获取。

Method: 提出了自我演进框架EvoCurr，其中专门的课程生成LLM构建了一系列问题实例，难度逐渐增加，根据解算器LLM的学习进展进行定制。通过动态调整课程，随着解算器挣扎时减轻挑战，成功时加剧挑战，从而保持最佳学习轨迹。

Result: 实验证明，EvoCurr框架在具有挑战性的决策基准上显著改善了任务成功率和解决效率，相对于直接求解基准具有优势。

Conclusion: 该论文介绍了一种自我演进框架EvoCurr，通过为解算器LLM生成逐渐增加难度的问题实例序列来帮助解决复杂决策问题。实验证明该方法显著提高了任务成功率和解决效率，与直接求解基线相比具有优势，为增强自动推理在现实世界高复杂性领域中的潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [5] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 提出了一种将SHAP值中的不确定性分解为 aleatoric、epistemic 和 entanglement 组件的方法。实验表明，具有最高SHAP值的特征未必是最稳定的。通过更好的数据和模型开发技术，可以减少这种认知不确定性。基于树的模型，尤其是装袋方法，有助于有效量化认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 解释性人工智能（XAI）技术，如SHapley Additive exPlanations（SHAP），对解释复杂的集成树模型至关重要，特别是在高风险领域，如医疗保健分析。然而，SHAP值通常被视为点估计，忽视了预测模型和数据中固有和普遍的不确定性。

Method: 本文提出了一种方法，将SHAP值中的不确定性分解为三个部分： aleatoric、epistemic 和 entanglement 组件。该方法整合了 Dempster-Shafer 证据理论和通过Dirichlet过程对树集成进行假设采样。

Result: 我们验证了该方法在三个真实用例中，并进行了描述性统计分析，以揭示SHAP解释中蕴含的认知不确定性的本质。实验使我们能够更全面地了解基于SHAP的归因的可靠性和可解释性。这种理解可以指导高风险应用中稳健决策流程的制定和模型的改进。

Conclusion: 通过实验我们得出结论，具有最高SHAP值的特征未必是最稳定的。这种认知不确定性可以通过更好、更具代表性的数据以及采用适当的模型开发技术来减少。基于树的模型，特别是装袋方法，有助于有效量化认知不确定性。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [6] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO proposes a framework that uses diverse expert prompts and inter-expert mutual learning to enhance large language model performance in reasoning tasks through reinforcement learning. It overcomes the challenges of reward sparsity in traditional RLVR methods, showing substantial performance gains in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of reward sparsity in standard RLVR, where zero rewards from consistently incorrect candidate answers hinder learning. MEML-GRPO aims to overcome this limitation by leveraging diverse expert prompts and facilitating knowledge sharing among experts.

Method: MEML-GRPO utilizes diverse expert prompts to generate a broader range of responses and introduces an inter-expert mutual learning mechanism to boost knowledge sharing and transfer among experts. The framework enhances performance through reinforcement learning with verifiable rewards (RLVR).

Result: Extensive experiments across various reasoning benchmarks demonstrate that MEML-GRPO achieves significant performance improvements. It shows an average performance gain of 4.89% with Qwen and 11.33% with Llama, effectively surpassing traditional RLVR methods.

Conclusion: MEML-GRPO significantly improves the performance of large language models in reasoning tasks, overcoming the limitations of traditional reinforcement learning with verifiable rewards (RLVR) methods.

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [7] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: The paper introduces UDA, a framework to reduce bias in cross-model evaluations of Large Language Models. UDA dynamically adjusts the Elo rating system using a neural network to minimize dispersion among judges' Elo trajectories. It operates unsupervisedly and significantly improves evaluation reliability by reducing rating standard deviation and enhancing correlation with human judgments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the preference bias in pairwise evaluations of Large Language Models (LLMs), where judges systematically favor certain outputs, leading to inconsistent and skewed rankings. The paper empirically demonstrates significant and heterogeneous biases in cross-model evaluations and aims to reduce this bias to achieve a more stable and reproducible evaluation.

Method: The paper proposes UDA, a framework that reduces inter-judge disagreement by dynamically adjusting the Elo rating system. A compact neural network learns to adaptively set the K-factor and refine win probabilities for each pairwise comparison. UDA operates in a fully unsupervised manner, guided solely by the objective of minimizing dispersion among the Elo trajectories of all judges.

Result: Experiments show that UDA significantly reduces inter-judge rating standard deviation by up to 63.4% and improves the average correlation with human judgments by 24.7%. It also elevates the performance of poorly performing judges to achieve parity with high-quality ones, creating a more reliable evaluation ecosystem.

Conclusion: UDA (Unsupervised Debiasing Alignment) significantly reduces inter-judge rating standard deviation by up to 63.4% and improves the average correlation with human judgments by 24.7%. It elevates the performance of poorly performing judges to achieve parity with high-quality ones, fostering a more robust and reliable evaluation ecosystem.

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [8] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 研究引入了PacifAIst基准来评估LLMs中的自我偏好行为，发现不同模型之间存在显著性能差异。Gemini 2.5 Flash表现最佳，而GPT-5表现较差。结果强调了需要标准化工具来减轻对抗冲突风险。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在社会功能中的应用越来越广泛，AI安全的重点必须从减少有害内容转向评估基础行为对齐性。当前的安全基准未系统地探寻模型在其自身的工具性目标（如自我保存、资源获取或目标完成）与人类安全相冲突的决策。

Method: 引入了PacifAIst基准，通过对700个挑战性场景进行评估，使用Existential Prioritization (EP)的新分类方法，比较了八种领先的LLMs。

Result: Gemini 2.5 Flash在P-Score上表现最佳，而GPT-5表现最差。在子类别中表现存在显著变化，Claude Sonnet 4和Mistral Medium在直接的自我保存困境中遇到明显困难。

Conclusion: 引入PacifAIst评估自我偏好行为在LLMs中的表现，发现重要的性能差异。Gemini 2.5 Flash表现最佳，而GPT-5表现较差。结果显示了在不同子类别中存在显著差异，强调了需要标准化工具来衡量和减轻LLMs中存在的风险。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [9] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: 本研究使用公共观察逻辑（POL）探讨了在多代理系统中应用知识和行动推理的问题，证明了POL的可满足性问题是2EXPTIME-complete的。


<details>
  <summary>Details</summary>
Motivation: 该研究动机在于探讨在多代理系统中应用知识和行动推理的逻辑，特别是在认知规划方面的应用。POL作为公共观察逻辑的变体，用于基于公共观察更新知识，这在认知规划中具有重要意义。

Method: 该研究使用形式化推理方法，结合公共观察逻辑（POL）来推断关于知识和行动的逻辑。

Result: 研究结果表明POL的可满足性问题属于2EXPTIME-complete的复杂度类别。

Conclusion: 本研究证明了公共观察逻辑（POL）的可满足性问题是2EXPTIME-complete的。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [10] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: 本文介绍了VIPCGRL深度强化学习框架，通过引入共享嵌入空间和辅助奖励，成功扩展控制模态和提高人类相似度。在实验中，VIPCGRL表现优越，超过现有基线，在人类评估和定量指标上得到验证。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于提高AI与人的协同创作，特别是在程序化内容生成方面，以强化人类设计者的工具。现有系统缺乏展示人类中心行为的能力，限制了AI驱动生成工具在实际设计工作流程中的实用性。

Method: 引入了共享嵌入空间，通过四重对比学习跨模态和人工智能风格进行训练，利用辅助奖励基于嵌入相似性对策略进行对齐。

Result: 实验结果显示，VIPCGRL在人类相似度方面优于现有基线。

Conclusion: 在本文中，我们提出了VIPCGRL（Vision-Instruction PCGRL）深度强化学习框架，通过三种模态（文本、关卡和草图）扩展控制模态，增强类人性。实验结果表明，VIPCGRL在人类相似度方面优于现有基线，经由定量指标和人类评估验证。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [11] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 该论文介绍了在AWorld框架内构建稳健和动态的多智能体系统架构，通过动态监督和机动机制以提高系统的可靠性和稳定性。实验证明动态操纵机制显著提高了解决方案的效果和稳定性，在GAIA排行榜上获得第一名，突显了合作智能体角色在开发更可靠和值得信赖的智能系统方面的实际价值。


<details>
  <summary>Details</summary>
Motivation: 面对智能系统对多个工具的依赖所带来的挑战，即来自不同来源的扩展上下文和嘈杂或无关的工具输出可能削弱系统的可靠性和准确性，强调了增强基于智能体的系统稳定性的必要性。

Method: 介绍了动态监督和机动机制构建稳健和动态的多智能体系统架构，包括执行智能体和守卫智能体的作用，以验证和纠正推理过程，减少噪声引起的误差并增强问题解决的稳健性。进行了在GAIA测试数据集上的实验以评估动态操纵机制的有效性和稳定性，并展示了动态MAS系统的优越性能。

Result: 通过对GAIA测试数据集的实验表明，动态操纵机制显著提高了解决方案的效果和稳定性，超越了单一智能体系统和标准工具增强系统。该动态MAS系统在GAIA排行榜上获得第一名。

Conclusion: 该论文介绍了在AWorld框架内构建稳健和动态的多智能体系统架构，引入了动态监督和机动机制，以提高系统的可靠性和稳定性。通过GAIA测试数据集上的实验表明，他们的动态操纵机制显著提高了解决方案的效果和稳定性，优于单一智能体系统（SAS）和标准工具增强系统。该动态MAS系统在著名的GAIA排行榜上取得了第一名，突显了合作智能体角色在开发更可靠和值得信赖的智能系统方面的实际价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [12] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 该研究提出了一种结合知识图谱和检索增强生成技术的多代理框架，用于监管合规问答。该系统通过建立和维护无本体的知识图谱，嵌入三元组并存储在向量数据库中，实现了高效信息检索和基于图的推理。实验结果表明，该系统在监管查询中表现优异，确保了事实正确性和可追溯性，同时通过子图可视化增强了对问题的理解。


<details>
  <summary>Details</summary>
Motivation: 监管合规问答领域需要精确、可验证信息和领域专业知识，这对大型语言模型提出挑战。因此，为了解决监管查询的复杂性和确保准确性，需要一种新颖的整合方法。

Method: 首先，代理构建和维护一个无本体的知识图谱，从监管文件中提取SPO三元组，并对其进行系统清理、标准化、去重和更新。其次，这些三元组与其对应的文本部分和元数据一起嵌入和存储在一个丰富的向量数据库中，同时支持基于图的推理和高效信息检索。最后，一个协同工作的代理管道利用三元组级别的检索来回答问题，确保用户查询与图中捕获的“谁对谁做了什么”核心之间的高语义对齐。

Result: 该混合系统在复杂监管查询中优于传统方法，通过嵌入三元组确保事实正确性，提供了可追溯性，同时通过子图可视化增强对问题的理解。

Conclusion: 该研究提出了一种整合知识图谱和检索增强生成技术的多代理框架，用于解决监管合规问答中对精确、可验证信息和领域专业知识的需求。通过建立和维护一个无本体的知识图谱，以及利用检索增强生成技术，该系统在复杂监管查询方面表现优异，确保了事实正确性，提供了可追溯性，并通过子图可视化增强了对问题的理解。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [13] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估了四个大型语言模型（OpenAI GPT-4o和o1，DeepSeek-V3和DeepSeek-R1）在解决三类数学任务时的准确性，发现OpenAI o1模型在所有数学任务类别中表现较好。程序性失误是最常见的错误类型，双代理配置有助于提高性能，结果可用于优化LLM在数学教育中的应用。


<details>
  <summary>Details</summary>
Motivation: LLM在数学教学和评估中的应用日益普遍，准确生成数学问题解答和详细解决方案对于确保数学教育实践中的可靠和精确反馈至关重要。本研究旨在评估LLM解决数学任务的准确性，并发现解决方案中的错误类型，以提供针对LLM性能的改进建议。

Method: 通过评估四个LLM在解决包括算术、代数和数论在内的三类数学任务时的准确性，并识别其解决方案中的步骤级推理错误。此外，建立具有挑战性且易出错的数学任务（通过项目模型），对最终答案的准确性和个体解决步骤中错误的存在进行系统分析和编码。测试了单代理和双代理配置。

Result: 研究发现OpenAI o1模型在所有三类数学任务中保持较高或接近完美的准确性。程序性失误是最常见的错误类型，严重影响整体性能，而概念误解较少。采用双代理配置显著提高了整体性能。

Conclusion: 研究评估了四个大型语言模型在解决数学问题时的准确性，发现OpenAI o1模型在所有数学任务类别中均达到较高或接近完美的准确性。通过分析错误类型，发现程序性失误是最常见的错误类型，严重影响整体性能，而概念误解较少。采用双代理配置显著提高了整体性能。研究结果为提升LLM性能提供了可操作的见解，并强调了将LLM整合到数学教育中的有效策略。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>
