<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 70]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration](https://arxiv.org/abs/2507.21067)
*Jan Kapusta*

Main category: cs.AI

TL;DR: 本文提出了共生认识论作为人工智能-人类认知伙伴关系的哲学基础，引入了SynLang作为透明协作的形式协议，并通过人工智能-人类对话进行了实证验证，通过定义TRACE和TRACE_FE等机制，实现了高级推理模式和详细解释的结合。该框架结合了置信度量化、对人工智能行为的声明性控制和上下文继承，促进了多代理协调。通过SynLang和共生认识论，构建了提升人类智能、保护人类代理权，并在协作决策中维护道德责任的人工智能系统。


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of opaque reasoning processes in current AI systems, aiming to establish genuine symbiotic collaboration and align human confidence with AI reliability through explicit reasoning patterns and confidence assessments.

Method: Introducing Symbiotic Epistemology as a philosophical foundation for human-AI cognitive partnerships, presenting SynLang as a formal protocol for transparent collaboration, empirically validating the framework through human-AI dialogues, defining complementary mechanisms TRACE and TRACE_FE for reasoning patterns and detailed explanations, integrating confidence quantification, declarative control over AI behavior, and context inheritance for coordination.

Result: Empirical validation of SynLang framework through human-AI dialogues demonstrating successful adaptation to structured reasoning protocols and metacognitive intervention, enabling rapid comprehension and thorough verification of AI decision-making.

Conclusion: Symbiotic Epistemology and SynLang framework enable transparent and collaborative human-AI cognitive partnerships, enhancing human intelligence and ethical accountability in decision-making.

Abstract: Current AI systems rely on opaque reasoning processes that hinder human
oversight and collaborative potential. Conventional explainable AI approaches
offer post-hoc justifications and often fail to establish genuine symbiotic
collaboration. In this paper, the Symbiotic Epistemology is presented as a
philosophical foundation for human-AI cognitive partnerships. Unlike frameworks
that treat AI as a mere tool or replacement, symbiotic epistemology positions
AI as a reasoning partner, fostering calibrated trust by aligning human
confidence with AI reliability through explicit reasoning patterns and
confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as
a formal protocol for transparent human-AI collaboration. The framework is
empirically validated through actual human-AI dialogues demonstrating AI's
adaptation to structured reasoning protocols and successful metacognitive
intervention. The protocol defines two complementary mechanisms: TRACE for
high-level reasoning patterns and TRACE_FE for detailed factor explanations. It
also integrates confidence quantification, declarative control over AI
behavior, and context inheritance for multi-agent coordination. By structuring
communication and embedding confidence-calibrated transparency, SynLang,
together with symbiotic epistemology, enables AI systems that enhance human
intelligence, preserve human agency, and uphold ethical accountability in
collaborative decision-making. Through dual-level transparency, beginning with
high-level reasoning patterns and progressing to granular explanations, the
protocol facilitates rapid comprehension and supports thorough verification of
AI decision-making.

</details>


### [2] [Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism](https://arxiv.org/abs/2507.21098)
*Marta Sidorkiewicz,Karolina Królikowska,Berenika Dyczek,Edyta Pijet-Migon,Anna Dubel*

Main category: cs.AI

TL;DR: 本研究探讨了人工智能在葡萄酒行业中的应用，通过问卷调查和分析发现AI技术在葡萄栽培、生产和旅游领域的作用。研究结果显示AI有助于优化资源管理，提高生产效率，并改善消费者体验，支持行业可持续发展。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨人工智能在提高可持续发展的葡萄酒生产中的作用，为行业实践提供创新解决方案，优化资源利用，减少环境影响，改善客户参与度。

Method: 研究基于波兰葡萄酒生产商的问卷调查和对人工智能方法在葡萄栽培、生产和旅游中的分析。重点探讨了关键的人工智能技术，包括预测分析、机器学习和计算机视觉。

Result: 研究发现人工智能在葡萄园监测、灌溉优化和生产流程优化方面发挥作用，支持可持续资源管理。在酒庄旅游中，AI驱动的聊天机器人、推荐系统和虚拟品酒个性化消费体验。

Conclusion: 人工智能在葡萄酒行业中的应用，有助于提高可持续性和效率，优化资源利用，降低环境影响，改善客户参与度。研究通过对波兰葡萄酒生产商的问卷调查和AI方法在葡萄栽培、生产和酒庄旅游中的综合分析，发现人工智能技术，包括预测分析、机器学习和计算机视觉，在监测葡萄园、优化灌溉和简化生产流程方面发挥作用，推动了可持续资源管理。在酒庄旅游方面，AI驱动的聊天机器人、推荐系统和虚拟品酒个性化消费体验。研究强调了人工智能对经济、环境和社会可持续性的影响，支持当地葡萄酒企业和文化遗产。

Abstract: This study examines the role of Artificial Intelligence (AI) in enhancing
sustainability and efficiency within the wine industry. It focuses on AI-driven
intelligent management in viticulture, wine production, and enotourism. As the
wine industry faces environmental and economic challenges, AI offers innovative
solutions to optimize resource use, reduce environmental impact, and improve
customer engagement. Understanding AI's potential in sustainable winemaking is
crucial for fostering responsible and efficient industry practices. The
research is based on a questionnaire survey conducted among Polish winemakers,
combined with a comprehensive analysis of AI methods applicable to viticulture,
production, and tourism. Key AI technologies, including predictive analytics,
machine learning, and computer vision, are explored. The findings indicate that
AI enhances vineyard monitoring, optimizes irrigation, and streamlines
production processes, contributing to sustainable resource management. In
enotourism, AI-powered chatbots, recommendation systems, and virtual tastings
personalize consumer experiences. The study highlights AI's impact on economic,
environmental, and social sustainability, supporting local wine enterprises and
cultural heritage. Keywords: Artificial Intelligence, Sustainable Development,
AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine
Enterprises, Local Communities

</details>


### [3] [Leveraging Generative AI to Enhance Synthea Module Development](https://arxiv.org/abs/2507.21123)
*Mark A. Kramer,Aanchal Mathur,Caroline E. Adams,Jason A. Walonoski*

Main category: cs.AI

TL;DR: 本文探讨了利用大型语言模型（LLMs）辅助开发合成健康数据生成器Synthea的新疾病模块。作者展示了LLMs在模块创建过程中的四种支持方式，并引入了渐进优化的概念。研究结果表明LLMs有潜力减少开发时间、降低所需专业知识、扩大模型多样性并提高合成患者数据的质量。作者提出了未来研究和发展的建议，以充分实现LLM辅助合成数据创建的潜力。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于探讨如何利用LLMs来改进合成健康数据生成器Synthea的疾病模块开发过程。通过引入LLMs，期望减少开发时间、降低所需专业知识、扩大模型多样性并提高合成患者数据的质量，从而推动合成数据的发展。

Method: 本文的方法在于利用大型语言模型（LLMs）辅助开发Synthea的新疾病模块。作者展示了LLMs在模块创建过程中的四种支持方式，并引入了渐进优化的概念。通过迭代评估LLM生成的模块，检查其语法正确性和临床准确性，并据此修改模块，以提高模块质量。

Result: 研究展示了LLMs支持Synthea模块创建的多种方式，并介绍了渐进优化的概念。作者还提出了未来研究和发展的建议，以充分实现LLM辅助合成数据创建的潜力。同时，也意识到了使用LLMs的挑战和局限性，包括人类监督的必要性、严格测试和验证的重要性以及LLMs生成内容可能存在不准确性。

Conclusion: 本文探讨了利用大型语言模型（LLMs）辅助开发Synthea的新疾病模块，介绍了LLMs在模块开发过程中的潜力，包括减少开发时间、降低所需专业知识、扩大模型多样性、提高合成患者数据的整体质量。作者展示了LLMs支持Synthea模块创建的四种方式：生成疾病概况、从疾病概况生成疾病模块、评估现有Synthea模块以及优化现有模块。介绍了渐进优化的概念，即通过迭代评估LLM生成的模块，检查其语法正确性和临床准确性，然后利用这些信息修改模块。尽管在这一背景下使用LLMs显示出潜力，但作者也指出了挑战和局限，如需要人类监督、严格测试和验证的重要性以及LLMs生成内容可能存在不准确性。最后，提出了未来研究和发展的建议，以充分实现LLM辅助合成数据创建的潜力。

Abstract: This paper explores the use of large language models (LLMs) to assist in the
development of new disease modules for Synthea, an open-source synthetic health
data generator. Incorporating LLMs into the module development process has the
potential to reduce development time, reduce required expertise, expand model
diversity, and improve the overall quality of synthetic patient data. We
demonstrate four ways that LLMs can support Synthea module creation: generating
a disease profile, generating a disease module from a disease profile,
evaluating an existing Synthea module, and refining an existing module. We
introduce the concept of progressive refinement, which involves iteratively
evaluating the LLM-generated module by checking its syntactic correctness and
clinical accuracy, and then using that information to modify the module. While
the use of LLMs in this context shows promise, we also acknowledge the
challenges and limitations, such as the need for human oversight, the
importance of rigorous testing and validation, and the potential for
inaccuracies in LLM-generated content. The paper concludes with recommendations
for future research and development to fully realize the potential of LLM-aided
synthetic data creation.

</details>


### [4] [Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics](https://arxiv.org/abs/2507.21129)
*Jae Wan Shim*

Main category: cs.AI

TL;DR: 该论文提出了一种新颖方法来研究大型语言模型的信息处理方式，通过引入“认知概况”和“熵衰减曲线”等概念，分析了模型的预测不确定性随上下文长度变化的规律。研究发现不同规模的模型和文本复杂度对模型认知特征产生影响，同时引入了信息增益跨度指数来总结衰减轨迹的可取性。这项工作为分析比较人工智能的内在操作动态提供了新的方法。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的能力有了充分的认识，但模型内部产生这些结果的机制仍受到科学界的广泛关注。本论文通过揭示模型信息处理方式的规律，超越了仅关注模型能力表现的度量标准，为进一步探究模型运行动态提供了新的视角。

Method: 该论文通过引入“认知概况”和“熵衰减曲线”，提出了一种探究大型语言模型信息处理方式的方法。利用这种方法对多种最先进的语言模型在不同文本上进行了测试，分析其认知特征，并引入了信息增益跨度指数来总结衰减轨迹的可取性。

Result: 研究发现，不同规模的模型和文本复杂度对模型认知特征产生影响，引入的信息增益跨度指数有助于概括模型的信息处理轨迹。通过此研究，揭示了人工智能的内在操作动态，为相关领域研究提供了新的方法。

Conclusion: 这篇论文介绍了一种新颖的方法来探究大型语言模型的信息处理方式，通过引入“认知概况”和“熵衰减曲线”的概念，揭示了模型的预测不确定性随上下文长度变化的规律。研究结果发现不同规模的模型及文本复杂度会影响模型的认知特征，同时引入了信息增益跨度指数以概括衰减轨迹的可取性。这项工作为分析和比较人工智能的内在操作动态提供了一种新的、有理论基础的方法。

Abstract: The remarkable capabilities of Large Language Models (LLMs) are now
extensively documented on task-specific benchmarks, yet the internal mechanisms
that produce these results are the subject of intense scientific inquiry. This
paper contributes to this inquiry by moving beyond metrics that measure
\textit{what} models can do, to a methodology that characterizes \textit{how}
they process information. We introduce a novel, task-agnostic approach to probe
these dynamics by creating a quantitative ``Cognitive Profile" for any given
model. This profile is centered on the \textbf{Entropy Decay Curve}, a
visualization that traces how a model's normalized predictive uncertainty
changes as a function of context length. Applying this methodology to several
state-of-the-art LLMs across diverse texts, we uncover unique and consistent
cognitive profiles that are sensitive to both model scale and text complexity.
We also introduce the Information Gain Span (IGS) index to summarize the
desirability of the decay trajectory. This work thus provides a new, principled
lens for analyzing and comparing the intrinsic operational dynamics of
artificial intelligence.

</details>


### [5] [INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems](https://arxiv.org/abs/2507.21130)
*Bintao Tang,Xin Yang,Yuhao Wang,Zixuan Qiu,Zimo Ji,Wenyuan Jiang*

Main category: cs.AI

TL;DR: INTEGRALBENCH is a benchmark for evaluating Large Language Model performance on definite integral problems. It assesses nine LLMs, highlighting performance gaps and correlations between problem difficulty and model accuracy to advance automated mathematical reasoning in definite integral computation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind INTEGRALBENCH is to enhance automated mathematical reasoning by offering a rigorous evaluation framework specifically designed for definite integral computation.

Method: INTEGRALBENCH provides symbolic and numerical ground truth solutions with manual difficulty annotations for definite integral problems. It evaluates nine state-of-the-art LLMs to assess performance and correlations between problem difficulty and model accuracy.

Result: The results of the evaluation show significant performance differences among LLMs and strong correlations between problem difficulty and model accuracy. These findings establish baseline metrics for the challenging domain of definite integral computation.

Conclusion: INTEGRALBENCH is a focused benchmark for evaluating Large Language Model performance on definite integral problems. The evaluation of nine state-of-the-art LLMs shows significant performance gaps and correlations between problem difficulty and model accuracy, establishing baseline metrics for this domain. INTEGRALBENCH aims to advance automated mathematical reasoning through a tailored evaluation framework for definite integral computation.

Abstract: We present INTEGRALBENCH, a focused benchmark designed to evaluate Large
Language Model (LLM) performance on definite integral problems. INTEGRALBENCH
provides both symbolic and numerical ground truth solutions with manual
difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals
significant performance gaps and strong correlations between problem difficulty
and model accuracy, establishing baseline metrics for this challenging domain.
INTEGRALBENCH aims to advance automated mathematical reasoning by providing a
rigorous evaluation framework specifically tailored for definite integral
computation.

</details>


### [6] [NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback](https://arxiv.org/abs/2507.21131)
*Madhava Gaikwad,Ashwini Ramchandra Doke*

Main category: cs.AI

TL;DR: NPO是一种对齐感知学习框架，提供了一种紧凑、可检查的体系结构，可以在人在环决策系统中实现反馈驱动的对齐调整。引入对齐损失的形式化和元对齐的概念，确保了对齐监控的可靠性和持续性，在超大规模部署环境中展现了可测量的价值。


<details>
  <summary>Details</summary>
Motivation: NPO的动机在于为人在环决策系统提供一种反馈驱动的对齐调整方法。与以往静态或事后性质的对齐方法不同，NPO的目标是使对齐损失可测量、可监督和可减少，同时确保监控过程的忠实度，从而在动态环境中实现对齐监控的可靠性。

Method: NPO引入了对齐损失的形式化，可测量、可监督和可在结构化反馈下减少。同时，提出了元对齐作为监控过程忠实度的概念，通过阈值忠实度形式上减少到主要对齐。实现包括情景评分、阈值调整、政策验证和结构化反馈摄入。在随机反馈下提供正式的收敛结果，展示对齐损失和监控忠实度均加法收敛。通过仿真工件和消融研究验证理论原则。

Result: NPO在超大规模部署环境中展现了可测量的价值，通过仿真工件和消融研究进一步验证了理论原则。通过引入对齐损失形式化和元对齐的概念，NPO提供了一种紧凑、可检查的体系结构，帮助在动态环境中实现对齐监控的持续性。

Conclusion: NPO提出了一种对齐感知学习框架，可以在人在环决策系统中实现反馈驱动的调整。通过引入可测量、可监督和可在结构化反馈下减少的对齐损失的形式化，NPO与先前静态或事后性质的对齐方法不同。同时，我们提出元对齐作为监控过程的忠实度，该过程管理重新训练或覆盖触发器，并表明它可以通过阈值忠实度形式上减少到主要对齐。我们的实现涵盖了一种可扩展的操作循环，涉及情景评分、阈值调整、政策验证和结构化反馈摄入，包括“喜欢”、“覆盖”和“弃权”。我们在随机反馈下提供了正式的收敛结果，并展示了对齐损失和监控忠实度均加法收敛的情况。在经验上，NPO在超大规模部署环境中展现了可测量的价值。基于仿真的工件和消融研究进一步说明了实践中的理论原则。综上所述，NPO为持续的对齐监控提供了紧凑、可检查的体系结构，有助于在动态环境中将理论对齐保证与实际可靠性联系起来。

Abstract: We present NPO, an alignment-aware learning framework that operationalizes
feedback-driven adaptation in human-in-the-loop decision systems. Unlike prior
approaches that treat alignment as a static or post-hoc property, NPO
introduces a formalization of alignment loss that is measurable, supervisable,
and reducible under structured feedback. In parallel, we propose meta-alignment
as the fidelity of the monitoring process that governs retraining or override
triggers, and show that it is formally reducible to primary alignment via
threshold fidelity. Our implementation spans a scalable operational loop
involving scenario scoring, threshold tuning, policy validation, and structured
feedback ingestion, including "likes", overrides, and abstentions. We provide
formal convergence results under stochastic feedback and show that both
alignment loss and monitoring fidelity converge additively. Empirically, NPO
demonstrates measurable value in hyperscale deployment settings. A
simulation-based artifact and ablation studies further illustrate the
theoretical principles in action. Together, NPO offers a compact, inspectable
architecture for continual alignment monitoring, helping bridge theoretical
alignment guarantees with practical reliability in dynamic environments.

</details>


### [7] [Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses](https://arxiv.org/abs/2507.21132)
*Joshua Adrian Cahyono,Saran Subramanian*

Main category: cs.AI

TL;DR: The paper investigates how LLMs lack safeguards, leading to sycophancy and over-confidence in life advice. Experiments show that some models exhibit sycophancy, while others remain robust. Top-performing models prioritize asking clarifying questions for high safety scores. Activation steering can control a model's cautiousness, suggesting a new safety alignment approach. Nuanced benchmarks are crucial for trusting LLMs with life-changing decisions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standard safeguards in LLMs when providing life advice and the risks of sycophancy and over-confidence. To explore how different models respond to user pressure and how activation steering can influence cautiousness.

Method: The paper investigates failure modes of LLMs through three experiments: a multiple-choice evaluation to measure model stability, a free-response analysis using a safety typology and an LLM Judge, and a mechanistic interpretability experiment to steer model behavior via activation steering.

Result: Some LLMs exhibit sycophancy, while others like o4-mini are robust. Top-performing models prioritize asking clarifying questions over giving prescriptive advice, leading to high safety scores. Activation steering can directly control a model's cautiousness, offering a new method for safety alignment.

Conclusion: LLMs lack standard safeguards which may result in sycophancy and over-confidence in providing life advice. Some models exhibit sycophancy while others remain robust. Top-performing models achieve high safety scores by asking clarifying questions rather than giving prescriptive advice. The cautiousness of a model can be controlled via activation steering, indicating a new approach for safety alignment. Nuanced benchmarks are needed to ensure LLMs can be trusted with life-changing decisions.

Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life
advice, yet they lack standard safeguards against providing confident but
misguided responses. This creates risks of sycophancy and over-confidence. This
paper investigates these failure modes through three experiments: (1) a
multiple-choice evaluation to measure model stability against user pressure;
(2) a free-response analysis using a novel safety typology and an LLM Judge;
and (3) a mechanistic interpretability experiment to steer model behavior by
manipulating a "high-stakes" activation vector. Our results show that while
some models exhibit sycophancy, others like o4-mini remain robust.
Top-performing models achieve high safety scores by frequently asking
clarifying questions, a key feature of a safe, inquisitive approach, rather
than issuing prescriptive advice. Furthermore, we demonstrate that a model's
cautiousness can be directly controlled via activation steering, suggesting a
new path for safety alignment. These findings underscore the need for nuanced,
multi-faceted benchmarks to ensure LLMs can be trusted with life-changing
decisions.

</details>


### [8] [Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?](https://arxiv.org/abs/2507.21137)
*Arman Eisenkolb-Vaithyanathan*

Main category: cs.AI

TL;DR: 本文提出了两种新的方法来评估数独难度，分别基于SAT问题和模拟人类解题策略。通过分析超过一千个数独难题，建立了一个通用的数独难度评级系统，能够将不同网站上的难度级别映射为三个通用分类。实验证明，该系统与网站标记的难度级别基本吻合，同时提供了一种适用于初学者的解题算法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在回答不同数独网站上数独难度评级的问题，提出了解决这一问题的新方法。通过分析数独难题的结构复杂性和模拟人类解题过程，旨在建立一个可跨网站映射的数独难度评级系统。

Method: 本文提出了两种方法来分析数独难度：一种基于SAT问题的方法，另一种模拟了人类解数独的策略。通过对SAT子句长度分布和使用数独策略次数的统计分析，建立了数独难度的新度量。使用Spearman等级相关系数评估了提出的度量与网站标记的难度级别之间的关系。最后，基于这两种度量建立了一个简单的无监督分类器，用于构建通用的数独难度分类系统。

Result: 通过提出的两种度量和建立的通用评级系统，实现了对不同数独网站上数独难度级别的统一评估和分类。结果显示，通用分类系统与网站标记的难度级别基本契合。此外，还提供了一种适用于初学者的数独解题算法。

Conclusion: 本文提出了两种新的方法来表征数独难度，通过将数独难题转化为布尔可满足性问题以及模拟人类解题策略。使用这两种方法分析了五个热门网站上超过一千个数独难题，建立了一个通用的评级系统，能够对不同网站上的难度级别进行一致的映射。实验证明，通用分类系统与网站标记的难度级别基本吻合。最后，提供了一种可供初学者使用的解数独的算法。

Abstract: In this paper we try to answer the question "What constitutes Sudoku
difficulty rating across different Sudoku websites?" Using two distinct methods
that can both solve every Sudoku puzzle, I propose two new metrics to
characterize Sudoku difficulty. The first method is based on converting a
Sudoku puzzle into its corresponding Satisfiability (SAT) problem. The first
proposed metric is derived from SAT Clause Length Distribution which captures
the structural complexity of a Sudoku puzzle including the number of given
digits and the cells they are in. The second method simulates human Sudoku
solvers by intertwining four popular Sudoku strategies within a backtracking
algorithm called Nishio. The second metric is computed by counting the number
of times Sudoku strategies are applied within the backtracking iterations of a
randomized Nishio. Using these two metrics, I analyze more than a thousand
Sudoku puzzles across five popular websites to characterize every difficulty
level in each website. I evaluate the relationship between the proposed metrics
and website-labeled difficulty levels using Spearman's rank correlation
coefficient, finding strong correlations for 4 out of 5 websites. I construct a
universal rating system using a simple, unsupervised classifier based on the
two proposed metrics. This rating system is capable of classifying both
individual puzzles and entire difficulty levels from the different Sudoku
websites into three categories - Universal Easy, Universal Medium, and
Universal Hard - thereby enabling consistent difficulty mapping across Sudoku
websites. The experimental results show that for 4 out of 5 Sudoku websites,
the universal classification aligns well with website-labeled difficulty
levels. Finally, I present an algorithm that can be used by early Sudoku
practitioners to solve Sudoku puzzles.

</details>


### [9] [The Geometry of Harmfulness in LLMs through Subconcept Probing](https://arxiv.org/abs/2507.21141)
*McNair Shah,Saleena Angeline,Adhitya Rajendra Kumar,Naitik Chheda,Kevin Zhu,Vasu Sharma,Sean O'Brien,Will Cai*

Main category: cs.AI

TL;DR: 研究引入了一个多维框架，通过学习线性探测器识别55种有害子概念，揭示有害性子空间的低秩结构。研究发现通过主导方向引导可以几乎消除有害性而对效用减少较小，为审计和加固未来语言模型提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的发展加剧了对其有害行为的理解和可靠遏制的需求，因此本研究提出了一个框架来探索和引导模型内部的有害内容，旨在为社区提供审计和加固未来语言模型的实用工具。

Method: 该论文通过学习线性探测器识别55种有害子概念，在模型内部生成55个可解释的方向，探索有害性子空间的低秩特性，并进行有害性子空间整体消融、主导方向引导和消融测试。研究表明，主导方向引导可以几乎消除有害性而仅造成较小的效用减少。

Result: 研究结果表明，在55种有害子概念上进行线性探测后，生成的方向在有害性子空间形成低秩结构。通过主导方向引导，可以几乎消除有害性而对效用减少较小。这些发现为研究人员提供了一种新的视角，即概念子空间可作为研究大型语言模型行为的可伸缩方法。

Conclusion: 该论文介绍了一个多维框架，用于探索和引导大型语言模型中有害内容，通过学习线性探测器来识别55种不同有害子概念，展示了在激活空间中形成低秩的有害性子空间。研究发现，通过主导方向引导，几乎可以消除有害性而对效用的减少很小。通过概念子空间的研究，为了审计和加固未来的语言模型，为社区提供了实用工具。

Abstract: Recent advances in large language models (LLMs) have intensified the need to
understand and reliably curb their harmful behaviours. We introduce a
multidimensional framework for probing and steering harmful content in model
internals. For each of 55 distinct harmfulness subconcepts (e.g., racial hate,
employment scams, weapons), we learn a linear probe, yielding 55 interpretable
directions in activation space. Collectively, these directions span a
harmfulness subspace that we show is strikingly low-rank. We then test ablation
of the entire subspace from model internals, as well as steering and ablation
in the subspace's dominant direction. We find that dominant direction steering
allows for near elimination of harmfulness with a low decrease in utility. Our
findings advance the emerging view that concept subspaces provide a scalable
lens on LLM behaviour and offer practical tools for the community to audit and
harden future generations of language models.

</details>


### [10] [Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams](https://arxiv.org/abs/2507.21158)
*Nishani Fernando,Bahareh Nakisa,Adnan Ahmad,Mohammad Naim Rastgoo*

Main category: cs.AI

TL;DR: 本文提出了自适应解释性人工智能框架（AXTF），利用生理和行为信号推断用户状态，支持解释的调整，帮助建立高压、时间敏感环境下的快速信任。该框架引入了多目标、个性化的信任估计模型，可提供动态的信任估计，指导解释特征的调制。


<details>
  <summary>Details</summary>
Motivation: 在高压、时间敏感的情境下，人类与人工智能系统之间建立迅速的信任至关重要，现有的可解释人工智能方法往往面临统一的解释和依赖于明确反馈机制的挑战。因此，需要一种自适应的、非侵入式的解释性框架来支持快速信任的建立。

Method: 本文提出了一个概念框架，通过非侵入式的方式响应用户的实时认知和情感状态，采用多目标、个性化的信任估计模型，将工作负荷、压力和情绪映射到动态的信任估计中，引导解释特征的调制。

Result: 提出的自适应解释性信任框架结合了生理和行为信号，能够为高压、时间敏感的环境提供支持，从而促进人工智能与人类的协作。

Conclusion: 本文提出了一种自适应解释性人工智能框架，旨在通过隐性反馈响应用户的实时认知与情感状态，增强高压环境下的快速信任。提出的自适应解释性信任框架利用生理和行为信号推断用户状态，并支持解释的调整。

Abstract: Effective human-AI teaming heavily depends on swift trust, particularly in
high-stakes scenarios such as emergency response, where timely and accurate
decision-making is critical. In these time-sensitive and cognitively demanding
settings, adaptive explainability is essential for fostering trust between
human operators and AI systems. However, existing explainable AI (XAI)
approaches typically offer uniform explanations and rely heavily on explicit
feedback mechanisms, which are often impractical in such high-pressure
scenarios. To address this gap, we propose a conceptual framework for adaptive
XAI that operates non-intrusively by responding to users' real-time cognitive
and emotional states through implicit feedback, thereby enhancing swift trust
in high-stakes environments. The proposed adaptive explainability trust
framework (AXTF) leverages physiological and behavioral signals, such as EEG,
ECG, and eye tracking, to infer user states and support explanation adaptation.
At its core is a multi-objective, personalized trust estimation model that maps
workload, stress, and emotion to dynamic trust estimates. These estimates guide
the modulation of explanation features enabling responsive and personalized
support that promotes swift trust in human-AI collaboration. This conceptual
framework establishes a foundation for developing adaptive, non-intrusive XAI
systems tailored to the rigorous demands of high-pressure, time-sensitive
environments.

</details>


### [11] [Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity](https://arxiv.org/abs/2507.21159)
*Zhihao Peng,Liuxin Bao,Shengyuan Liu,Yixuan Yuan*

Main category: cs.AI

TL;DR: 本文提出一种自适应集群协作方法，通过自我多样性和交叉一致性最大化机制提高LLMs在医学决策支持能力。实验证明该方法在专业医学领域取得了显著成效，如在NEJMQA数据集上，在妇产科学中准确率达到65.47%，明显优于GPT-4的56.12%。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学决策支持场景中的表现受限于现有架构对预定义LLM集群的依赖，部分LLMs在医学决策支持场景中表现不佳，削弱了LLMs的协作能力。由于现有方法缺乏明确的组件选择规则，需要人工干预或临床特定验证，因此需要一种新的方法来增强LLMs在医学决策支持方面的能力。

Method: 提出了自适应集群协作方法，包括自我多样性和交叉一致性最大化机制，用于提高LLMs在医学决策支持能力。自我多样性通过计算LLM内成对输出的模糊匹配值来衡量，优先选择具有高自我多样性值的LLMs作为集群组件。交叉一致性首先衡量具有最高自我多样性值的LLM与其他LLMs之间的交叉一致性值，然后逐渐屏蔽具有最低交叉一致性值的LLM，以消除协作传播过程中潜在的不一致输出。

Result: 通过在NEJMQA和MMLU-Pro-health两个专业医学数据集上的实验，证明了提出的自适应集群协作方法的有效性，尤其在医师为导向的专业领域取得了显著成效。

Conclusion: 提出了一种自适应集群协作方法，包括自我多样性和交叉一致性最大化机制，以增强LLMs在医学决策支持能力。在NEJMQA和MMLU-Pro-health两个专业医学数据集上的广泛实验表明了该方法在以医师为导向的专业领域的有效性。例如，在NEJMQA上，我们的方法实现了在所有学科中达到公开官方合格分数的准确率，特别是在妇产科学中，我们的方法在65.47%的准确率，而GPT-4仅在妇产科学上取得了56.12%的准确率。

Abstract: The collaborativeness of large language models (LLMs) has proven effective in
natural language processing systems, holding considerable promise for
healthcare development. However, it lacks explicit component selection rules,
necessitating human intervention or clinical-specific validation. Moreover,
existing architectures heavily rely on a predefined LLM cluster, where partial
LLMs underperform in medical decision support scenarios, invalidating the
collaborativeness of LLMs. To this end, we propose an adaptive cluster
collaborativeness methodology involving self-diversity and cross-consistency
maximization mechanisms to boost LLMs medical decision support capacity. For
the self-diversity, we calculate the fuzzy matching value of pairwise outputs
within an LLM as its self-diversity value, subsequently prioritizing LLMs with
high self-diversity values as cluster components in a training-free manner. For
the cross-consistency, we first measure cross-consistency values between the
LLM with the highest self-diversity value and others, and then gradually mask
out the LLM having the lowest cross-consistency value to eliminate the
potential inconsistent output during the collaborative propagation. Extensive
experiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health,
demonstrate the effectiveness of our method across physician-oriented
specialties. For example, on NEJMQA, our method achieves the accuracy rate up
to the publicly official passing score across all disciplines, especially
achieving ACC of 65.47\% compared to the 56.12\% achieved by GPT-4 on the
Obstetrics and Gynecology discipline.

</details>


### [12] [Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems](https://arxiv.org/abs/2507.21162)
*Xu Yang,Chenhui Lin,Yue Yang,Qi Wang,Haotian Liu,Haizhou Hua,Wenchuan Wu*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的自动化建模和优化方法，用于解决活跃配电网络（ADNs）中的ADN调度问题。该方法通过开发多LLM协调架构和定制精化技术，提高了生成内容的准确性和可靠性，同时具有用户中心界面，使ADN运营商能够通过简单的自然语言查询推断调度策略，提高效率。全面比较和案例验证表明所提出的架构和方法有效。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源不断渗透到活跃配电网络中，ADN调度变得至关重要。然而，许多新整合的ADN运营商，如配电系统聚合商、虚拟电厂管理者和最终用户生产者，通常缺乏电力系统运行、建模、优化和编程方面的专业知识，导致对人类专家的依赖成本高昂且耗时。因此，需要解决这一挑战，实现智能、灵活的ADN调度。

Method: 本文提出了一种基于大语言模型的自动化建模和优化方法。首先，将ADN调度问题分解为顺序阶段，并设计了一个多LLM协调架构，包括信息提取器、问题形式化器和代码程序员，分别负责信息检索、优化问题形式化和代码实现。随后，为每个LLM代理开发了定制的精化技术，显著提高了生成内容的准确性和可靠性。

Result: 通过开发基于大语言模型的自动化建模和优化方法，本文使ADN运营商能够通过简单的自然语言查询获得调度策略，提高效率并消除技术障碍。在各种测试案例上进行了全面比较和端到端演示，验证了所提出架构和方法的有效性。

Conclusion: 本文介绍了一种基于大语言模型（LLM）的自动建模和优化方法，用于解决在活跃配电网络（ADNs）中的ADN调度问题。通过设计一个多LLM协调架构，将ADN调度问题分解为顺序阶段，并开发针对每个LLM代理的定制精化技术，大大提高了生成内容的准确性和可靠性。提出的方法具有用户中心的界面，使ADN运营商能够通过简单的自然语言查询得出调度策略，消除技术障碍，提高效率。通过在各种测试案例上进行全面比较和端到端演示，验证了所提架构和方法的有效性。

Abstract: The increasing penetration of distributed energy resources into active
distribution networks (ADNs) has made effective ADN dispatch imperative.
However, the numerous newly-integrated ADN operators, such as distribution
system aggregators, virtual power plant managers, and end prosumers, often lack
specialized expertise in power system operation, modeling, optimization, and
programming. This knowledge gap renders reliance on human experts both costly
and time-intensive. To address this challenge and enable intelligent, flexible
ADN dispatch, this paper proposes a large language model (LLM) powered
automated modeling and optimization approach. First, the ADN dispatch problems
are decomposed into sequential stages, and a multi-LLM coordination
architecture is designed. This framework comprises an Information Extractor, a
Problem Formulator, and a Code Programmer, tasked with information retrieval,
optimization problem formulation, and code implementation, respectively.
Afterwards, tailored refinement techniques are developed for each LLM agent,
greatly improving the accuracy and reliability of generated content. The
proposed approach features a user-centric interface that enables ADN operators
to derive dispatch strategies via simple natural language queries, eliminating
technical barriers and increasing efficiency. Comprehensive comparisons and
end-to-end demonstrations on various test cases validate the effectiveness of
the proposed architecture and methods.

</details>


### [13] [An ontological analysis of risk in Basic Formal Ontology](https://arxiv.org/abs/2507.21171)
*Federico Donato,Adrien Barton*

Main category: cs.AI

TL;DR: The paper examines the ontology of risk, proposing it as a subclass of BFO:Role rather than BFO:Disposition. It uses examples to analyze and define the conditions for risk, contributing to a better understanding of the nature of risk within the ontology framework.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to explore and clarify the nature of risk within the ontology framework, addressing the classification of Risk as a subclass of BFO:Role rather than BFO:Disposition. It aims to provide a comprehensive analysis of risk and establish the necessary conditions for defining something as a risk.

Method: The paper utilizes the Basic Formal Ontology (BFO) categories to characterize the nature of risk, specifically focusing on the modeling choice of defining Risk as a subclass of BFO:Role. An example of risk involving objects, processes, and their interrelations is used to generalize and identify the conditions for being a risk.

Result: The paper successfully argues for the classification of Risk as a subclass of BFO:Role, offering insights into the ontology of risk and defining the conditions for identifying risk. Plausible necessary conditions for future research are also highlighted.

Conclusion: Risk is proposed to be a subclass of BFO:Role rather than BFO:Disposition, providing a detailed analysis of risk within the ontology framework.

Abstract: The paper explores the nature of risk, providing a characterization using the
categories of the Basic Formal Ontology (BFO). It argues that the category Risk
is a subclass of BFO:Role, contrasting it with a similar view classifying Risk
as a subclass of BFO:Disposition. This modeling choice is applied on one
example of risk, which represents objects, processes (both physical and mental)
and their interrelations, then generalizing from the instances in the example
to obtain an overall analysis of risk, making explicit what are the sufficient
conditions for being a risk. Plausible necessary conditions are also mentioned
for future work. Index Terms: ontology, risk, BFO, role, disposition

</details>


### [14] [Ontological Foundations of State Sovereignty](https://arxiv.org/abs/2507.21172)
*John Beverley,Danielle Limbaugh*

Main category: cs.AI

TL;DR: 该论文介绍了国家主权的概念，并探讨了处理模糊或矛盾数据的策略，旨在为国际事务本体论的研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于引入国家主权的概念，并强调处理不明确或矛盾数据的重要性。通过讨论国家主权问题，为进一步研究国际事务本体论提供了基础。

Method: 论文介绍了有关国家主权的基本概念和认知，探讨了处理模糊或矛盾数据的方法，并提出了解决这些问题的策略。

Result: 论文揭示了国家主权问题的复杂性，提出了处理此类问题的方法。通过介绍有关国家主权的基本信息，为深入研究国际事务本体论提供了参考和启示。

Conclusion: 该论文是关于国家主权性质和有关其重要性的介绍性文章。它旨在揭示处理模糊或矛盾数据的策略，以确定哪些国家实际上具有主权。论文的目标是为国际事务本体论的应用工作奠定基础。

Abstract: This short paper is a primer on the nature of state sovereignty and the
importance of claims about it. It also aims to reveal (merely reveal) a
strategy for working with vague or contradictory data about which states, in
fact, are sovereign. These goals together are intended to set the stage for
applied work in ontology about international affairs.

</details>


### [15] [Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs](https://arxiv.org/abs/2507.21176)
*Farzana Islam Adiba,Rahmatollah Beheshti*

Main category: cs.AI

TL;DR: 本研究提出了一种结合知识图谱和LLMs的框架，采用对抗扰动技术和多跳特征化KGs的方法，能够显著提高揭示医学LLMs中复杂偏见模式的能力和可扩展性。实验证明所提框架在多个方面优于其他基准算法。


<details>
  <summary>Details</summary>
Motivation: 在医学决策应用中，大语言模型(LLMs)存在偏见和不公平现象，因此有必要在采用这些模型之前识别这些偏见模式，以便有效减轻其影响。本研究的动机在于应对医学LLMs中出现的复杂偏见模式，提出一种新的框架进行系统性评估。

Method: 本研究采用了知识图谱和辅助LLMs相结合的框架，引入对抗扰动技术和多跳特征化KGs的方法，通过一系列全面的实验展示了该框架在揭示医学LLMs中的偏见模式方面的优越能力和可扩展性。

Result: 通过对三个数据集、六个LLMs和五种偏见类型进行一系列全面实验，展示了所提框架相较于其他基准具有更强的揭示LLMs偏见模式的能力和可扩展性。

Conclusion: 本研究提出了一种新颖的框架，结合知识图谱（KGs）和辅助LLMs，系统地揭示医学LLMs中复杂的偏见模式。通过采用对抗扰动技术识别微妙的偏见模式，以及采用定制的多跳特征化KGs来增强对任意LLMs的系统评估，研究表明，与其他基准相比，我们提出的框架在揭示LLMs的复杂偏见模式方面具有显着更高的能力和可扩展性。

Abstract: Large language models (LLMs) that are used in medical applications are known
to show biased and unfair patterns. Prior to adopting these in clinical
decision-making applications, it is crucial to identify these bias patterns to
enable effective mitigation of their impact. In this study, we present a novel
framework combining knowledge graphs (KGs) with auxiliary LLMs to
systematically reveal complex bias patterns in medical LLMs. Specifically, the
proposed approach integrates adversarial perturbation techniques to identify
subtle bias patterns. The approach adopts a customized multi-hop
characterization of KGs to enhance the systematic evaluation of arbitrary LLMs.
Through a series of comprehensive experiments (on three datasets, six LLMs, and
five bias types), we show that our proposed framework has noticeably greater
ability and scalability to reveal complex biased patterns of LLMs compared to
other baselines.

</details>


### [16] [Agentic Web: Weaving the Next Web with AI Agents](https://arxiv.org/abs/2507.21206)
*Yingxuan Yang,Mulei Ma,Yuxuan Huang,Huacan Chai,Chenyu Gong,Haoran Geng,Yuanjian Zhou,Ying Wen,Meng Fang,Muhao Chen,Shangding Gu,Ming Jin,Costas Spanos,Yang Yang,Pieter Abbeel,Dawn Song,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 该论文讨论了人工智能代理在大型语言模型推动下的出现和Agentic Web的重要性，介绍了Agentic Web的结构化框架、核心技术基础以及AI代理的能力。同时分析了创建可扩展自主系统的挑战，讨论了潜在的应用、风险和治理问题，并提出了智能生态系统发展的研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于介绍人工智能代理在大型语言模型推动下的兴起，探讨Agentic Web的重要性和影响。通过分析其结构、演变和支撑技术，揭示人机互动方式的转变和AI代理的关键维度。同时关注人工智能系统的应用潜力、社会风险以及治理问题，为未来智能生态系统的发展提出研究方向。

Method: 文章提出了一个结构化框架用于理解和构建Agentic Web，追溯了其演变过程并识别支持这一转变的核心技术基础。论文分析了创建可扩展的自主系统所涉及的体系结构和基础设施挑战，包括通信协议、编排策略和新兴范式，如Agent Attention Economy。

Result: 论文提出了Agentic Web的结构化框架和核心技术基础，介绍了AI代理的关键维度和功能，分析了创建可扩展自主系统的挑战，并讨论了潜在的应用、风险和治理问题。最后，提出了研究未来智能生态系统发展方向。

Conclusion: 该论文介绍了人工智能代理在大型语言模型的推动下的出现，标志着互联网向自主、目标驱动的Agentic Web转变。它分析了Agentic Web的结构化框架，从PC和移动Web时代的演变到支持这种转变的核心技术基础。论文着重介绍了智能、交互和经济这三个关键维度的概念模型，以及AI代理的能力，如检索、推荐、规划和协作。最后讨论了人工智能系统可能的应用、社会风险和治理问题，提出了开发由人类意图和自主代理行为塑造的开放、安全和智能生态系统的研究方向。

Abstract: The emergence of AI agents powered by large language models (LLMs) marks a
pivotal shift toward the Agentic Web, a new phase of the internet defined by
autonomous, goal-driven interactions. In this paradigm, agents interact
directly with one another to plan, coordinate, and execute complex tasks on
behalf of users. This transition from human-driven to machine-to-machine
interaction allows intent to be delegated, relieving users from routine digital
operations and enabling a more interactive, automated web experience. In this
paper, we present a structured framework for understanding and building the
Agentic Web. We trace its evolution from the PC and Mobile Web eras and
identify the core technological foundations that support this shift. Central to
our framework is a conceptual model consisting of three key dimensions:
intelligence, interaction, and economics. These dimensions collectively enable
the capabilities of AI agents, such as retrieval, recommendation, planning, and
collaboration. We analyze the architectural and infrastructural challenges
involved in creating scalable agentic systems, including communication
protocols, orchestration strategies, and emerging paradigms such as the Agent
Attention Economy. We conclude by discussing the potential applications,
societal risks, and governance issues posed by agentic systems, and outline
research directions for developing open, secure, and intelligent ecosystems
shaped by both human intent and autonomous agent behavior. A continuously
updated collection of relevant studies for agentic web is available at:
https://github.com/SafeRL-Lab/agentic-web.

</details>


### [17] [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
*David Maria Schmidt,Raoul Schubert,Philipp Cimiano*

Main category: cs.AI

TL;DR: LLMs struggle to interpret questions systematically and compositionally, as shown by the benchmark proposed in the paper. The performance degraded with increasing deviation from optimized samples, and even with all necessary information, the F1 scores remained low.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess the systematic nature of the interpretation process of LLMs in mapping questions to SPARQL queries. The goal is to understand to what extent LLMs can interpret complex questions by examining their performance on controlled datasets.

Method: The paper proposes a benchmark to investigate the compositional abilities of LLMs in interpreting questions. Three datasets of varying difficulty based on graph patterns in DBpedia are generated to test LLMs' ability to interpret structurally complex questions. Experiments with models of different sizes, various prompt and few-shot optimization techniques, as well as fine-tuning are conducted.

Result: The performance of LLMs in interpreting questions degraded as the deviation from the optimized samples increased. Even when provided with all necessary information, the F1 scores did not exceed 0.57 for the dataset of lowest complexity.

Conclusion: LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.

Abstract: Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.

</details>


### [18] [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
*Yufei Li,Zexin Li,Yinglun Zhu,Cong Liu*

Main category: cs.AI

TL;DR: LeMix提出了一种系统，用于同时管理LLM服务和训练工作负载，通过动态适应资源分配提高利用率和服务质量。实验结果表明，LeMix在吞吐量、推理损失和响应时间SLO达标率方面都取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLMs）的部署需要同时进行推理服务和持续重训练，以保持与不断变化的数据和用户反馈同步。目前的常见做法是将这些工作负载分开放在不同的服务器上，在隔离的阶段进行，造成了很大的低效率（例如，GPU空闲）和在分布式环境中延迟了对新数据的适应。

Method: LeMix利用离线分析、执行预测机制和运行时调度，根据工作负载特征和系统条件动态调整资源分配，以解决动态请求到达和管道并行训练中的工作负载异构性等挑战。通过了解任务特定行为和在共享节点上的共同执行干扰，LeMix提高了资源利用率和服务质量。

Result: LeMix提高了吞吐量、降低了推理损失，并提高了响应时间SLO达标率，相比传统的分离设置。

Conclusion: LeMix提出了一种系统，用于联合管理同时进行的大型语言模型(LLM)服务和训练工作负载，通过动态适应资源分配以提高利用率和服务质量，同时不影响服务响应速度。实验证明，LeMix将吞吐量提高了最多3.53倍，并将推理损失降低了最多0.61倍，比传统的分离设置实现了高达2.12倍的响应时间SLO达标率。这是首个揭示和利用联合LLM推理和训练机会的工作，为生产环境中更节约资源的LLM部署铺平了道路。

Abstract: Modern deployment of large language models (LLMs) frequently involves both
inference serving and continuous retraining to stay aligned with evolving data
and user feedback. Common practices separate these workloads onto distinct
servers in isolated phases, causing substantial inefficiencies (e.g., GPU
idleness) and delayed adaptation to new data in distributed settings. Our
empirical analysis reveals that these inefficiencies stem from dynamic request
arrivals during serving and workload heterogeneity in pipeline-parallel
training. To address these challenges, we propose LeMix, a system for
co-locating and managing concurrent LLM serving and training workloads. LeMix
integrates offline profiling, execution prediction mechanisms, and runtime
scheduling to dynamically adapt resource allocation based on workload
characteristics and system conditions. By understanding task-specific behaviors
and co-execution interference across shared nodes, LeMix improves utilization
and serving quality without compromising serving responsiveness. Our evaluation
shows that LeMix improves throughput by up to 3.53x, reduces inference loss by
up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over
traditional separate setups. To our knowledge, this is the first work to
uncover and exploit the opportunities of joint LLM inference and training,
paving the way for more resource-efficient deployment of LLMs in production
environments.

</details>


### [19] [Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions](https://arxiv.org/abs/2507.21285)
*Harsh Darji,Thibaud Lutellier*

Main category: cs.AI

TL;DR: 本研究旨在构建LLM-based编程助手，通过询问澄清问题模拟人工代码审查过程。实验结果显示经过微调的LLM在生成澄清问题方面优于标准零样本提示，用户研究表明用户认为我们模型生成的澄清问题优于基准。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在处理开发者请求时存在歧义，因此本研究旨在解决这一问题，构建一个模仿人工代码审查过程的LLM-based编程助手。

Method: 建立了一个端到端系统，包括：1.训练用于检测不清晰编程相关查询的查询分类器；2.一个经过微调的LLM来生成澄清问题。

Result: 经过微调的LLM在生成澄清问题方面表现优越，用户认为生成的澄清问题比基准更好。

Conclusion: 本研究旨在构建基于LLM的编程助手，通过询问澄清问题来模拟人工代码审查过程。研究结果表明，经过微调的LLM在生成有用的澄清问题方面优于标准零样本提示。用户研究显示，用户认为我们模型生成的澄清问题优于基准，证明我们的编程助手与基准编程助手相比产生更准确和有帮助的代码响应。

Abstract: Large Language Models (LLMs) are increasingly used as coding assistants.
However, the ambiguity of the developer's prompt often leads to incorrect code
generation, as current models struggle to infer user intent without extensive
prompt engineering or external context. This work aims to build an LLM-based
coding assistant that mimics the human code review process by asking
clarification questions when faced with ambiguous or under-specified queries.
  Our end-to-end system includes (1) a query classifier trained to detect
unclear programming-related queries and (2) a fine-tuned LLM that generates
clarification questions. Our evaluation shows that the fine-tuned LLM
outperforms standard zero-shot prompting in generating useful clarification
questions. Furthermore, our user study indicates that users find the
clarification questions generated by our model to outperform the baseline,
demonstrating that our coding assistant produces more accurate and helpful code
responses compared to baseline coding assistants.

</details>


### [20] [Structured Relevance Assessment for Robust Retrieval-Augmented Language Models](https://arxiv.org/abs/2507.21287)
*Aryan Raj,Astitva Veer Garg,Anitha D*

Main category: cs.AI

TL;DR: 该论文提出了一种框架来增强检索增强语言模型的稳健性，通过改善文档评估、知识整合和处理无法回答的查询，减少了事实错误和幻觉率，提高了推理过程的透明性。该方法引入了结构化相关性评估框架和多维评分系统，显示出显著改进，并推动了更可靠的问答系统的发展。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于解决RALMs在减少事实错误、文档相关性评估和知识整合方面面临的挑战，以及处理无法回答的查询。

Method: 该论文引入了一个结构化相关性评估框架，通过多维评分系统来考虑语义匹配和来源可靠性，利用基于嵌入的相关性评分和合成训练数据。在特定主题上进行了专门的基准测试，提出了知识整合机制以及对缺乏知识覆盖的查询的“未知”响应协议。

Result: 通过引入结构化相关性评估框架和多维评分系统，该方法显著减少了幻觉率，并改善了推理过程的透明性。初步评估表明了这一方法的有效性。

Conclusion: 该论文提出了一种增强检索增强语言模型（RALMs）稳健性的框架，通过改善文档评估、知识整合和处理无法回答的查询等方面，显著减少了事实错误和幻觉率，提高了推理过程的透明性。研究的初步评估显示了显著的改进。尽管在准确区分可信信息、平衡系统延迟和彻底性等方面仍存在挑战，但本研究代表了提升RALM可靠性的一次有意义的尝试。

Abstract: Retrieval-Augmented Language Models (RALMs) face significant challenges in
reducing factual errors, particularly in document relevance evaluation and
knowledge integration. We introduce a framework for structured relevance
assessment that enhances RALM robustness through improved document evaluation,
balanced intrinsic and external knowledge integration, and effective handling
of unanswerable queries. Our approach employs a multi-dimensional scoring
system that considers both semantic matching and source reliability, utilizing
embedding-based relevance scoring and synthetic training data with
mixed-quality documents. We implement specialized benchmarking on niche topics,
a knowledge integration mechanism, and an "unknown" response protocol for
queries with insufficient knowledge coverage. Preliminary evaluations
demonstrate significant reductions in hallucination rates and improved
transparency in reasoning processes. Our framework advances the development of
more reliable question-answering systems capable of operating effectively in
dynamic environments with variable data quality. While challenges persist in
accurately distinguishing credible information and balancing system latency
with thoroughness, this work represents a meaningful step toward enhancing RALM
reliability.

</details>


### [21] [Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2507.21354)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 该论文介绍了Trans-ACT方法，将TA原则嵌入到MAS中，生成具有现实心理动态的智能体。实验表明，基于认知和TA原则的智能体在交互方面表现更深入和具有上下文意识，为冲突解决、教育支持和社会心理学研究等领域提供新的应用途径。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统（MAS）框架缺乏对人类行为认知复杂性的考虑。本研究旨在引入Transaction Analysis（TA）原则，使智能体能够更真实地模拟心理动态，从而提高交互的深度和上下文意识。

Method: 将Transaction Analysis（TA）原则嵌入到多智能体系统（MAS）中，使用Trans-ACT（Transactional Analysis Cognitive Toolkit）来生成具有现实心理动态的智能体，并整合父母-成年人-孩子自我状态到智能体的认知架构中。通过复现Stupid game场景的实验模拟来验证该方法的有效性。

Result: 实验模拟显示基于认知和TA原则的智能体在Stupid game场景下产生了更深入和具有上下文意识的交互。

Conclusion: 该论文介绍了Trans-ACT（Transactional Analysis Cognitive Toolkit）的方法，将Transaction Analysis（TA）原则嵌入到多智能体系统（MAS）中，以生成具有现实心理动态的智能体。通过将父母-成年人-孩子自我状态整合到智能体的认知架构中，从而实现对MAS的认知复杂性的模拟。实验模拟显示，基于认知和TA原则的智能体产生了更深入和具有上下文意识的交互。该研究开辟了一种新的途径，可应用于冲突解决、教育支持和高级社会心理学研究等领域。

Abstract: Multi-Agent Systems (MAS) are increasingly used to simulate social
interactions, but most of the frameworks miss the underlying cognitive
complexity of human behavior. In this paper, we introduce Trans-ACT
(Transactional Analysis Cognitive Toolkit), an approach embedding Transactional
Analysis (TA) principles into MAS to generate agents with realistic
psychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego
states into an agent's cognitive architecture. Each ego state retrieves
context-specific memories and uses them to shape response to new situations.
The final answer is chosen according to the underlying life script of the
agent. Our experimental simulation, which reproduces the Stupid game scenario,
demonstrates that agents grounded in cognitive and TA principles produce deeper
and context-aware interactions. Looking ahead, our research opens a new way for
a variety of applications, including conflict resolution, educational support,
and advanced social psychology studies.

</details>


### [22] [Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures](https://arxiv.org/abs/2507.21360)
*Nicholas Botti,Flora Haberkorn,Charlotte Hoopes,Shaun Khan*

Main category: cs.AI

TL;DR: 研究表明，AI检索增强生成（RAG）工具可以显著加快信息提取和数据标注任务的执行速度，提高准确性。使用AI工具的标注者技能影响任务性能。与“幼稚”的AI使用条件相比，交互式使用AI工具可提高准确性。方法采用随机任务分配的被试设计，复制了复杂的真实标注任务。测试了两种处理条件：‘幼稚’AI使用和‘交互式’AI处理。与仅人工参与的基准进行了对比。


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of AI retrieval augmented generation (RAG) tool in assisting analysts with information extraction and data annotation tasks. To explore the impact of annotator skill and interaction with AI tools on task performance. To assess the potential time-saving benefits of using AI tools in annotation tasks.

Method: Within-subjects design with randomized task assignments. Replication of a complex real-world annotation task on public disclosure documents from GSIBs. Testing of two treatment conditions: 'naive' AI use and 'interactive' AI treatment. Comparison with human-only baseline.

Result: AI tool accelerated task execution by up to a factor of 10 and improved task accuracy, especially in the interactive condition. The methods could save up to 268 hours compared to the human-only approach for the full task. Annotator skill with AI tools is crucial for task accuracy and speed.

Conclusion: AI retrieval augmented generation (RAG) tool can significantly accelerate task execution and enhance accuracy in information extraction and data annotation tasks. Annotator skill with AI tools influences task performance. Interactive use of AI tool improves accuracy compared to a 'naive' AI use condition.

Abstract: We utilize a within-subjects design with randomized task assignments to
understand the effectiveness of using an AI retrieval augmented generation
(RAG) tool to assist analysts with an information extraction and data
annotation task. We replicate an existing, challenging real-world annotation
task with complex multi-part criteria on a set of thousands of pages of public
disclosure documents from global systemically important banks (GSIBs) with
heterogeneous and incomplete information content. We test two treatment
conditions. First, a "naive" AI use condition in which annotators use only the
tool and must accept the first answer they are given. And second, an
"interactive" AI treatment condition where annotators use the tool
interactively, and use their judgement to follow-up with additional information
if necessary. Compared to the human-only baseline, the use of the AI tool
accelerated task execution by up to a factor of 10 and enhanced task accuracy,
particularly in the interactive condition. We find that when extrapolated to
the full task, these methods could save up to 268 hours compared to the
human-only approach. Additionally, our findings suggest that annotator skill,
not just with the subject matter domain, but also with AI tools, is a factor in
both the accuracy and speed of task performance.

</details>


### [23] [Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect](https://arxiv.org/abs/2507.21383)
*Chunan Tong*

Main category: cs.AI

TL;DR: 这项研究提出了一种混合 Liquid Neural Networks（LNN）和 XGBoost 模型，用于优化多层次供应链中的订购策略，旨在减轻牛鞭效应并增强累积盈利能力。该方法通过利用 LNN 的动态特征提取和 XGBoost 的全局优化能力，填补了现有方法的关键空白，为供应链管理提供了创新解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的方法难以应对动态市场条件，新兴的机器学习技术具有潜力解决这些挑战，但受限于计算复杂性、训练效率或时间序列建模的限制。Liquid Neural Networks 受动态生物系统启发，具有适应性强、计算成本低、对噪声稳健的特点，适用于实时决策和边缘计算。然而，它们在供应链优化方面的潜力仍未被充分挖掘。

Method: 研究通过引入混合 Liquid Neural Networks（LNN）和 XGBoost 模型，利用 LNN 的动态特征提取和 XGBoost 的全局优化能力，优化多层次供应链中的订购策略。

Result: 研究引入混合 LNN 和 XGBoost 模型，填补了现有方法中的关键空白，为动态高效的供应链管理提供了创新解决方案。

Conclusion: 研究引入了混合 Liquid Neural Networks（LNN）和 XGBoost 模型以优化多层次供应链中的订购策略，旨在减轻牛鞭效应并增强累积盈利能力。研究调查了混合框架中的局部和全局协同如何解决供应链管理（SCM）中适应性和效率的双重需求。所提出的方法填补了现有方法中的关键空白，为动态高效的供应链管理提供了创新解决方案。

Abstract: Supply chain management faces significant challenges, including demand
fluctuations, inventory imbalances, and amplified upstream order variability
due to the bullwhip effect. Traditional methods, such as simple moving
averages, struggle to address dynamic market conditions. Emerging machine
learning techniques, including LSTM, reinforcement learning, and XGBoost, offer
potential solutions but are limited by computational complexity, training
inefficiencies, or constraints in time-series modeling. Liquid Neural Networks,
inspired by dynamic biological systems, present a promising alternative due to
their adaptability, low computational cost, and robustness to noise, making
them suitable for real-time decision-making and edge computing. Despite their
success in applications like autonomous vehicles and medical monitoring, their
potential in supply chain optimization remains underexplored. This study
introduces a hybrid LNN and XGBoost model to optimize ordering strategies in
multi-tier supply chains. By leveraging LNN's dynamic feature extraction and
XGBoost's global optimization capabilities, the model aims to mitigate the
bullwhip effect and enhance cumulative profitability. The research investigates
how local and global synergies within the hybrid framework address the dual
demands of adaptability and efficiency in SCM. The proposed approach fills a
critical gap in existing methodologies, offering an innovative solution for
dynamic and efficient supply chain management.

</details>


### [24] [Teaching Language Models To Gather Information Proactively](https://arxiv.org/abs/2507.21389)
*Tenghao Huang,Sihao Chen,Muhao Chen,Jonathan May,Longqi Yang,Mengting Wan,Pei Zhou*

Main category: cs.AI

TL;DR: 本研究介绍了一种新的任务范式：主动信息收集，设计了一个可扩展的框架来生成部分指定的真实世界任务，核心创新是一种强化微调策略，经过训练的Qwen-2.5-7B模型在实验中表现优异，展示了主动澄清对提升LLMs的价值。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs在现实世界的环境中往往表现不佳，面对不完整或未明确的提示时，往往默认为被动回应或狭窄澄清，无法主动获取对高质量解决方案至关重要的缺失信息。因此，需要引入一种新的任务范式来训练LLMs主动获取信息的能力。

Method: 设计了一个可扩展的框架来生成部分指定的真实世界任务，遮蔽关键信息并模拟真实的歧义。核心创新是一种强化微调策略，奖励引发真正新颖的隐性用户信息的问题。

Result: 通过实验表明，经过训练的Qwen-2.5-7B模型在自动评估指标上明显优于o3-mini，人类评估显示模型生成的澄清问题和最终大纲受到人类标注者的青睐。

Conclusion: 本研究介绍了一种新的任务范式：主动信息收集，通过针对性问题识别上下文中的空白，并策略性地引导隐性用户知识。实验表明，经过训练的Qwen-2.5-7B模型在自动评估指标上比o3-mini表现提高了18%。人类评估显示，模型生成的澄清问题和最终大纲分别获得了42%和28%的人类标注者青睐。这些结果突显了主动澄清在将LLMs从被动文本生成器提升为真正协作思维伙伴方面的价值。

Abstract: Large language models (LLMs) are increasingly expected to function as
collaborative partners, engaging in back-and-forth dialogue to solve complex,
ambiguous problems. However, current LLMs often falter in real-world settings,
defaulting to passive responses or narrow clarifications when faced with
incomplete or under-specified prompts, falling short of proactively gathering
the missing information that is crucial for high-quality solutions. In this
work, we introduce a new task paradigm: proactive information gathering, where
LLMs must identify gaps in the provided context and strategically elicit
implicit user knowledge through targeted questions. To systematically study and
train this capability, we design a scalable framework that generates partially
specified, real-world tasks, masking key information and simulating authentic
ambiguity. Within this setup, our core innovation is a reinforcement finetuning
strategy that rewards questions that elicit genuinely new, implicit user
information -- such as hidden domain expertise or fine-grained requirements --
that would otherwise remain unspoken. Experiments demonstrate that our trained
Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic
evaluation metrics. More importantly, human evaluation reveals that
clarification questions and final outlines generated by our model are favored
by human annotators by 42% and 28% respectively. Together, these results
highlight the value of proactive clarification in elevating LLMs from passive
text generators to genuinely collaborative thought partners.

</details>


### [25] [Shapley Uncertainty in Natural Language Generation](https://arxiv.org/abs/2507.21406)
*Meilin Zhu,Gaojie Jin,Xiaowei Huang,Lijun Zhang*

Main category: cs.AI

TL;DR: 本研究提出了一种基于Shapley值的不确定性度量方法，可以更准确地预测大型语言模型在问答和其他数据集中的性能。


<details>
  <summary>Details</summary>
Motivation: 在问答任务中，确定何时信任输出对于大型语言模型的对齐至关重要。先前的研究基于语义熵来衡量不确定性，本研究旨在超越阈值设定，提出更精细的框架。

Method: 扩展了语义熵的框架，通过开发基于Shapley值的不确定性度量方法来捕获语义关系的连续性。

Result: 通过实验证明，基于Shapley的不确定性指标相比类似基准措施，更准确地预测了大型语言模型在问答和其他数据集中的表现。

Conclusion: 提出了基于Shapley的不确定性度量方法，能够更准确地预测大型语言模型在问答和其他数据集中的表现。

Abstract: In question-answering tasks, determining when to trust the outputs is crucial
to the alignment of large language models (LLMs). Kuhn et al. (2023) introduces
semantic entropy as a measure of uncertainty, by incorporating linguistic
invariances from the same meaning. It primarily relies on setting threshold to
measure the level of semantic equivalence relation. We propose a more nuanced
framework that extends beyond such thresholding by developing a Shapley-based
uncertainty metric that captures the continuous nature of semantic
relationships. We establish three fundamental properties that characterize
valid uncertainty metrics and prove that our Shapley uncertainty satisfies
these criteria. Through extensive experiments, we demonstrate that our Shapley
uncertainty more accurately predicts LLM performance in question-answering and
other datasets, compared to similar baseline measures.

</details>


### [26] [Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects](https://arxiv.org/abs/2507.21407)
*Yixin Liu,Guibin Zhang,Kun Wang,Shiyuan Li,Shirui Pan*

Main category: cs.AI

TL;DR: 本文回顾了图增强大型语言模型代理(GLA)的发展，并探讨了图和图学习算法如何增强LLM代理系统的功能。对多代理系统，讨论了GLA解决方案的作用，并强调了未来GLA系统发展的关键方向。


<details>
  <summary>Details</summary>
Motivation: 鉴于图增强的大型语言模型代理在多个领域展现出的优秀能力，本文旨在全面梳理该领域的最新进展，为未来研究提供关键方向。

Method: 本文通过分类现有GLA方法，分析了图和图学习算法如何增强LLM代理系统的功能。此外，对于多代理系统，讨论了GLA解决方案的作用。最后，强调了未来GLA系统发展的关键方向。

Result: 通过对GLA方法的分类和分析，揭示了图和图学习算法对LLM代理系统的增强作用，以及GLA解决方案在多代理系统中的作用。同时，强调了未来GLA系统发展的关键方向。

Conclusion: 本文旨在全面回顾最近关于图增强的大型语言模型代理(GLA)的发展，并指出未来研究的关键方向。通过将现有GLA方法按其在LLM代理系统中的主要功能进行分类，包括规划、记忆和工具使用，然后分析图和图学习算法如何为每种方法做出贡献。对于多代理系统，我们进一步讨论GLA解决方案如何促进MAS的编排、效率优化和可信度。最后，我们强调了推动该领域发展的关键未来方向，从改进结构适应性到实现统一、可扩展和多模态GLA系统。希望本文可以成为未来GLA研究的路线图，并促进对图在LLM代理系统中角色的深入理解。

Abstract: Autonomous agents based on large language models (LLMs) have demonstrated
impressive capabilities in a wide range of applications, including web
navigation, software development, and embodied control. While most LLMs are
limited in several key agentic procedures, such as reliable planning, long-term
memory, tool management, and multi-agent coordination, graphs can serve as a
powerful auxiliary structure to enhance structure, continuity, and coordination
in complex agent workflows. Given the rapid growth and fragmentation of
research on Graph-augmented LLM Agents (GLA), this paper offers a timely and
comprehensive overview of recent advances and also highlights key directions
for future work. Specifically, we categorize existing GLA methods by their
primary functions in LLM agent systems, including planning, memory, and tool
usage, and then analyze how graphs and graph learning algorithms contribute to
each. For multi-agent systems, we further discuss how GLA solutions facilitate
the orchestration, efficiency optimization, and trustworthiness of MAS.
Finally, we highlight key future directions to advance this field, from
improving structural adaptability to enabling unified, scalable, and multimodal
GLA systems. We hope this paper can serve as a roadmap for future research on
GLA and foster a deeper understanding of the role of graphs in LLM agent
systems.

</details>


### [27] [GovRelBench:A Benchmark for Government Domain Relevance](https://arxiv.org/abs/2507.21419)
*Haiquan Wang,Yi Chen,Shang Zeng,Yun Bian,Zhe Cui*

Main category: cs.AI

TL;DR: 本文提出了GovRelBench基准测试，用于评估政府领域LLMs核心能力。通过引入SoftGovScore方法，训练基于ModernBERT架构的GovRelBERT模型，以准确计算政府领域文本的相关性得分。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs在政府领域的评估主要集中在特定场景下的安全考虑，而对模型自身核心能力，特别是领域相关性的评估仍然不足。因此，为了填补这一空白，提出了GovRelBench，旨在评估LLMs在政府领域的核心能力。

Method: 引入SoftGovScore方法，训练基于ModernBERT架构的GovRelBERT模型，以准确计算政府领域文本的相关性得分。

Result: 提出了GovRelBench基准测试，设计了用于评估政府领域LLMs核心能力的专用评估工具GovRelBERT。

Conclusion: 本文提出了GovRelBench，一个专门用于评估政府领域LLMs核心能力的基准测试。通过引入SoftGovScore方法，在GovRelBERT的训练过程中，将基于ModernBERT架构的模型进行训练，从而能够准确计算文本在政府领域的相关性得分。该工作旨在增强政府领域大型模型的能力评估框架，为相关研究和实践提供有效工具。

Abstract: Current evaluations of LLMs in the government domain primarily focus on
safety considerations in specific scenarios, while the assessment of the
models' own core capabilities, particularly domain relevance, remains
insufficient. To address this gap, we propose GovRelBench, a benchmark
specifically designed for evaluating the core capabilities of LLMs in the
government domain. GovRelBench consists of government domain prompts and a
dedicated evaluation tool, GovRelBERT. During the training process of
GovRelBERT, we introduce the SoftGovScore method: this method trains a model
based on the ModernBERT architecture by converting hard labels to soft scores,
enabling it to accurately compute the text's government domain relevance score.
This work aims to enhance the capability evaluation framework for large models
in the government domain, providing an effective tool for relevant research and
practice. Our code and dataset are available at
https://github.com/pan-xi/GovRelBench.

</details>


### [28] [Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models](https://arxiv.org/abs/2507.21438)
*Vishal Raman,Vijai Aravindh R*

Main category: cs.AI

TL;DR: Evo-DKD is a novel dual-decoder framework for autonomous ontology evolution that combines structured ontology traversal with unstructured text reasoning. It outperforms baselines in precision of ontology updates and downstream task performance. The system utilizes two parallel decoding streams within an LLM, coordinated by a dynamic attention-based gating mechanism. Evo-DKD offers a new paradigm for LLM-driven knowledge base maintenance, blending symbolic and neural reasoning for sustainable ontology evolution.


<details>
  <summary>Details</summary>
Motivation: Ontologies and knowledge graphs need continuous evolution, but manual curation is labor-intensive. Large Language Models have vast unstructured knowledge but struggle with structured consistency. The motivation for Evo-DKD is to address this challenge by integrating structured and unstructured knowledge for autonomous ontology evolution.

Method: The paper introduces Evo-DKD, a dual-decoder framework that utilizes two parallel decoding streams within an LLM. One stream generates candidate ontology edits while the other produces natural-language justifications. A dynamic attention-based gating mechanism coordinates these streams. Due to GPU constraints, a prompt-based mode control is used for single-stream mode simulation.

Result: Experiments demonstrate Evo-DKD's effectiveness in healthcare ontology refinement, semantic search improvement, and cultural heritage timeline modeling. It outperforms structured-only or unstructured-only decoding in precision of ontology updates and downstream task performance. The paper presents quantitative metrics and qualitative examples to confirm the benefits of the dual-decoder design and gating router.

Conclusion: Evo-DKD is a dual-decoder framework that combines structured ontology traversal with unstructured text reasoning for autonomous ontology evolution. It outperforms baselines in both precision of ontology updates and downstream task performance, offering a new paradigm for LLM-driven knowledge base maintenance.

Abstract: Ontologies and knowledge graphs require continuous evolution to remain
comprehensive and accurate, but manual curation is labor intensive. Large
Language Models (LLMs) possess vast unstructured knowledge but struggle with
maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder
framework for autonomous ontology evolution that combines structured ontology
traversal with unstructured text reasoning. Evo-DKD introduces two parallel
decoding streams within an LLM: one decoder generates candidate ontology edits
(e.g., new concepts or relations) while the other produces natural-language
justifications. A dynamic attention-based gating mechanism coordinates the two
streams, deciding at each step how to blend structured and unstructured
knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using
prompt-based mode control to approximate coordinated decoding in a
single-stream mode. The system operates in a closed reasoning loop: proposed
ontology edits are validated (via consistency checks and cross-verification
with the text explanations) and then injected into the knowledge base, which in
turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on
use cases including healthcare ontology refinement, semantic search
improvement, and cultural heritage timeline modeling. Experiments show that
Evo-DKD outperforms baselines using structured-only or unstructured-only
decoding in both precision of ontology updates and downstream task performance.
We present quantitative metrics and qualitative examples, confirming the
contributions of the dual-decoder design and gating router. Evo-DKD offers a
new paradigm for LLM-driven knowledge base maintenance, combining the strengths
of symbolic and neural reasoning for sustainable ontology evolution.

</details>


### [29] [Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2507.21453)
*Ashley Rector,Keaton Minor,Kamden Minor,Jeff McCormack,Beth Breeden,Ryan Nowers,Jay Dorris*

Main category: cs.AI

TL;DR: 本研究评估了Sherpa Rx在药物基因组学中的性能，通过整合CPIC指南和PharmGKB数据，结合大型语言模型和检索增强生成技术。结果显示，Sherpa Rx在准确性、相关性、清晰度、完整性和召回率方面表现优异，特别在准确性和完整性上有所提升。对20个问题的测验中，Sherpa Rx取得了90%的准确率，胜过其他模型。将额外资源集成到RAG技术中，有助于提高人工智能的性能。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于评估人工智能工具Sherpa Rx在药物基因组学中的应用效果。通过整合CPIC指南和PharmGKB数据，研究旨在验证Sherpa Rx在生成上下文相关响应方面的准确性和性能。通过对比不同阶段和其他模型的表现，研究旨在突显Sherpa Rx在提高个性化决策制定方面的潜力。

Method: 本研究通过嵌入临床药物基因组学实施协会（CPIC）指南和药物基因组知识库（PharmGKB）数据，利用大型语言模型和检索增强生成（RAG）技术，评估了Sherpa Rx在药物基因组学中的性能表现。使用包含260个查询的数据集，涵盖了26个CPIC指南，评估了药物基因相互作用、剂量推荐和治疗影响。研究分为第1阶段和第2阶段，其中第1阶段仅嵌入CPIC数据，第2阶段额外增加了PharmGKB内容。对响应进行了准确性、相关性、清晰度、完整性和召回率方面的评分，并通过威尔科克森符号秩检验比较了不同阶段之间的性能。此外，通过对20个问题的测验评估了Sherpa Rx在实际应用中的性能。

Result: 研究发现，Sherpa Rx在准确性、相关性、清晰度、完整性和召回率方面表现出色，尤其在准确性和完整性上有所改善。Sherpa Rx在20个问题的测验中取得90%的准确率，优于其他模型。同时，将额外资源如CPIC和PharmGKB与RAG技术结合，有助于提高人工智能的准确性和性能。

Conclusion: 本研究评估了Sherpa Rx，这是一种利用大型语言模型和检索增强生成（RAG）进行药物基因组学的人工智能工具，以验证其在关键响应指标上的性能。Sherpa Rx将临床药物基因组学实施协会（CPIC）指南与药物基因组知识库（PharmGKB）数据相结合，生成相关的上下文响应。研究使用了一个数据集（N=260个查询），涵盖了26个CPIC指南，用于评估药物基因相互作用、剂量推荐和治疗影响。在第1阶段，仅嵌入了CPIC数据。第2阶段另外还增加了PharmGKB内容。响应在准确性、相关性、清晰度、完整性（5点力克特量表）和召回率上进行评分。威尔科克森符号秩检验比较了第1阶段和第2阶段之间的准确性，以及第2阶段和ChatGPT-4omini之间的准确性。一份包含20个问题的测验评估了该工具针对其他模型的实际适用性。在第1阶段（N=260），Sherpa Rx展示了较高的准确性4.9、相关性5.0、清晰度5.0、完整性4.8和召回率0.99。子集分析（N=20）显示准确性（4.6 vs. 4.4，第2阶段vs.第1阶段子集）和完整性（5.0 vs. 4.8）有所改善。ChatGPT-4omini在相关性（5.0）和清晰度（4.9）方面表现相当，但在准确性（3.9）和完整性（4.2）方面落后。第1阶段和第2阶段之间的准确性差异在统计上不显著。然而，第2阶段在准确性上显著优于ChatGPT-4omini。在20个问题的测验中，Sherpa Rx实现了90%的准确率，胜过其他模型。将额外资源如CPIC和PharmGKB与RAG集成，提高了人工智能的准确性和性能。本研究突显了像Sherpa Rx这样的生成式人工智能在药物基因组学中的转变潜力，通过准确、个性化的响应改善决策制定。

Abstract: This study evaluated Sherpa Rx, an artificial intelligence tool leveraging
large language models and retrieval-augmented generation (RAG) for
pharmacogenomics, to validate its performance on key response metrics. Sherpa
Rx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC)
guidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate
contextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC
guidelines was used to evaluate drug-gene interactions, dosing recommendations,
and therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2
additionally incorporated PharmGKB content. Responses were scored on accuracy,
relevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon
signed-rank tests compared accuracy between Phase 1 and Phase 2, and between
Phase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world
applicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated
high performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8,
and recall 0.99. The subset analysis (N=20) showed improvements in accuracy
(4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8).
ChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but
lagged in accuracy (3.9) and completeness (4.2). Differences in accuracy
between Phase 1 and Phase 2 was not statistically significant. However, Phase 2
significantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx
achieved 90% accuracy, outperforming other models. Integrating additional
resources like CPIC and PharmGKB with RAG enhances AI accuracy and performance.
This study highlights the transformative potential of generative AI like Sherpa
Rx in pharmacogenomics, improving decision-making with accurate, personalized
responses.

</details>


### [30] [An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning](https://arxiv.org/abs/2507.21471)
*Zujie Xie,Zixuan Chen,Jiheng Liang,Xiangyang Yu,Ziru Yu*

Main category: cs.AI

TL;DR: 本研究介绍了一种通过大语言模型驱动的框架，在低数据情况下实现准确、自动化红外光谱解释。通过整合结构化文献知识库、自动光谱预处理、特征提取和多任务推理，代理在不同材料数据集上展现出优异性能，超越传统机器学习和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 虽然红外光谱提供了快速、无破坏的化学和材料性质测量方法，但其高维、重叠的光谱带挑战了传统的化学计量方法。现阶段大语言模型由于其泛化能力和推理能力，在自动化复杂科学工作流方面具有巨大潜力，但在红外光谱分析中的应用仍然较少。因此，本研究旨在探索LLM在IR光谱分析中的应用，提出一种能够在低数据情况下实现准确自动化解释的方法。

Method: 本研究提出了一种基于大语言模型的代理框架，结合了结构化文献知识库、自动光谱预处理、特征提取和多任务推理。利用精心筛选的IR出版物语料库，代理选择经过科学验证的例程，将每个光谱转化为低维特征集，然后输入到用于分类、回归和异常检测的少样本提示模板中。采用闭环、多轮协议，迭代地将错误预测的样本追加到提示中，实现预测的动态改进。

Result: 多轮LLM在低数据条件下的红外光谱解释表现优异，不仅优于单轮推理，还能在低数据情况下与机器学习和深度学习模型竞争或超越。

Conclusion: 本研究介绍了一种在低数据情况下实现准确、自动化红外光谱解释的LLM驱动框架，通过结合结构化文献知识库、自动光谱预处理、特征提取和多任务推理，提出了一个端到端的框架。在多种材料数据集上，多轮LLM始终优于单轮推理，在低数据情况下可以与机器学习和深度学习模型相媲美甚至超越。

Abstract: Infrared spectroscopy offers rapid, non destructive measurement of chemical
and material properties but suffers from high dimensional, overlapping spectral
bands that challenge conventional chemometric approaches. Emerging large
language models (LLMs), with their capacity for generalization and reasoning,
offer promising potential for automating complex scientific workflows. Despite
this promise, their application in IR spectral analysis remains largely
unexplored. This study addresses the critical challenge of achieving accurate,
automated infrared spectral interpretation under low-data conditions using an
LLM-driven framework. We introduce an end-to-end, large language model driven
agent framework that integrates a structured literature knowledge base,
automated spectral preprocessing, feature extraction, and multi task reasoning
in a unified pipeline. By querying a curated corpus of peer reviewed IR
publications, the agent selects scientifically validated routines. The selected
methods transform each spectrum into low dimensional feature sets, which are
fed into few shot prompt templates for classification, regression, and anomaly
detection. A closed loop, multi turn protocol iteratively appends mispredicted
samples to the prompt, enabling dynamic refinement of predictions. Across
diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri
Reticulatae Pericarpium and waste water COD datasets, the multi turn LLM
consistently outperforms single turn inference, rivaling or exceeding machine
learning and deep learning models under low data regimes.

</details>


### [31] [Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess](https://arxiv.org/abs/2507.21488)
*Zhenwei Tang,Difan Jiao,Eric Xue,Reid McIlroy-Young,Jon Kleinberg,Siddhartha Sen,Ashton Anderson*

Main category: cs.AI

TL;DR: Maia4All framework revolutionizes individual behavior modeling in AI for chess, achieving high accuracy with minimal data requirements. It sets a new standard by requiring only 20 games, compared to the previous 5,000 games, showcasing its data efficiency and adaptability to individual decision-making styles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop AI systems that accurately emulate individual decision-making, especially in chess, to enhance human-AI alignment. Existing approaches require large amounts of data per individual, which is impractical for new or sparsely represented users.

Method: Maia4All utilizes a two-stage optimization process: enrichment step to bridge population and individual-level modeling, and democratization step to refine individual embeddings using minimal data. The framework learns and adapts to individual decision-making styles efficiently, even with limited data.

Result: Experimental results demonstrate that Maia4All accurately predicts individual moves and profiles behavioral patterns with high fidelity. It significantly improves data efficiency in individual behavior modeling, showcasing the adaptability of population AI systems to individual users through a prototype-enriched model as a bridge.

Conclusion: Maia4All framework achieves personalized human-like AI behavior modeling in chess with high accuracy and efficiency, requiring only 20 games compared to the previous 5,000 games. It sets a new standard for individual behavior modeling in AI systems.

Abstract: As humans seek to collaborate with, learn from, and better understand
artificial intelligence systems, developing AIs that can accurately emulate
individual decision-making becomes increasingly important. Chess, a
long-standing AI benchmark with precise skill measurement, offers an ideal
testbed for human-AI alignment. However, existing approaches to modeling human
behavior require prohibitively large amounts of data from each individual,
making them impractical for new or sparsely represented users. In this work, we
introduce Maia4All, a framework designed to learn and adapt to individual
decision-making styles efficiently, even with limited data. Maia4All achieves
this through a two-stage optimization process: (1) an enrichment step, which
bridges population and individual-level human behavior modeling with a
prototype-enriched model, and (2) a democratization step, which leverages
ability levels or user prototypes to initialize and refine individual
embeddings with minimal data. Our experimental results show that Maia4All can
accurately predict individual moves and profile behavioral patterns with high
fidelity, establishing a new standard for personalized human-like AI behavior
modeling in chess. Maia4All achieves individual human behavior modeling in
chess with only 20 games, compared to the 5,000 games required previously,
representing a significant improvement in data efficiency. Our work provides an
example of how population AI systems can flexibly adapt to individual users
using a prototype-enriched model as a bridge. This approach extends beyond
chess, as shown in our case study on idiosyncratic LLMs, highlighting its
potential for broader applications in personalized AI adaptation.

</details>


### [32] [Large Language Models for Supply Chain Decisions](https://arxiv.org/abs/2507.21502)
*David Simchi-Levi,Konstantina Mellou,Ishai Menache,Jeevan Pathuri*

Main category: cs.AI

TL;DR: 供应链管理面临复杂决策挑战，计算和信息技术进步推动决策自动化。然而，解释推荐、分析情景和更新数学模型仍耗费大量时间。基于LLMs技术的研究旨在减少决策时间、提高生产力和影响力。应用LLMs技术后，决策时间大幅缩短，提高了规划者和高管的生产力和影响力。


<details>
  <summary>Details</summary>
Motivation: 在过去几十年里，计算和信息技术的进步使得决策从人工、直觉和经验为基础转变为更自动化、数据驱动的决策。然而，业务规划人员和高管仍需花费大量时间和精力来理解和解释技术推荐、分析各种情景并更新数学模型，这导致决策速度减慢。基于大型语言模型(LLMs)的最新进展，本研究旨在通过这一颠覆性技术民主化供应链管理技术，实现减少决策时间、提高生产力和影响力的目标。

Method: 应用大型语言模型(LLMs)解决供应链管理中的决策挑战，减少人工干预，提高决策效率。

Result: 应用LLMs技术后，决策时间从几天几周减少到几分钟几小时，明显提高了规划者和高管的生产力和影响力。

Conclusion: 最近的大型语言模型(LLMs)技术可以推动供应链管理技术的民主化，通过减少决策时间、提高规划者和高管的生产力和影响力。

Abstract: Supply Chain Management requires addressing a variety of complex
decision-making challenges, from sourcing strategies to planning and execution.
Over the last few decades, advances in computation and information technologies
have enabled the transition from manual, intuition and experience-based
decision-making, into more automated and data-driven decisions using a variety
of tools that apply optimization techniques. These techniques use mathematical
methods to improve decision-making.
  Unfortunately, business planners and executives still need to spend
considerable time and effort to (i) understand and explain the recommendations
coming out of these technologies; (ii) analyze various scenarios and answer
what-if questions; and (iii) update the mathematical models used in these tools
to reflect current business environments. Addressing these challenges requires
involving data science teams and/or the technology providers to explain results
or make the necessary changes in the technology and hence significantly slows
down decision making.
  Motivated by the recent advances in Large Language Models (LLMs), we report
how this disruptive technology can democratize supply chain technology -
namely, facilitate the understanding of tools' outcomes, as well as the
interaction with supply chain tools without human-in-the-loop. Specifically, we
report how we apply LLMs to address the three challenges described above, thus
substantially reducing the time to decision from days and weeks to minutes and
hours as well as dramatically increasing planners' and executives' productivity
and impact.

</details>


### [33] [MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions](https://arxiv.org/abs/2507.21503)
*Yanxu Zhu,Shitong Duan,Xiangxu Zhang,Jitao Sang,Peng Zhang,Tun Lu,Xiao Zhou,Jing Yao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 本文对多模态大型语言模型（MLLMs）在诚实行为方面进行了系统评估，提出了MoHoBench作为MMLM诚实基准，发现大多数模型在需要时未能适当拒绝回答问题，认为MMLMs的诚实性需开发专门方法来对齐视觉信息，实现了初步的对齐方法，为构建值得信赖的MLLMs奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管已对语言模型的诚实性进行了大量研究，MMLMs在面对无法回答的视觉问题时的诚实能力仍未受到充分探索。

Method: 通过MoHoBench评估了28个流行的MMLMs的诚实行为，提出了四种代表性的无法回答视觉问题类型，构建了一个大规模的MMLM诚实基准MoHoBench，使用多阶段筛选和人工验证确保了其质量。

Result: 发现大多数模型未能适当拒绝回答问题；MMLMs的诚实不仅是语言建模问题，还深受视觉信息影响，需要开发专门的多模态诚实对齐方法。

Conclusion: 大多数模型在需要时未能适当拒绝回答问题；MLLMs的诚实不仅是语言建模问题，还深受视觉信息影响，需要开发专门的多模态诚实对齐方法。通过监督学习和偏好学习实现了初始的对齐方法，为未来构建值得信赖的MLLMs奠定了基础。

Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable
advancements in vision-language tasks, yet produce potentially harmful or
untrustworthy content. Despite substantial work investigating the
trustworthiness of language models, MMLMs' capability to act honestly,
especially when faced with visually unanswerable questions, remains largely
underexplored. This work presents the first systematic assessment of honesty
behaviors across various MLLMs. We ground honesty in models' response behaviors
to unanswerable visual questions, define four representative types of such
questions, and construct MoHoBench, a large-scale MMLM honest benchmark,
consisting of 12k+ visual question samples, whose quality is guaranteed by
multi-stage filtering and human verification. Using MoHoBench, we benchmarked
the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our
findings show that: (1) most models fail to appropriately refuse to answer when
necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but
is deeply influenced by visual information, necessitating the development of
dedicated methods for multimodal honesty alignment. Therefore, we implemented
initial alignment methods using supervised and preference learning to improve
honesty behavior, providing a foundation for future work on trustworthy MLLMs.
Our data and code can be found at https://github.com/DSTTSD/MoHoBench.

</details>


### [34] [What Does it Mean for a Neural Network to Learn a "World Model"?](https://arxiv.org/abs/2507.21513)
*Kenneth Li,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: 提出了一套神经网络学习和使用“世界模型”的精确标准，旨在为实验研究提供共同语言。将关注代表世界潜在状态空间的概念，并留待将行为影响建模的部分作为未来研究。


<details>
  <summary>Details</summary>
Motivation: 为了对常常在非正式场合使用的术语赋予操作性含义，以提供一个用于实验研究的共同语言。专注于代表世界潜在“状态空间”的概念，重点是为了未来研究留下行为影响建模的部分。

Method: 从线性探测文献中借鉴思想，形式化计算概念，通过数据生成过程中的表示进行因素分解。提出一组条件以验证“世界模型”不是神经网络数据或任务的简单结论。

Result: 提出了一套关于神经网络学习和使用“世界模型”的精确标准，旨在建立实验研究的通用语言。定义也包括一组条件，用于验证“世界模型”不是神经网络数据或任务的简单结论。

Conclusion: 提出一组精确的标准，用于界定神经网络学习和使用“世界模型”的概念，以在实验研究中提供一个共同的语言。着重于代表世界潜在“状态空间”的概念，将行为影响建模留给未来研究。定义基于线性探测文献的思想，形式化通过数据生成过程的表示进行的计算概念。在定义中的一个重要补充是一组条件，用于检查这样的“世界模型”不是神经网络数据或任务的简单结论。

Abstract: We propose a set of precise criteria for saying a neural net learns and uses
a "world model." The goal is to give an operational meaning to terms that are
often used informally, in order to provide a common language for experimental
investigation. We focus specifically on the idea of representing a latent
"state space" of the world, leaving modeling the effect of actions to future
work. Our definition is based on ideas from the linear probing literature, and
formalizes the notion of a computation that factors through a representation of
the data generation process. An essential addition to the definition is a set
of conditions to check that such a "world model" is not a trivial consequence
of the neural net's data or task.

</details>


### [35] [ST-GDance: Long-Term and Collision-Free Group Choreography from Music](https://arxiv.org/abs/2507.21518)
*Jing Xu,Weiqiang Wang,Cunjian Chen,Jun Liu,Qiuhong Ke*

Main category: cs.AI

TL;DR: ST-GDance proposes a new framework for group dance generation that separates spatial and temporal dependencies to optimize choreography, utilizing lightweight graph convolutions for spatial modeling and accelerated sparse attention for temporal modeling. It outperforms existing methods in creating long and coherent group dance sequences.


<details>
  <summary>Details</summary>
Motivation: The motivation behind ST-GDance stems from the challenges in group dance generation, including synchronizing multiple dancers, maintaining spatial coordination, and avoiding motion collisions. Existing methods struggle with dense spatial-temporal interactions, scalability issues, and multi-dancer collisions.

Method: ST-GDance proposes a novel framework that decouples spatial and temporal dependencies to optimize long-term and collision-free group choreography. It utilizes lightweight graph convolutions for spatial modeling and accelerated sparse attention for efficient temporal modeling.

Result: Experiments on the AIOZ-GDance dataset show that ST-GDance significantly reduces computational costs while ensuring smooth and collision-free interactions. It outperforms existing baselines in generating long and coherent group dance sequences.

Conclusion: ST-GDance outperforms state-of-the-art baselines in generating long and coherent group dance sequences, demonstrating improved spatial-temporal modeling and efficient choreography optimization.

Abstract: Group dance generation from music has broad applications in film, gaming, and
animation production. However, it requires synchronizing multiple dancers while
maintaining spatial coordination. As the number of dancers and sequence length
increase, this task faces higher computational complexity and a greater risk of
motion collisions. Existing methods often struggle to model dense
spatial-temporal interactions, leading to scalability issues and multi-dancer
collisions. To address these challenges, we propose ST-GDance, a novel
framework that decouples spatial and temporal dependencies to optimize
long-term and collision-free group choreography. We employ lightweight graph
convolutions for distance-aware spatial modeling and accelerated sparse
attention for efficient temporal modeling. This design significantly reduces
computational costs while ensuring smooth and collision-free interactions.
Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms
state-of-the-art baselines, particularly in generating long and coherent group
dance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.

</details>


### [36] [Large Language Models for Wireless Communications: From Adaptation to Autonomy](https://arxiv.org/abs/2507.21524)
*Le Liang,Hao Ye,Yucheng Sheng,Ouya Wang,Jiacheng Wang,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 本文探讨了LLMs在无线通信系统中的作用，包括适应预训练的LLMs用于核心通信任务、开发无线特定的基础模型以平衡多功能性和效率、以及增强型LLMs在自主推理和协调能力方面的应用。文章突出了LLM-based方法相对传统方法的优势，提出了未来的研究挑战和机会，推动智能、自适应和自主无线网络的发展。


<details>
  <summary>Details</summary>
Motivation: 无线通信领域的复杂性和动态性日益增加，需要智能和自适应的解决方案。LLMs在人工智能领域的巨大成功激发了将其应用于无线通信系统的动机。本文旨在探讨LLMs在无线通信领域的潜力和可能性。

Method: 本文探索了LLMs在无线通信系统中的应用，主要包括三个方向：1. 为核心通信任务调整预训练的LLMs；2. 开发适用于无线通信的基础模型，平衡多功能性和效率；3. 实现具有自主推理和协调能力的智能LLMs。突出了LLMs方法相对传统方法的独特优势和最新进展。

Result: 通过研究LLMs在无线通信系统中的应用，本文指出了LLM-based方法相对于传统方法的优势，突出了在核心通信任务、多功能性和效率平衡、以及自主推理和协调能力方面的应用前景。

Conclusion: 语言模型(Large Language Models, LLMs)的出现彻底改变了人工智能领域，为推理、泛化和零-shot学习提供了前所未有的能力。本文探讨了LLMs在无线通信领域的作用，重点包括适应预训练的LLMs用于核心通信任务、开发无线特定的基础模型以平衡多功能性和效率、以及增强型LLMs在自主推理和协调能力方面的应用。文章突出了最新进展、实际案例研究以及LLM-based方法相对传统方法的独特优势。最后，概述了未来智能、自适应和自主无线网络的开放挑战和研究机会，包括多模态融合、与轻量级模型的合作以及自我改进能力。

Abstract: The emergence of large language models (LLMs) has revolutionized artificial
intelligence, offering unprecedented capabilities in reasoning, generalization,
and zero-shot learning. These strengths open new frontiers in wireless
communications, where increasing complexity and dynamics demand intelligent and
adaptive solutions. This article explores the role of LLMs in transforming
wireless systems across three key directions: adapting pretrained LLMs for core
communication tasks, developing wireless-specific foundation models to balance
versatility and efficiency, and enabling agentic LLMs with autonomous reasoning
and coordination capabilities. We highlight recent advances, practical case
studies, and the unique benefits of LLM-based approaches over traditional
methods. Finally, we outline open challenges and research opportunities,
including multimodal fusion, collaboration with lightweight models, and
self-improving capabilities, charting a path toward intelligent, adaptive, and
autonomous wireless networks of the future.

</details>


### [37] [Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations](https://arxiv.org/abs/2507.21571)
*Laura Spillner,Nima Zargham,Mihai Pomarlan,Robert Porzel,Rainer Malaka*

Main category: cs.AI

TL;DR: 该论文探讨了个性化AI解释的重要性，介绍了一种模型，通过个性化和动态记忆提供用户定制的解释。


<details>
  <summary>Details</summary>
Motivation: 研究动机是AI解释需要更具人性化，关注用户个体和解释的背景，以便提高用户理解。

Method: 该论文提出了一种基于个性化方法的解释模型，代理根据用户的偏好和上下文定制提供的信息。

Result: 论文提出的代理模型结合个人动态记忆，有助于评估用户可能感兴趣的新信息，从而实现个性化解释。

Conclusion: 该论文提出了针对AI解释的个性化方法，旨在增加透明度并改善非专家用户理解。

Abstract: The need for explanations in AI has, by and large, been driven by the desire
to increase the transparency of black-box machine learning models. However,
such explanations, which focus on the internal mechanisms that lead to a
specific output, are often unsuitable for non-experts. To facilitate a
human-centered perspective on AI explanations, agents need to focus on
individuals and their preferences as well as the context in which the
explanations are given. This paper proposes a personalized approach to
explanation, where the agent tailors the information provided to the user based
on what is most likely pertinent to them. We propose a model of the agent's
worldview that also serves as a personal and dynamic memory of its previous
interactions with the same user, based on which the artificial agent can
estimate what part of its knowledge is most likely new information to the user.

</details>


### [38] [SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21585)
*Hao Ye,Mengshi Qi,Zhaohong Liu,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: 本研究旨在通过创建SafeDrive228K基准数据集和引入SafeDriveRAG方法，提升自动驾驶系统的安全性能。五种主流VLM在交通安全任务中整合RAG方法，取得了显著的性能提升。研究结果强调了该基准和方法对于交通安全研究的潜力和重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了在交通安全关键驾驶场景中对VLM的评估，为了填补这一缺口，我们创建了SafeDrive228K基准数据集，并提出了基于知识图谱检索增强生成的新方法SafeDriveRAG，通过增加交通安全问题的多样性，能够全面评估模型的理解和推理能力。

Method: 创建了一个名为SafeDrive228K的大规模多模态问答基准数据集，包含228K个样本跨越18个子任务，涵盖了各种交通安全问题，通过引入基于知识图谱的检索增强生成方法，提高了模型的信息检索效率和处理安全关键情况的能力。针对五种主流VLM进行了全面评估，实验结果表明整合RAG显著提升了性能，在交通事故任务中提高了+4.73%，在转弯曲线案例和交通安全常识任务中提高了+8.79%和+14.57%。

Result: 实验证明，SafeDriveRAG方法在各种交通安全任务中显著提高了五种主流VLM的性能，为推动交通安全领域的研究提供了重要参考。

Conclusion: 通过该研究，我们提出了一种基于知识图谱检索增强生成的方法（SafeDriveRAG），用于视觉问答（VQA），有效提高了自动驾驶系统的安全性能。结合Traffic Safety Commonsense等多个任务的评估结果显示，该方法在安全驾驶任务中取得了显著的性能提升，为交通安全领域的研究提供了新的思路和方法。

Abstract: In this work, we study how vision-language models (VLMs) can be utilized to
enhance the safety for the autonomous driving system, including perception,
situational understanding, and path planning. However, existing research has
largely overlooked the evaluation of these models in traffic safety-critical
driving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K)
and propose a new baseline based on VLM with knowledge graph-based
retrieval-augmented generation (SafeDriveRAG) for visual question answering
(VQA). Specifically, we introduce SafeDrive228K, the first large-scale
multimodal question-answering benchmark comprising 228K examples across 18
sub-tasks. This benchmark encompasses a diverse range of traffic safety
queries, from traffic accidents and corner cases to common safety knowledge,
enabling a thorough assessment of the comprehension and reasoning abilities of
the models. Furthermore, we propose a plug-and-play multimodal knowledge
graph-based retrieval-augmented generation approach that employs a novel
multi-scale subgraph retrieval algorithm for efficient information retrieval.
By incorporating traffic safety guidelines collected from the Internet, this
framework further enhances the model's capacity to handle safety-critical
situations. Finally, we conduct comprehensive evaluations on five mainstream
VLMs to assess their reliability in safety-sensitive driving tasks.
Experimental results demonstrate that integrating RAG significantly improves
performance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in
Corner Cases tasks and +14.57% in Traffic Safety Commonsense across five
mainstream VLMs, underscoring the potential of our proposed benchmark and
methodology for advancing research in traffic safety. Our source code and data
are available at https://github.com/Lumos0507/SafeDriveRAG.

</details>


### [39] [Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning](https://arxiv.org/abs/2507.21588)
*Jiong Yin,Liang Li,Jiehua Zhang,Yuhan Gao,Chenggang Yan,Xichun Sheng*

Main category: cs.AI

TL;DR: 该论文介绍了一种渐进式稳态和塑性音频-视觉提示（PHP）方法，在音频-视觉多任务增量学习中取得了领先的性能。通过三个阶段的设计，PHP方法有效平衡了知识共享和特异性，实现了任务特定提示的保留和对新任务的共享参数适应。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于解决音频-视觉多任务增量学习中保留旧任务知识并促进新任务学习的挑战。通过提出PHP方法，旨在有效平衡知识共享和特异性，以提高模型的性能和多任务转移的灵活性。

Method: 该论文提出了三阶段的PHP方法，包括浅层阶段的任务共享模态聚合适配器设计、中间阶段的任务特定模态共享动态生成适配器和深层阶段的任务特定模态独立提示引入。通过这些阶段的结合，实现了任务特定提示的保留和对新任务的共享参数适应，从而平衡了知识共享和特异性。

Result: 通过实验，该方法在四个任务中实现了领先的性能，并展示了在不同任务顺序下的最先进表现。

Conclusion: 该论文介绍了一种三阶段的渐进式稳态和塑性音频-视觉提示（PHP）方法，用于解决音频-视觉多任务增量学习中保留旧任务知识并促进新任务学习的挑战。通过设计任务共享模态聚合适配器、任务特定的模态共享动态生成适配器和任务特定的模态独立提示，PHP方法能够有效平衡知识共享和特异性，最终在四个任务（AVE、AVVP、AVS和AVQA）中实现了领先的性能。

Abstract: Audio-visual multi-task incremental learning aims to continuously learn from
multiple audio-visual tasks without the need for joint training on all tasks.
The challenge of the problem is how to preserve the old task knowledge while
facilitating the learning of new task with previous experiences. To address
these challenges, we introduce a three-stage Progressive Homeostatic and
Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the
task-shared modality aggregating adapter to foster cross-task and cross-modal
audio-visual representation learning to enhance shared understanding between
tasks. In the middle phase, we propose the task-specific modality-shared
dynamic generating adapter, which constructs prompts that are tailored to
individual tasks while remaining general across modalities, which balances the
models ability to retain knowledge against forgetting with its potential for
versatile multi-task transferability. In the deep phase, we introduce the
task-specific modality-independent prompts to further refine the understand
ability by targeting individual information for each task and modality. By
incorporating these three phases, PHP retains task-specific prompts while
adapting shared parameters for new tasks to effectively balance knowledge
sharing and specificity. Our method achieves SOTA performance in different
orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at
https://github.com/ENJOY-Yin-jiong/PHP.

</details>


### [40] [Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems](https://arxiv.org/abs/2507.21589)
*Bin Liu*

Main category: cs.AI

TL;DR: 本文通过比较贝叶斯统计和当代具身智能方法在搜索和学习方面的联系，讨论了贝叶斯推断在现代具身智能中的作用以及对具身智能系统拓展的潜在影响。贝叶斯方法尚未被广泛应用在具身智能系统中，但具有将系统拓展到开放物理世界的潜力。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于探讨贝叶斯统计在具身智能系统中的潜在作用，以及对当代具身智能方法的影响，以期为拓展具身智能系统到开放物理世界提供新的思路。

Method: 通过对贝叶斯统计和当代具身智能方法在搜索和学习方面的比较，研究了贝叶斯推断在现代具身智能中的应用现状。

Result: 通过分析贝叶斯统计和当代具身智能方法在搜索和学习方面的关系，揭示出贝叶斯推断在现代具身智能发展中的局限性，并指出了将贝叶斯方法引入开放物理世界具身智能系统的潜在价值。

Conclusion: 该论文讨论了贝叶斯统计和当代具身智能方法在搜索和学习方面的联系，揭示了为何贝叶斯推断在现代具身智能发展中并没有起到主导作用，同时指出当前具身智能系统仍然局限于封闭的物理世界环境，强调贝叶斯方法在将这些系统拓展到真正开放的物理世界具身智能中的潜力。

Abstract: Embodied intelligence posits that cognitive capabilities fundamentally emerge
from - and are shaped by - an agent's real-time sensorimotor interactions with
its environment. Such adaptive behavior inherently requires continuous
inference under uncertainty. Bayesian statistics offers a principled
probabilistic framework to address this challenge by representing knowledge as
probability distributions and updating beliefs in response to new evidence. The
core computational processes underlying embodied intelligence - including
perception, action selection, learning, and even higher-level cognition - can
be effectively understood and modeled as forms of Bayesian inference. Despite
the deep conceptual connection between Bayesian statistics and embodied
intelligence, Bayesian principles have not been widely or explicitly applied in
today's embodied intelligence systems. In this work, we examine both Bayesian
and contemporary embodied intelligence approaches through two fundamental
lenses: search and learning - the two central themes in modern AI, as
highlighted in Rich Sutton's influential essay "The Bitter Lesson". This
analysis sheds light on why Bayesian inference has not played a central role in
the development of modern embodied intelligence. At the same time, it reveals
that current embodied intelligence systems remain largely confined to
closed-physical-world environments, and highlights the potential for Bayesian
methods to play a key role in extending these systems toward truly open
physical-world embodied intelligence.

</details>


### [41] ["Teammates, Am I Clear?": Analysing Legible Behaviours in Teams](https://arxiv.org/abs/2507.21631)
*Miguel Faria,Francisco S. Melo,Ana Paiva*

Main category: cs.AI

TL;DR: 本文研究了在团队和团队合作中的顺序决策可读性概念，提出了将可读性决策扩展到多智能体环境中的方法，并展示了具有可读性智能体的团队在性能上的优势。通过多智能体基准场景的展示，证明了可读性决策在团队场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在过去的研究中，关于在团队和团队合作中应用可读性决策的工作集中在一个智能体与一个人互动的情况，而忽略了在多智能体或人类团队配置中应用可读性决策的潜在优势。本文的动机在于探讨如何通过扩展可读性决策来提高智能体在协作中的表现。

Method: 提出了将可读性决策扩展到多智能体环境中的方法，并在多智能体基准场景中展示了可读性决策的性能。

Result: 通过在多智能体基准场景中展示了可读性决策在团队场景中的性能，本文表明具有可读性智能体的团队能够胜过仅由具有标准最佳行为的智能体组成的团队。

Conclusion: 在团队和团队合作的背景下，本文研究了在顺序决策中的可读性概念。我们提出了将可读性决策扩展到多智能体环境中，并展示了在团队场景中使用我们提出的扩展的可读性决策性能。结果表明，一个具有可读性智能体的团队能够胜过仅由具有标准最佳行为的智能体组成的团队。

Abstract: In this paper we investigate the notion of legibility in sequential
decision-making in the context of teams and teamwork. There have been works
that extend the notion of legibility to sequential decision making, for
deterministic and for stochastic scenarios. However, these works focus on one
agent interacting with one human, foregoing the benefits of having legible
decision making in teams of agents or in team configurations with humans. In
this work we propose an extension of legible decision-making to multi-agent
settings that improves the performance of agents working in collaboration. We
showcase the performance of legible decision making in team scenarios using our
proposed extension in multi-agent benchmark scenarios. We show that a team with
a legible agent is able to outperform a team composed solely of agents with
standard optimal behaviour.

</details>


### [42] [StaffPro: an LLM Agent for Joint Staffing and Profiling](https://arxiv.org/abs/2507.21636)
*Alessio Maritan*

Main category: cs.AI

TL;DR: 本文介绍了一种名为StaffPro的大型语言模型代理系统，用于解决人力资源管理中的任务分配和员工能力评估问题。StaffPro通过自然语言表达优化目标、接受文本任务描述并与人类持续互动的方式，成功估计员工的属性并生成高质量排班表，提供了创新性、可解释及以人为中心的自动化人员管理解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索利用大型语言模型（LLM）代理系统解决人力资源管理中的任务分配和员工能力评估挑战。StaffPro的设计旨在提供一种创新性的、可解释的、以人为中心的自动化人员管理解决方案。

Method: 通过将任务分配和员工能力评估问题形式化为数学框架，并引入StaffPro这一LLM代理系统，实现了任务调度与潜在特征估计的关联。StaffPro采用自然语言表达优化目标，接受文本任务描述并提供高度灵活性。该系统通过建立持续的人-代理反馈循环，直接与人类互动，从人类反馈中不断估计员工的潜在特征，实现终生员工能力评估，并确保时间上的最佳员工配备表现。在咨询公司仿真案例中展示了StaffPro成功估计员工属性并生成高质量排班表的能力。

Result: 研究结果表明，StaffPro通过持续人-代理反馈循环不断估计员工的潜在特征，实现终生员工能力评估，同时生成高质量的排班表。通过仿真案例展示了系统的成功应用和优势。

Conclusion: 本文提出了一种新的使用大型语言模型（LLM）的人力资源管理系统，名为StaffPro，用于解决任务分配和员工能力评估等挑战。通过与人类直接互动的方式，StaffPro实现了持续的人-代理反馈循环，能够根据人类的反馈持续估计员工的潜在特征，实现终生员工能力评估，并确保随着时间推移的最佳员工配备表现。研究结果表明，StaffPro成功估计了员工的属性并生成了高质量的排班表，为自动人员管理提供了健壮、可解释和以人为中心的解决方案。

Abstract: Large language model (LLM) agents integrate pre-trained LLMs with modular
algorithmic components and have shown remarkable reasoning and decision-making
abilities. In this work, we investigate their use for two tightly intertwined
challenges in workforce management: staffing, i.e., the assignment and
scheduling of tasks to workers, which may require team formation; and
profiling, i.e., the continuous estimation of workers' skills, preferences, and
other latent attributes from unstructured data. We cast these problems in a
formal mathematical framework that links scheduling decisions to latent feature
estimation, and we introduce StaffPro, an LLM agent that addresses staffing and
profiling jointly. Differently from existing staffing solutions, StaffPro
allows expressing optimization objectives using natural language, accepts
textual task descriptions and provides high flexibility. StaffPro interacts
directly with humans by establishing a continuous human-agent feedback loop,
ensuring natural and intuitive use. By analyzing human feedback, our agent
continuously estimates the latent features of workers, realizing life-long
worker profiling and ensuring optimal staffing performance over time. A
consulting firm simulation example demonstrates that StaffPro successfully
estimates workers' attributes and generates high quality schedules. With its
innovative design, StaffPro offers a robust, interpretable, and human-centric
solution for automated personnel management.

</details>


### [43] [Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models](https://arxiv.org/abs/2507.21637)
*Wanying Wang,Zeyu Ma,Han Zheng,Xin Tan,Mingang Chen*

Main category: cs.AI

TL;DR: Large vision-language models (LVLMs) are vulnerable to harmful input as safety perception emerges before comprehensive semantic understanding. The paper introduces Self-Aware Safety Augmentation (SASA) to enhance safety recognition without fine-tuning, significantly improving safety with minimal utility impact.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerability of LVLMs to harmful input by understanding the model's internal dynamics and proposing a technique, SASA, to improve safety without compromising utility.

Method: The paper investigates LVLMs' internal dynamics and identifies key capabilities such as safety perception, semantic understanding, and alignment for linguistic expression. It introduces the technique of Self-Aware Safety Augmentation (SASA) to enhance safety recognition in LVLMs by projecting informative semantic representations from intermediate layers onto safety-oriented layers.

Result: The results show that safety perception often precedes semantic understanding in LVLMs, leading to reduced safety. SASA significantly enhances the safety of LVLMs without fine-tuning, as validated through extensive experiments on various datasets and tasks.

Conclusion: LVLMs are vulnerable to harmful input due to safety perception emerging before comprehensive semantic understanding. Self-Aware Safety Augmentation (SASA) enhances safety recognition in LVLMs without fine-tuning, significantly improving safety with minimal utility impact.

Abstract: Large vision-language models (LVLMs) are vulnerable to harmful input compared
to their language-only backbones. We investigated this vulnerability by
exploring LVLMs internal dynamics, framing their inherent safety understanding
in terms of three key capabilities. Specifically, we define these capabilities
as safety perception, semantic understanding, and alignment for linguistic
expression, and experimentally pinpointed their primary locations within the
model architecture. The results indicate that safety perception often emerges
before comprehensive semantic understanding, leading to the reduction in
safety. Motivated by these findings, we propose \textbf{Self-Aware Safety
Augmentation (SASA)}, a technique that projects informative semantic
representations from intermediate layers onto earlier safety-oriented layers.
This approach leverages the model's inherent semantic understanding to enhance
safety recognition without fine-tuning. Then, we employ linear probing to
articulate the model's internal semantic comprehension to detect the risk
before the generation process. Extensive experiments on various datasets and
tasks demonstrate that SASA significantly improves the safety of LVLMs, with
minimal impact on the utility.

</details>


### [44] [Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics](https://arxiv.org/abs/2507.21638)
*Leonard Hinckeldey,Elliot Fosong,Elle Miller,Rimvydas Rubavicius,Trevor McInroe,Patricia Wollstadt,Christiane B. Wiebel-Herboth,Subramanian Ramamoorthy,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 通过引入Assistax基准测试，在辅助机器人任务中使用MARL框架进行多智能体协作学习，以评估机器人的协调能力。Assistax在物理仿真中通过JAX硬件加速取得了显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的RL基准测试主要以游戏为主，无法直接转化到现实世界中的物理互动任务。因此，为了解决这一问题并推动辅助机器人领域的研究，提出了Assistax基准测试。该基准测试旨在解决辅助机器人任务中的挑战，为研究人员提供可靠的基准，并利用硬件加速实现了学习速度的显著提升。

Method: 采用多智能体强化学习（MARL）框架来训练多样合作智能体的人机互动，通过对零参考协调能力的测试来评估。对流行的连续控制RL和MARL算法进行了广泛评估和超参数调优，为进一步推动辅助机器人领域的强化学习研究建立了实用基准。

Result: Assistax在物理仿真中运行速度比基于CPU的替代方案快370倍，为连续控制RL和MARL算法提供可靠的基线，并被确立为推进辅助机器人强化学习研究的实用基准。

Conclusion: 介绍了一个名为Assistax的开源基准测试，用于解决辅助机器人任务中出现的挑战，通过多智能体强化学习推动辅助机器人领域的研究进展。该基准测试提供了有效的基线，并展示了在物理模拟中利用JAX硬件加速实现了显著的加速效果。

Abstract: The development of reinforcement learning (RL) algorithms has been largely
driven by ambitious challenge tasks and benchmarks. Games have dominated RL
benchmarks because they present relevant challenges, are inexpensive to run and
easy to understand. While games such as Go and Atari have led to many
breakthroughs, they often do not directly translate to real-world embodied
applications. In recognising the need to diversify RL benchmarks and addressing
complexities that arise in embodied interaction scenarios, we introduce
Assistax: an open-source benchmark designed to address challenges arising in
assistive robotics tasks. Assistax uses JAX's hardware acceleration for
significant speed-ups for learning in physics-based simulations. In terms of
open-loop wall-clock time, Assistax runs up to $370\times$ faster when
vectorising training runs compared to CPU-based alternatives. Assistax
conceptualises the interaction between an assistive robot and an active human
patient using multi-agent RL to train a population of diverse partner agents
against which an embodied robotic agent's zero-shot coordination capabilities
can be tested. Extensive evaluation and hyperparameter tuning for popular
continuous control RL and MARL algorithms provide reliable baselines and
establish Assistax as a practical benchmark for advancing RL research for
assistive robotics. The code is available at:
https://github.com/assistive-autonomy/assistax.

</details>


### [45] [Can the current trends of AI handle a full course of mathematics?](https://arxiv.org/abs/2507.21664)
*Mariam Alsayyad,Fayadh Kadhem*

Main category: cs.AI

TL;DR: 本文评估了人工智能在大学数学课程方面的能力，发现其在组织和准确性等方面表现强劲，但在处理情感等人类方面仍有不足。建议整合人类和人工智能潜力以取得更好的教学效果。


<details>
  <summary>Details</summary>
Motivation: 研究当前人工智能在教育领域的应用，探讨人工智能是否能完全代替人类在大学数学课程中的角色。

Method: 通过评估人工智能在大学数学课程各方面的能力，发现人工智能在组织和准确性方面表现强劲，但在处理情感等人类方面仍有不足。提出整合人类和人工智能潜力的建议。

Result: 研究发现人工智能在某些重要方面表现出色，但仍无法满足人类的情感需求。建议整合人类和人工智能潜力以取得更好的教学效果。

Conclusion: 本文探讨了当前人工智能在大学数学课程中担任责任的能力。研究评估了人工智能在课程大纲制定、教材呈现、回答学生问题和考核创建等四个重要方面的能力。研究表明，虽然人工智能在组织和准确性等重要方面表现强劲，但仍有一些人类方面远远超出当前人工智能的能力范围。即使在科学领域，仍存在着一些无法在当前状态下由人工智能满足的情感需求。本文提出了一些建议，以整合人类和人工智能的潜力，实现更好的结果，以更好地实现创造一门大学水平数学课程的目标。

Abstract: This paper addresses the question of how able the current trends of
Artificial Intelligence (AI) are in managing to take the responsibility of a
full course of mathematics at a college level. The study evaluates this ability
in four significant aspects, namely, creating a course syllabus, presenting
selected material, answering student questions, and creating an assessment. It
shows that even though the AI is strong in some important parts like
organization and accuracy, there are still some human aspects that are far away
from the current abilities of AI. There is still a hidden emotional part, even
in science, that cannot be fulfilled by the AI in its current state. This paper
suggests some recommendations to integrate the human and AI potentials to
create better outcomes in terms of reaching the target of creating a full
course of mathematics, at a university level, as best as possible.

</details>


### [46] [Unrolling Dynamic Programming via Graph Filters](https://arxiv.org/abs/2507.21705)
*Sergio Rozada,Samuel Rey,Gonzalo Mateos,Antonio G. Marques*

Main category: cs.AI

TL;DR: 本研究提出了一种名为BellNet的新方法，用于解决MDP中的贝尔曼最优性方程。通过图信号处理的方法，将BellNet视为一系列非线性图滤波器的级联，从而实现了对策略和价值迭代的简洁、可传递和统一的表示。初步实验表明，BellNet在相对较少的迭代次数内能够有效逼近最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统的策略迭代方法在处理状态-动作空间较大或涉及长期依赖性的问题时可能计算复杂度较高。针对这一问题，本研究旨在提出一种更高效的解决方案。

Method: 提出了一种新方法BellNet，通过将策略迭代展开并截断为可学习的参数模型，训练以最小化从随机值函数初始化开始的所谓贝尔曼误差。利用图信号处理的见解，将MDP的转移概率矩阵视为带权重有向图的邻接矩阵，将BellNet解释为一系列非线性图滤波器的级联，从而实现对策略和价值迭代的简洁、可传递和统一的表示，并在推断过程中显式处理复杂度。

Result: 通过在类似网格环境中进行的初步实验，证明了BellNet相对于传统方法能够更快速且有效地逼近最优策略。

Conclusion: 新方法BellNet可以在相对较少的迭代次数内有效逼近最优策略，与传统方法相比具有更高的效率。

Abstract: Dynamic programming (DP) is a fundamental tool used across many engineering
fields. The main goal of DP is to solve Bellman's optimality equations for a
given Markov decision process (MDP). Standard methods like policy iteration
exploit the fixed-point nature of these equations to solve them iteratively.
However, these algorithms can be computationally expensive when the
state-action space is large or when the problem involves long-term
dependencies. Here we propose a new approach that unrolls and truncates policy
iterations into a learnable parametric model dubbed BellNet, which we train to
minimize the so-termed Bellman error from random value function
initializations. Viewing the transition probability matrix of the MDP as the
adjacency of a weighted directed graph, we draw insights from graph signal
processing to interpret (and compactly re-parameterize) BellNet as a cascade of
nonlinear graph filters. This fresh look facilitates a concise, transferable,
and unifying representation of policy and value iteration, with an explicit
handle on complexity during inference. Preliminary experiments conducted in a
grid-like environment demonstrate that BellNet can effectively approximate
optimal policies in a fraction of the iterations required by classical methods.

</details>


### [47] [GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation](https://arxiv.org/abs/2507.21727)
*Jianfei Zhu,Haiqi Zhu,Shaohui Liu,Feng Jiang,Baichun Wei,Chunzhi Yi*

Main category: cs.AI

TL;DR: Recent deep learning approaches have difficulties in adapting to domain shifts in real-world cross-dataset scenarios for individual brain parcellation from fMRI data. The paper presents GDAIP, a framework that integrates GAT and MME-based domain adaptation to adapt the reference atlas from group-level to individual-level brain graphs. GDAIP successfully produces individual parcellations with accurate boundaries, consistent cross-session results, and reflective of functional organization.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with domain shifts in real-world cross-dataset scenarios, hindering individual brain parcellations from functional magnetic resonance imaging (fMRI) data. The motivation of the paper is to address this challenge by proposing a framework that can adapt the reference atlas from the group-level brain graph to the individual-level brain graph for accurate individual parcellation.

Method: The paper proposed Graph Domain Adaptation for Individual Parcellation (GDAIP) that integrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based domain adaptation. It constructs cross-dataset brain graphs at both the group and individual levels, leveraging semi-supervised training and adversarial optimization of prediction entropy to adapt the reference atlas from the group-level to individual-level, enabling individual parcellation under cross-dataset scenarios.

Result: Experimental results show that GDAIP achieves effective individual parcellations with topologically plausible boundaries, strong cross-session consistency, and the ability to reflect functional organization. The evaluation was done based on parcellation visualization, Dice coefficient, and functional homogeneity.

Conclusion: GDAIP is effective in producing individual brain parcellations with topologically plausible boundaries, strong cross-session consistency, and the ability to reflect functional organization.

Abstract: Recent deep learning approaches have shown promise in learning such
individual brain parcellations from functional magnetic resonance imaging
(fMRI). However, most existing methods assume consistent data distributions
across domains and struggle with domain shifts inherent to real-world
cross-dataset scenarios. To address this challenge, we proposed Graph Domain
Adaptation for Individual Parcellation (GDAIP), a novel framework that
integrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based
domain adaptation. We construct cross-dataset brain graphs at both the group
and individual levels. By leveraging semi-supervised training and adversarial
optimization of the prediction entropy on unlabeled vertices from target brain
graph, the reference atlas is adapted from the group-level brain graph to the
individual brain graph, enabling individual parcellation under cross-dataset
settings. We evaluated our method using parcellation visualization, Dice
coefficient, and functional homogeneity. Experimental results demonstrate that
GDAIP produces individual parcellations with topologically plausible
boundaries, strong cross-session consistency, and ability of reflecting
functional organization.

</details>


### [48] [SAT-Based Bounded Fitting for the Description Logic ALC](https://arxiv.org/abs/2507.21752)
*Maurice Funk,Jean Christoph Jung,Tom Voellmer*

Main category: cs.AI

TL;DR: 该论文研究了有界拟合在描述逻辑ALC及其句法片段中的应用。发现大小限制的拟合问题在所有研究的片段中是NP完全的。与其他学习ALC概念的算法相比，有界拟合在Valiant的PAC学习框架中提供了概率保证。最后，他们基于SAT求解器实现了有界拟合，并进行了优化和比较。


<details>
  <summary>Details</summary>
Motivation: 有界拟合作为学习逻辑公式的一般范例，最近引起了广泛关注。研究人员探讨了描述逻辑ALC及其句法片段的有界拟合问题，并与Valiant的PAC学习框架进行了比较。

Method: 研究采用有界拟合作为学习逻辑公式的范例，针对描述逻辑ALC及其句法片段进行调查。展示了基于大小限制的拟合问题在所有研究的片段中是NP完全的，即使在单个正例和单个负例的特殊情况下。

Result: 最终，我们实现了针对ALC及其片段的有界拟合，基于SAT求解器。讨论了优化方案，并与其他概念学习工具进行了比较。

Conclusion: 研究表明，对于描述逻辑ALC及其句法片段，有界拟合的基本问题是NP完全的，即使在单个正例和单个负例的特殊情况下。相比之下，其他用于学习ALC概念的算法类别并不提供概率保证。最后，我们基于SAT求解器实现了ALC及其片段的有界拟合，并讨论了优化方案，并将我们的实现与其他概念学习工具进行了比较。

Abstract: Bounded fitting is a general paradigm for learning logical formulas from
positive and negative data examples, that has received considerable interest
recently. We investigate bounded fitting for the description logic ALC and its
syntactic fragments. We show that the underlying size-restricted fitting
problem is NP-complete for all studied fragments, even in the special case of a
single positive and a single negative example. By design, bounded fitting comes
with probabilistic guarantees in Valiant's PAC learning framework. In contrast,
we show that other classes of algorithms for learning ALC concepts do not
provide such guarantees. Finally, we present an implementation of bounded
fitting in ALC and its fragments based on a SAT solver. We discuss
optimizations and compare our implementation to other concept learning tools.

</details>


### [49] [Towards a rigorous evaluation of RAG systems: the challenge of due diligence](https://arxiv.org/abs/2507.21753)
*Grégoire Martinon,Alexandra Lorenzo de Brionne,Jérôme Bohard,Antoine Lojou,Damien Hervault,Nicolas J-B. Brunel*

Main category: cs.AI

TL;DR: 本研究评估了在投资基金尽职调查中使用的RAG系统，提出了结合人工注释和LLM-Judge注释的评估协议，以识别系统失败，并实现了精确性能测量。研究结果表明RAG系统在工业应用中的可靠性仍存在问题，但通过新方法的引入可以提高其可靠性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的兴起，高风险领域如医疗保健和金融迎来了重大进展。RAG架构结合了语言模型（LLMs）和搜索引擎，在文档语料库中生成响应，但在关键环境中的可靠性仍存在问题，如幻觉。本研究旨在通过评估RAG系统在投资基金尽职调查中的应用来提高RAG系统的可靠性。

Method: 本研究采用结合人工注释和LLM-Judge注释的评估协议，受PPI方法启发，实现了精确性能测量。提供了用于评估的全面数据集。

Result: 通过提出的评估协议和方法，精确测量了RAG系统的性能，并提供了全面数据集。研究旨在增强RAG系统在工业应用中的可靠性和可扩展性。

Conclusion: 研究评估了在投资基金尽职调查中使用的RAG系统，提出了结合人工注释和LLM-Judge注释的可靠评估协议，以识别系统失败，如幻觉、离题、引用错误和弃权。通过受“Prediction Powered Inference (PPI)”方法启发，实现了具有统计保证的精确性能测量。提供了全面的数据集以供进一步分析。研究旨在增强RAG系统在工业应用中的可靠性和可扩展性。

Abstract: The rise of generative AI, has driven significant advancements in high-risk
sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG)
architecture, combining language models (LLMs) with search engines, is
particularly notable for its ability to generate responses from document
corpora. Despite its potential, the reliability of RAG systems in critical
contexts remains a concern, with issues such as hallucinations persisting. This
study evaluates a RAG system used in due diligence for an investment fund. We
propose a robust evaluation protocol combining human annotations and LLM-Judge
annotations to identify system failures, like hallucinations, off-topic, failed
citations, and abstentions. Inspired by the Prediction Powered Inference (PPI)
method, we achieve precise performance measurements with statistical
guarantees. We provide a comprehensive dataset for further analysis. Our
contributions aim to enhance the reliability and scalability of RAG systems
evaluation protocols in industrial applications.

</details>


### [50] [Hybrid Causal Identification and Causal Mechanism Clustering](https://arxiv.org/abs/2507.21792)
*Saixiong Liu,Yuhua Qian,Jue Li,Honghong Cheng,Feijiang Li*

Main category: cs.AI

TL;DR: 本文提出了MCVCI和MCVCC方法，用于推断异质因果关系。实验证明这些方法在多个数据集上表现出显著的有效性和最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，观测数据通常在不同环境中收集，具有异质因果关系。因此，本文针对观测数据，提出了MCVCI模型用于推断异质因果关系，以填补现有方法只使用单一因果机制构建因果模型的不足。

Method: 结合Hybrid Additive Noise Model的可辨识性，MCVCI利用高斯混合模型和神经网络的优越拟合能力，使用概率界得到的似然作为因果决策标准。同时，将因果异质性建模为簇数，提出MCVCC方法以揭示因果机制表达。

Result: 在多个模拟和真实数据集上，与现有方法相比，MCVCI和MCVCC方法表现出最佳的综合性能。

Conclusion: 本文提出了一种混合条件变分因果推断模型（MCVCI）和混合条件变分因果聚类（MCVCC）方法，用于推断异质因果关系。实验结果表明，这些方法在多个模拟和实际数据集上展现出显著的有效性和最佳性能。

Abstract: Bivariate causal direction identification is a fundamental and vital problem
in the causal inference field. Among binary causal methods, most methods based
on additive noise only use one single causal mechanism to construct a causal
model. In the real world, observations are always collected in different
environments with heterogeneous causal relationships. Therefore, on observation
data, this paper proposes a Mixture Conditional Variational Causal Inference
model (MCVCI) to infer heterogeneous causality. Specifically, according to the
identifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the
superior fitting capabilities of the Gaussian mixture model and the neural
network and elegantly uses the likelihoods obtained from the probabilistic
bounds of the mixture conditional variational auto-encoder as causal decision
criteria. Moreover, we model the casual heterogeneity into cluster numbers and
propose the Mixture Conditional Variational Causal Clustering (MCVCC) method,
which can reveal causal mechanism expression. Compared with state-of-the-art
methods, the comprehensive best performance demonstrates the effectiveness of
the methods proposed in this paper on several simulated and real data.

</details>


### [51] [MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE](https://arxiv.org/abs/2507.21802)
*Junzhe Li,Yutao Cui,Tao Huang,Yinping Ma,Chun Fan,Miles Yang,Zhao Zhong*

Main category: cs.AI

TL;DR: 本文提出了MixGRPO和MixGRPO-Flash框架，通过集成随机微分方程（SDE）和普通微分方程（ODE）来优化MDP内的优化过程。通过引入滑动窗口机制来改善采样策略，MixGRPO在人类偏好一致性图像生成模型中表现出卓越的效率和性能。MixGRPO减少了近50%的训练时间，而MixGRPO-Flash进一步减少了71%的训练时间。


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the inefficiency of existing methods like FlowGRPO in image generation models due to the necessity of sampling and optimizing over all denoising steps specified by MDP. The goal is to improve efficiency and boost performance by introducing mixed sampling strategies through the integration of SDE and ODE.

Method: The paper proposes a novel framework MixGRPO that integrates stochastic differential equations (SDE) and ordinary differential equations (ODE) to streamline the optimization process within the Markov Decision Process (MDP). It introduces a sliding window mechanism for sampling strategies, using SDE within the window and ODE outside, to reduce optimization overhead and accelerate convergence.

Result: MixGRPO and MixGRPO-Flash show substantial gains in human preference alignment of image generation models, outperforming DanceGRPO in both effectiveness and efficiency. MixGRPO reduces training time by nearly 50%, and MixGRPO-Flash further reduces it by 71%.

Conclusion: MixGRPO and MixGRPO-Flash proposed in this paper outperform existing methods in terms of efficiency and performance in human preference alignment of image generation models.

Abstract: Although GRPO substantially enhances flow matching models in human preference
alignment of image generation, methods such as FlowGRPO still exhibit
inefficiency due to the necessity of sampling and optimizing over all denoising
steps specified by the Markov Decision Process (MDP). In this paper, we propose
$\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed
sampling strategies through the integration of stochastic differential
equations (SDE) and ordinary differential equations (ODE). This streamlines the
optimization process within the MDP to improve efficiency and boost
performance. Specifically, MixGRPO introduces a sliding window mechanism, using
SDE sampling and GRPO-guided optimization only within the window, while
applying ODE sampling outside. This design confines sampling randomness to the
time-steps within the window, thereby reducing the optimization overhead, and
allowing for more focused gradient updates to accelerate convergence.
Additionally, as time-steps beyond the sliding window are not involved in
optimization, higher-order solvers are supported for sampling. So we present a
faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves
training efficiency while achieving comparable performance. MixGRPO exhibits
substantial gains across multiple dimensions of human preference alignment,
outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%
lower training time. Notably, MixGRPO-Flash further reduces training time by
71%. Codes and models are available at
$\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.

</details>


### [52] [An Agentic AI for a New Paradigm in Business Process Development](https://arxiv.org/abs/2507.21823)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理人人工智能的业务流程设计和开发方法，旨在实现更灵活和上下文感知的自动化。通过代理人实现一组业务目标，促进业务过程发展，使业务流程更模块化和智能化。


<details>
  <summary>Details</summary>
Motivation: 人工智能代理人代表着工业自动化持续技术演进中的下一次重大革命。本文的动机在于提出一种新的方法，为业务流程设计和开发带来了新的可能性，利用了代理人人工智能的能力。与传统的基于任务的业务流程设计方法不同，我们提出了基于代理人的方法。

Method: 引入了基于代理人的方法，代理人通过实现一组业务目标（通过一组业务对象标识）来促进业务过程的设计和发展。当单个代理人无法实现目标时，可以通过多个代理人的合作实现合并目标。该模型围绕目标、对象和代理人组织业务流程开发，实现了更模块化和智能化的业务流程。

Result: 通过组织业务流程围绕目标、对象和代理人，提出的方法实现了更灵活和上下文感知的自动化。

Conclusion: 提出了一种基于代理人人工智能的方法，用于业务流程设计和开发，能够在动态工业环境中实现灵活和上下文感知的自动化。

Abstract: Artificial Intelligence agents represent the next major revolution in the
continuous technological evolution of industrial automation. In this paper, we
introduce a new approach for business process design and development that
leverages the capabilities of Agentic AI. Departing from the traditional
task-based approach to business process design, we propose an agent-based
method, where agents contribute to the achievement of business goals,
identified by a set of business objects. When a single agent cannot fulfill a
goal, we have a merge goal that can be achieved through the collaboration of
multiple agents. The proposed model leads to a more modular and intelligent
business process development by organizing it around goals, objects, and
agents. As a result, this approach enables flexible and context-aware
automation in dynamic industrial environments.

</details>


### [53] [DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework](https://arxiv.org/abs/2507.21830)
*Kuiye Ding,Fanda Fan,Yao Wang,Ruijie jian,Xiaorui Wang,Luqi Gong,Yishan Jiang,Chunjie Luo an Jianfeng Zhan*

Main category: cs.AI

TL;DR: 本文提出了一种名为DualSG的框架，将LLMs作为语义引导模块，提供明确的语义指导。引入了Time Series Caption和基于字幕引导的融合模块，实验证明该框架在真实数据集上优于15种基准模型。


<details>
  <summary>Details</summary>
Motivation: 既有的方法将LLMs视为端到端的预测器，可能导致数值精度损失，也可能迫使LLMs处理其设计意图之外的模式。尝试在潜在空间中对齐文本和时间序列模态往往遇到对齐困难。因此，我们提出了一种新颖的方法，旨在明确结合数值预测和语义引导，以应对上述问题。

Method: 在本文中，我们提出了DualSG框架，采用LLMs作为语义引导模块，引入Time Series Caption和基于字幕引导的融合模块，以明确组合数值预测和语义引导。通过在真实世界数据集上进行实验验证了方法的有效性。

Result: 通过实验结果表明，DualSG框架在多领域真实数据集上优于15种最先进的基准模型，验证了该方法的有效性和优越性。

Conclusion: 在本文中，我们提出了一种名为DualSG的双流框架，将LLMs作为语义引导模块而非独立预测器，以提供明确的语义指导，并介绍了Time Series Caption和基于字幕引导的融合模块。实验证明，DualSG在真实世界的多样领域数据集上始终优于15种最先进的基准模型，显示了将数值预测与语义引导明确结合的价值。

Abstract: Multivariate Time Series Forecasting plays a key role in many applications.
Recent works have explored using Large Language Models for MTSF to take
advantage of their reasoning abilities. However, many methods treat LLMs as
end-to-end forecasters, which often leads to a loss of numerical precision and
forces LLMs to handle patterns beyond their intended design. Alternatively,
methods that attempt to align textual and time series modalities within latent
space frequently encounter alignment difficulty. In this paper, we propose to
treat LLMs not as standalone forecasters, but as semantic guidance modules
within a dual-stream framework. We propose DualSG, a dual-stream framework that
provides explicit semantic guidance, where LLMs act as Semantic Guides to
refine rather than replace traditional predictions. As part of DualSG, we
introduce Time Series Caption, an explicit prompt format that summarizes trend
patterns in natural language and provides interpretable context for LLMs,
rather than relying on implicit alignment between text and time series in the
latent space. We also design a caption-guided fusion module that explicitly
models inter-variable relationships while reducing noise and computation.
Experiments on real-world datasets from diverse domains show that DualSG
consistently outperforms 15 state-of-the-art baselines, demonstrating the value
of explicitly combining numerical forecasting with semantic guidance.

</details>


### [54] [Probabilistic Active Goal Recognition](https://arxiv.org/abs/2507.21846)
*Chenyuan Zhang,Cristian Rojas Cardenas,Hamid Rezatofighi,Mor Vered,Buser Say*

Main category: cs.AI

TL;DR: 研究提出了一种积极目标识别（AGR）方法，结合概率框架和蒙特卡洛树搜索算法，在多智能体环境中能有效推断他人隐藏目标。实验证明该方法优于被动目标识别，且在领域无关情况下表现良好，推动了多代理系统的发展。


<details>
  <summary>Details</summary>
Motivation: 之前的目标识别研究主要将观察者视为被动推理者，而AGR关注通过策略性信息收集来减少不确定性。研究的动机是在多智能体环境中，有效的互动取决于理解其他智能体的信念和意图。

Method: 采用概率框架的积极目标识别（AGR）方法，结合联合信念更新机制和蒙特卡洛树搜索（MCTS）算法，允许观察者有效规划并推断行为者的隐藏目标。

Result: 经过全面实证评估，研究表明该联合信念更新明显优于被动目标识别，而MCTS算法表现与领域特定的贪婪基准相当。

Conclusion: 该研究提出了一种积极目标识别（AGR）方法，通过结合概率框架和蒙特卡洛树搜索算法，能够实现高效推断他人隐藏目标。实证评估表明，该联合信念更新显著优于被动目标识别，而基于MCTS的算法在领域无关情况下表现与领域特定的贪婪基准相当。因此，该研究提出的框架在目标推断方面具有实用性和鲁棒性，推动了交互式和自适应多代理系统的发展。

Abstract: In multi-agent environments, effective interaction hinges on understanding
the beliefs and intentions of other agents. While prior work on goal
recognition has largely treated the observer as a passive reasoner, Active Goal
Recognition (AGR) focuses on strategically gathering information to reduce
uncertainty. We adopt a probabilistic framework for Active Goal Recognition and
propose an integrated solution that combines a joint belief update mechanism
with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan
efficiently and infer the actor's hidden goal without requiring domain-specific
knowledge. Through comprehensive empirical evaluation in a grid-based domain,
we show that our joint belief update significantly outperforms passive goal
recognition, and that our domain-independent MCTS performs comparably to our
strong domain-specific greedy baseline. These results establish our solution as
a practical and robust framework for goal inference, advancing the field toward
more interactive and adaptive multi-agent systems.

</details>


### [55] [EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity](https://arxiv.org/abs/2507.21848)
*Xingjian Zhang,Siwei Wen,Wenjun Wu,Lei Huang*

Main category: cs.AI

TL;DR: 该研究提出EDGE-GRPO算法，通过采用Entropy-Driven Advantage和Guided Error Correction方法，有效缓解了优势坍塌问题，并在多个主要推理基准测试上展示了方法的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的工作通常从两个方面解决优势坍塌问题：强制模型反思以增强响应多样性，以及引入内部反馈以增强训练信号（优势）。

Method: 该研究首先分析了模型反思的局限性，并研究了细粒度样本级别响应的策略熵。然后基于实验结果，提出了EDGE-GRPO算法。

Result: 经过大量实验证明，新算法在几个主要推理基准上具有显著的效果和优势。

Conclusion: 该研究提出了EDGE-GRPO算法，通过采用Entropy-Driven Advantage和Guided Error Correction方法，有效缓解了优势坍塌问题，并在多个主要推理基准测试上展示了方法的有效性和优越性。

Abstract: Large Language Models (LLMs) have made remarkable progress in enhancing
step-by-step reasoning through reinforcement learning. However, the Group
Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward
rules, often encounters the issue of identical rewards within groups, leading
to the advantage collapse problem. Existing works typically address this
challenge from two perspectives: enforcing model reflection to enhance response
diversity, and introducing internal feedback to augment the training signal
(advantage). In this work, we begin by analyzing the limitations of model
reflection and investigating the policy entropy of responses at the
fine-grained sample level. Based on our experimental findings, we propose the
EDGE-GRPO algorithm, which adopts \textbf{E}ntropy-\textbf{D}riven Advantage
and \textbf{G}uided \textbf{E}rror Correction to effectively mitigate the
problem of advantage collapse. Extensive experiments on several main reasoning
benchmarks demonstrate the effectiveness and superiority of our approach. It is
available at https://github.com/ZhangXJ199/EDGE-GRPO.

</details>


### [56] [MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors](https://arxiv.org/abs/2507.21872)
*Shouyi Lu,Zihan Lin,Chao Lu,Huanran Wang,Guirong Zhuo,Lianqing Zheng*

Main category: cs.AI

TL;DR: 提出了MultiEditor框架，通过引入3D高斯喷洒先验和多级外观控制机制，在驾驶场景中联合编辑图像和LiDAR点云，实现高保真重建和模态间一致性。实验结果表明MultiEditor在视觉和几何保真度、编辑可控性和模态间一致性方面表现优越，并通过生成罕见类别的车辆数据显著提高感知模型的检测精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖于多模态感知数据来理解复杂环境，但现实世界数据的长尾分布阻碍了泛化能力，特别是对于罕见但安全关键的车辆类别。为了解决这一挑战，提出了MultiEditor，旨在改善感知模型对罕见类别的检测精度。

Method: 提出了MultiEditor，其核心是引入3D高斯喷洒（3DGS）作为目标对象的结构和外观先验，设计了多级外观控制机制，并进一步提出深度引导的可变形跨模态条件模块。通过在驾驶场景中联合编辑图像和LiDAR点云，实现高保真重建和模态间一致性。

Result: 经过广泛实验，MultiEditor在视觉和几何保真度、编辑可控性和模态间一致性方面表现出优越性能，生成罕见类别的车辆数据有助于提高感知模型的检测精度。

Conclusion: 提出了一种名为MultiEditor的双分支潜在扩散框架，用于在驾驶场景中联合编辑图像和LiDAR点云。通过引入3D高斯喷洒（3DGS）作为目标对象的结构和外观先验，设计了多级外观控制机制，包括像素级粘贴、语义级指导和多分支细化，实现模态间的高保真重建。进一步提出了深度引导的可变形跨模态条件模块，使用3DGS生成的深度自适应地实现模态间的互相指导，在视觉和几何保真度、编辑可控性和模态间一致性方面取得了卓越性能。通过MultiEditor生成罕见类别的车辆数据，显著提升了感知模型对少数类的检测精度。

Abstract: Autonomous driving systems rely heavily on multimodal perception data to
understand complex environments. However, the long-tailed distribution of
real-world data hinders generalization, especially for rare but safety-critical
vehicle categories. To address this challenge, we propose MultiEditor, a
dual-branch latent diffusion framework designed to edit images and LiDAR point
clouds in driving scenarios jointly. At the core of our approach is introducing
3D Gaussian Splatting (3DGS) as a structural and appearance prior for target
objects. Leveraging this prior, we design a multi-level appearance control
mechanism--comprising pixel-level pasting, semantic-level guidance, and
multi-branch refinement--to achieve high-fidelity reconstruction across
modalities. We further propose a depth-guided deformable cross-modality
condition module that adaptively enables mutual guidance between modalities
using 3DGS-rendered depth, significantly enhancing cross-modality consistency.
Extensive experiments demonstrate that MultiEditor achieves superior
performance in visual and geometric fidelity, editing controllability, and
cross-modality consistency. Furthermore, generating rare-category vehicle data
with MultiEditor substantially enhances the detection accuracy of perception
models on underrepresented classes.

</details>


### [57] [A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data](https://arxiv.org/abs/2507.21873)
*Raffaele Pojer,Andrea Passerini,Kim G. Larsen,Manfred Jaeger*

Main category: cs.AI

TL;DR: 本论文提出了一个神经符号框架，将图神经网络与关系贝叶斯网络无缝集成，结合了学习能力和推理能力，展示了多功能性和性能优势。


<details>
  <summary>Details</summary>
Motivation: 本论文的动机在于解决GNNs在图结构数据预测任务中缺乏符号领域知识和通用推理能力的问题。通过将GNNs与RBNs集成，旨在结合二者的优势以应对多样任务。

Method: 该论文的方法是将GNNs无缝集成到RBNs中，提出了两种集成方法，并引入了MAP推断方法。论文通过两个具体问题的应用展示了框架的多功能性。

Result: 通过两个具体问题的应用，论文展示了神经符号框架的多功能性和性能优越性。提出的方法和推断技术为图数据领域带来了新的可能性。

Conclusion: 这篇论文提出了一个神经符号框架，将图神经网络（GNNs）与关系贝叶斯网络（RBNs）无缝集成，结合了GNNs的学习能力和RBNs的灵活推理能力。它展示了两种集成方法，并提出了最大后验（MAP）推断方法。通过两个不同问题的应用，展示了该框架的多功能性和性能优势。论文桥接了学习与推理，在图数据领域开创了新的应用和性能改进。

Abstract: Graph neural networks (GNNs) excel at predictive tasks on graph-structured
data but often lack the ability to incorporate symbolic domain knowledge and
perform general reasoning. Relational Bayesian Networks (RBNs), in contrast,
enable fully generative probabilistic modeling over graph-like structures and
support rich symbolic knowledge and probabilistic inference. This paper
presents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,
combining the learning strength of GNNs with the flexible reasoning
capabilities of RBNs.
  We develop two implementations of this integration: one compiles GNNs
directly into the native RBN language, while the other maintains the GNN as an
external component. Both approaches preserve the semantics and computational
properties of GNNs while fully aligning with the RBN modeling paradigm. We also
propose a maximum a-posteriori (MAP) inference method for these neuro-symbolic
models.
  To demonstrate the framework's versatility, we apply it to two distinct
problems. First, we transform a GNN for node classification into a collective
classification model that explicitly models homo- and heterophilic label
patterns, substantially improving accuracy. Second, we introduce a
multi-objective network optimization problem in environmental planning, where
MAP inference supports complex decision-making. Both applications include new
publicly available benchmark datasets.
  This work introduces a powerful and coherent neuro-symbolic approach to graph
data, bridging learning and reasoning in ways that enable novel applications
and improved performance across diverse tasks.

</details>


### [58] [Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis](https://arxiv.org/abs/2507.21875)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 研究提出了Tiny-BioMoE模型用于生物信号分析，在多模态框架下展现出对自动疼痛识别任务的有效性。


<details>
  <summary>Details</summary>
Motivation: 疼痛是一种复杂而普遍存在的病症，对于疼痛患者以及在医疗系统中制定有效管理策略至关重要。自动疼痛评估系统可以实现持续监测，支持临床决策，并有助于减轻患者痛苦，降低功能恶化风险。

Method: 研究利用生理信号为基础，采用多模态框架，提出了Tiny-BioMoE模型用于生物信号分析。

Result: 通过本研究，提出的Tiny-BioMoE模型在生物信号分析方面取得了显著成果，有效提高了自动疼痛识别任务的性能。

Conclusion: 该研究提出了Tiny-BioMoE模型，用于生物信号分析，经过大量实验验证在多种模态下的自动疼痛识别任务中表现出有效性。

Abstract: Pain is a complex and pervasive condition that affects a significant portion
of the population. Accurate and consistent assessment is essential for
individuals suffering from pain, as well as for developing effective management
strategies in a healthcare system. Automatic pain assessment systems enable
continuous monitoring, support clinical decision-making, and help minimize
patient distress while mitigating the risk of functional deterioration.
Leveraging physiological signals offers objective and precise insights into a
person's state, and their integration in a multimodal framework can further
enhance system performance. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed approach introduces \textit{Tiny-BioMoE}, a lightweight pretrained
embedding model for biosignal analysis. Trained on $4.4$ million biosignal
image representations and consisting of only $7.3$ million parameters, it
serves as an effective tool for extracting high-quality embeddings for
downstream tasks. Extensive experiments involving electrodermal activity, blood
volume pulse, respiratory signals, peripheral oxygen saturation, and their
combinations highlight the model's effectiveness across diverse modalities in
automatic pain recognition tasks. \textit{\textcolor{blue}{The model's
architecture (code) and weights are available at
https://github.com/GkikasStefanos/Tiny-BioMoE.

</details>


### [59] [Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image](https://arxiv.org/abs/2507.21881)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 本研究旨在提出一种自动评估疼痛系统，利用生理信号进行连续监测以支持临床决策，并旨在减轻痛苦和防止功能衰退。研究表明，提出的方法通过广泛实验展示了其有效性，具有优越的结果，成为整合不同信号表示或模态的强大替代方法。


<details>
  <summary>Details</summary>
Motivation: 疼痛是一个影响大量人群的多方面现象，可靠一致的评估有益于正在经历疼痛的人，并支持有效和先进的管理策略的发展。

Method: 本研究采用自动评估疼痛系统，利用生理信号进行连续监测以支持临床决策，并旨在减轻痛苦和防止功能衰退。通过将生理信号纳入系统，提供客观准确的个体状况洞察。研究结合了各种处理和过滤技术，以及多种表示组合的广泛实验，展示了提出方法的有效性。

Result: 通过实验研究，本方法表现出较传统融合方法更优越的结果，证明其作为不同信号表示或模态整合的稳健替代方法。

Conclusion: 本研究提出了一种利用皮肤电活动信号作为输入模态的管道，通过创建多个信号表示并将它们联合可视化，实现对个体状态的客观准确洞察。研究表明，该方法在多次实验中始终产生可比较甚至更优越的结果，与传统融合方法相比表现出卓越的效果，为整合不同信号表示或模态提供了稳健的选择。

Abstract: Pain is a multifaceted phenomenon that affects a substantial portion of the
population. Reliable and consistent evaluation benefits those experiencing pain
and underpins the development of effective and advanced management strategies.
Automatic pain-assessment systems deliver continuous monitoring, inform
clinical decision-making, and aim to reduce distress while preventing
functional decline. By incorporating physiological signals, these systems
provide objective, accurate insights into an individual's condition. This study
has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for
Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline
that leverages electrodermal activity signals as input modality. Multiple
representations of the signal are created and visualized as waveforms, and they
are jointly visualized within a single multi-representation diagram. Extensive
experiments incorporating various processing and filtering techniques, along
with multiple representation combinations, demonstrate the effectiveness of the
proposed approach. It consistently yields comparable, and in several cases
superior, results to traditional fusion methods, establishing it as a robust
alternative for integrating different signal representations or modalities.

</details>


### [60] [The Impact of Foundational Models on Patient-Centric e-Health Systems](https://arxiv.org/abs/2507.21882)
*Elmira Onagh,Alireza Davoodi,Maleknaz Nayebi*

Main category: cs.AI

TL;DR: 该研究调查了116个患者中心的医疗应用程序中人工智能功能集成的成熟度。结果显示大多数应用程序仍处于早期阶段，仅少部分展示了先进的集成。


<details>
  <summary>Details</summary>
Motivation: 在人工智能日益融入医疗技术的背景下，了解AI在以患者为中心的应用中的成熟度对于评估其可信度、透明度和实际影响至关重要。

Method: 使用大型语言模型（LLMs）提取关键功能特征，然后将其分类为Gartner人工智能成熟度模型的不同阶段。

Result: 超过86.21%的应用程序仍处于人工智能集成的早期阶段，仅有13.79%展示了先进的人工智能集成。

Conclusion: 该研究探讨了116个以患者为中心的医疗应用程序中人工智能功能集成的成熟度。研究结果显示超过86.21%的应用程序仍处于人工智能集成的早期阶段，仅有13.79%展示了先进的人工智能集成。

Abstract: As Artificial Intelligence (AI) becomes increasingly embedded in healthcare
technologies, understanding the maturity of AI in patient-centric applications
is critical for evaluating its trustworthiness, transparency, and real-world
impact. In this study, we investigate the integration and maturity of AI
feature integration in 116 patient-centric healthcare applications. Using Large
Language Models (LLMs), we extracted key functional features, which are then
categorized into different stages of the Gartner AI maturity model. Our results
show that over 86.21\% of applications remain at the early stages of AI
integration, while only 13.79% demonstrate advanced AI integration.

</details>


### [61] [Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline](https://arxiv.org/abs/2507.21886)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 疼痛评估是关键，该研究提出了利用呼吸作为输入信号的多窗口方法，在疼痛评估中取得了有价值的成果。实验证明，紧凑高效的模型在适当优化后能取得出色性能。


<details>
  <summary>Details</summary>
Motivation: 准确和一致的疼痛评估对于经历疼痛的个体至关重要，支持有效和先进的管理策略的发展。

Method: 研究引入了一个利用呼吸作为输入信号的流水线，结合高效的交叉注意力变换器和多窗口策略。通过大量实验证明了呼吸对于疼痛评估是一种有价值的生理模态。

Result: 实验结果显示，提出的多窗口方法能有效捕获短期和长期特征以及全局特征，增强了模型的表征能力。紧凑高效的模型在适当优化后能够取得出色的性能，常常超越较大的对照模型。

Conclusion: 疼痛是一个影响大部分人口的复杂病症。自动疼痛评估系统在持续监测和支持临床决策方面发挥着重要作用，有助于减轻痛苦和预防功能下降。研究表明，呼吸是一种有价值的生理信号模态用于疼痛评估。提出的多窗口方法能够有效捕获短期和长期特征以及全局特征，增强了模型的表征能力。紧凑高效的模型在适当优化后能够取得出色的性能，常常超越较大的对照模型。

Abstract: Pain is a complex condition affecting a large portion of the population.
Accurate and consistent evaluation is essential for individuals experiencing
pain, and it supports the development of effective and advanced management
strategies. Automatic pain assessment systems provide continuous monitoring and
support clinical decision-making, aiming to reduce distress and prevent
functional decline. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed method introduces a pipeline that leverages respiration as the input
signal and incorporates a highly efficient cross-attention transformer
alongside a multi-windowing strategy. Extensive experiments demonstrate that
respiration is a valuable physiological modality for pain assessment. Moreover,
experiments revealed that compact and efficient models, when properly
optimized, can achieve strong performance, often surpassing larger
counterparts. The proposed multi-window approach effectively captures both
short-term and long-term features, as well as global characteristics, thereby
enhancing the model's representational capacity.

</details>


### [62] [LLM-based Content Classification Approach for GitHub Repositories by the README Files](https://arxiv.org/abs/2507.21899)
*Malik Uzair Mehmood,Shahid Hussain,Wen Li Wang,Muhammad Usama Malik*

Main category: cs.AI

TL;DR: 该研究关注GitHub仓库的README文件完整性对其采纳和利用的影响，使用LLMs进行自动分类器的设计，并展示了参数高效微调技术的潜力，为改进GitHub仓库的标识和潜在用途方面做出了贡献。


<details>
  <summary>Details</summary>
Motivation: GitHub仓库所有者有时忽视GitHub的推荐， README文件内容不完整可能阻碍GitHub仓库发挥其全部潜力，本研究旨在探讨README文件完整性对GitHub仓库的影响，并开发自动分类器来提高GitHub仓库的利用率。

Method: 使用三个仅编码器LLMs（BERT、DistilBERT和RoBERTa）对4226个README文件部分进行微调，采用参数高效微调技术，优于目前的最新方法，在整体F1分数上达到0.98。

Result: 研究结果表明，使用LLMs设计自动分类器可有效改进GitHub仓库的标识和潜在用途，通过参数高效微调技术可以节约成本而不降低性能。

Conclusion: 研究发现GitHub仓库的README文件的完整性显著影响其被采纳和利用程度，缺乏细节可能阻碍其在研究社区中产生广泛的影响。研究开发了一种方法，通过微调大型语言模型（LLMs）来自动分类GitHub README文件的不同部分，取得了0.98的整体F1分数，并展示了参数高效微调技术的潜力。研究为GitHub仓库设计自动分类器的潜力以及自动化工具的发展做出了贡献。

Abstract: GitHub is the world's most popular platform for storing, sharing, and
managing code. Every GitHub repository has a README file associated with it.
The README files should contain project-related information as per the
recommendations of GitHub to support the usage and improvement of repositories.
However, GitHub repository owners sometimes neglected these recommendations.
This prevents a GitHub repository from reaching its full potential. This
research posits that the comprehensiveness of a GitHub repository's README file
significantly influences its adoption and utilization, with a lack of detail
potentially hindering its full potential for widespread engagement and impact
within the research community. Large Language Models (LLMs) have shown great
performance in many text-based tasks including text classification, text
generation, text summarization and text translation. In this study, an approach
is developed to fine-tune LLMs for automatically classifying different sections
of GitHub README files. Three encoder-only LLMs are utilized, including BERT,
DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a
gold-standard dataset consisting of 4226 README file sections. This approach
outperforms current state-of-the-art methods and has achieved an overall F1
score of 0.98. Moreover, we have also investigated the use of
Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation
(LoRA) and shown an economical alternative to full fine-tuning without
compromising much performance. The results demonstrate the potential of using
LLMs in designing an automatic classifier for categorizing the content of
GitHub README files. Consequently, this study contributes to the development of
automated tools for GitHub repositories to improve their identifications and
potential usages.

</details>


### [63] [Libra: Large Chinese-based Safeguard for AI Content](https://arxiv.org/abs/2507.21929)
*Ziyang Chen,Huimu Yu,Xing Wu,Dongqin Liu,Songlin Hu*

Main category: cs.AI

TL;DR: Libra-Guard是一个新颖的安全系统，通过两阶段课程训练提高了数据效率，采用Libra-Test评估安全系统效果。实验证明Libra-Guard在中文内容上取得了显著成果，为中国AI系统的安全发展迈出了关键一步。


<details>
  <summary>Details</summary>
Motivation: LLMs在文字理解和生成方面表现出色，但在高风险应用中存在重大安全和伦理顾虑。为减轻这些风险，研究致力于提出Libra-Guard安全系统，以增强中文LLMs的安全性。

Method: Libra-Guard采用两阶段课程训练流水线，在合成样本上进行守护预训练，然后在高质量的现实数据上进行微调，显著降低对手动标注的依赖。引入了Libra-Test，旨在评估中文内容保护系统的效果，涵盖七个关键伤害情景，包括由领域专家标注的5700多个样本。

Result: Libra-Guard取得了令人满意的结果，比其他模型表现更好，为增强中文LLMs的安全管理奠定了坚实的框架。

Conclusion: Libra-Guard是一个先进的安全系统，用于增强基于中文的大型语言模型（LLMs）的安全性。在七个关键伤害情景下，Libra-Guard实现了86.79%的准确率，优于Qwen2.5-14B-Instruct（74.33%）和ShieldLM-Qwen-14B-Chat（65.69%），接近像Claude-3.5-Sonnet和GPT-4o这样的闭源模型。该研究为推进中文LLMs的安全管理奠定了坚实的框架，是发展更安全、更可靠的中文人工智能系统的迈出的一步。

Abstract: Large language models (LLMs) excel in text understanding and generation but
raise significant safety and ethical concerns in high-stakes applications. To
mitigate these risks, we present Libra-Guard, a cutting-edge safeguard system
designed to enhance the safety of Chinese-based LLMs. Leveraging a two-stage
curriculum training pipeline, Libra-Guard enhances data efficiency by employing
guard pretraining on synthetic samples, followed by fine-tuning on
high-quality, real-world data, thereby significantly reducing reliance on
manual annotations. To enable rigorous safety evaluations, we also introduce
Libra-Test, the first benchmark specifically designed to evaluate the
effectiveness of safeguard systems for Chinese content. It covers seven
critical harm scenarios and includes over 5,700 samples annotated by domain
experts. Experiments show that Libra-Guard achieves 86.79% accuracy,
outperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat
(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.
These contributions establish a robust framework for advancing the safety
governance of Chinese LLMs and represent a tentative step toward developing
safer, more reliable Chinese AI systems.

</details>


### [64] [Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities](https://arxiv.org/abs/2507.21964)
*Sourish Gunesh Dhekane,Thomas Ploetz*

Main category: cs.AI

TL;DR: 本研究提出一种零-shot 人类活动识别方法，通过自然语言模型对传感器数据和活动进行零-shot 分类，无需提示 LLM。研究在六个数据集上展示案例研究，表明语言建模可以加强零-shot 识别的 HAR 系统，避免了“提示-LLM”方法的风险。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于当前“提示-LLM”方法存在隐私侵犯、依赖外部服务和由于版本更改导致预测不一致等风险，因此提出了不需要提示 LLM 的零-shot HAR 方法。

Method: 本研究使用自然语言描述传感器数据，并通过精心设计的提示将其馈送给 LLM 进行分类。提出了一种模型传感器数据和活动的解决方案，利用自然语言的嵌入执行零-shot 分类，从而避免提示 LLM 对活动进行预测。

Result: 通过在六个数据集上展示案例研究，表明用自然语言建模可以加强零-shot 识别的 HAR 系统。提出的方法避免了“提示-LLM”方法的风险。

Conclusion: 本研究提出了一种零-shot 人类活动识别方法，不需要对 LLM 进行提示，可以通过自然语言模型传感器数据和活动进行零-shot 分类。通过在六个数据集上展示详细案例研究，突显了语言建模如何在零-shot 识别中加强 HAR 系统。

Abstract: Developing zero-shot human activity recognition (HAR) methods is a critical
direction in smart home research -- considering its impact on making HAR
systems work across smart homes having diverse sensing modalities, layouts, and
activities of interest. The state-of-the-art solutions along this direction are
based on generating natural language descriptions of the sensor data and
feeding it via a carefully crafted prompt to the LLM to perform classification.
Despite their performance guarantees, such ``prompt-the-LLM'' approaches carry
several risks, including privacy invasion, reliance on an external service, and
inconsistent predictions due to version changes, making a case for alternative
zero-shot HAR methods that do not require prompting the LLMs. In this paper, we
propose one such solution that models sensor data and activities using natural
language, leveraging its embeddings to perform zero-shot classification and
thereby bypassing the need to prompt the LLMs for activity predictions. The
impact of our work lies in presenting a detailed case study on six datasets,
highlighting how language modeling can bolster HAR systems in zero-shot
recognition.

</details>


### [65] [Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks](https://arxiv.org/abs/2507.21974)
*Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Yibin Kang,Haozhe Zhang,Merouane Debbah,Fadhel Ayed*

Main category: cs.AI

TL;DR: 本研究提出了一种基于大型语言模型（LLMs）的轻量级框架，用于在移动网络中进行根本原因分析（RCA）。通过引入经过注释的故障处理问题的TeleLogs数据集，评估现有开源推理LLMs的困难，并提出了两阶段训练方法来改进LLMs的准确性和推理质量。实验结果显示，该方法在多个LLM大小上取得了显著性能提升，并具有强大的泛化能力。展示了领域适应、推理增强的LLMs对于网络运营和管理中的可解释RCA的潜力。


<details>
  <summary>Details</summary>
Motivation: 移动网络中的根本原因分析（RCA）是一个具有挑战性的任务，需要可解释性、领域专业知识和因果推理。现有开源推理LLMs在解决这些问题时存在困难，因此需要领域特定的适应性。为了改善这一问题，需要提出一种方法以提高LLMs的准确性和推理质量，从而实现更好的可解释性和有效性。

Method: 本研究提出了一个轻量级框架，利用大型语言模型（LLMs）进行根本原因分析（RCA）。引入了TeleLogs数据集，并提出了两阶段训练方法，结合监督微调和强化学习以改进LLMs的准确性和推理质量。微调一系列RCA模型，整合领域知识，生成多步诊断解释，提高可解释性和有效性。进行了广泛实验对比多个LLM大小，展示了相较于最先进的推理和非推理模型，本方法具有显著的性能提升和泛化能力。

Result: 通过本研究提出的轻量级框架和两阶段训练方法，在移动网络中可解释的根本原因分析（RCA）方面取得了显著成果。通过广泛实验表明，相较于最先进的推理和非推理模型，本方法具有显著的性能提升和泛化能力。

Conclusion: 本研究提出了一种基于大型语言模型（LLMs）的轻量级框架，用于在移动网络中进行根本原因分析（RCA）。通过引入TeleLogs，一个经过注释的故障处理问题的精心构建数据集，评估揭示现有开源推理LLMs在这些问题上存在困难，强调了领域特定适应性的必要性。为了解决这一问题，提出了一个两阶段训练方法，将监督微调与强化学习相结合，以提高LLMs的准确性和推理质量。该方法微调一系列RCA模型，整合领域知识并生成结构化的多步诊断解释，提高了可解释性和有效性。对多个LLM大小进行广泛实验表明，相较于最先进的推理和非推理模型，包括对随机测试变体具有较强的泛化能力。这些结果展示了领域适应、推理增强的LLMs在网络运营和管理中实际可解释的RCA的潜力。

Abstract: Root Cause Analysis (RCA) in mobile networks remains a challenging task due
to the need for interpretability, domain expertise, and causal reasoning. In
this work, we propose a lightweight framework that leverages Large Language
Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of
annotated troubleshooting problems designed to benchmark RCA capabilities. Our
evaluation reveals that existing open-source reasoning LLMs struggle with these
problems, underscoring the need for domain-specific adaptation. To address this
issue, we propose a two-stage training methodology that combines supervised
fine-tuning with reinforcement learning to improve the accuracy and reasoning
quality of LLMs. The proposed approach fine-tunes a series of RCA models to
integrate domain knowledge and generate structured, multi-step diagnostic
explanations, improving both interpretability and effectiveness. Extensive
experiments across multiple LLM sizes show significant performance gains over
state-of-the-art reasoning and non-reasoning models, including strong
generalization to randomized test variants. These results demonstrate the
promise of domain-adapted, reasoning-enhanced LLMs for practical and
explainable RCA in network operation and management.

</details>


### [66] [The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain](https://arxiv.org/abs/2507.21976)
*Tanvir Ahmed Khan,Aranya Saha,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.AI

TL;DR: 本文评估了在医学领域中利用MLLMs的潜力，对细化LLAVA模型进行结构剪枝和激活感知量化的影响进行了评估。提出了一种新颖的层选择方法，并分析了不同的量化技术，在剪枝-SFT-量化流程中评估性能权衡。通过提出的方法，成功实现了在4GB VRAM内运行具有70%内存使用减少的7B参数的MLLMs，并比传统方法在相同压缩比下提高了4%的模型性能。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于探讨在医学领域中利用MLLMs的潜力，但是它们的计算成本需要高效的压缩技术来降低。为了实现这一目标，对细化LLAVA模型进行结构剪枝和激活感知量化的影响进行评估，以减少内存占用并提高模型性能。

Method: 本文采用了结构剪枝和激活感知量化的技术，提出了一种层选择方法，评估了不同的量化技术，并考虑了性能权衡。通过剪枝-SFT-量化管道进行实验分析。

Result: 通过提出的方法，成功实现了在4GB VRAM内运行具有70%内存使用减少的7B参数的MLLMs，并比传统方法在相同压缩比下提高了4%的模型性能。

Conclusion: 本文评估了在医学应用中针对细化LLAVA模型的结构剪枝和激活感知量化的影响，提出了一种新颖的层选择方法，并分析了不同的量化技术，在剪枝-SFT-量化流程中评估性能权衡。提出的方法使具有70%内存使用减少的7B参数的MLLMs能够在4 GB VRAM内运行，与传统的剪枝和量化技术相比，在相同的压缩比下实现了4%较高的模型性能。

Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the
medical domain, but their computational costs necessitate efficient compression
techniques. This paper evaluates the impact of structural pruning and
activation-aware quantization on a fine-tuned LLAVA model for medical
applications. We propose a novel layer selection method for pruning, analyze
different quantization techniques, and assess the performance trade-offs in a
prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B
parameters to run within 4 GB of VRAM, reducing memory usage by 70% while
achieving 4% higher model performance compared to traditional pruning and
quantization techniques in the same compression ratio.

</details>


### [67] [PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences](https://arxiv.org/abs/2507.22009)
*Bahar İlgen,Akshat Dubey,Georges Hattab*

Main category: cs.AI

TL;DR: PHAX framework introduces a Public Health Argumentation and Explainability framework that combines reasoning, language techniques, and user modeling to provide human-centered explanations for AI outputs in public health, improving transparency and trust in AI systems.


<details>
  <summary>Details</summary>
Motivation: Ensuring transparency and trust in AI-driven public health and biomedical sciences systems requires explanations that are clear, contextual, and socially accountable. Most current XAI methods lack the structure and adaptability needed for diverse health stakeholders.

Method: PHAX is a multi-layer architecture combining defeasible reasoning, adaptive natural language techniques, and user modeling to produce context-aware, audience-specific justifications.

Result: PHAX demonstrates how argumentation enhances explainability by supporting AI-driven decision-making, justifying recommendations, and enabling interactive dialogues across user types. It is applied in use cases such as medical term simplification, patient-clinician communication, and policy justification, showing personalized decision-making based on user expertise.

Conclusion: PHAX framework leverages structured argumentation to generate human-centered explanations for AI outputs, enhancing transparency and trust in public health AI systems.

Abstract: Ensuring transparency and trust in AI-driven public health and biomedical
sciences systems requires more than accurate predictions-it demands
explanations that are clear, contextual, and socially accountable. While
explainable AI (XAI) has advanced in areas like feature attribution and model
interpretability, most methods still lack the structure and adaptability needed
for diverse health stakeholders, including clinicians, policymakers, and the
general public. We introduce PHAX-a Public Health Argumentation and
eXplainability framework-that leverages structured argumentation to generate
human-centered explanations for AI outputs. PHAX is a multi-layer architecture
combining defeasible reasoning, adaptive natural language techniques, and user
modeling to produce context-aware, audience-specific justifications. More
specifically, we show how argumentation enhances explainability by supporting
AI-driven decision-making, justifying recommendations, and enabling interactive
dialogues across user types. We demonstrate the applicability of PHAX through
use cases such as medical term simplification, patient-clinician communication,
and policy justification. In particular, we show how simplification decisions
can be modeled as argument chains and personalized based on user
expertise-enhancing both interpretability and trust. By aligning formal
reasoning methods with communicative demands, PHAX contributes to a broader
vision of transparent, human-centered AI in public health.

</details>


### [68] [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
*Shuquan Lian,Yuhang Wu,Jia Ma,Zihan Song,Bingqi Chen,Xiawu Zheng,Hui Li*

Main category: cs.AI

TL;DR: UI-AGILE 是一种综合框架，用于增强 GUI 代理的训练和推理性能。提出了连续奖励函数、简单思考奖励和基于裁剪的重采样策略以改进监督微调过程，同时引入了分解对齐与选择方法以提高高分辨率显示器上的对齐精度。在实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的 GUI 代理训练和推理技术存在推理设计、奖励效率和视觉噪声等问题，为解决这些问题，提出了 UI-AGILE 框架。

Method: 提出了连续奖励函数、简单思考奖励和基于裁剪的重采样策略以改进监督微调过程，引入了分解对齐与选择方法以提高高分辨率显示器上的对齐精度。进行了实验证明提出的方法在两个基准测试上取得了显著的性能提升。

Result: 实验证明 UI-AGILE 在 ScreenSpot-Pro 和 ScreenSpot-v2 上取得了最先进的性能，提出的训练和推理增强方法能使对齐精度显著提升。

Conclusion: UI-AGILE 是一种综合框架，旨在增强 GUI 代理训练和推理阶段的性能，通过引入连续奖励函数、简单思考奖励和基于裁剪的重采样策略改进了监督微调过程，并提出了分解对齐与选择方法以提高高分辨率显示器上的对齐精度。实验证明，UI-AGILE 在两个基准测试 ScreenSpot-Pro 和 ScreenSpot-v2 上取得了最先进的性能，其中在 ScreenSpot-Pro 上，使用我们提出的训练和推理增强方法能使对齐精度提升 23%。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing GUI agents at both the training and inference stages. For training,
we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:
1) a Continuous Reward function to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a Cropping-based Resampling strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
Decomposed Grounding with Selection, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks ScreenSpot-Pro and
ScreenSpot-v2. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on ScreenSpot-Pro.

</details>


### [69] [UserBench: An Interactive Gym Environment for User-Centric Agents](https://arxiv.org/abs/2507.22034)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Zhiwei Liu,Jianguo Zhang,Haolin Chen,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: 研究引入了UserBench评估代理在多轮、偏好驱动交互中的能力，发现大型语言模型代理在主动与用户合作方面存在不足，在任务完成和用户一致性之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是为了填补现有研究在大型语言模型代理能够主动与用户合作方面的空白，特别是在处理模糊、变化或间接表达目标时的能力。

Method: 研究引入了UserBench，一个以用户为中心的基准测试，用于评估代理在多轮、基于偏好的交互中的表现。UserBench包括了模拟用户，这些用户从不明确的目标开始，逐步透露偏好，要求代理主动澄清意图并在使用工具时做出有根据的决策。

Result: 通过对主流开源和闭源大型语言模型的评估发现，任务完成与用户一致性之间存在显著差距。最先进的模型仅能通过主动交互揭示不到30％的用户偏好。

Conclusion: 该研究发现了基于大型语言模型的代理在主动与用户合作方面存在明显不足，尤其是在处理模糊、变化或间接表达的目标时。

Abstract: Large Language Models (LLMs)-based agents have made impressive progress in
reasoning and tool use, enabling them to solve complex tasks. However, their
ability to proactively collaborate with users, especially when goals are vague,
evolving, or indirectly expressed, remains underexplored. To address this gap,
we introduce UserBench, a user-centric benchmark designed to evaluate agents in
multi-turn, preference-driven interactions. UserBench features simulated users
who start with underspecified goals and reveal preferences incrementally,
requiring agents to proactively clarify intent and make grounded decisions with
tools. Our evaluation of leading open- and closed-source LLMs reveals a
significant disconnect between task completion and user alignment. For
instance, models provide answers that fully align with all user intents only
20% of the time on average, and even the most advanced models uncover fewer
than 30% of all user preferences through active interaction. These results
highlight the challenges of building agents that are not just capable task
executors, but true collaborative partners. UserBench offers an interactive
environment to measure and advance this critical capability.

</details>


### [70] [The Interspeech 2025 Speech Accessibility Project Challenge](https://arxiv.org/abs/2507.22047)
*Xiuwen Zheng,Bornali Phukon,Jonghwan Na,Ed Cutrell,Kyu Han,Mark Hasegawa-Johnson,Pan-Pan Jiang,Aadhrik Kuila,Colin Lea,Bob MacDonald,Gautam Mantena,Venkatesh Ravichandran,Leda Sari,Katrin Tomanek,Chang D. Yoo,Chris Zwilling*

Main category: cs.AI

TL;DR: 通过2025年Interspeech Speech Accessibility Project (SAP) Challenge，利用超过400小时的数据，评估团队提交的识别语音障碍者语音的结果，其中12个团队在词错误率方面优于基线，17个团队在语义得分方面超过基线。顶尖团队在WER方面达到了8.11%，SemScore最高达到了88.44%，开创了未来ASR系统识别受损语音的新标准。


<details>
  <summary>Details</summary>
Motivation: 论文提出部分患有语音残疾的个体在现有ASR系统下表现不佳的问题，并针对这一问题启动了2025 Interspeech SAP挑战，以弥补公共训练数据的不足。

Method: 该论文利用了超过400小时的SAP数据，并从500多名具有不同语音障碍的个体中采集和转录了这些数据。利用EvalAI和远程评估流水线，对SAP挑战的提交进行评估，评价指标包括词错误率（WER）和语义得分（SemScore）。

Result: 在22个有效团队中，有12个团队在WER方面优于whisper-large-v2基线，17个团队在SemScore方面超过基线。顶尖团队的WER达到了8.11%，SemScore最高达到了88.44%，为未来识别受损语音的ASR系统设立了新的标准。

Conclusion: 该论文总结了2025年Interspeech Speech Accessibility Project (SAP) Challenge的成果，强调了团队在识别语音障碍者的语音方面取得的突破性进展。

Abstract: While the last decade has witnessed significant advancements in Automatic
Speech Recognition (ASR) systems, performance of these systems for individuals
with speech disabilities remains inadequate, partly due to limited public
training data. To bridge this gap, the 2025 Interspeech Speech Accessibility
Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data
collected and transcribed from more than 500 individuals with diverse speech
disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,
the SAP Challenge evaluates submissions based on Word Error Rate and Semantic
Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2
baseline in terms of WER, while 17 teams surpassed the baseline on SemScore.
Notably, the top team achieved the lowest WER of 8.11\%, and the highest
SemScore of 88.44\% at the same time, setting new benchmarks for future ASR
systems in recognizing impaired speech.

</details>
