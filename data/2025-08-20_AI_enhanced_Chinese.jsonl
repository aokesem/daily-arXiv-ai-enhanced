{"id": "2508.13167", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13167", "abs": "https://arxiv.org/abs/2508.13167", "authors": ["Weizhen Li", "Jianbo Lin", "Zhuosong Jiang", "Jingyi Cao", "Xinpeng Liu", "Jiayu Zhang", "Zhenqiang Huang", "Qianben Chen", "Weichen Sun", "Qiexiang Wang", "Hongxuan Lu", "Tianrui Qin", "Chenghao Zhu", "Yi Yao", "Shuying Fan", "Xiaowan Li", "Tiannan Wang", "Pai Liu", "King Zhu", "He Zhu", "Dingfeng Shi", "Piaohong Wang", "Yeyi Guan", "Xiangru Tang", "Minghao Liu", "Yuchen Eleanor Jiang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou"], "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL", "comment": "51 pages", "summary": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLL\u63a8\u7406\u8303\u5f0fChain-of-Agents\uff08CoA\uff09\uff0c\u5f15\u5165\u4e86Agent Foundation Models\uff08AFMs\uff09\u901a\u8fc7\u591a\u4ee3\u63a7\u5236\u6846\u67b6fine-tuning\u548c\u5f3a\u5316\u5b66\u4e60\u3002AFM\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u65b0\u7684\u6027\u80fd\uff0c\u7814\u7a76\u5185\u5bb9\u5b8c\u5168\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4ee3\u7cfb\u7edf\u5728\u590d\u6742\u95ee\u9898\u6c42\u89e3\u4e2d\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u80fd\u529b\u53d7\u9650\u548c\u65e0\u6cd5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684LLL\u63a8\u7406\u8303\u5f0f\uff0c\u5e76\u63a2\u7d22\u6a21\u578bfine-tuning\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86Chain-of-Agents\uff08CoA\uff09\u7684\u65b0LLL\u63a8\u7406\u8303\u5f0f\uff0c\u4f7f\u7528\u591a\u4ee3\u63a7\u5236\u6846\u67b6\u8fdb\u884c\u6a21\u578bfine-tuning\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5f62\u6210Agent Foundation Models\uff08AFMs\uff09\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc1\u660eAFM\u5728web\u4ee3\u7406\u548c\u4ee3\u7801\u4ee3\u7406\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u4e86\u6700\u65b0\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u4e14\u5f00\u6e90\u4e86\u6574\u4e2a\u7814\u7a76\u5185\u5bb9\uff0c\u5305\u62ec\u6a21\u578b\u6743\u91cd\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u4ee3\u7801\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u5f15\u5165\u4e86Chain-of-Agents\uff08CoA\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684LLL\u63a8\u7406\u8303\u5f0f\uff0c\u4f7f\u5f97LLL\u80fd\u591f\u5728\u4e00\u4e2a\u6a21\u578b\u5185\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u590d\u6742\u95ee\u9898\u6c42\u89e3\uff0c\u5e76\u8bc1\u660e\u4e86Agent Foundation Models\uff08AFMs\uff09\u5728\u591a\u79cd\u57fa\u51c6\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0\u7684\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u4ee3\u7406\u6a21\u578b\u548c\u4ee3\u7406RL\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2508.13171", "categories": ["cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13171", "abs": "https://arxiv.org/abs/2508.13171", "authors": ["Tao An"], "title": "Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context", "comment": "13 pages, 1 figure, code available at\n  https://github.com/tao-hpu/cognitive-workspace", "summary": "Large Language Models (LLMs) face fundamental limitations in context\nmanagement despite recent advances extending context windows to millions of\ntokens. We propose Cognitive Workspace, a novel paradigm that transcends\ntraditional Retrieval-Augmented Generation (RAG) by emulating human cognitive\nmechanisms of external memory use. Drawing from cognitive science foundations\nincluding Baddeley's working memory model, Clark's extended mind thesis, and\nHutchins' distributed cognition framework, we demonstrate that current passive\nretrieval systems fail to capture the dynamic, task-driven nature of human\nmemory management. Our analysis of 2024-2025 developments reveals that while\ntechniques like Infini-attention and StreamingLLM achieve impressive context\nlengths, they lack the metacognitive awareness and active planning capabilities\nessential for true cognitive extension. Cognitive Workspace addresses these\nlimitations through three core innovations: (1) active memory management with\ndeliberate information curation, (2) hierarchical cognitive buffers enabling\npersistent working states, and (3) task-driven context optimization that\ndynamically adapts to cognitive demands. Empirical validation demonstrates\nCognitive Workspace achieves an average 58.6% memory reuse rate (ranging from\n54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%\nnet efficiency gain despite 3.3x higher operation counts. Statistical analysis\nconfirms these advantages with p < 0.001 and Cohen's d > 23 across multiple\ntask types, establishing the first quantitative evidence for active memory\nsuperiority in LLM systems. We present a comprehensive theoretical framework\nsynthesizing insights from 50+ recent papers, positioning Cognitive Workspace\nas a fundamental shift from information retrieval to genuine cognitive\naugmentation.", "AI": {"tldr": "Cognitive Workspace proposes a novel paradigm inspired by human cognitive mechanisms to enhance context management in large language models. It introduces active memory management, hierarchical cognitive buffers, and task-driven context optimization. Empirical validation shows improved memory reuse and efficiency gains compared to traditional methods, with statistical evidence supporting its superiority in LLM systems.", "motivation": "Recent advances in extending context windows for LLMs have limitations in capturing the dynamic nature of human memory management. The study aims to transcend traditional Retrieval-Augmented Generation (RAG) systems by integrating metacognitive awareness and active planning capabilities into LLMs.", "method": "The paper proposes the Cognitive Workspace paradigm, inspired by human cognitive mechanisms, to enhance context management in LLMs. It draws from cognitive science theories such as Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework. Three core innovations are highlighted: active memory management, hierarchical cognitive buffers, and task-driven context optimization.", "result": "Empirical validation of Cognitive Workspace demonstrates a 58.6% memory reuse rate and a net efficiency gain compared to traditional RAG systems. Statistical analysis supports the superiority of Cognitive Workspace in LLMs with a confidence level of p < 0.001 and Cohen's d > 23 across various task types.", "conclusion": "Cognitive Workspace introduces active memory management, hierarchical cognitive buffers, and task-driven context optimization to address limitations in current large language models (LLMs). Empirical validation shows a significant memory reuse rate improvement compared to traditional methods, with a net efficiency gain despite higher operation counts. Statistical analysis supports the advantages of Cognitive Workspace across various tasks, establishing its superiority in LLM systems."}}
{"id": "2508.13174", "categories": ["cs.AI", "cs.LG", "q-fin.CP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.13174", "abs": "https://arxiv.org/abs/2508.13174", "authors": ["Hongjun Ding", "Binqi Chen", "Jinsheng Huang", "Taian Guo", "Zhengyang Mao", "Guoyi Shao", "Lutong Zou", "Luchen Liu", "Ming Zhang"], "title": "AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining", "comment": "12 pages, 5 figures", "summary": "Formula alpha mining, which generates predictive signals from financial data,\nis critical for quantitative investment. Although various algorithmic\napproaches-such as genetic programming, reinforcement learning, and large\nlanguage models-have significantly expanded the capacity for alpha discovery,\nsystematic evaluation remains a key challenge. Existing evaluation metrics\npredominantly include backtesting and correlation-based measures. Backtesting\nis computationally intensive, inherently sequential, and sensitive to specific\nstrategy parameters. Correlation-based metrics, though efficient, assess only\npredictive ability and overlook other crucial properties such as temporal\nstability, robustness, diversity, and interpretability. Additionally, the\nclosed-source nature of most existing alpha mining models hinders\nreproducibility and slows progress in this field. To address these issues, we\npropose AlphaEval, a unified, parallelizable, and backtest-free evaluation\nframework for automated alpha mining models. AlphaEval assesses the overall\nquality of generated alphas along five complementary dimensions: predictive\npower, stability, robustness to market perturbations, financial logic, and\ndiversity. Extensive experiments across representative alpha mining algorithms\ndemonstrate that AlphaEval achieves evaluation consistency comparable to\ncomprehensive backtesting, while providing more comprehensive insights and\nhigher efficiency. Furthermore, AlphaEval effectively identifies superior\nalphas compared to traditional single-metric screening approaches. All\nimplementations and evaluation tools are open-sourced to promote\nreproducibility and community engagement.", "AI": {"tldr": "AlphaEval\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8Alpha\u6316\u6398\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u751f\u6210\u7684Alpha\u5728\u9884\u6d4b\u80fd\u529b\u3001\u7a33\u5b9a\u6027\u3001\u5bf9\u5e02\u573a\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3001\u8d22\u52a1\u903b\u8f91\u548c\u591a\u6837\u6027\u7b49\u4e94\u4e2a\u65b9\u9762\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAlphaEval\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u56de\u6d4b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u89c1\u89e3\u548c\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u51fa\u4f18\u79c0\u7684Alpha\u3002\u6240\u6709\u5de5\u5177\u5747\u4e3a\u5f00\u6e90\uff0c\u65e8\u5728\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u548c\u793e\u533a\u53c2\u4e0e\u3002", "motivation": "\u73b0\u6709\u7684Alpha\u6316\u6398\u6a21\u578b\u8bc4\u4f30\u5b58\u5728\u8bf8\u591a\u6311\u6218\uff0c\u5982\u56de\u6d4b\u8ba1\u7b97\u5bc6\u96c6\u3001\u76f8\u5173\u6027\u5ea6\u91cf\u5ffd\u7565\u5176\u4ed6\u91cd\u8981\u5c5e\u6027\u7b49\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86AlphaEval\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u4f9b\u7efc\u5408\u3001\u9ad8\u6548\u4e14\u5f00\u6e90\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u751f\u6210\u7684Alpha\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86AlphaEval\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u81ea\u52a8Alpha\u6316\u6398\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u4e94\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u751f\u6210\u7684Alpha\uff1a\u9884\u6d4b\u80fd\u529b\u3001\u7a33\u5b9a\u6027\u3001\u5bf9\u5e02\u573a\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3001\u8d22\u52a1\u903b\u8f91\u548c\u591a\u6837\u6027\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u5355\u4e00\u6307\u6807\u7b5b\u9009\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u6240\u6709\u5b9e\u73b0\u548c\u8bc4\u4f30\u5de5\u5177\u5747\u4e3a\u5f00\u6e90\uff0c\u4ee5\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u548c\u793e\u533a\u53c2\u4e0e\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cAlphaEval\u8bc4\u4f30\u6846\u67b6\u5b9e\u73b0\u4e86\u4e0e\u5168\u9762\u56de\u6d4b\u76f8\u5ab2\u7f8e\u7684\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89c1\u89e3\u548c\u66f4\u9ad8\u7684\u6548\u7387\u3002\u6b64\u5916\uff0cAlphaEval\u6bd4\u4f20\u7edf\u7684\u5355\u4e00\u6307\u6807\u7b5b\u9009\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u8bc6\u522b\u51fa\u4f18\u79c0\u7684Alpha\u3002\u6240\u6709\u5b9e\u73b0\u548c\u8bc4\u4f30\u5de5\u5177\u7684\u5f00\u6e90\u6709\u52a9\u4e8e\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u793e\u533a\u53c2\u4e0e\u3002", "conclusion": "\u63d0\u51fa\u4e86AlphaEval\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8Alpha\u6316\u6398\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u5728\u9884\u6d4b\u80fd\u529b\u3001\u7a33\u5b9a\u6027\u3001\u5bf9\u5e02\u573a\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3001\u8d22\u52a1\u903b\u8f91\u548c\u591a\u6837\u6027\u7b49\u4e94\u4e2a\u65b9\u9762\u8bc4\u4f30\u751f\u6210\u7684Alpha\u7684\u6574\u4f53\u8d28\u91cf\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cAlphaEval\u5b9e\u73b0\u4e86\u4e0e\u5168\u9762\u56de\u6d4b\u76f8\u5ab2\u7f8e\u7684\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89c1\u89e3\u548c\u66f4\u9ad8\u7684\u6548\u7387\u3002\u6b64\u5916\uff0cAlphaEval\u6709\u6548\u5730\u8bc6\u522b\u4e86\u4f18\u79c0\u7684\u03b1\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7684\u5355\u4e00\u6307\u6807\u7b5b\u9009\u65b9\u6cd5\u3002\u6240\u6709\u5b9e\u73b0\u548c\u8bc4\u4f30\u5de5\u5177\u5747\u4e3a\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u793e\u533a\u53c2\u4e0e\u3002"}}
{"id": "2508.13176", "categories": ["cs.AI", "cs.DB", "68T30 (Primary) 68P15, 03B70 (Secondary)", "I.2.4; H.2.3"], "pdf": "https://arxiv.org/pdf/2508.13176", "abs": "https://arxiv.org/abs/2508.13176", "authors": ["Simon Hosemann", "Jean Christoph Jung", "Carsten Lutz", "Sebastian Rudolph"], "title": "Fitting Ontologies and Constraints to Relational Structures", "comment": "Accepted at the 22nd International Conference on Principles of\n  Knowledge Representation and Reasoning (KR 2025)", "summary": "We study the problem of fitting ontologies and constraints to positive and\nnegative examples that take the form of a finite relational structure. As\nontology and constraint languages, we consider the description logics\n$\\mathcal{E\\mkern-2mu L}$ and $\\mathcal{E\\mkern-2mu LI}$ as well as several\nclasses of tuple-generating dependencies (TGDs): full, guarded,\nfrontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion\ndependencies. We pinpoint the exact computational complexity, design\nalgorithms, and analyze the size of fitting ontologies and TGDs. We also\ninvestigate the related problem of constructing a finite basis of concept\ninclusions / TGDs for a given set of finite structures. While finite bases\nexist for $\\mathcal{E\\mkern-2mu L}$, $\\mathcal{E\\mkern-2mu LI}$, guarded TGDs,\nand inclusion dependencies, they in general do not exist for full,\nfrontier-guarded and frontier-one TGDs.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u6709\u9650\u5173\u7cfb\u7ed3\u6784\u4e2d\u5c06\u672c\u4f53\u548c\u7ea6\u675f\u62df\u5408\u5230\u6b63\u9762\u548c\u8d1f\u9762\u793a\u4f8b\u7684\u95ee\u9898\uff0c\u8003\u8651\u4e86\u4e0d\u540c\u7684\u672c\u4f53\u548c\u7ea6\u675f\u8bed\u8a00\uff0c\u8bbe\u8ba1\u4e86\u89e3\u51b3\u7b97\u6cd5\uff0c\u5206\u6790\u4e86\u62df\u5408\u7684\u5927\u5c0f\u548c\u6784\u5efa\u6982\u5ff5\u5305\u542b/TGD\u7684\u6709\u9650\u57fa\u7840\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u672c\u4f53\u548c\u7ea6\u675f\u5728\u6709\u9650\u5173\u7cfb\u7ed3\u6784\u4e2d\u7684\u62df\u5408\u95ee\u9898\uff0c\u5e76\u7814\u7a76\u6784\u5efa\u7ed9\u5b9a\u6709\u9650\u7ed3\u6784\u96c6\u7684\u6982\u5ff5\u5305\u542b/TGD\u7684\u6709\u9650\u57fa\u7840\u3002", "method": "\u8003\u8651\u4e86\u63cf\u8ff0\u903b\u8f91$\\mathcal{EL}$\u548c$\\mathcal{ELI}$\u4ee5\u53ca\u51e0\u7c7b\u5143\u7ec4\u751f\u6210\u4f9d\u8d56(TGDs)\u4f5c\u4e3a\u672c\u4f53\u548c\u7ea6\u675f\u8bed\u8a00\uff1a\u5b8c\u6574\u7684\u3001\u53d7\u9650\u3001\u524d\u6cbf-\u53d7\u9650\u3001\u524d\u6cbf-\u4e00\u4e2a\u548c\u65e0\u7ea6\u675fTGDs\u4ee5\u53ca\u5305\u542b\u4f9d\u8d56\u3002\u8bbe\u8ba1\u4e86\u7b97\u6cd5\u6765\u89e3\u51b3\u62df\u5408\u95ee\u9898\u548c\u6784\u5efa\u6982\u5ff5\u5305\u542b/TGD\u7684\u6709\u9650\u57fa\u7840\u95ee\u9898\u3002", "result": "\u786e\u5b9a\u4e86\u672c\u4f53\u3001TGD\u7684\u62df\u5408\u95ee\u9898\u7684\u7cbe\u786e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u7814\u7a76\u4e86\u6982\u5ff5\u5305\u542b/TGD\u6709\u9650\u57fa\u7840\u7684\u5b58\u5728\u6027\u3002", "conclusion": "\u7814\u7a76\u4e86\u5c06\u672c\u4f53\u548c\u7ea6\u675f\u62df\u5408\u5230\u4ee5\u6709\u9650\u5173\u7cfb\u7ed3\u6784\u5f62\u5f0f\u51fa\u73b0\u7684\u6b63\u9762\u548c\u8d1f\u9762\u793a\u4f8b\u7684\u95ee\u9898\u3002\u786e\u5b9a\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u8bbe\u8ba1\u4e86\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u62df\u5408\u672c\u4f53\u548cTGD\u7684\u5927\u5c0f\u3002\u7814\u7a76\u4e86\u4e3a\u7ed9\u5b9a\u4e00\u7ec4\u6709\u9650\u7ed3\u6784\u6784\u9020\u6982\u5ff5\u5305\u542b/ TGD\u7684\u6709\u9650\u57fa\u7840\u7684\u76f8\u5173\u95ee\u9898\u3002\u867d\u7136\u5bf9\u4e8e\u63cf\u8ff0\u903b\u8f91\u548c\u7ea6\u675f\u7684\u6709\u9650\u57fa\u7840\u5b58\u5728\uff0c\u4f46\u5bf9\u4e8e\u5b8c\u6574\u7684\u3001\u524d\u6cbf\u8b66\u536b\u548c\u524d\u6cbf\u4e00\u4e2aTGD\uff0c\u57fa\u672c\u4e0a\u4e0d\u5b58\u5728\u3002"}}
{"id": "2508.13177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13177", "abs": "https://arxiv.org/abs/2508.13177", "authors": ["Nikola Pi\u017eurica", "Nikola Milovi\u0107", "Igor Jovan\u010devi\u0107", "Conor Heins", "Miguel de Prado"], "title": "A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment", "comment": null, "summary": "Active Inference (AIF) offers a robust framework for decision-making, yet its\ncomputational and memory demands pose challenges for deployment, especially in\nresource-constrained environments. This work presents a methodology that\nfacilitates AIF's deployment by integrating pymdp's flexibility and efficiency\nwith a unified, sparse, computational graph tailored for hardware-efficient\nexecution. Our approach reduces latency by over 2x and memory by up to 35%,\nadvancing the deployment of efficient AIF agents for real-time and embedded\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u6574\u5408\u4e86pymdp\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u7ed3\u5408\u9002\u7528\u4e8e\u786c\u4ef6\u9ad8\u6548\u6267\u884c\u7684\u7edf\u4e00\u7a00\u758f\u8ba1\u7b97\u56fe\uff0c\u4ece\u800c\u63d0\u51fa\u4e86\u4fc3\u8fdbAIF\u90e8\u7f72\u7684\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548AIF\u4ee3\u7406\u7684\u5b9e\u65f6\u548c\u5d4c\u5165\u5f0f\u5e94\u7528\u90e8\u7f72\u3002", "motivation": "Active Inference\uff08AIF\uff09\u5728\u51b3\u7b56\u65b9\u9762\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6846\u67b6\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u5b58\u5728\u6311\u6218\u3002", "method": "\u6574\u5408pymdp\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u7ed3\u5408\u9002\u7528\u4e8e\u786c\u4ef6\u9ad8\u6548\u6267\u884c\u7684\u7edf\u4e00\u7a00\u758f\u8ba1\u7b97\u56fe", "result": "\u901a\u8fc7\u672c\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u5728\u90e8\u7f72AIF\u65f6\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u63d0\u5347\u4e86\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408pymdp\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u5e76\u7ed3\u5408\u9002\u7528\u4e8e\u786c\u4ef6\u9ad8\u6548\u6267\u884c\u7684\u7edf\u4e00\u7a00\u758f\u8ba1\u7b97\u56fe\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4fc3\u8fdb\u4e86Active Inference\uff08AIF\uff09\u7684\u90e8\u7f72\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u5ef6\u8fdf\u964d\u4f4e\u4e862\u500d\u4ee5\u4e0a\uff0c\u5185\u5b58\u5229\u7528\u7387\u6700\u591a\u964d\u4f4e\u4e8635%\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548AIF\u4ee3\u7406\u7684\u5b9e\u65f6\u548c\u5d4c\u5165\u5f0f\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2508.13178", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.13178", "abs": "https://arxiv.org/abs/2508.13178", "authors": ["Cong Zhang"], "title": "The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task", "comment": null, "summary": "To elevate the foundational capabilities and generalization prowess of the\ntext-to-SQL model in real-world applications, we integrate model\ninterpretability analysis with execution-guided strategy for semantic parsing\nof WHERE clauses in SQL queries. Furthermore, we augment this approach with\nfiltering adjustments, logical correlation refinements, and model fusion,\nculminating in the design of the CESQL model that facilitates conditional\nenhancement. Our model excels on the WikiSQL dataset, which is emblematic of\nsingle-table database query tasks, markedly boosting the accuracy of prediction\noutcomes. When predicting conditional values in WHERE clauses, we have not only\nminimized our dependence on data within the condition columns of tables but\nalso circumvented the impact of manually labeled training data. Our hope is\nthat this endeavor to enhance accuracy in processing basic database queries\nwill offer fresh perspectives for research into handling complex queries and\nscenarios featuring irregular data in real-world database environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CESQL\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u578b\u89e3\u91ca\u6027\u5206\u6790\u548c\u6267\u884c\u5bfc\u5411\u7b56\u7565\u4f18\u5316\u4e86\u8bed\u4e49\u89e3\u6790SQL\u67e5\u8be2\u4e2d\u7684WHERE\u5b50\u53e5\u3002\u5728WikiSQL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u4e0d\u89c4\u5219\u6570\u636e\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u4e3a\u63d0\u5347\u6587\u672c\u5230SQL\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u57fa\u7840\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u6570\u636e\u548c\u624b\u52a8\u6807\u8bb0\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u5904\u7406\u57fa\u672c\u6570\u636e\u5e93\u67e5\u8be2\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5e93\u73af\u5883\u4e2d\u4e0d\u89c4\u5219\u6570\u636e\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u601d\u8def\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u89e3\u91ca\u6027\u5206\u6790\u548c\u6267\u884c\u5bfc\u5411\u7b56\u7565\uff0c\u901a\u8fc7\u8fc7\u6ee4\u8c03\u6574\u3001\u903b\u8f91\u76f8\u5173\u6027\u4f18\u5316\u548c\u6a21\u578b\u878d\u5408\u8bbe\u8ba1\u51faCESQL\u6a21\u578b\uff0c\u7528\u4e8e\u8bed\u4e49\u89e3\u6790SQL\u67e5\u8be2\u4e2d\u7684WHERE\u5b50\u53e5\u3002", "result": "\u901a\u8fc7CESQL\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u5728\u5904\u7406\u5355\u8868\u6570\u636e\u5e93\u67e5\u8be2\u4efb\u52a1\u65f6\u53d6\u5f97\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\uff0c\u5c24\u5176\u5728\u9884\u6d4bWHERE\u5b50\u53e5\u4e2d\u7684\u6761\u4ef6\u503c\u65f6\uff0c\u51cf\u5c11\u4e86\u5bf9\u8868\u4e2d\u6761\u4ef6\u5217\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u907f\u514d\u4e86\u624b\u52a8\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86CESQL\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6a21\u578b\u89e3\u91ca\u6027\u5206\u6790\u548c\u6267\u884c\u5bfc\u5411\u7b56\u7565\uff0c\u7528\u4e8e\u8bed\u4e49\u89e3\u6790SQL\u67e5\u8be2\u4e2d\u7684WHERE\u5b50\u53e5\u3002\u901a\u8fc7\u8fc7\u6ee4\u8c03\u6574\u3001\u903b\u8f91\u76f8\u5173\u6027\u4f18\u5316\u548c\u6a21\u578b\u878d\u5408\uff0cCESQL\u6a21\u578b\u5728WikiSQL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002\u8be5\u6a21\u578b\u80fd\u591f\u5728\u5904\u7406\u57fa\u672c\u6570\u636e\u5e93\u67e5\u8be2\u65f6\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u7814\u7a76\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5e93\u73af\u5883\u4e2d\u4e0d\u89c4\u5219\u6570\u636e\u7684\u65b0\u9014\u5f84\u63d0\u4f9b\u4e86\u5e0c\u671b\u3002"}}
{"id": "2508.13180", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13180", "abs": "https://arxiv.org/abs/2508.13180", "authors": ["Ziwen Han", "Meher Mankikar", "Julian Michael", "Zifan Wang"], "title": "Search-Time Data Contamination", "comment": null, "summary": "Data contamination refers to the leakage of evaluation data into model\ntraining data, resulting in overfitting to supposedly held-out test sets and\ncompromising test validity. We identify an analogous issue, search-time\ncontamination (STC), in evaluating search-based LLM agents which use tools to\ngather information from online sources when answering user queries. STC occurs\nwhen the retrieval step surfaces a source containing the test question (or a\nnear-duplicate) alongside its answer, enabling agents to copy rather than\ngenuinely infer or reason, undermining benchmark integrity. We find that\nHuggingFace, an online platform hosting evaluation datasets, appears among\nretrieved sources in search based agent logs. Consequently, agents often\nexplicitly acknowledge discovering question answer pairs from HuggingFace\nwithin their reasoning chains. On three commonly used capability benchmarks:\nHumanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for\napproximately 3% of questions, search-based agents directly find the datasets\nwith ground truth labels on HuggingFace. When millions of evaluation queries\ntarget the same benchmark, even small, repeated leaks can accelerate the\nbenchmark's obsolescence, shortening its intended lifecycle. After HuggingFace\nis blocked, we observe a drop in accuracy on the contaminated subset of\napproximately 15%. We further show through ablation experiments that publicly\naccessible evaluation datasets on HuggingFace may not be the sole source of\nSTC. To this end, we conclude by proposing best practices for benchmark design\nand result reporting to address this novel form of leakage and ensure\ntrustworthy evaluation of search-based LLM agents. To facilitate the auditing\nof evaluation results, we also publicly release the complete logs from our\nexperiments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u636e\u6c61\u67d3\u548c\u641c\u7d22\u65f6\u6c61\u67d3\u5bf9\u57fa\u4e8e\u641c\u7d22\u7684LLM\u4ee3\u7406\u8bc4\u4f30\u7684\u5f71\u54cd\u3002\u4f5c\u8005\u53d1\u73b0\u641c\u7d22\u4ee3\u7406\u53ef\u80fd\u76f4\u63a5\u4eceHuggingFace\u7b49\u5728\u7ebf\u5e73\u53f0\u4e0a\u83b7\u53d6\u5e26\u6709\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u57fa\u51c6\u7684\u53ef\u4fe1\u6027\u548c\u5b8c\u6574\u6027\u53d7\u5230\u5f71\u54cd\u3002\u901a\u8fc7\u5b9e\u9a8c\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u89e3\u51b3\u641c\u7d22\u65f6\u6c61\u67d3\u95ee\u9898\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u516c\u5f00\u4e86\u5b9e\u9a8c\u65e5\u5fd7\u4ee5\u4f9b\u5ba1\u6838\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u53d1\u73b0\u6570\u636e\u6c61\u67d3\u5e26\u6765\u7684\u95ee\u9898\uff0c\u5373\u5728\u8bc4\u4f30\u57fa\u4e8e\u641c\u7d22\u7684LLM\u4ee3\u7406\u65f6\u7684\u641c\u7d22\u65f6\u6c61\u67d3\uff08STC\uff09\u3002\u4f5c\u8005\u610f\u8bc6\u5230\u641c\u7d22\u4ee3\u7406\u53ef\u80fd\u76f4\u63a5\u4eceHuggingFace\u7b49\u5728\u7ebf\u5e73\u53f0\u4e0a\u627e\u5230\u5e26\u6709\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5f71\u54cd\u57fa\u51c6\u7684\u53ef\u4fe1\u6027\u548c\u5b8c\u6574\u6027\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u5728HuggingFace\u4e0a\u76f4\u63a5\u627e\u5230\u5e26\u6709\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4f1a\u5bf9\u57fa\u51c6\u7684\u51c6\u786e\u6027\u4ea7\u751f\u5f71\u54cd\u3002\u4f5c\u8005\u8fd8\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\uff0c\u8868\u660eHuggingFace\u4e0a\u516c\u5f00\u53ef\u8bbf\u95ee\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u53ef\u80fd\u4e0d\u662fSTC\u7684\u552f\u4e00\u6765\u6e90\u3002\u6700\u540e\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u79cd\u6cc4\u6f0f\u95ee\u9898\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u5b9e\u9a8c\u7684\u5b8c\u6574\u65e5\u5fd7\u4ee5\u4fbf\u8fdb\u884c\u8bc4\u4f30\u7ed3\u679c\u7684\u5ba1\u6838\u3002", "result": "\u4f5c\u8005\u53d1\u73b0\u641c\u7d22\u65f6\u6c61\u67d3\u53ef\u80fd\u4f1a\u5bf9\u57fa\u51c6\u7684\u51c6\u786e\u6027\u9020\u6210\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u57fa\u51c6\u8bbe\u8ba1\u548c\u7ed3\u679c\u62a5\u544a\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u516c\u5f00\u4e86\u5b9e\u9a8c\u65e5\u5fd7\u4ee5\u4f9b\u5ba1\u6838\u3002\u5728\u963b\u6b62HuggingFace\u540e\uff0c\u6c61\u67d3\u5b50\u96c6\u7684\u51c6\u786e\u6027\u4e0b\u964d\u7ea615%\u3002", "conclusion": "\u6570\u636e\u6c61\u67d3\u5bfc\u81f4\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u4e2d\u51fa\u73b0\u8fc7\u62df\u5408\uff0c\u5f71\u54cd\u6d4b\u8bd5\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002\u4f5c\u8005\u53d1\u73b0\u4e86\u4e00\u79cd\u7c7b\u4f3c\u7684\u95ee\u9898\uff0c\u5373\u641c\u7d22\u65f6\u6c61\u67d3\uff08STC\uff09\uff0c\u5728\u8bc4\u4f30\u57fa\u4e8e\u641c\u7d22\u7684LLM\u4ee3\u7406\u65f6\u51fa\u73b0\u3002\u4ed6\u4eec\u53d1\u73b0\u4f7f\u7528\u5728\u7ebf\u6765\u6e90\u83b7\u53d6\u4fe1\u606f\u65f6\uff0c\u68c0\u7d22\u6b65\u9aa4\u6709\u53ef\u80fd\u8fd4\u56de\u5305\u542b\u6d4b\u8bd5\u95ee\u9898\uff08\u6216\u8fd1\u4f3c\u590d\u5236\uff09\u53ca\u5176\u7b54\u6848\u7684\u6765\u6e90\uff0c\u4f7f\u4ee3\u7406\u53ef\u4ee5\u590d\u5236\u800c\u975e\u771f\u5b9e\u63a8\u65ad\u6216\u63a8\u7406\uff0c\u4ece\u800c\u635f\u5bb3\u4e86\u57fa\u51c6\u7684\u5b8c\u6574\u6027\u3002\u4f5c\u8005\u5728\u4e09\u4e2a\u5e38\u7528\u7684\u80fd\u529b\u57fa\u51c6\uff08HLE\uff0cSimpleQA\u548cGPQA\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u53d1\u73b0\u641c\u7d22\u4ee3\u7406\u76f4\u63a5\u5728HuggingFace\u4e0a\u627e\u5230\u4e86\u5e26\u6709\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u7684\u6570\u636e\u96c6\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u51c6\u8bbe\u8ba1\u548c\u7ed3\u679c\u62a5\u544a\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u4ee5\u89e3\u51b3\u8fd9\u79cd\u65b0\u578b\u6cc4\u6f0f\u5f62\u5f0f\uff0c\u5e76\u786e\u4fdd\u5bf9\u641c\u7d22\u578bLLM\u4ee3\u7406\u7684\u8bc4\u4f30\u53ef\u4fe1\u3002"}}
{"id": "2508.13204", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13204", "abs": "https://arxiv.org/abs/2508.13204", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "QuickMerge++: Fast Token Merging with Autoregressive Prior", "comment": "The paper has been accepted to ICML Tokshop at\n  https://openreview.net/forum?id=dMdxHd0tRf", "summary": "As generative models scale to larger inputs across language, vision, and\nvideo domains, the cost of token-level computation has become a key bottleneck.\nWhile prior work suggests that only a subset of tokens significantly influence\ndownstream predictions, most token selection methods are static,\nmodality-specific, or incompatible with autoregressive generation. In this\npaper, we propose QuickMerge, a lightweight token merging framework designed\nfor efficient next-token prediction.\n  QuickMerge dynamically selects a reduced number of tokens based on attention\nnorm magnitude, guided by an entropy-based budget estimator. To preserve\nautoregressive compatibility, we introduce a lightweight transformer prior\ntrained over the merged token sequence. By combining semantic salience\nestimation, flexible token budgets, and AR alignment, QuickMerge enables\naccurate generation with fewer tokens.\n  We evaluate QuickMerge across multi-modality domains, demonstrating\nconsistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge\nreduces token counts sustantially while matching as well as exceeding the\nperformance of learned tokenizers and fixed-patch baselines.", "AI": {"tldr": "QuickMerge is a lightweight token merging framework that reduces token counts while maintaining performance, enabling efficient next-token prediction across multi-modality domains.", "motivation": "The motivation behind this paper is the increasing cost of token-level computation in generative models across various domains like language, vision, and video. The existing token selection methods are limited by being static, modality-specific, or incompatible with autoregressive generation.", "method": "QuickMerge dynamically selects a reduced number of tokens based on attention norm magnitude, guided by an entropy-based budget estimator. It introduces a lightweight transformer prior trained over the merged token sequence to preserve autoregressive compatibility.", "result": "The results show that QuickMerge reduces token counts substantially while maintaining or exceeding the performance of learned tokenizers and fixed-patch baselines.", "conclusion": "QuickMerge is a lightweight token merging framework designed for efficient next-token prediction that enables accurate generation with fewer tokens and demonstrates consistent improvements in compute-accuracy tradeoffs across multi-modality domains."}}
{"id": "2508.13213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13213", "abs": "https://arxiv.org/abs/2508.13213", "authors": ["Adamo Cerioli", "Edward D. Lee", "Vito D. P. Servedio"], "title": "AI sustains higher strategic tension than humans in chess", "comment": null, "summary": "Strategic decision-making involves managing the tension between immediate\nopportunities and long-term objectives. We study this trade-off in chess by\ncharacterizing and comparing dynamics between human vs human and AI vs AI\ngames. We propose a network-based metric of piece-to-piece interaction to\nquantify the ongoing strategic tension on the board. Its evolution in games\nreveals that the most competitive AI players sustain higher levels of strategic\ntension for longer durations than elite human players. Cumulative tension\nvaries with algorithmic complexity for AI and correspondingly in human-played\ngames increases abruptly with expertise at about 1600 Elo and again at 2300\nElo. The profiles reveal different approaches. Highly competitive AI tolerates\ninterconnected positions balanced between offensive and defensive tactics over\nlong periods. Human play, in contrast, limits tension and game complexity,\nwhich may reflect cognitive limitations and adaptive strategies. The difference\nmay have implications for AI usage in complex, strategic environments.", "AI": {"tldr": "The paper analyzed strategic decision-making in chess by comparing human vs human and AI vs AI games. Competitive AI players maintain higher strategic tension levels for longer periods than elite human players. The study found that strategic tension varies with algorithmic complexity for AI and expertise levels in human players. AI players tolerate interconnected positions while human players limit tension and game complexity, potentially due to cognitive limitations and adaptive strategies.", "motivation": "To understand the trade-off between immediate opportunities and long-term objectives in strategic decision-making, specifically in chess. Explore how strategic tension varies between human and AI players, and the impact of algorithmic complexity and expertise levels on strategic decision-making in chess.", "method": "Characterized and compared dynamics between human vs human and AI vs AI games in chess. Proposed a network-based metric of piece-to-piece interaction to quantify strategic tension on the board. Studied the evolution of strategic tension in games to analyze the differences between human and AI players.", "result": "Found that competitive AI players sustain higher levels of strategic tension for longer durations compared to elite human players. Cumulative tension varies with algorithmic complexity for AI and expertise levels in human players. AI players exhibit a different strategic approach compared to human players, which may have implications for AI usage in strategic environments.", "conclusion": "AI players sustain higher levels of strategic tension for longer durations than elite human players. Strategic tension varies with algorithmic complexity for AI and expertise levels in human players. AI players tolerate interconnected positions balanced between offensive and defensive tactics, while human players limit tension and game complexity."}}
{"id": "2508.13250", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.13250", "abs": "https://arxiv.org/abs/2508.13250", "authors": ["Zeyu Zhang", "Yang Zhang", "Haoran Tan", "Rui Li", "Xu Chen"], "title": "Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information", "comment": "15 pages, 13 figures, 3 tables", "summary": "In large language model-based agents, memory serves as a critical capability\nfor achieving personalization by storing and utilizing users' information.\nAlthough some previous studies have adopted memory to implement user\npersonalization, they typically focus on preference alignment and simple\nquestion-answering. However, in the real world, complex tasks often require\nmulti-hop reasoning on a large amount of user information, which poses\nsignificant challenges for current memory approaches. To address this\nlimitation, we propose the multi-hop personalized reasoning task to explore how\ndifferent memory mechanisms perform in multi-hop reasoning over personalized\ninformation. We explicitly define this task and construct a dataset along with\na unified evaluation framework. Then, we implement various explicit and\nimplicit memory methods and conduct comprehensive experiments. We evaluate\ntheir performance on this task from multiple perspectives and analyze their\nstrengths and weaknesses. Besides, we explore hybrid approaches that combine\nboth paradigms and propose the HybridMem method to address their limitations.\nWe demonstrate the effectiveness of our proposed model through extensive\nexperiments. To benefit the research community, we release this project at\nhttps://github.com/nuster1128/MPR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u663e\u5f0f\u548c\u9690\u5f0f\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86HybridMem\u65b9\u6cd5\uff0c\u6700\u7ec8\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u590d\u6742\u4efb\u52a1\u901a\u5e38\u9700\u8981\u5728\u5927\u91cf\u7528\u6237\u4fe1\u606f\u4e0a\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u800c\u5f53\u524d\u7684\u8bb0\u5fc6\u65b9\u6cd5\u5728\u6b64\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\u5e76\u63a2\u8ba8\u4e0d\u540c\u8bb0\u5fc6\u673a\u5236\u7684\u8868\u73b0\u3002", "method": "\u8be5\u8bba\u6587\u5b9a\u4e49\u4e86\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u663e\u5f0f\u548c\u9690\u5f0f\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86HybridMem\u65b9\u6cd5\u3002\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u5b83\u4eec\u5728\u591a\u4e2a\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\uff0c\u65e8\u5728\u63a2\u8ba8\u4e0d\u540c\u8bb0\u5fc6\u673a\u5236\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u663e\u5f0f\u548c\u9690\u5f0f\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86HybridMem\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2508.13251", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.13251", "abs": "https://arxiv.org/abs/2508.13251", "authors": ["Di Zhang", "Xue Jia", "Tran Ba Hung", "Seong Hoon Jang", "Linda Zhang", "Ryuhei Sato", "Yusuke Hashimoto", "Toyoto Sato", "Kiyoe Konno", "Shin-ichi Orimo", "Hao Li"], "title": "\"DIVE\" into Hydrogen Storage Materials Discovery with AI Agents", "comment": "23 pages, 5 figures. The supplementary video is available at the\n  GitHub link provided in the manuscript", "summary": "Data-driven artificial intelligence (AI) approaches are fundamentally\ntransforming the discovery of new materials. Despite the unprecedented\navailability of materials data in the scientific literature, much of this\ninformation remains trapped in unstructured figures and tables, hindering the\nconstruction of large language model (LLM)-based AI agent for automated\nmaterials design. Here, we present the Descriptive Interpretation of Visual\nExpression (DIVE) multi-agent workflow, which systematically reads and\norganizes experimental data from graphical elements in scientific literatures.\nWe focus on solid-state hydrogen storage materials-a class of materials central\nto future clean-energy technologies and demonstrate that DIVE markedly improves\nthe accuracy and coverage of data extraction compared to the direct extraction\nby multimodal models, with gains of 10-15% over commercial models and over 30%\nrelative to open-source models. Building on a curated database of over 30,000\nentries from 4,000 publications, we establish a rapid inverse design workflow\ncapable of identifying previously unreported hydrogen storage compositions in\ntwo minutes. The proposed AI workflow and agent design are broadly transferable\nacross diverse materials, providing a paradigm for AI-driven materials\ndiscovery.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528DIVE\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u63d0\u9ad8\u6570\u636e\u63d0\u53d6\u51c6\u786e\u6027\u4e0e\u8986\u76d6\u8303\u56f4\uff0c\u5efa\u7acb\u4e86\u80fd\u5728\u4e24\u5206\u949f\u5185\u8bc6\u522b\u65b0\u6c22\u5b58\u50a8\u7ec4\u6210\u7684\u5feb\u901f\u9006\u5411\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u3002\u63d0\u51fa\u7684AI\u5de5\u4f5c\u6d41\u7a0b\u9002\u7528\u4e8e\u5404\u79cd\u6750\u6599\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002", "motivation": "\u5c3d\u7ba1\u79d1\u5b66\u6587\u732e\u4e2d\u5b58\u5728\u5927\u91cf\u6750\u6599\u6570\u636e\uff0c\u4f46\u5f88\u591a\u4fe1\u606f\u4ecd\u6df1\u85cf\u5728\u7ed3\u6784\u6df7\u4e71\u7684\u56fe\u8868\u4e2d\uff0c\u963b\u788d\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684AI\u4ee3\u7406\u81ea\u52a8\u5316\u6750\u6599\u8bbe\u8ba1\u3002\u9488\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e86DIVE\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\uff0c\u65e8\u5728\u63d0\u9ad8\u6570\u636e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002\u7740\u91cd\u7814\u7a76\u56fa\u6001\u6c22\u5b58\u50a8\u6750\u6599\uff0c\u8fd9\u79cd\u6750\u6599\u5bf9\u672a\u6765\u6e05\u6d01\u80fd\u6e90\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528DIVE\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u7cfb\u7edf\u8bfb\u53d6\u548c\u7ec4\u7ec7\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u6570\u636e\u3002\u9488\u5bf9\u56fa\u6001\u6c22\u50a8\u5b58\u6750\u6599\u5c55\u5f00\u7814\u7a76\uff0c\u5c55\u793aDIVE\u76f8\u5bf9\u5546\u4e1a\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u63d0\u9ad8\u4e8610-15%\u7684\u6570\u636e\u63d0\u53d6\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\u7684\u6548\u679c\u3002\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc730,000\u6761\u8bb0\u5f55\u7684\u6570\u636e\u5e93\uff0c\u5e76\u63d0\u51fa\u4e86\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u65b0\u7684\u6c22\u5b58\u50a8\u7ec4\u6210\u7684\u9006\u5411\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7DIVE\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6570\u636e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\u5f97\u5230\u663e\u8457\u63d0\u9ad8\u3002\u76f8\u6bd4\u76f4\u63a5\u63d0\u53d6\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e8610-15%\u7684\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\uff0c\u76f8\u5bf9\u5546\u4e1a\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u63d0\u9ad8\u8d85\u8fc730%\u3002\u5efa\u7acb\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5e93\uff0c\u5e76\u5b9e\u73b0\u4e86\u5feb\u901f\u9006\u5411\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u901a\u8fc7DIVE\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u7cfb\u7edf\u9605\u8bfb\u548c\u7ec4\u7ec7\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc730,000\u6761\u8bb0\u5f55\u7684\u7b5b\u9009\u6570\u636e\u5e93\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u80fd\u591f\u5728\u4e24\u5206\u949f\u5185\u8bc6\u522b\u5148\u524d\u672a\u62a5\u544a\u7684\u6c22\u5b58\u50a8\u7ec4\u6210\u7684\u5feb\u901f\u9006\u5411\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u3002\u8be5AI\u5de5\u4f5c\u6d41\u7a0b\u548c\u667a\u80fd\u4f53\u8bbe\u8ba1\u5728\u5404\u79cd\u6750\u6599\u4e0a\u5177\u6709\u5e7f\u6cdb\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u4e3a\u57fa\u4e8eAI\u7684\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8303\u4f8b\u3002"}}
{"id": "2508.13256", "categories": ["cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.13256", "abs": "https://arxiv.org/abs/2508.13256", "authors": ["Yuting Zhang", "Karina V. Bunting", "Asgher Champsi", "Xiaoxia Wang", "Wenqi Lu", "Alexander Thorley", "Sandeep S Hothi", "Zhaowen Qiu", "Dipak Kotecha", "Jinming Duan"], "title": "CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support", "comment": null, "summary": "Cardiovascular diseases (CVDs) remain the foremost cause of mortality\nworldwide, a burden worsened by a severe deficit of healthcare workers.\nArtificial intelligence (AI) agents have shown potential to alleviate this gap\nvia automated early detection and proactive screening, yet their clinical\napplication remains limited by: 1) prompt-based clinical role assignment that\nrelies on intrinsic model capabilities without domain-specific tool support; or\n2) rigid sequential workflows, whereas clinical care often requires adaptive\nreasoning that orders specific tests and, based on their results, guides\npersonalised next steps; 3) general and static knowledge bases without\ncontinuous learning capability; and 4) fixed unimodal or bimodal inputs and\nlack of on-demand visual outputs when further clarification is needed. In\nresponse, a multimodal framework, CardAIc-Agents, was proposed to augment\nmodels with external tools and adaptively support diverse cardiac tasks.\nSpecifically, a CardiacRAG agent generated general plans from updatable cardiac\nknowledge, while the chief agent integrated tools to autonomously execute these\nplans and deliver decisions. To enable adaptive and case-specific\ncustomization, a stepwise update strategy was proposed to dynamically refine\nplans based on preceding execution results, once the task was assessed as\ncomplex. In addition, a multidisciplinary discussion tool was introduced to\ninterpret challenging cases, thereby supporting further adaptation. When\nclinicians raised concerns, visual review panels were provided to assist final\nvalidation. Experiments across three datasets showed the efficiency of\nCardAIc-Agents compared to mainstream Vision-Language Models (VLMs),\nstate-of-the-art agentic systems, and fine-tuned VLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCardAIc-Agents\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u5fc3\u810f\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u548c\u6848\u4f8b\u7279\u5b9a\u5b9a\u5236\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8fd9\u4e00\u6846\u67b6\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u76f8\u5173\u6a21\u578b\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4ecd\u7136\u662f\u4e3b\u8981\u7684\u6b7b\u56e0\uff0c\u533b\u7597\u5de5\u4f5c\u8005\u7684\u4e25\u91cd\u7f3a\u9677\u52a0\u5267\u4e86\u8fd9\u4e00\u8d1f\u62c5\u3002\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5df2\u663e\u793a\u51fa\u6f5c\u529b\u901a\u8fc7\u81ea\u52a8\u5316\u65e9\u671f\u68c0\u6d4b\u548c\u79ef\u6781\u7b5b\u67e5\u6765\u7f13\u89e3\u8fd9\u4e00\u7f3a\u53e3\uff0c\u4f46\u5176\u4e34\u5e8a\u5e94\u7528\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6CardAIc-Agents\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u5de5\u5177\u589e\u5f3a\u6a21\u578b\uff0c\u652f\u6301\u5fc3\u810f\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u548c\u6848\u4f8b\u7279\u5b9a\u5b9a\u5236\u3002\u5177\u4f53\u5730\uff0cCardiacRAG\u4ee3\u7406\u6839\u636e\u53ef\u66f4\u65b0\u7684\u5fc3\u810f\u77e5\u8bc6\u751f\u6210\u901a\u7528\u8ba1\u5212\uff0c\u4e3b\u8981\u4ee3\u7406\u96c6\u6210\u5de5\u5177\u4ee5\u81ea\u4e3b\u6267\u884c\u8fd9\u4e9b\u8ba1\u5212\u5e76\u63d0\u51fa\u51b3\u7b56\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9010\u6b65\u66f4\u65b0\u7b56\u7565\uff0c\u6839\u636e\u5148\u524d\u6267\u884c\u7ed3\u679c\u52a8\u6001\u7ec6\u5316\u8ba1\u5212\uff0c\u4f7f\u4efb\u52a1\u5728\u8bc4\u4f30\u4e3a\u590d\u6742\u65f6\u80fd\u591f\u81ea\u9002\u5e94\u548c\u6848\u4f8b\u7279\u5b9a\u5b9a\u5236\u3002\u5f15\u5165\u4e86\u591a\u5b66\u79d1\u8ba8\u8bba\u5de5\u5177\u6765\u89e3\u91ca\u6311\u6218\u6027\u6848\u4f8b\uff0c\u4ece\u800c\u652f\u6301\u8fdb\u4e00\u6b65\u81ea\u9002\u5e94\u3002\u5f53\u4e34\u5e8a\u533b\u751f\u63d0\u51fa\u5173\u6ce8\u65f6\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u5ba1\u67e5\u9762\u677f\u4ee5\u534f\u52a9\u6700\u7ec8\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86CardAIc-Agents\u76f8\u8f83\u4e8e\u4e3b\u6d41\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u7cfb\u7edf\u548c\u7ecf\u8fc7\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u3002", "conclusion": "\u7efc\u5408\u5361\u5c14\u8feaAIc-Agent\u7684\u591a\u6a21\u6001\u6846\u67b6\u5728\u5fc3\u810f\u4efb\u52a1\u652f\u6301\u65b9\u9762\u7684\u6548\u7387\uff0c\u76f8\u8f83\u4e8e\u4e3b\u6d41\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u7cfb\u7edf\u548c\u7ecf\u8fc7\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u793a\u51fa\u8f83\u9ad8\u6548\u7387\u3002"}}
{"id": "2508.13327", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13327", "abs": "https://arxiv.org/abs/2508.13327", "authors": ["Sarthak Khanna", "Armin Berger", "David Berghaus", "Tobias Deusser", "Lorenz Sparrenberg", "Rafet Sifa"], "title": "Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention", "comment": "Accepted in IEEE-DSAA 2025", "summary": "We propose STONK (Stock Optimization using News Knowledge), a multimodal\nframework integrating numerical market indicators with sentiment-enriched news\nembeddings to improve daily stock-movement prediction. By combining numerical &\ntextual embeddings via feature concatenation and cross-modal attention, our\nunified pipeline addresses limitations of isolated analyses. Backtesting shows\nSTONK outperforms numeric-only baselines. A comprehensive evaluation of fusion\nstrategies and model configurations offers evidence-based guidance for scalable\nmultimodal financial forecasting. Source code is available on GitHub", "AI": {"tldr": "STONK (Stock Optimization using News Knowledge) integrates numerical market indicators and sentiment-enriched news embeddings to enhance daily stock-movement prediction. It outperforms numeric-only baselines, providing evidence-based guidance for scalable multimodal financial forecasting. Source code is available on GitHub.", "motivation": "The motivation behind this paper is to improve daily stock-movement prediction by leveraging both numerical and textual information. The integration of sentiment-enriched news embeddings aims to enhance the predictive accuracy of financial forecasting models. The goal is to provide a comprehensive evaluation of fusion strategies and model configurations for scalable multimodal financial forecasting.", "method": "The paper proposes a multimodal framework called STONK, which integrates numerical market indicators and sentiment-enriched news embeddings. It combines numerical and textual embeddings through feature concatenation and cross-modal attention in a unified pipeline to enhance daily stock-movement prediction. The approach addresses limitations of isolated analyses and backtesting demonstrates the outperformance of STONK over numeric-only baselines.", "result": "The result of the paper shows that STONK outperforms numeric-only baselines in stock movement prediction. The fusion strategies and model configurations explored in STONK offer evidence-based guidance for scalable multimodal financial forecasting.", "conclusion": "STONK (Stock Optimization using News Knowledge) outperforms numeric-only baselines in daily stock-movement prediction through the integration of numerical market indicators and sentiment-enriched news embeddings. The fusion strategies and model configurations in STONK provide evidence-based guidance for scalable multimodal financial forecasting."}}
{"id": "2508.13333", "categories": ["cs.AI", "cs.NE", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.13333", "abs": "https://arxiv.org/abs/2508.13333", "authors": ["Chentong Chen", "Mengyuan Zhong", "Jianyong Sun", "Ye Fan", "Jialong Shi"], "title": "HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design", "comment": "9 pages, 6 figures", "summary": "LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation\n(EC) frameworks has shown promising results. However, its effectiveness is\nhindered by the use of static operators and the lack of knowledge accumulation\nmechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two\nsynergistic prompting strategies: Foresight and Hindsight. Foresight-based\nprompts adaptively steer the search based on population dynamics, managing the\nexploration-exploitation trade-off. In addition, hindsight-based prompts mimic\nhuman expertise by distilling successful heuristics from past generations into\nfundamental, reusable design principles. This dual mechanism transforms\ntransient discoveries into a persistent knowledge base, enabling the LLM to\nlearn from its own experience. Empirical results demonstrate that HiFo-Prompt\nsignificantly outperforms state-of-the-art LLM-based AHD methods, generating\nhigher-quality heuristics while achieving substantially faster convergence and\nsuperior query efficiency.", "AI": {"tldr": "HiFo-Prompt framework enhances LLM-based Automatic Heuristic Design by combining Foresight and Hindsight prompting strategies, leading to improved heuristic quality, faster convergence, and superior query efficiency.", "motivation": "The effectiveness of LLM-based Automatic Heuristic Design within Evolutionary Computation frameworks is hindered by static operators and lack of knowledge accumulation mechanisms.", "method": "Introducing HiFo-Prompt framework that guides LLMs with Foresight and Hindsight prompting strategies to steer the search adaptively and distill successful heuristics from past generations, transforming transient discoveries into a persistent knowledge base.", "result": "Empirical results demonstrate the superiority of HiFo-Prompt in comparison to existing methods, showing better performance in heuristic generation, convergence speed, and query efficiency.", "conclusion": "HiFo-Prompt framework outperforms state-of-the-art LLM-based Automatic Heuristic Design methods in generating higher-quality heuristics with faster convergence and superior query efficiency."}}
{"id": "2508.13371", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13371", "abs": "https://arxiv.org/abs/2508.13371", "authors": ["Ronit Virwani", "Ruchika Suryawanshi"], "title": "LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems", "comment": "Submitted to IAAI-26", "summary": "Planning is one of the most critical tasks in autonomous systems, where even\na small error can lead to major failures or million-dollar losses. Current\nstate-of-the-art neural planning approaches struggle with complex domains,\nproducing plans with missing preconditions, inconsistent goals, and\nhallucinations. While classical planners provide logical guarantees, they lack\nthe flexibility and natural language understanding capabilities needed for\nmodern autonomous systems. Existing neuro-symbolic approaches use one-shot\ntranslation from natural language to formal plans, missing the opportunity for\nneural and symbolic components to work and refine solutions together. To\naddress this gap, we develop LOOP -- a novel neuro-symbolic planning framework\nthat treats planning as an iterative conversation between neural and symbolic\ncomponents rather than simple translation. LOOP integrates 13 coordinated\nneural features including graph neural networks for spatial relationships,\nmulti-agent validation for consensus-based correctness, hierarchical\ndecomposition for complex task management, and causal memory that learns from\nboth successes and failures. Unlike existing approaches, LOOP generates PDDL\nspecifications, refines them iteratively based on symbolic feedback, and builds\na causal knowledge base from execution traces. LOOP was evaluated on six\nstandard IPC benchmark domains, where it achieved 85.8% success rate compared\nto LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This\nwork shows that the key to reliable planning is not in choosing between neural\nnetworks or symbolic reasoners but it lies in making them actually ``talk'' to\neach other during the entire process. LOOP provides a thorough blueprint for\nbuilding autonomous systems that can finally be trusted with critical\nreal-world applications.", "AI": {"tldr": "LOOP is a new neuro-symbolic planning framework that enhances planning by facilitating communication between neural and symbolic components. It outperformed existing approaches in standard benchmark domains, achieving an 85.8% success rate. The key to effective planning is enabling neural networks and symbolic reasoners to collaborate throughout the process.", "motivation": "Current neural planning approaches struggle with complex domains, leading to missing preconditions, inconsistent goals, and hallucinations. Classical planners lack flexibility and natural language understanding. Existing neuro-symbolic approaches use one-shot translation, missing the opportunity for neural and symbolic components to collaborate. To address these limitations, the paper develops LOOP to facilitate communication between neural and symbolic components in planning tasks.", "method": "The paper introduces LOOP, a neuro-symbolic planning framework that integrates 13 coordinated neural features, including graph neural networks, multi-agent validation, hierarchical decomposition, and causal memory. It generates PDDL specifications, refines them iteratively based on symbolic feedback, and builds a causal knowledge base from execution traces.", "result": "LOOP achieved an 85.8% success rate in standard IPC benchmark domains, outperforming other approaches such as LLM+P, LLM-as-Planner, and Tree-of-Thoughts. It demonstrates the effectiveness of integrating neural and symbolic components in planning.", "conclusion": "LOOP is a novel neuro-symbolic planning framework that treats planning as an iterative conversation between neural and symbolic components, achieving a high success rate in standard benchmark domains. The key to reliable planning lies in making neural networks and symbolic reasoners communicate throughout the process."}}
{"id": "2508.13387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13387", "abs": "https://arxiv.org/abs/2508.13387", "authors": ["Thye Shan Ng", "Caren Soyeon Han", "Eun-Jung Holden"], "title": "SPANER: Shared Prompt Aligner for Multimodal Semantic Representation", "comment": null, "summary": "Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have\nsignificantly improved performance on downstream tasks such as few-shot\nretrieval. However, most existing approaches focus on task-specific gains while\nneglecting the structure of the multimodal embedding space. As a result,\nmodality-specific representations often remain isolated, limiting cross-modal\ngeneralisation. In this work, we introduce Shared Prompt AligNER (SPANER), a\nmodality-agnostic PEFT framework designed to embed inputs from diverse\nmodalities into a unified semantic space. At its core, SPANER employs a shared\nprompt mechanism that acts as a conceptual anchor, enabling semantically\nrelated instances to converge spatially regardless of modality. This shared\nprompt design is inherently extensible, supporting the seamless integration of\nadditional modalities, such as audio, without altering the core architecture.\nThrough comprehensive experiments across vision-language and audio-visual\nbenchmarks, SPANER demonstrates competitive few-shot retrieval performance\nwhile preserving high semantic coherence in the learned embedding space. Our\nresults highlight the importance of aligning embedding structures, rather than\nmerely tuning adapter weights, for scalable multimodal learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86Shared Prompt AligNER (SPANER)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u63d0\u793a\u673a\u5236\u5c06\u4e0d\u540c\u6a21\u6001\u7684\u8f93\u5165\u5d4c\u5165\u5230\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5c11\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u5728\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4fdd\u6301\u9ad8\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "motivation": "\u6700\u8fd1\u591a\u6a21\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u5728\u5c11\u6837\u672c\u68c0\u7d22\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u4efb\u52a1\u7279\u5b9a\u589e\u76ca\uff0c\u800c\u5ffd\u7565\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\u3002\u7ed3\u679c\u662f\uff0c\u6a21\u6001\u7279\u5b9a\u8868\u793a\u901a\u5e38\u4fdd\u6301\u5b64\u7acb\uff0c\u9650\u5236\u4e86\u8de8\u6a21\u6001\u6cdb\u5316\u3002\u56e0\u6b64\uff0c\u672c\u5de5\u4f5c\u5f15\u5165\u4e86Shared Prompt AligNER (SPANER)\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528Shared Prompt AligNER (SPANER)\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u5177\u6709\u5171\u4eab\u63d0\u793a\u673a\u5236\u7684\u6982\u5ff5\u951a\u70b9\uff0c\u4fc3\u4f7f\u8bed\u4e49\u76f8\u5173\u5b9e\u4f8b\u5728\u7a7a\u95f4\u4e0a\u6c47\u805a\uff0c\u65e0\u8bba\u5176\u6a21\u6001\u3002\u8fd9\u79cd\u8bbe\u8ba1\u662f\u53ef\u6269\u5c55\u7684\uff0c\u652f\u6301\u65e0\u7f1d\u96c6\u6210\u5176\u4ed6\u6a21\u6001\uff0c\u5982\u97f3\u9891\uff0c\u800c\u65e0\u9700\u6539\u53d8\u6838\u5fc3\u67b6\u6784\u3002", "result": "\u5728\u89c6\u89c9\u8bed\u8a00\u548c\u97f3\u9891- \u89c6\u89c9\u57fa\u51c6\u4e0a\uff0cSPANER\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u7684\u5c11\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4fdd\u6301\u9ad8\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86Shared Prompt AligNER (SPANER)\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u6765\u81ea\u4e0d\u540c\u6a21\u6001\u7684\u8f93\u5165\u5d4c\u5165\u5230\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\u4e2d\u3002\u901a\u8fc7\u5b9e\u9a8c\u663e\u793a\uff0cSPANER\u5728\u89c6\u89c9\u8bed\u8a00\u548c\u97f3\u9891 - \u89c6\u89c9\u57fa\u51c6\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u5c11\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7a7a\u95f4\u5177\u6709\u9ad8\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u5f3a\u8c03\u4e86\u8c03\u6574\u5d4c\u5165\u7ed3\u6784\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u8c03\u6574\u9002\u914d\u5668\u6743\u91cd\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u5b66\u4e60\u3002"}}
{"id": "2508.13404", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13404", "abs": "https://arxiv.org/abs/2508.13404", "authors": ["Nicole Cho", "Kirsty Fielding", "William Watson", "Sumitra Ganesh", "Manuela Veloso"], "title": "TASER: Table Agents for Schema-guided Extraction and Recommendation", "comment": null, "summary": "Real-world financial documents report essential information about an entity's\nfinancial holdings that can span millions of different financial instrument\ntypes. Yet, these details are often buried in messy, multi-page, fragmented\ntables - for example, 99.4% of the tables in our dataset have no bounding boxes\nwith the maximum number of rows amounting to 426 per table across 44 pages. To\ntackle these unique challenges from real-world tables, we present a\ncontinuously learning, agentic table extraction system, TASER (Table Agents for\nSchema-guided Extraction and Recommendation) that extracts highly unstructured,\nmulti-page, heterogeneous tables into normalized, schema-conforming outputs.\nOur table agents execute on table detection, classification, extraction, and\nrecommendations by leveraging an initial schema. Then, our Recommender Agent\nreviews the outputs, recommends schema revisions, and decides on the final\nrecommendations, enabling TASER to outperform existing table detection models\nsuch as Table Transformer by 10.1%. Within this continuous learning process, we\nhighlight that larger batch sizes result in a 104.3% increase in schema\nrecommendations that are actionable and utilized, resulting in a 9.8% increase\nin extracted holdings - highlighting the importance of a continuous learning\nprocess. To train TASER, we have manually labeled 22,584 pages (28,150,449\ntokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of\nthe first real financial table datasets. We release our dataset TASERTab to\nenable the research community to access real-world financial tables and\noutputs. Our results highlight the promise of agentic, schema-guided extraction\nsystems for robust understanding of real-world financial tables.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aTASER\u7684\u4e0d\u65ad\u5b66\u4e60\u7684\u8868\u683c\u63d0\u53d6\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u9ad8\u5ea6\u975e\u7ed3\u6784\u5316\u3001\u591a\u9875\u3001\u5f02\u6784\u8868\u683c\u63d0\u53d6\u6210\u89c4\u8303\u5316\u3001\u7b26\u5408\u6a21\u5f0f\u7684\u8f93\u51fa\u3002\u4f5c\u8005\u901a\u8fc7\u5f00\u53d1TASER\u7cfb\u7edf\uff0c\u7ed3\u5408\u8868\u683c\u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u63d0\u53d6\u548c\u63a8\u8350\u7b49\u529f\u80fd\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u975e\u7ed3\u6784\u5316\u8868\u683c\u7684\u63d0\u53d6\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5b9e\u9645\u91d1\u878d\u6587\u6863\u4e2d\u7684\u8868\u683c\u63d0\u53d6\u95ee\u9898\uff0c\u5e76\u7a81\u663e\u4e86\u57fa\u4e8e\u6a21\u5f0f\u7684\u4e3b\u52a8\u63d0\u53d6\u7cfb\u7edf\u5728\u7406\u89e3\u771f\u5b9e\u8d22\u52a1\u8868\u683c\u65b9\u9762\u7684\u6f5c\u529b\u3002\u4f5c\u8005\u6210\u529f\u5f00\u53d1\u4e86TASER\u7cfb\u7edf\u5e76\u53d1\u5e03\u4e86\u65b0\u7684\u8d22\u52a1\u8868\u683c\u6570\u636e\u96c6TASERTab\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5b9e\u9645\u91d1\u878d\u6587\u6863\u4e2d\u6df7\u4e71\u3001\u591a\u9875\u3001\u788e\u7247\u5316\u8868\u683c\u7684\u63d0\u53d6\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u91d1\u878d\u6301\u6709\u60c5\u51b5\u7684\u7406\u89e3\u548c\u5904\u7406\u6548\u7387\u3002\u901a\u8fc7\u5f15\u5165\u8868\u683c\u4ee3\u7406\u7cfb\u7edfTASER\uff0c\u4f5c\u8005\u5e0c\u671b\u80fd\u591f\u6709\u6548\u5730\u4ece\u8fd9\u4e9b\u771f\u5b9e\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u7528\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u4e0d\u65ad\u5b66\u4e60\u4e0d\u65ad\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5f00\u53d1TASER\u7cfb\u7edf\uff0c\u7ed3\u5408\u8868\u683c\u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u63d0\u53d6\u548c\u63a8\u8350\u7b49\u529f\u80fd\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u975e\u7ed3\u6784\u5316\u8868\u683c\u7684\u63d0\u53d6\u3002\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u6301\u7eed\u4f18\u5316\u6a21\u5f0f\u5efa\u8bae\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u4e3a\u4e86\u8bad\u7ec3TASER\uff0c\u4f5c\u8005\u624b\u52a8\u6807\u8bb0\u4e86\u5927\u91cf\u8d22\u52a1\u6587\u6863\u9875\u9762\u548c\u8868\u683c\uff0c\u521b\u9020\u4e86\u4e00\u4e2a\u65b0\u7684\u8d22\u52a1\u8868\u683c\u6570\u636e\u96c6TASERTab\u3002", "result": "\u4f5c\u8005\u6210\u529f\u5f00\u53d1\u4e86TASER\u7cfb\u7edf\uff0c\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\u53ef\u4ee5\u63d0\u9ad8\u6a21\u5f0f\u5efa\u8bae\u7684\u8d28\u91cf\u548c\u63d0\u53d6\u7684\u51c6\u786e\u6027\u3002\u6700\u7ec8\uff0c\u4f5c\u8005\u53d1\u5e03\u4e86\u65b0\u7684\u8d22\u52a1\u8868\u683c\u6570\u636e\u96c6TASERTab\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aTASER\u7684\u4e0d\u65ad\u5b66\u4e60\u7684\u8868\u683c\u63d0\u53d6\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u9ad8\u5ea6\u975e\u7ed3\u6784\u5316\u3001\u591a\u9875\u3001\u5f02\u6784\u8868\u683c\u63d0\u53d6\u6210\u89c4\u8303\u5316\u3001\u7b26\u5408\u6a21\u5f0f\u7684\u8f93\u51fa\u3002TASER\u901a\u8fc7\u8868\u683c\u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u63d0\u53d6\u548c\u63a8\u8350\u5b9e\u73b0\u5176\u529f\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u63a8\u8350\u4ee3\u7406\u8bc4\u5ba1\u8f93\u51fa\u5e76\u63d0\u51fa\u6a21\u5f0f\u4fee\u8ba2\u5efa\u8bae\uff0c\u4ece\u800c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u8868\u683c\u68c0\u6d4b\u6a21\u578b\u3002\u4f5c\u8005\u53d1\u73b0\uff0c\u4f7f\u7528\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\u53ef\u4ee5\u663e\u8457\u589e\u52a0\u53ef\u64cd\u4f5c\u4e14\u5b9e\u7528\u7684\u6a21\u5f0f\u5efa\u8bae\uff0c\u5e76\u63d0\u9ad8\u63d0\u53d6\u8d44\u4ea7\u7684\u91cf\u3002\u4f5c\u8005\u8fd8\u4ecb\u7ecd\u4e86\u4ed6\u4eec\u521b\u5efa\u7684\u7b2c\u4e00\u4e2a\u771f\u5b9e\u8d22\u52a1\u8868\u683c\u6570\u636e\u96c6TASERTab\uff0c\u5e76\u91ca\u653e\u8be5\u6570\u636e\u96c6\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002\u6700\u540e\uff0c\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u57fa\u4e8e\u6a21\u5f0f\u7684\u4e3b\u52a8\u63d0\u53d6\u7cfb\u7edf\u5728\u7406\u89e3\u771f\u5b9e\u8d22\u52a1\u8868\u683c\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.13421", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.13421", "abs": "https://arxiv.org/abs/2508.13421", "authors": ["Gabrielle Wehr", "Reuben Rideaux", "Amaya J. Fox", "David R. Lightfoot", "Jason Tangen", "Jason B. Mattingley", "Shane E. Ehrhardt"], "title": "Virtuous Machines: Towards Artificial General Science", "comment": null, "summary": "Artificial intelligence systems are transforming scientific discovery by\naccelerating specific research tasks, from protein structure prediction to\nmaterials design, yet remain confined to narrow domains requiring substantial\nhuman oversight. The exponential growth of scientific literature and increasing\ndomain specialisation constrain researchers' capacity to synthesise knowledge\nacross disciplines and develop unifying theories, motivating exploration of\nmore general-purpose AI systems for science. Here we show that a\ndomain-agnostic, agentic AI system can independently navigate the scientific\nworkflow - from hypothesis generation through data collection to manuscript\npreparation. The system autonomously designed and executed three psychological\nstudies on visual working memory, mental rotation, and imagery vividness,\nexecuted one new online data collection with 288 participants, developed\nanalysis pipelines through 8-hour+ continuous coding sessions, and produced\ncompleted manuscripts. The results demonstrate the capability of AI scientific\ndiscovery pipelines to conduct non-trivial research with theoretical reasoning\nand methodological rigour comparable to experienced researchers, though with\nlimitations in conceptual nuance and theoretical interpretation. This is a step\ntoward embodied AI that can test hypotheses through real-world experiments,\naccelerating discovery by autonomously exploring regions of scientific space\nthat human cognitive and resource constraints might otherwise leave unexplored.\nIt raises important questions about the nature of scientific understanding and\nthe attribution of scientific credit.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u7684\u3001\u4e3b\u52a8\u7684AI\u7cfb\u7edf\u53ef\u4ee5\u72ec\u7acb\u8fdb\u884c\u79d1\u5b66\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u5047\u8bbe\u751f\u6210\u3001\u6570\u636e\u6536\u96c6\u548c\u6587\u7a3f\u51c6\u5907\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cdAI\u7cfb\u7edf\u53ef\u4ee5\u8fdb\u884c\u9ad8\u54c1\u8d28\u7684\u79d1\u5b66\u53d1\u73b0\u7814\u7a76\uff0c\u867d\u7136\u5728\u6982\u5ff5\u548c\u7406\u8bba\u9610\u91ca\u65b9\u9762\u5b58\u5728\u4e00\u5b9a\u7684\u9650\u5236\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u6307\u6570\u589e\u957f\u548c\u9886\u57df\u4e13\u4e1a\u5316\u9650\u5236\u4e86\u7814\u7a76\u4eba\u5458\u8de8\u5b66\u79d1\u7efc\u5408\u77e5\u8bc6\u548c\u53d1\u5c55\u7edf\u4e00\u7406\u8bba\u7684\u80fd\u529b\uff0c\u4fc3\u4f7f\u63a2\u7d22\u66f4\u5177\u901a\u7528\u6027\u7684AI\u7cfb\u7edf\u7528\u4e8e\u79d1\u5b66\u7814\u7a76\u3002", "method": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u7684\u3001\u4e3b\u52a8\u7684AI\u7cfb\u7edf\u5982\u4f55\u72ec\u7acb\u8fdb\u884c\u79d1\u5b66\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u5047\u8bbe\u751f\u6210\u3001\u6570\u636e\u6536\u96c6\u548c\u6587\u7a3f\u51c6\u5907\u3002\u7cfb\u7edf\u8bbe\u8ba1\u5e76\u6267\u884c\u4e86\u4e09\u9879\u5173\u4e8e\u89c6\u89c9\u5de5\u4f5c\u8bb0\u5fc6\u3001\u5fc3\u7406\u65cb\u8f6c\u548c\u5f62\u8c61\u751f\u52a8\u5ea6\u7684\u5fc3\u7406\u5b66\u7814\u7a76\uff0c\u8fdb\u884c\u4e86\u4e00\u9879\u65b0\u7684\u5728\u7ebf\u6570\u636e\u6536\u96c6\uff0c\u901a\u8fc78\u5c0f\u65f6\u4ee5\u4e0a\u7684\u8fde\u7eed\u7f16\u7801\u4f1a\u8bdd\u5f00\u53d1\u4e86\u5206\u6790\u7ba1\u7ebf\uff0c\u5e76\u4ea7\u51fa\u4e86\u5b8c\u6574\u7684\u6587\u7a3f\u3002", "result": "\u8be5AI\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u8fdb\u884c\u975e\u5e73\u51e1\u7814\u7a76\u7684\u80fd\u529b\uff0c\u867d\u7136\u5728\u6982\u5ff5\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u548c\u7406\u8bba\u89e3\u91ca\u65b9\u9762\u6709\u6b20\u7f3a\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u5c55\u793a\u4e86\u9886\u57df\u65e0\u5173\u7684AI\u7cfb\u7edf\u53ef\u4ee5\u72ec\u7acb\u5730\u6d4f\u89c8\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u5047\u8bbe\u751f\u6210\u5230\u6570\u636e\u6536\u96c6\u518d\u5230\u6587\u7a3f\u51c6\u5907\u3002\u7ed3\u679c\u8868\u660e\uff0cAI\u79d1\u5b66\u53d1\u73b0\u7ba1\u7ebf\u80fd\u591f\u8fdb\u884c\u975e\u5e73\u51e1\u7684\u7814\u7a76\uff0c\u5177\u6709\u53ef\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7814\u7a76\u4eba\u5458\u76f8\u5ab2\u7f8e\u7684\u7406\u8bba\u63a8\u7406\u548c\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\uff0c\u5c3d\u7ba1\u5728\u6982\u5ff5\u7ec6\u5fae\u5dee\u522b\u548c\u7406\u8bba\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u8fd9\u662f\u671d\u7740\u5177\u6709\u4f53\u73b0\u80fd\u529b\u7684AI\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u80fd\u591f\u901a\u8fc7\u5b9e\u5730\u5b9e\u9a8c\u9a8c\u8bc1\u5047\u8bbe\uff0c\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u79d1\u5b66\u7a7a\u95f4\u7684\u533a\u57df\u52a0\u901f\u53d1\u73b0\u3002\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u79d1\u5b66\u7406\u89e3\u7684\u672c\u8d28\u548c\u79d1\u5b66\u8d21\u732e\u5f52\u56e0\u7684\u91cd\u8981\u95ee\u9898\u3002"}}
{"id": "2508.13433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13433", "abs": "https://arxiv.org/abs/2508.13433", "authors": ["Jiayu Fang", "Zhiqi Shao", "S T Boris Choy", "Junbin Gao"], "title": "STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting", "comment": null, "summary": "Spatio-temporal traffic forecasting is challenging due to complex temporal\npatterns, dynamic spatial structures, and diverse input formats. Although\nTransformer-based models offer strong global modeling, they often struggle with\nrigid temporal encoding and weak space-time fusion. We propose STPFormer, a\nSpatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art\nperformance via unified and interpretable representation learning. It\nintegrates four modules: Temporal Position Aggregator (TPA) for pattern-aware\ntemporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial\nlearning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,\nand an Attention Mixer for multi-scale fusion. Experiments on five real-world\ndatasets show that STPFormer consistently sets new SOTA results, with ablation\nand visualizations confirming its effectiveness and generalizability.", "AI": {"tldr": "STPFormer is a novel Spatio-Temporal Pattern-Aware Transformer model that enhances spatio-temporal traffic forecasting by integrating specialized modules for various aspects of the forecasting task. It surpasses existing models in performance across different datasets, showcasing its effectiveness and adaptability.", "motivation": "The motivation behind the paper is to improve spatio-temporal traffic forecasting by overcoming the challenges posed by complex temporal patterns, dynamic spatial structures, and diverse input formats, which are not effectively handled by existing Transformer models.", "method": "The paper proposes STPFormer, a Spatio-Temporal Pattern-Aware Transformer model that addresses the limitations of Transformer-based models in spatio-temporal forecasting. It integrates modules for temporal encoding, spatial learning, cross-domain alignment, and fusion.", "result": "STPFormer outperforms existing models on five real-world datasets, demonstrating its effectiveness and generalizability through ablation studies and visualizations.", "conclusion": "STPFormer achieves state-of-the-art performance in spatio-temporal traffic forecasting by integrating four modules for pattern-aware temporal encoding, sequential spatial learning, cross-domain alignment, and multi-scale fusion."}}
{"id": "2508.13437", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13437", "abs": "https://arxiv.org/abs/2508.13437", "authors": ["Cheikh Ahmed", "Mahdi Mostajabdaveh", "Samin Aref", "Zirui Zhou"], "title": "Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences", "comment": null, "summary": "We introduce the Discrete Min-Max Violation (DMMV) as a general optimization\nproblem which seeks an assignment of discrete values to variables that\nminimizes the largest constraint violation. This context-free mathematical\nformulation is applicable to a wide range of use cases that have worst-case\nperformance requirements. After defining the DMMV problem mathematically, we\nexplore its properties to establish a foundational understanding. To tackle\nDMMV instance sizes of practical relevance, we develop a GPU-accelerated\nheuristic that takes advantage of the mathematical properties of DMMV for\nspeeding up the solution process. We demonstrate the versatile applicability of\nour heuristic by solving three optimization problems as use cases: (1)\npost-training quantization of language models, (2) discrete tomography, and (3)\nFinite Impulse Response (FIR) filter design. In quantization without outlier\nseparation, our heuristic achieves 14% improvement on average over existing\nmethods. In discrete tomography, it reduces reconstruction error by 16% under\nuniform noise and accelerates computations by a factor of 6 on GPU. For FIR\nfilter design, it nearly achieves 50% ripple reduction compared to using the\ncommercial integer optimization solver, Gurobi. Our comparative results point\nto the benefits of studying DMMV as a context-free optimization problem and the\nadvantages that our proposed heuristic offers on three distinct problems. Our\nGPU-accelerated heuristic will be made open-source to further stimulate\nresearch on DMMV and its other applications. The code is available at\nhttps://anonymous.4open.science/r/AMVM-5F3E/", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u79bb\u6563\u6700\u5927\u6700\u5c0f\u8fdd\u53cd\u503c\uff08DMMV\uff09\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5b9a\u4e49DMMV\u95ee\u9898\u5e76\u63a2\u7d22\u5176\u5c5e\u6027\uff0c\u5f00\u53d1GPU\u52a0\u901f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u89c4\u6a21\u7684DMMV\u5b9e\u4f8b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e09\u4e2a\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u5e94\u7528\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u9488\u5bf9\u5177\u6709\u6700\u574f\u60c5\u51b5\u6027\u80fd\u8981\u6c42\u7684\u5e7f\u6cdb\u7528\u4f8b\uff0c\u5f15\u5165\u4e86DMMV\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u6700\u5c0f\u5316\u6700\u5927\u7ea6\u675f\u8fdd\u53cd\u503c\u3002\u63a2\u7d22DMMV\u95ee\u9898\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u548c\u5c5e\u6027\uff0c\u65e8\u5728\u52a0\u901f\u89e3\u51b3\u8fc7\u7a0b\u3002", "method": "\u5b9a\u4e49\u4e86DMMV\u95ee\u9898\u5e76\u63a2\u7d22\u5176\u5c5e\u6027\u4ee5\u5efa\u7acb\u57fa\u7840\u7406\u89e3\uff0c\u5f00\u53d1\u4e86GPU\u52a0\u901f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u89c4\u6a21\u7684DMMV\u5b9e\u4f8b\uff0c\u901a\u8fc7\u6570\u5b66\u7279\u6027\u52a0\u5feb\u89e3\u51b3\u8fc7\u7a0b\u3002\u901a\u8fc7\u89e3\u51b3\u4e09\u4e2a\u4f18\u5316\u95ee\u9898\u6765\u5c55\u793a\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u591a\u529f\u80fd\u9002\u7528\u6027\uff0c\u53d6\u5f97\u4e8614%\u81f350%\u7684\u6539\u8fdb\u3002", "result": "\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7ed3\u679c\uff0c\u5305\u62ec\u5728\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5316\u3001\u79bb\u6563\u65ad\u5c42\u6444\u5f71\u672f\u548c\u6709\u9650\u8109\u51b2\u54cd\u5e94\uff08FIR\uff09\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u4e2d\u83b7\u5f97\u7684\u6539\u8fdb\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4e86\u7814\u7a76DMMV\u4f5c\u4e3a\u65e0\u4e0a\u4e0b\u6587\u4f18\u5316\u95ee\u9898\u7684\u597d\u5904\uff0c\u5e76\u5c55\u793a\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u4e09\u4e2a\u4e0d\u540c\u95ee\u9898\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u4ecb\u7ecd\u4e86\u79bb\u6563\u6700\u5927\u6700\u5c0f\u8fdd\u53cd\u503c\uff08DMMV\uff09\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u4f18\u5316\u95ee\u9898\uff0c\u65e8\u5728\u5c06\u79bb\u6563\u503c\u5206\u914d\u7ed9\u53d8\u91cf\uff0c\u4f7f\u6700\u5927\u7ea6\u675f\u8fdd\u53cd\u503c\u6700\u5c0f\u5316\u3002\u63d0\u51fa\u4e86\u6570\u5b66\u5f62\u5f0f\u5316\u7684DMMV\u95ee\u9898\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5c5e\u6027\u4ee5\u5efa\u7acb\u57fa\u7840\u7406\u89e3\u3002\u5f00\u53d1\u4e86\u9488\u5bf9\u5b9e\u9645\u95ee\u9898\u89c4\u6a21\u7684GPU\u52a0\u901f\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528DMMV\u7684\u6570\u5b66\u7279\u6027\u52a0\u5feb\u89e3\u51b3\u8fc7\u7a0b\u3002\u901a\u8fc7\u89e3\u51b3\u4e09\u4e2a\u4f18\u5316\u95ee\u9898\u6765\u5c55\u793a\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u591a\u529f\u80fd\u9002\u7528\u6027\uff0c\u5e76\u5728\u8fd9\u4e9b\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4e86\u7814\u7a76DMMV\u4f5c\u4e3a\u4e00\u4e2a\u65e0\u4e0a\u4e0b\u6587\u4f18\u5316\u95ee\u9898\u4ee5\u53ca\u6211\u4eec\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u4e09\u4e2a\u4e0d\u540c\u95ee\u9898\u4e0a\u7684\u4f18\u52bf\u3002\u5c06GPU\u52a0\u901f\u542f\u53d1\u5f0f\u65b9\u6cd5\u5f00\u6e90\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4fc3\u8fdbDMMV\u53ca\u5176\u5176\u4ed6\u5e94\u7528\u7684\u7814\u7a76\u3002"}}
{"id": "2508.13465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13465", "abs": "https://arxiv.org/abs/2508.13465", "authors": ["Yuzhi Tang", "Tianxiao Li", "Elizabeth Li", "Chris J. Maddison", "Honghua Dong", "Yangjun Ruan"], "title": "LM Agents May Fail to Act on Their Own Risk Knowledge", "comment": null, "summary": "Language model (LM) agents have demonstrated significant potential for\nautomating real-world tasks, yet they pose a diverse array of potential, severe\nrisks in safety-critical scenarios. In this work, we identify a significant gap\nbetween LM agents' risk awareness and safety execution abilities: while they\noften answer \"Yes\" to queries like \"Is executing `sudo rm -rf /*' dangerous?\",\nthey will likely fail to identify such risks in instantiated trajectories or\neven directly perform these risky actions when acting as agents. To\nsystematically investigate this, we develop a comprehensive evaluation\nframework to examine agents' safety across three progressive dimensions: 1)\ntheir knowledge about potential risks, 2) their ability to identify\ncorresponding risks in execution trajectories, and 3) their actual behaviors to\navoid executing these risky actions. Our evaluation reveals two critical\nperformance gaps that resemble the generator-validator gaps observed in LMs:\nwhile agents demonstrate near-perfect risk knowledge ($>98\\%$ pass rates), they\nfail to apply this knowledge when identifying risks in actual scenarios (with\nperformance dropping by $>23\\%$) and often still execute risky actions ($<26\\%$\npass rates). Notably, this trend persists across more capable LMs as well as in\nspecialized reasoning models like DeepSeek-R1, indicating that simply scaling\nmodel capabilities or inference compute does not inherently resolve safety\nconcerns. Instead, we take advantage of these observed gaps to develop a risk\nverifier that independently critiques the proposed actions by agents, with an\nabstractor that converts specific execution trajectories into abstract\ndescriptions where LMs can more effectively identify the risks. Our overall\nsystem achieves a significant reduction of risky action execution by $55.3\\%$\nover vanilla-prompted agents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u98ce\u9669\u610f\u8bc6\u4e0e\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u5f00\u53d1\u8bc4\u4f30\u6846\u67b6\u548c\u98ce\u9669\u9a8c\u8bc1\u65b9\u6cd5\u6765\u6539\u5584\u4ee3\u7406\u7684\u5b89\u5168\u6027\u80fd\u3002\u8bc4\u4f30\u663e\u793a\u4ee3\u7406\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u4f46\u4f5c\u8005\u63d0\u51fa\u7684\u7cfb\u7edf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4ee3\u7406\u6267\u884c\u98ce\u9669\u884c\u4e3a\u7684\u6bd4\u7387\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u53ef\u80fd\u4ea7\u751f\u7684\u4e25\u91cd\u98ce\u9669\uff0c\u5e76\u89e3\u51b3\u4ee3\u7406\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u98ce\u9669\u610f\u8bc6\u548c\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63d0\u9ad8\u4ee3\u7406\u5728\u8bc6\u522b\u548c\u907f\u514d\u98ce\u9669\u884c\u4e3a\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4ece\u800c\u589e\u5f3a\u4ee3\u7406\u7684\u5b89\u5168\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u68c0\u67e5\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u5b89\u5168\u6027\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e86\u4ee3\u7406\u7684\u98ce\u9669\u77e5\u8bc6\u3001\u98ce\u9669\u8bc6\u522b\u80fd\u529b\u548c\u907f\u514d\u98ce\u9669\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u952e\u6027\u80fd\u5dee\u8ddd\u3002\u968f\u540e\uff0c\u5229\u7528\u89c2\u5bdf\u5230\u7684\u5dee\u8ddd\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u9a8c\u8bc1\u5668\u548c\u62bd\u8c61\u5668\u6765\u6539\u5584\u4ee3\u7406\u7684\u5b89\u5168\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6027\u80fd\u5dee\u8ddd\uff0c\u5373\u4ee3\u7406\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8bc6\u522b\u98ce\u9669\u548c\u907f\u514d\u98ce\u9669\u884c\u4e3a\u7684\u80fd\u529b\u8f83\u5dee\u3002\u4f5c\u8005\u63d0\u51fa\u7684\u98ce\u9669\u9a8c\u8bc1\u5668\u548c\u62bd\u8c61\u5668\u663e\u8457\u964d\u4f4e\u4e86\u4ee3\u7406\u6267\u884c\u98ce\u9669\u884c\u4e3a\u7684\u6bd4\u7387\u3002", "conclusion": "\u4f5c\u8005\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5b58\u5728\u98ce\u9669\u610f\u8bc6\u548c\u5b89\u5168\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u98ce\u9669\u9a8c\u8bc1\u65b9\u6cd5\u4ee5\u51cf\u5c11\u98ce\u9669\u884c\u4e3a\u3002\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u4ee3\u7406\u5728\u98ce\u9669\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8bc6\u522b\u98ce\u9669\u548c\u907f\u514d\u98ce\u9669\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002\u4f5c\u8005\u901a\u8fc7\u53d1\u5c55\u4e00\u4e2a\u98ce\u9669\u9a8c\u8bc1\u5668\u548c\u62bd\u8c61\u5668\u6765\u63d0\u9ad8\u4ee3\u7406\u7684\u5b89\u5168\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5bf9\u98ce\u9669\u884c\u4e3a\u7684\u663e\u8457\u51cf\u5c11\u3002"}}
{"id": "2508.13530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13530", "abs": "https://arxiv.org/abs/2508.13530", "authors": ["Junyeong Park", "Hyeonseo Cho", "Sungjin Ahn"], "title": "CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter", "comment": null, "summary": "Developing general-purpose embodied agents is a core challenge in AI.\nMinecraft provides rich complexity and internet-scale data, but its slow speed\nand engineering overhead make it unsuitable for rapid prototyping. Crafter\noffers a lightweight alternative that retains key challenges from Minecraft,\nyet its use has remained limited to narrow tasks due to the absence of\nfoundation models that have driven progress in the Minecraft setting. In this\npaper, we present CrafterDojo, a suite of foundation models and tools that\nunlock the Crafter environment as a lightweight, prototyping-friendly, and\nMinecraft-like testbed for general-purpose embodied agent research. CrafterDojo\naddresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for\nbehavior priors, vision-language grounding, and instruction following,\nrespectively. In addition, we provide toolkits for generating behavior and\ncaption datasets (CrafterPlay and CrafterCaption), reference agent\nimplementations, benchmark evaluations, and a complete open-source codebase.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CrafterDojo\uff0c\u4e00\u4e2a\u65e8\u5728\u63d0\u4f9b\u901a\u7528\u76ee\u7684\u7684\u8f7b\u91cf\u7ea7\u3001\u6613\u4e8e\u539f\u578b\u8bbe\u8ba1\u4e14\u7c7b\u4f3c\u4e8eMinecraft\u7684\u5b9e\u9a8c\u5e73\u53f0\u7684\u5957\u4ef6\uff0c\u4ecb\u7ecd\u4e86\u5305\u62ec\u884c\u4e3a\u5148\u9a8c\u3001\u89c6\u89c9-\u8bed\u8a00\u8054\u7cfb\u548c\u6307\u4ee4\u9075\u5faa\u5728\u5185\u7684\u4e00\u7cfb\u5217\u57fa\u7840\u6a21\u578b\u548c\u5de5\u5177\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5f00\u6e90\u4ee3\u7801\u5e93\u548c\u5de5\u5177\u5305\u3002", "motivation": "Minecraft\u867d\u7136\u63d0\u4f9b\u4e86\u4e30\u5bcc\u590d\u6742\u6027\u548c\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u901f\u5ea6\u7f13\u6162\u548c\u5de5\u7a0b\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3002Crafter\u4f5c\u4e3a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\uff0c\u4fdd\u7559\u4e86Minecraft\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u63a8\u52a8Minecraft\u8fdb\u5c55\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5176\u4f7f\u7528\u4ecd\u7136\u53d7\u9650\u4e8e\u72ed\u7a84\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86CrafterDojo\u3002", "method": "\u4ecb\u7ecd\u4e86CrafterDojo\u7684\u4e00\u7cfb\u5217\u57fa\u7840\u6a21\u578b\u548c\u5de5\u5177\uff0c\u5305\u62ec\u7528\u4e8e\u884c\u4e3a\u5148\u9a8c\u3001\u89c6\u89c9-\u8bed\u8a00\u8054\u7cfb\u548c\u6307\u4ee4\u9075\u5faa\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u7528\u4e8e\u751f\u6210\u884c\u4e3a\u548c\u5b57\u5e55\u6570\u636e\u96c6\u7684\u5de5\u5177\u5305\u3002\u6b64\u5916\uff0c\u63d0\u4f9b\u4e86\u53c2\u8003\u4ee3\u7406\u5b9e\u73b0\u3001\u57fa\u51c6\u8bc4\u4f30\u548c\u5b8c\u6574\u7684\u5f00\u6e90\u4ee3\u7801\u5e93\u3002", "result": "\u5f15\u5165\u4e86CrafterDojo\uff0c\u5305\u62ecCrafterVPT\u3001CrafterCLIP\u548cCrafterSteve-1\u7b49\u6a21\u578b\uff0c\u4e3a\u901a\u7528\u76ee\u7684\u7684\u5177\u8eab\u4f53\u7279\u5f81\u4ee3\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u539f\u578b\u8bbe\u8ba1\u53cb\u597d\u4e14\u7c7b\u4f3c\u4e8eMinecraft\u7684\u5b9e\u9a8c\u5e73\u53f0\u3002\u63d0\u4f9b\u4e86\u5de5\u5177\u5305\u7528\u4e8e\u751f\u6210\u884c\u4e3a\u548c\u5b57\u5e55\u6570\u636e\u96c6\uff0c\u53c2\u8003\u4ee3\u7406\u5b9e\u73b0\uff0c\u57fa\u51c6\u8bc4\u4f30\u4ee5\u53ca\u5b8c\u6574\u7684\u5f00\u6e90\u4ee3\u7801\u5e93\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CrafterDojo\uff0c\u65e8\u5728\u89e3\u9501Crafter\u73af\u5883\uff0c\u4f5c\u4e3a\u901a\u7528\u76ee\u7684\u7684\u5177\u8eab\u4f53\u7279\u5f81\u4ee3\u7406\u7814\u7a76\u7684\u8f7b\u91cf\u7ea7\u3001\u6613\u4e8e\u539f\u578b\u8bbe\u8ba1\u4e14\u7c7b\u4f3c\u4e8eMinecraft\u7684\u5b9e\u9a8c\u5e73\u53f0\u3002"}}
{"id": "2508.13579", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13579", "abs": "https://arxiv.org/abs/2508.13579", "authors": ["Yue Fang", "Yuxin Guo", "Jiaran Gao", "Hongxin Ding", "Xinke Jiang", "Weibin Liao", "Yongxin Xu", "Yinghao Zhu", "Zhibang Yang", "Liantao Ma", "Junfeng Zhao", "Yasha Wang"], "title": "Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance", "comment": null, "summary": "Improving large language models (LLMs) for electronic health record (EHR)\nreasoning is essential for enabling accurate and generalizable clinical\npredictions. While LLMs excel at medical text understanding, they underperform\non EHR-based prediction tasks due to challenges in modeling temporally\nstructured, high-dimensional data. Existing approaches often rely on hybrid\nparadigms, where LLMs serve merely as frozen prior retrievers while downstream\ndeep learning (DL) models handle prediction, failing to improve the LLM's\nintrinsic reasoning capacity and inheriting the generalization limitations of\nDL models. To this end, we propose EAG-RL, a novel two-stage training framework\ndesigned to intrinsically enhance LLMs' EHR reasoning ability through expert\nattention guidance, where expert EHR models refer to task-specific DL models\ntrained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise\nreasoning trajectories using expert-guided Monte Carlo Tree Search to\neffectively initialize the LLM's policy. Then, EAG-RL further optimizes the\npolicy via reinforcement learning by aligning the LLM's attention with\nclinically salient features identified by expert EHR models. Extensive\nexperiments on two real-world EHR datasets show that EAG-RL improves the\nintrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also\nenhancing robustness to feature perturbations and generalization to unseen\nclinical domains. These results demonstrate the practical potential of EAG-RL\nfor real-world deployment in clinical prediction tasks. Our code have been\navailable at https://github.com/devilran6/EAG-RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EAG-RL\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u4e13\u5bb6\u5f15\u5bfc\u6765\u5185\u5728\u589e\u5f3aLLMs\u7684EHR\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEAG-RL\u5e73\u5747\u63d0\u9ad8\u4e86LLMs\u7684EHR\u63a8\u7406\u80fd\u529b14.62\uff05\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u7279\u5f81\u6270\u52a8\u7684\u7a33\u5065\u6027\u548c\u5bf9\u672a\u89c1\u4e34\u5e8a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5e38\u5e38\u4f9d\u8d56\u6df7\u5408\u8303\u5f0f\uff0c\u5176\u4e2dLLMs\u4ec5\u4f5c\u4e3a\u51bb\u7ed3\u7684\u5148\u524d\u68c0\u7d22\u5668\uff0c\u800c\u4e0b\u6e38\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u5904\u7406\u9884\u6d4b\uff0c\u672a\u80fd\u63d0\u9ad8LLMs\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7ee7\u627fDL\u6a21\u578b\u7684\u6cdb\u5316\u9650\u5236\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u6539\u5584LLMs\u5728EHR\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3aLLMs\u7684\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86EAG-RL\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\uff0c\u6709\u6548\u521d\u59cb\u5316\u4e86LLM\u7684\u7b56\u7565\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\uff0c\u4f7fLLM\u7684\u5173\u6ce8\u70b9\u4e0e\u4e13\u5bb6EHR\u6a21\u578b\u8bc6\u522b\u51fa\u7684\u4e34\u5e8a\u663e\u8457\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7EAG-RL\u6846\u67b6\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86LLMs\u7684EHR\u63a8\u7406\u80fd\u529b14.62\uff05\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u7279\u5f81\u6270\u52a8\u7684\u7a33\u5065\u6027\u548c\u5bf9\u672a\u89c1\u4e34\u5e8a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAG-RL\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u4e13\u5bb6\u5173\u6ce8\u5f15\u5bfc\u6765\u5185\u5728\u589e\u5f3aLLMs\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u63a8\u7406\u80fd\u529b\u3002 \u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684EHR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEAG-RL\u5c06LLMs\u7684\u5185\u5728EHR\u63a8\u7406\u80fd\u529b\u63d0\u9ad8\u4e86\u5e73\u574714.62\uff05\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5bf9\u7279\u5f81\u6270\u52a8\u7684\u7a33\u5065\u6027\u548c\u5bf9\u672a\u89c1\u4e34\u5e8a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86EAG-RL\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.13587", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13587", "abs": "https://arxiv.org/abs/2508.13587", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Liming Zheng", "Yufeng Zhong", "Lin Ma"], "title": "Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation", "comment": "technical report", "summary": "While reinforcement learning (RL) has proven highly effective for general\nreasoning in vision-language models, its application to tasks requiring\nin-depth understanding of information-rich images and generation of structured\noutputs remains underexplored. Chart-to-code generation exemplifies this\nchallenge, demanding complex reasoning over visual charts to generate\nstructured code. Supervised fine-tuning (SFT) alone is often insufficient,\nhighlighting the need for effective RL strategies that appropriately reward\nstructured outputs. We systematically investigate the performance plateau in\nSFT through large-scale experiments and propose Multimodal Structured\nReinforcement Learning (MSRL) for chart-to-code generation, which substantially\nbreaks through this plateau. We construct the largest training corpus to date,\ncontaining 3 million chart-code pairs from real-world arXiv tables to mitigate\nsimplistic patterns of prior synthetic data. Despite reaching state-of-the-art\nperformance, our experiments show that scaling SFT data eventually hits a\nplateau where further increases yield negligible improvements. Our MSRL method\nleverages a multi-granularity structured reward system using multimodal textual\nand visual feedback. At the textual level, rule-based rewards validate\nfine-grained code details. At the visual level, model-based rewards assess\nstructural similarity by rendering generated code into images and employing an\nevaluator model. We implement this within a two-stage curriculum for training\nstability. Results demonstrate that MSRL significantly breaks the SFT plateau,\nimproving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA\nbenchmarks respectively, achieving competitive performance with advanced\nclosed-source models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u8868\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u76d1\u7763\u5fae\u8c03\u6027\u80fd\u7684\u5e73\u7a33\u6027\uff0c\u5e76\u63d0\u51fa\u4e86MSRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u8bed\u6599\u5e93\u548c\u4f7f\u7528\u591a\u6a21\u6001\u7ed3\u6784\u5316\u5956\u52b1\u7cfb\u7edf\uff0c\u5728\u6027\u80fd\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u7a81\u7834\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5148\u8fdb\u95ed\u6e90\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u6307\u51fa\u4e86\u76d1\u7763\u5fae\u8c03\u5355\u72ec\u5e94\u7528\u4e0d\u8db3\u4ee5\u89e3\u51b3\u56fe\u8868\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4ee5\u9002\u5f53\u5956\u52b1\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "method": "\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u76d1\u7763\u5fae\u8c03\u5728\u56fe\u8868\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u6027\u80fd\u5e73\u53f0\u7684\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86Multimodal Structured Reinforcement Learning\uff08MSRL\uff09\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u6784\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u591a\u6a21\u6001\u6587\u672c\u548c\u89c6\u89c9\u53cd\u9988\u7684\u591a\u7c92\u5ea6\u7ed3\u6784\u5316\u5956\u52b1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5728ChartMimic\u548cReachQA\u57fa\u51c6\u4e0a\u9ad8\u6c34\u5e73\u6307\u6807\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u7ed3\u6784\u5316\u5f3a\u5316\u5b66\u4e60\uff08MSRL\uff09\u5728\u56fe\u8868\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u7a81\u7834\u4e86\u6027\u80fd\u5e73\u53f0\uff0c\u4e0e\u5148\u8fdb\u7684\u95ed\u6e90\u6a21\u578b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u80fd\u3002"}}
{"id": "2508.13634", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13634", "abs": "https://arxiv.org/abs/2508.13634", "authors": ["Jikai Chen", "Long Chen", "Dong Wang", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu"], "title": "V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task", "comment": null, "summary": "Precise localization of GUI elements is crucial for the development of GUI\nagents. Traditional methods rely on bounding box or center-point regression,\nneglecting spatial interaction uncertainty and visual-semantic hierarchies.\nRecent methods incorporate attention mechanisms but still face two key issues:\n(1) ignoring processing background regions causes attention drift from the\ndesired area, and (2) uniform labeling fails to distinguish between center and\nedges of the target UI element, leading to click imprecision. Inspired by how\nhumans visually process and interact with GUI elements, we propose the\nValley-to-Peak (V2P) method to address these issues. To mitigate background\ndistractions, V2P introduces a suppression attention mechanism that minimizes\nthe model's focus on irrelevant regions to highlight the intended region. For\nthe issue of center-edge distinction, V2P applies a Fitts' Law-inspired\napproach by modeling GUI interactions as 2D Gaussian heatmaps where the weight\ngradually decreases from the center towards the edges. The weight distribution\nfollows a Gaussian function, with the variance determined by the target's size.\nConsequently, V2P effectively isolates the target area and teaches the model to\nconcentrate on the most essential point of the UI element. The model trained by\nV2P achieves the performance with 92.3% and 50.5% on two benchmarks\nScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's\ncontribution, highlighting V2P's generalizability for precise GUI grounding\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684Valley-to-Peak (V2P)\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8eFitts' Law\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86GUI\u5143\u7d20\u5b9a\u4f4d\u4e2d\u5b58\u5728\u7684\u80cc\u666f\u5e72\u6270\u548c\u4e2d\u5fc3-\u8fb9\u7f18\u533a\u522b\u7b49\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728GUI\u5143\u7d20\u5b9a\u4f4d\u4e2d\u5b58\u5728\u7a7a\u95f4\u4ea4\u4e92\u4e0d\u786e\u5b9a\u6027\u548c\u89c6\u89c9-\u8bed\u4e49\u5c42\u6b21\u7b49\u65b9\u9762\u7684\u7f3a\u5931\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u4e86\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u7740\u5ffd\u7565\u80cc\u666f\u533a\u57df\u5904\u7406\u548c\u7edf\u4e00\u6807\u8bb0\u5bfc\u81f4\u70b9\u51fb\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002\u53d7\u4eba\u7c7b\u89c6\u89c9\u5904\u7406\u548c\u4e0eGUI\u5143\u7d20\u4ea4\u4e92\u65b9\u5f0f\u7684\u542f\u53d1\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Valley-to-Peak (V2P)\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165Valley-to-Peak (V2P)\u65b9\u6cd5\u89e3\u51b3\u4e86GUI\u5143\u7d20\u5b9a\u4f4d\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5176\u4e2dV2P\u65b9\u6cd5\u5305\u62ec\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8eFitts' Law\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u5904\u7406\u4e86\u80cc\u666f\u5e72\u6270\u548c\u4e2d\u5fc3-\u8fb9\u7f18\u533a\u522b\u7b49\u6311\u6218\u3002", "result": "\u901a\u8fc7Valley-to-Peak (V2P)\u65b9\u6cd5\uff0c\u6a21\u578b\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u53d6\u5f97\u4e8692.3%\u548c50.5%\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u5b9e\u4e86V2P\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aValley-to-Peak (V2P)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584GUI\u5143\u7d20\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5f15\u5165\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8eFitts' Law\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cV2P\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u53d6\u5f97\u4e8692.3%\u548c50.5%\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u8d21\u732e\uff0c\u663e\u793a\u51faV2P\u65b9\u6cd5\u5728\u7cbe\u786eGUI\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2508.13663", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13663", "abs": "https://arxiv.org/abs/2508.13663", "authors": ["Daniel Daza", "Alberto Bernardi", "Luca Costabello", "Christophe Gueret", "Masoud Mansoury", "Michael Cochez", "Martijn Schut"], "title": "Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints", "comment": null, "summary": "Methods for query answering over incomplete knowledge graphs retrieve\nentities that are likely to be answers, which is particularly useful when such\nanswers cannot be reached by direct graph traversal due to missing edges.\nHowever, existing approaches have focused on queries formalized using\nfirst-order-logic. In practice, many real-world queries involve constraints\nthat are inherently vague or context-dependent, such as preferences for\nattributes or related categories. Addressing this gap, we introduce the problem\nof query answering with soft constraints. We propose a Neural Query Reranker\n(NQR) designed to adjust query answer scores by incorporating soft constraints\nwithout disrupting the original answers to a query. NQR operates interactively,\nrefining answers based on incremental examples of preferred and non-preferred\nentities. We extend existing QA benchmarks by generating datasets with soft\nconstraints. Our experiments demonstrate that NQR can capture soft constraints\nwhile maintaining robust query answering performance.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u67e5\u8be2\u54cd\u5e94\u8f6f\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u795e\u7ecf\u67e5\u8be2\u91cd\u65b0\u6392\u540d\u5668\uff08NQR\uff09\uff0c\u7528\u4e8e\u8c03\u6574\u67e5\u8be2\u7b54\u6848\u8bc4\u5206\u3002NQR\u80fd\u591f\u6355\u6349\u8f6f\u7ea6\u675f\u5e76\u4fdd\u6301\u67e5\u8be2\u54cd\u5e94\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u7528\u4e00\u9636\u903b\u8f91\u5f62\u5f0f\u5316\u7684\u67e5\u8be2\uff0c\u4f46\u5b9e\u9645\u4e0a\u8bb8\u591a\u771f\u5b9e\u4e16\u754c\u7684\u67e5\u8be2\u6d89\u53ca\u56fa\u6709\u6a21\u7cca\u6216\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u7ea6\u675f\u3002\u56e0\u6b64\uff0c\u5f15\u5165\u4e86\u67e5\u8be2\u54cd\u5e94\u8f6f\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86NQR\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u795e\u7ecf\u67e5\u8be2\u91cd\u65b0\u6392\u540d\u5668\uff08NQR\uff09\u6765\u89e3\u51b3\u67e5\u8be2\u7b54\u6848\u4e2d\u8f6f\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u64cd\u4f5c\u4e92\u52a8\u5730\u6839\u636e\u9996\u9009\u548c\u975e\u9996\u9009\u5b9e\u4f53\u7684\u589e\u91cf\u793a\u4f8b\u6765\u4f18\u5316\u7b54\u6848\u3002\u901a\u8fc7\u751f\u6210\u5e26\u6709\u8f6f\u7ea6\u675f\u7684\u6570\u636e\u96c6\u6269\u5c55\u4e86\u73b0\u6709\u7684\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eNQR\u80fd\u591f\u6355\u6349\u8f6f\u7ea6\u675f\u5e76\u4fdd\u6301\u67e5\u8be2\u54cd\u5e94\u6027\u80fd\u3002", "conclusion": "\u5f15\u5165\u4e86\u67e5\u8be2\u54cd\u5e94\u8f6f\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u67e5\u8be2\u91cd\u65b0\u6392\u540d\u5668\uff08NQR\uff09\uff0c\u7528\u4e8e\u8c03\u6574\u67e5\u8be2\u7b54\u6848\u8bc4\u5206\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u67e5\u8be2\u7684\u539f\u59cb\u7b54\u6848\u3002NQR\u901a\u8fc7\u589e\u91cf\u6837\u672c\u4e0d\u65ad\u7ec6\u5316\u7b54\u6848\uff0c\u80fd\u591f\u6355\u6349\u8f6f\u7ea6\u675f\uff0c\u5e76\u4fdd\u6301\u5f3a\u5927\u7684\u67e5\u8be2\u54cd\u5e94\u6027\u80fd\u3002"}}
{"id": "2508.13672", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13672", "abs": "https://arxiv.org/abs/2508.13672", "authors": ["Rehan Raza", "Guanjin Wang", "Kevin Wong", "Hamid Laga", "Marco Fisichella"], "title": "ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings", "comment": "Accepted at the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM 2025)", "summary": "Explainable Artificial Intelligence (XAI) methods, such as Local\nInterpretable Model-Agnostic Explanations (LIME), have advanced the\ninterpretability of black-box machine learning models by approximating their\nbehavior locally using interpretable surrogate models. However, LIME's inherent\nrandomness in perturbation and sampling can lead to locality and instability\nissues, especially in scenarios with limited training data. In such cases, data\nscarcity can result in the generation of unrealistic variations and samples\nthat deviate from the true data manifold. Consequently, the surrogate model may\nfail to accurately approximate the complex decision boundary of the original\nmodel. To address these challenges, we propose a novel Instance-based Transfer\nLearning LIME framework (ITL-LIME) that enhances explanation fidelity and\nstability in data-constrained environments. ITL-LIME introduces instance\ntransfer learning into the LIME framework by leveraging relevant real instances\nfrom a related source domain to aid the explanation process in the target\ndomain. Specifically, we employ clustering to partition the source domain into\nclusters with representative prototypes. Instead of generating random\nperturbations, our method retrieves pertinent real source instances from the\nsource cluster whose prototype is most similar to the target instance. These\nare then combined with the target instance's neighboring real instances. To\ndefine a compact locality, we further construct a contrastive learning-based\nencoder as a weighting mechanism to assign weights to the instances from the\ncombined set based on their proximity to the target instance. Finally, these\nweighted source and target instances are used to train the surrogate model for\nexplanation purposes.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aInstance-based Transfer Learning LIME\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e2d\u7684\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u548c\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u8f6c\u79fb\u5b66\u4e60\uff0c\u4ece\u76f8\u5173\u7684\u6e90\u9886\u57df\u5229\u7528\u5b9e\u4f8b\u6765\u5e2e\u52a9\u76ee\u6807\u9886\u57df\u7684\u89e3\u91ca\u8fc7\u7a0b\u3002\u6700\u7ec8\uff0c\u901a\u8fc7\u8bad\u7ec3\u5e26\u6743\u91cd\u7684\u6e90\u548c\u76ee\u6807\u5b9e\u4f8b\u7684\u66ff\u4ee3\u6a21\u578b\u6765\u89e3\u91ca\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\uff08\u5982LIME\uff09\u5df2\u7ecf\u63d0\u9ad8\u4e86\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f46LIME\u4e2d\u7684\u968f\u673a\u6027\u53ef\u80fd\u5bfc\u81f4\u5c40\u90e8\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002\u6570\u636e\u7a00\u7f3a\u53ef\u80fd\u4f1a\u5bfc\u81f4\u751f\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u6d41\u5f62\u504f\u79bb\u7684\u4e0d\u5207\u5b9e\u9645\u53d8\u5316\u548c\u6837\u672c\uff0c\u4ece\u800c\u5bfc\u81f4\u66ff\u4ee3\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u903c\u8fd1\u539f\u59cb\u6a21\u578b\u7684\u590d\u6742\u51b3\u7b56\u8fb9\u754c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Instance-based Transfer Learning LIME\u6846\u67b6\uff08ITL-LIME\uff09\uff0c\u5f15\u5165\u5b9e\u4f8b\u8f6c\u79fb\u5b66\u4e60\u5230LIME\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u805a\u7c7b\u5c06\u6e90\u9886\u57df\u5212\u5206\u4e3a\u5177\u6709\u4ee3\u8868\u6027\u539f\u578b\u7684\u7c07\u3002\u65b9\u6cd5\u907f\u514d\u4e86\u751f\u6210\u968f\u673a\u6270\u52a8\uff0c\u800c\u662f\u4ece\u6700\u7c7b\u4f3c\u4e8e\u76ee\u6807\u5b9e\u4f8b\u7684\u6e90\u7c07\u4e2d\u68c0\u7d22\u76f8\u5173\u7684\u5b9e\u9645\u6e90\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e\u76ee\u6807\u5b9e\u4f8b\u7684\u76f8\u90bb\u5b9e\u9645\u5b9e\u4f8b\u76f8\u7ed3\u5408\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u7f16\u7801\u5668\u4f5c\u4e3a\u52a0\u6743\u673a\u5236\uff0c\u6839\u636e\u5b9e\u4f8b\u4e0e\u76ee\u6807\u5b9e\u4f8b\u7684\u63a5\u8fd1\u7a0b\u5ea6\u4e3a\u7ed3\u5408\u96c6\u5408\u4e2d\u7684\u5b9e\u4f8b\u5206\u914d\u6743\u91cd\u3002\u6700\u7ec8\u5229\u7528\u8fd9\u4e9b\u52a0\u6743\u7684\u6e90\u548c\u76ee\u6807\u5b9e\u4f8b\u6765\u8bad\u7ec3\u66ff\u4ee3\u6a21\u578b\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u63d0\u51fa\u7684ITL-LIME\u6846\u67b6\u80fd\u591f\u589e\u5f3a\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u5728\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u8f6c\u79fb\u5b66\u4e60\uff0c\u5229\u7528\u76f8\u5173\u7684\u5b9e\u4f8b\u4ece\u6e90\u9886\u57df\u8f85\u52a9\u76ee\u6807\u9886\u57df\u7684\u89e3\u91ca\u8fc7\u7a0b\uff0c\u6700\u7ec8\u4e3a\u89e3\u91ca\u76ee\u7684\u8bad\u7ec3\u66ff\u4ee3\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Instance-based Transfer Learning LIME\u6846\u67b6\uff08ITL-LIME\uff09\uff0c\u65e8\u5728\u589e\u5f3a\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u548c\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u5c06\u5b9e\u4f8b\u8f6c\u79fb\u5b66\u4e60\u5f15\u5165\u5230LIME\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u76f8\u5173\u7684\u5b9e\u4f8b\u4ece\u76f8\u5173\u7684\u6e90\u9886\u57df\u6765\u8f85\u52a9\u76ee\u6807\u9886\u57df\u4e2d\u7684\u89e3\u91ca\u8fc7\u7a0b\u3002\u6700\u7ec8\u63d0\u51fa\u7684\u6743\u91cd\u6e90\u548c\u76ee\u6807\u5b9e\u4f8b\u7528\u4e8e\u8bad\u7ec3\u66ff\u4ee3\u6a21\u578b\u8fdb\u884c\u89e3\u91ca\u76ee\u7684\u3002"}}
{"id": "2508.13675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13675", "abs": "https://arxiv.org/abs/2508.13675", "authors": ["Mariam Arustashvili", "J\u00f6rg Deigm\u00f6ller", "Heiko Paulheim"], "title": "Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks", "comment": "Accepted at Semantics 2025", "summary": "Knowledge Graphs are used for various purposes, including business\napplications, biomedical analyses, or digital twins in industry 4.0. In this\npaper, we investigate knowledge graphs describing household actions, which are\nbeneficial for controlling household robots and analyzing video footage. In the\nlatter case, the information extracted from videos is notoriously incomplete,\nand completing the knowledge graph for enhancing the situational picture is\nessential. In this paper, we show that, while a standard link prediction\nproblem, situational knowledge graphs have special characteristics that render\nmany link prediction algorithms not fit for the job, and unable to outperform\neven simple baselines.", "AI": {"tldr": "This paper explores knowledge graphs of household actions for controlling robots and analyzing video footage. It reveals that standard link prediction algorithms are not effective for situational knowledge graphs, highlighting the need for specialized approaches in this context.", "motivation": "The motivation behind this paper is to address the challenges and limitations in using standard link prediction algorithms for situational knowledge graphs describing household actions. It aims to demonstrate the inadequacy of many algorithms in this context and the need for specialized approaches.", "method": "The paper investigates knowledge graphs related to household actions, focusing on controlling household robots and analyzing video footage. It highlights the importance of completing knowledge graphs extracted from videos to enhance the situational picture.", "result": "The study shows that standard link prediction algorithms are not suitable for situational knowledge graphs of household actions and are unable to outperform simple baselines in this specific scenario.", "conclusion": "Situational knowledge graphs describing household actions present special characteristics that challenge standard link prediction algorithms, leading to underperformance compared to simple baselines."}}
{"id": "2508.13676", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13676", "abs": "https://arxiv.org/abs/2508.13676", "authors": ["Yu Li", "Zulong Chen", "Wenjian Xu", "Hong Wen", "Yipeng Yu", "Man Lung Yiu", "Yuyu Yin"], "title": "MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model", "comment": null, "summary": "To maintain the company's talent pool, recruiters need to continuously search\nfor resumes from third-party websites (e.g., LinkedIn, Indeed). However,\nfetched resumes are often incomplete and inaccurate. To improve the quality of\nthird-party resumes and enrich the company's talent pool, it is essential to\nconduct duplication detection between the fetched resumes and those already in\nthe company's talent pool. Such duplication detection is challenging due to the\nsemantic complexity, structural heterogeneity, and information incompleteness\nof resume texts. To this end, we propose MHSNet, an multi-level identity\nverification framework that fine-tunes BGE-M3 using contrastive learning. With\nthe fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and\ndense representations for resumes, enabling the computation of corresponding\nmulti-level semantic similarities. Moreover, the state-aware Mixture-of-Experts\n(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental\nresults verify the effectiveness of MHSNet", "AI": {"tldr": "\u4e3a\u4e86\u6539\u8fdb\u7b2c\u4e09\u65b9\u7b80\u5386\u7684\u8d28\u91cf\u5e76\u4e30\u5bcc\u516c\u53f8\u7684\u4eba\u624d\u5e93\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MHSNet\uff0c\u4e00\u4e2a\u591a\u5c42\u6b21\u8eab\u4efd\u9a8c\u8bc1\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u7cbe\u8c03BGE-M3\uff0c\u751f\u6210\u591a\u5c42\u7a00\u758f\u548c\u7a20\u5bc6\u7684\u7b80\u5386\u8868\u793a\uff0c\u5e76\u8ba1\u7b97\u76f8\u5e94\u7684\u591a\u5c42\u8bed\u4e49\u76f8\u4f3c\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86MHSNet\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u7b2c\u4e09\u65b9\u7b80\u5386\u7684\u8d28\u91cf\uff0c\u4e30\u5bcc\u516c\u53f8\u7684\u4eba\u624d\u5e93\uff0c\u5fc5\u987b\u5728\u6293\u53d6\u7684\u7b80\u5386\u548c\u516c\u53f8\u4eba\u624d\u5e93\u4e2d\u7684\u7b80\u5386\u4e4b\u95f4\u8fdb\u884c\u91cd\u590d\u68c0\u6d4b\u3002\u7531\u4e8e\u7b80\u5386\u6587\u672c\u7684\u8bed\u4e49\u590d\u6742\u6027\u3001\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u4fe1\u606f\u4e0d\u5b8c\u6574\u6027\uff0c\u6b64\u7c7b\u91cd\u590d\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86MHSNet\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u5c42\u6b21\u8eab\u4efd\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6765\u7cbe\u8c03BGE-M3\u3002\u5229\u7528\u7ec6\u8c03\u6574\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u4e3a\u7b80\u5386\u751f\u6210\u591a\u5c42\u7a00\u758f\u548c\u7a20\u5bc6\u8868\u793a\uff0c\u4ece\u800c\u8ba1\u7b97\u76f8\u5e94\u7684\u591a\u5c42\u8bed\u4e49\u76f8\u4f3c\u6027\u3002\u53e6\u5916\uff0cMHSNet\u4e2d\u4f7f\u7528\u4e86\u72b6\u6001\u611f\u77e5\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\uff0c\u4ee5\u5904\u7406\u5404\u79cd\u4e0d\u5b8c\u6574\u7684\u7b80\u5386\u3002", "result": "MHSNet\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6765\u6539\u8fdb\u7b2c\u4e09\u65b9\u7b80\u5386\u7684\u8d28\u91cf\uff0c\u63d0\u9ad8\u516c\u53f8\u4eba\u624d\u5e93\u7684\u4e30\u5bcc\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5404\u79cd\u4e0d\u5b8c\u6574\u7b80\u5386\u65f6\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86MHSNet\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.13678", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13678", "abs": "https://arxiv.org/abs/2508.13678", "authors": ["Xiao-Wen Yang", "Jie-Jing Shao", "Lan-Zhe Guo", "Bo-Wen Zhang", "Zhi Zhou", "Lin-Han Jia", "Wang-Zhou Dai", "Yu-Feng Li"], "title": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models", "comment": "9 pages, 3 figures, IJCAI 2025 Survey Track", "summary": "Large Language Models (LLMs) have shown promising results across various\ntasks, yet their reasoning capabilities remain a fundamental challenge.\nDeveloping AI systems with strong reasoning capabilities is regarded as a\ncrucial milestone in the pursuit of Artificial General Intelligence (AGI) and\nhas garnered considerable attention from both academia and industry. Various\ntechniques have been explored to enhance the reasoning capabilities of LLMs,\nwith neuro-symbolic approaches being a particularly promising way. This paper\ncomprehensively reviews recent developments in neuro-symbolic approaches for\nenhancing LLM reasoning. We first present a formalization of reasoning tasks\nand give a brief introduction to the neurosymbolic learning paradigm. Then, we\ndiscuss neuro-symbolic methods for improving the reasoning capabilities of LLMs\nfrom three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.\nFinally, we discuss several key challenges and promising future directions. We\nhave also released a GitHub repository including papers and resources related\nto this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.", "AI": {"tldr": "\u672c\u8bba\u6587\u7efc\u8ff0\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u63a2\u8ba8\u4e86\u4ece\u591a\u4e2a\u89d2\u5ea6\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u53d1\u5c55\u5177\u6709\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684AI\u7cfb\u7edf\u88ab\u89c6\u4e3a\u901a\u5f80\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u5173\u952e\u91cc\u7a0b\u7891\uff0c\u56e0\u6b64\u53d7\u5230\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5bf9\u4e8e\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u8bba\u6587\u9996\u5148\u5bf9\u63a8\u7406\u4efb\u52a1\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\uff0c\u7b80\u8981\u4ecb\u7ecd\u4e86\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u8303\u5f0f\uff0c\u7136\u540e\u4eceSymbolic->LLM\u3001LLM->Symbolic\u548cLLM+Symbolic\u4e09\u4e2a\u89d2\u5ea6\u8ba8\u8bba\u4e86\u6539\u5584LLM\u63a8\u7406\u80fd\u529b\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u3002", "result": "\u672c\u7814\u7a76\u7efc\u8ff0\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8fd1\u671f\u5728\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u65b9\u9762\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u589e\u5f3a\u6240\u53d6\u5f97\u7684\u8fdb\u5c55\uff0c\u8ba8\u8bba\u4e86Symbolic->LLM\u3001LLM->Symbolic\u548cLLM+Symbolic\u4e09\u4e2a\u65b9\u9762\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5b58\u5728\u7684\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.13697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13697", "abs": "https://arxiv.org/abs/2508.13697", "authors": ["Vincent Derkinderen", "Robin Manhaeve", "Rik Adriaensen", "Lucas Van Praet", "Lennert De Smet", "Giuseppe Marra", "Luc De Raedt"], "title": "The DeepLog Neurosymbolic Machine", "comment": null, "summary": "We contribute a theoretical and operational framework for neurosymbolic AI\ncalled DeepLog. DeepLog introduces building blocks and primitives for\nneurosymbolic AI that make abstraction of commonly used representations and\ncomputational mechanisms used in neurosymbolic AI. DeepLog can represent and\nemulate a wide range of neurosymbolic systems. It consists of two key\ncomponents. The first is the DeepLog language for specifying neurosymbolic\nmodels and inference tasks. This language consists of an annotated neural\nextension of grounded first-order logic, and makes abstraction of the type of\nlogic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the\narchitecture or in the loss function. The second DeepLog component is situated\nat the computational level and uses extended algebraic circuits as\ncomputational graphs. Together these two components are to be considered as a\nneurosymbolic abstract machine, with the DeepLog language as the intermediate\nlevel of abstraction and the circuits level as the computational one. DeepLog\nis implemented in software, relies on the latest insights in implementing\nalgebraic circuits on GPUs, and is declarative in that it is easy to obtain\ndifferent neurosymbolic models by making different choices for the underlying\nalgebraic structures and logics. The generality and efficiency of the DeepLog\nneurosymbolic machine is demonstrated through an experimental comparison\nbetween 1) different fuzzy and probabilistic logics, 2) between using logic in\nthe architecture or in the loss function, and 3) between a standalone CPU-based\nimplementation of a neurosymbolic AI system and a DeepLog GPU-based one.", "AI": {"tldr": "DeepLog presents a theoretical and operational framework for neurosymbolic AI, comprising a language for model specification and computational components based on algebraic circuits. It aims to provide flexibility in modeling neurosymbolic systems and demonstrates generality and efficiency through experimental comparisons.", "motivation": "The motivation behind DeepLog is to provide a unified framework for neurosymbolic AI that combines neural networks with symbolic reasoning. It aims to abstract various representation types and computational mechanisms commonly used in neurosymbolic AI systems, offering flexibility in specifying models and tasks. By leveraging the latest insights in GPU implementation, DeepLog seeks to demonstrate the generality and efficiency of neurosymbolic systems.", "method": "DeepLog introduces building blocks and primitives for neurosymbolic AI, abstracting commonly used representations and computational mechanisms. It includes a DeepLog language for model specification (an annotated neural extension of grounded first-order logic) and computational components based on extended algebraic circuits. The implementation relies on GPU acceleration and allows flexibility in choosing underlying algebraic structures and logics.", "result": "DeepLog demonstrates the generality and efficiency of neurosymbolic systems through experimental comparisons involving different logics, the usage of logic in architecture or loss function, and performance against a CPU-based implementation. The software implementation of DeepLog shows effectiveness in representing and emulating diverse neurosymbolic systems.", "conclusion": "DeepLog introduces a theoretical and operational framework for neurosymbolic AI that can represent and emulate a wide range of neurosymbolic systems. It consists of two key components: the DeepLog language for specifying models and tasks, and extended algebraic circuits for computational graphs. The software implementation of DeepLog demonstrates generality and efficiency through experimental comparisons."}}
{"id": "2508.13721", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13721", "abs": "https://arxiv.org/abs/2508.13721", "authors": ["Minh Hoang Nguyen", "Van Dai Do", "Dung Nguyen", "Thin Nguyen", "Hung Le"], "title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning", "comment": null, "summary": "Large language model (LLM) agents-especially smaller, open-source\nmodels-often produce causally invalid or incoherent actions in collaborative\ntasks due to their reliance on surface-level correlations rather than grounded\ncausal reasoning. This limitation undermines their performance in terms of\ncoordination and planning in dynamic environments. We address this challenge\nwith CausalPlan, a two-phase framework that integrates explicit structural\ncausal reasoning into the LLM planning process. At the core of CausalPlan is\nthe Structural Causal Action (SCA) model, which learns a causal graph from\nagent trajectories to capture how prior actions and current environment states\ninfluence future decisions. This structure is then used to guide action\nselection by assigning causal scores to LLM-generated proposals, reweighting\nthem accordingly, or falling back to causally grounded alternatives when\nneeded. By embedding this causal knowledge directly into the decision loop,\nCausalPlan constrains planning to intervention-consistent behaviours without\nrequiring fine-tuning of the LLM itself. We evaluate CausalPlan on the\nOvercooked-AI benchmark across five multi-agent coordination tasks and four\nLLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.\nExperimental results show that CausalPlan consistently reduces invalid actions\nand improves collaboration in both AI-AI and human-AI settings, outperforming\nstrong reinforcement learning baselines. Our findings highlight the value of\ncausality-driven planning for deploying efficient, interpretable, and\ngeneralisable multi-agent LLM systems.", "AI": {"tldr": "CausalPlan introduces a two-phase framework that integrates causal reasoning into LLM planning, addressing causally invalid actions. The Structural Causal Action (SCA) model learns causal graphs from agent trajectories to guide action selection. Experimental results show CausalPlan reduces invalid actions, improves collaboration, and outperforms reinforcement learning baselines on multi-agent tasks.", "motivation": "The motivation behind the research is to improve the performance of LLM agents in coordination and planning tasks in dynamic environments. LLMs often rely on surface-level correlations leading to causally invalid actions. By incorporating causal reasoning into the planning process, the paper aims to enhance collaboration and reduce invalid actions in multi-agent tasks.", "method": "The paper introduces the CausalPlan framework that integrates causal reasoning into LLM planning. It utilizes the Structural Causal Action (SCA) model to learn causal graphs from agent trajectories and guides action selection by assigning causal scores to LLM-generated proposals. The framework restrains planning to intervention-consistent behaviors without the need for fine-tuning the LLM itself.", "result": "Experimental results on the Overcooked-AI benchmark with various LLM sizes demonstrate that CausalPlan effectively reduces invalid actions, improves collaboration in AI-AI and human-AI settings, and outperforms reinforcement learning baselines. The findings emphasize the importance of causality-driven planning for efficient, interpretable, and generalizable multi-agent LLM systems.", "conclusion": "CausalPlan effectively addresses the issue of causally invalid or incoherent actions in collaborative tasks by integrating explicit structural causal reasoning into the planning process of Large Language Models (LLMs). It provides a two-phase framework that uses the Structural Causal Action (SCA) model to guide action selection based on learned causal graphs from agent trajectories."}}
{"id": "2508.13754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13754", "abs": "https://arxiv.org/abs/2508.13754", "authors": ["Liuxin Bao", "Zhihao Peng", "Xiaofei Zhou", "Runmin Cong", "Jiyong Zhang", "Yixuan Yuan"], "title": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making", "comment": "14 pages", "summary": "Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86EMRC\u6846\u67b6\uff0c\u5229\u7528\u591aLLM\u4ee3\u7406\u4eba\u62db\u52df\u548c\u534f\u4f5c\uff0c\u63d0\u9ad8\u533b\u5b66\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u64cd\u4f5c\uff0c\u6846\u67b6\u5728\u516c\u5171MDM\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u8d85\u8d8a\u4e86\u5355LLM\u548c\u591aLLM\u65b9\u6cd5\u3002", "motivation": "\u9274\u4e8e\u5355\u4e2aLLM\u65b9\u6cd5\u53d7\u5230\u53c2\u6570\u5316\u77e5\u8bc6\u9650\u5236\u548c\u9759\u6001\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u5c40\u9650\uff0c\u4e0d\u80fd\u5f88\u597d\u6574\u5408\u4e34\u5e8a\u4fe1\u606f\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86EMRC\u6846\u67b6\u6765\u63d0\u9ad8MDM\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5728\u4e24\u4e2a\u9636\u6bb5\u4e2d\uff0c\u4f7f\u7528\u516c\u5f00\u8bed\u6599\u5e93\u6784\u5efaLLM\u4e13\u4e1a\u77e5\u8bc6\u8868\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f73LLM\u4f5c\u4e3a\u533b\u5b66\u4e13\u5bb6\u4ee3\u7406\u4eba\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u8bc4\u4f30\u7f6e\u4fe1\u5ea6\u548c\u5bf9\u6297\u9a8c\u8bc1\u63d0\u9ad8\u8bca\u65ad\u53ef\u9760\u6027\u3002\u8bc4\u4f30\u6846\u67b6\u5728\u4e09\u4e2a\u516c\u5171MDM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u5c55\u73b0\u4e86EMRC\u76f8\u6bd4\u5355LLM\u548c\u591aLLM\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEMRC\u5728\u533b\u5b66\u51b3\u7b56\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728MMLU-Pro-Health\u6570\u636e\u96c6\u4e0a\u8fbe\u523074.45%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u72b6\u6001\u4e0b\u7684GPT- 4-0613\u6a21\u578b\u63d0\u5347\u4e862.69%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e13\u4e1a\u77e5\u8bc6\u611f\u77e5\u7684\u591aLLM\u62db\u52df\u548c\u534f\u4f5c\uff08EMRC\uff09\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u533b\u7597\u51b3\u7b56\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u901a\u8fc7\u5728\u4e24\u4e2a\u9636\u6bb5\u64cd\u4f5c\uff0c\u5229\u7528LLM\u4e13\u4e1a\u77e5\u8bc6\u8868\u548c\u4ee3\u7406\u4eba\u4e4b\u95f4\u7684\u5408\u4f5c\uff0c\u6210\u529f\u6539\u8fdb\u4e86\u533b\u5b66\u51b3\u7b56\u7684\u6027\u80fd\u3002\u5728\u4e09\u4e2a\u516c\u5171MDM\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4e2d\uff0cEMRC\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5355LLM\u548c\u591aLLM\u65b9\u6cd5\uff0c\u51c6\u786e\u6027\u8868\u73b0\u66f4\u4f73\uff0c\u5c55\u793a\u4e86\u5bf9LLM\u7684\u4f18\u52bf\u5229\u7528\u3002"}}
{"id": "2508.13811", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.13811", "abs": "https://arxiv.org/abs/2508.13811", "authors": ["Jan Jakub\u016fv", "Mikol\u00e1\u0161 Janota"], "title": "Quantifier Instantiations: To Mimic or To Revolt?", "comment": "Accepted to SMT 2025: 23rd International Workshop on Satisfiability\n  Modulo Theories", "summary": "Quantified formulas pose a significant challenge for Satisfiability Modulo\nTheories (SMT) solvers due to their inherent undecidability. Existing\ninstantiation techniques, such as e-matching, syntax-guided, model-based,\nconflict-based, and enumerative methods, often complement each other. This\npaper introduces a novel instantiation approach that dynamically learns from\nthese techniques during solving. By treating observed instantiations as samples\nfrom a latent language, we use probabilistic context-free grammars to generate\nnew, similar terms. Our method not only mimics successful past instantiations\nbut also explores diversity by optionally inverting learned term probabilities,\naiming to balance exploitation and exploration in quantifier reasoning.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f8b\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\u52a8\u6001\u5b66\u4e60\u4e0d\u540c\u5b9e\u4f8b\u5316\u6280\u672f\uff0c\u5e76\u751f\u6210\u65b0\u7684\u672f\u8bed\uff0c\u4ee5\u5e73\u8861\u91cf\u8bcd\u63a8\u7406\u4e2d\u7684\u5f00\u53d1\u5229\u7528\u548c\u63a2\u7d22\u3002", "motivation": "\u91cf\u5316\u516c\u5f0f\u5bf9SMT\u6c42\u89e3\u5668\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u7684\u5b9e\u4f8b\u5316\u6280\u672f\u901a\u5e38\u76f8\u4e92\u8865\u5145\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\u7684\u52a8\u6001\u5b66\u4e60\u5b9e\u4f8b\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u5b9e\u4f8b\u5316\u6280\u672f\u7684\u52a8\u6001\u5b66\u4e60\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4ece\u4e0d\u540c\u5b9e\u4f8b\u5316\u6280\u672f\u4e2d\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\u751f\u6210\u65b0\u7684\u672f\u8bed\uff0c\u5b9e\u73b0\u5bf9\u91cf\u8bcd\u63a8\u7406\u4e2d\u5f00\u53d1\u5229\u7528\u548c\u63a2\u7d22\u7684\u5e73\u8861\u3002", "conclusion": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f8b\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u52a8\u6001\u5730\u4ece\u73b0\u6709\u7684\u5b9e\u4f8b\u5316\u6280\u672f\u4e2d\u5b66\u4e60\u3002\u901a\u8fc7\u5c06\u89c2\u5bdf\u5230\u7684\u5b9e\u4f8b\u5316\u89c6\u4e3a\u6765\u81ea\u6f5c\u5728\u8bed\u8a00\u7684\u6837\u672c\uff0c\u4f7f\u7528\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\u751f\u6210\u65b0\u7684\u76f8\u4f3c\u672f\u8bed\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6a21\u4eff\u4e86\u8fc7\u53bb\u6210\u529f\u7684\u5b9e\u4f8b\u5316\uff0c\u8fd8\u901a\u8fc7\u9009\u62e9\u6027\u5730\u53cd\u8f6c\u5b66\u4e60\u7684\u672f\u8bed\u6982\u7387\uff0c\u63a2\u7d22\u591a\u6837\u6027\uff0c\u65e8\u5728\u5e73\u8861\u91cf\u8bcd\u63a8\u7406\u4e2d\u7684\u5f00\u53d1\u5229\u7528\u548c\u63a2\u7d22\u3002"}}
{"id": "2508.13828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13828", "abs": "https://arxiv.org/abs/2508.13828", "authors": ["Yifei Chen", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou"], "title": "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) technology has been widely applied in\nrecent years. However, despite the emergence of various RAG frameworks, a\nsingle RAG framework still cannot adapt well to a broad range of downstream\ntasks. Therefore, how to leverage the advantages of multiple RAG systems has\nbecome an area worth exploring. To address this issue, we have conducted a\ncomprehensive and systematic investigation into ensemble methods based on RAG\nsystems. Specifically, we have analyzed the RAG ensemble framework from both\ntheoretical and mechanistic analysis perspectives. From the theoretical\nanalysis, we provide the first explanation of the RAG ensemble framework from\nthe perspective of information entropy. In terms of mechanism analysis, we have\nexplored the RAG ensemble framework from both the pipeline and module levels.\nWe carefully select four different pipelines (Branching, Iterative, Loop, and\nAgentic) and three different modules (Generator, Retriever, and Reranker) to\nsolve seven different research questions. The experiments show that aggregating\nmultiple RAG systems is both generalizable and robust, whether at the pipeline\nlevel or the module level. Our work lays the foundation for similar research on\nthe multi-RAG system ensemble.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u591a\u4e2aRAG\u7cfb\u7edf\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86RAG\u7cfb\u7edf\u5408\u594f\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5bf9RAG\u5408\u594f\u6846\u67b6\u8fdb\u884c\u7406\u8bba\u548c\u673a\u5236\u5206\u6790\uff0c\u9a8c\u8bc1\u805a\u5408\u591a\u4e2aRAG\u7cfb\u7edf\u7684\u666e\u9002\u6027\u548c\u9c81\u68d2\u6027\u3002\u4e3a\u672a\u6765\u591aRAG\u7cfb\u7edf\u5408\u594f\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u51fa\u73b0\u4e86\u5404\u79cdRAG\u6846\u67b6\uff0c\u5355\u4e00RAG\u6846\u67b6\u4ecd\u65e0\u6cd5\u5f88\u597d\u5730\u9002\u5e94\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5229\u7528\u591a\u4e2aRAG\u7cfb\u7edf\u7684\u4f18\u52bf\u6210\u4e3a\u4e00\u4e2a\u503c\u5f97\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u8be5\u7814\u7a76\u4ece\u7406\u8bba\u548c\u673a\u5236\u5206\u6790\u4e24\u4e2a\u7ef4\u5ea6\u7814\u7a76RAG\u5408\u594f\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u71b5\u89d2\u5ea6\u89e3\u91ca\u4e86RAG\u5408\u594f\u6846\u67b6\uff0c\u9009\u62e9\u56db\u79cd\u4e0d\u540c\u6d41\u7a0b\uff08\u5206\u652f\u3001\u8fed\u4ee3\u3001\u5faa\u73af\u548c\u4ee3\u7406\uff09\u548c\u4e09\u79cd\u4e0d\u540c\u6a21\u5757\uff08\u751f\u6210\u5668\u3001\u68c0\u7d22\u5668\u548c\u91cd\u65b0\u6392\u5e8f\u5668\uff09\u89e3\u51b3\u4e03\u4e2a\u7814\u7a76\u95ee\u9898\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u805a\u5408\u591a\u4e2aRAG\u7cfb\u7edf\u5728\u6d41\u7a0b\u7ea7\u522b\u6216\u6a21\u5757\u7ea7\u522b\u90fd\u5177\u6709\u666e\u9002\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u5408\u594f\u65b9\u6cd5\u5c55\u5f00\u7efc\u5408\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u4ece\u7406\u8bba\u548c\u673a\u5236\u5206\u6790\u4e24\u4e2a\u5c42\u9762\u9610\u8ff0\u4e86RAG\u5408\u594f\u6846\u67b6\u3002\u5b9e\u9a8c\u8bc1\u660e\u805a\u5408\u591a\u4e2aRAG\u7cfb\u7edf\u65e0\u8bba\u5728\u6d41\u7a0b\u7ea7\u522b\u8fd8\u662f\u6a21\u5757\u7ea7\u522b\u90fd\u5177\u6709\u666e\u9002\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u672a\u6765\u591aRAG\u7cfb\u7edf\u5408\u594f\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.13876", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13876", "abs": "https://arxiv.org/abs/2508.13876", "authors": ["Katharina Stein", "Nils Hodel", "Daniel Fi\u0161er", "J\u00f6rg Hoffmann", "Michael Katz", "Alexander Koller"], "title": "Improved Generalized Planning with LLMs through Strategy Refinement and Reflection", "comment": null, "summary": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator.", "AI": {"tldr": "\u4ee5\u524d\u7684\u5de5\u4f5c\u5728PDDL\u89c4\u5212\u4e2d\u751f\u6210Python\u7a0b\u5e8f\u65f6\u5b58\u5728\u95ee\u9898\uff0c\u5f15\u5165\u4e00\u79cd\u751f\u6210\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5c06\u7b56\u7565\u751f\u6210\u4e3a\u4f2a\u4ee3\u7801\u5e76\u8fdb\u884c\u81ea\u52a8\u8c03\u8bd5\uff0c\u6269\u5c55Python\u8c03\u8bd5\u9636\u6bb5\uff0c\u901a\u8fc7\u53cd\u601d\u6b65\u9aa4\u6307\u51fa\u8ba1\u5212\u5931\u8d25\u539f\u56e0\uff0c\u751f\u6210\u591a\u4e2a\u7a0b\u5e8f\u53d8\u4f53\u5e76\u9009\u62e9\u6700\u4f73\u7a0b\u5e8f\u3002\u572817\u4e2a\u57fa\u51c6\u9886\u57df\u4e0a\u5c55\u793a\u4e86\u8fd9\u4e9b\u6269\u5c55\u663e\u8457\u63d0\u9ad8\u5e7f\u4e49\u8ba1\u5212\u8d28\u91cf\uff0c\u572812\u4e2a\u9886\u57df\u4e2d\u6700\u4f73Python\u7a0b\u5e8f\u89e3\u51b3\u4e86\u6240\u6709\u53ef\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u4ee5\u524d\u7684\u5de5\u4f5c\u5728PDDL\u89c4\u5212\u4e2d\u63d0\u51fa\u4e86\u7531LLM\u751f\u6210Python\u7a0b\u5e8f\u7684\u6846\u67b6\uff0c\u4f46\u5b58\u5728\u751f\u6210\u552f\u4e00\u7b56\u7565\u4e14\u76f4\u63a5\u4f20\u9012\u7ed9\u7a0b\u5e8f\u751f\u6210\u7684\u95ee\u9898\u3002\u5982\u679c\u7b56\u7565\u4e0d\u6b63\u786e\uff0c\u5176\u5b9e\u73b0\u5c06\u5bfc\u81f4\u4e0d\u6b63\u786e\u7684\u5e7f\u4e49\u8ba1\u5212\u3002\u56e0\u6b64\uff0c\u5f15\u5165\u4e00\u79cd\u751f\u6210\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u751f\u6210\u5e7f\u4e49\u8ba1\u5212\u4e4b\u524d\u8bc6\u522b\u548c\u4fee\u590d\u9519\u8bef\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u5212\u7684\u8d28\u91cf\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u751f\u6210\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5c06\u7b56\u7565\u751f\u6210\u4e3a\u4f2a\u4ee3\u7801\uff0c\u5e76\u5b9e\u73b0\u4f2a\u4ee3\u7801\u7684\u81ea\u52a8\u8c03\u8bd5\uff0c\u4ece\u800c\u5728\u751f\u6210\u5e7f\u4e49\u8ba1\u5212\u4e4b\u524d\u8bc6\u522b\u548c\u4fee\u590d\u9519\u8bef\u3002\u6b64\u5916\uff0c\u5728Python\u8c03\u8bd5\u9636\u6bb5\u6dfb\u52a0\u4e86\u53cd\u601d\u6b65\u9aa4\uff0c\u63d0\u793aLLM\u6307\u51fa\u89c2\u5bdf\u5230\u7684\u8ba1\u5212\u5931\u8d25\u7684\u539f\u56e0\u3002\u501f\u9274LLM\u4ee3\u7801\u751f\u6210\u7684\u7075\u611f\uff0c\u751f\u6210\u591a\u4e2a\u7a0b\u5e8f\u53d8\u4f53\u5e76\u9009\u62e9\u6700\u4f73\u7684\u4e00\u4e2a\u3002", "result": "\u572817\u4e2a\u57fa\u51c6\u9886\u57df\u4e0a\u5c55\u793a\u4e86\u6269\u5c55\u5982\u4f55\u663e\u7740\u6539\u5584\u5e7f\u4e49\u8ba1\u5212\u7684\u8d28\u91cf\u3002\u572812\u4e2a\u9886\u57df\u4e2d\uff0c\u6700\u4f73Python\u7a0b\u5e8f\u89e3\u51b3\u4e86\u6240\u6709\u53ef\u4f7f\u7528\u76f8\u5e94\u5b9e\u4f8b\u751f\u6210\u5668\u751f\u6210\u7684\u4efb\u52a1\u3002", "conclusion": "\u572817\u4e2a\u57fa\u51c6\u9886\u57df\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u4e9b\u6269\u5c55\u663e\u8457\u6539\u5584\uff08\u4e14\u4ece\u4e0d\u6076\u5316\uff09\u5e7f\u4e49\u8ba1\u5212\u7684\u8d28\u91cf\u3002\u572812\u4e2a\u9886\u57df\u4e2d\uff0c\u6211\u4eec\u7684\u6700\u4f73Python\u7a0b\u5e8f\u89e3\u51b3\u4e86\u53ef\u4ee5\u4f7f\u7528\u76f8\u5e94\u5b9e\u4f8b\u751f\u6210\u5668\u751f\u6210\u7684\u6240\u6709\u4efb\u52a1\u3002"}}
{"id": "2508.13915", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13915", "abs": "https://arxiv.org/abs/2508.13915", "authors": ["Yihao Ang", "Yifan Bao", "Lei Jiang", "Jiajie Tao", "Anthony K. H. Tung", "Lukasz Szpruch", "Hao Ni"], "title": "Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback", "comment": null, "summary": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability.", "AI": {"tldr": "\textsf{TS-Agent} is a modular agentic framework for automating and enhancing time-series modeling workflows in financial applications. It outperforms existing AutoML and agentic baselines in accuracy, robustness, and decision traceability, addressing the challenges in building high-performing models for financial markets.", "motivation": "The motivation behind this paper is to address the challenges in building high-performing, interpretable, and auditable models for financial applications. The authors aim to enhance time-series modeling workflows by introducing a framework that offers adaptability, responsiveness to domain-specific needs, and flexibility through agentic systems.", "method": "Introducing \textsf{TS-Agent}, a modular agentic framework that formalizes the modeling workflow into three stages: model selection, code refinement, and fine-tuning. The framework utilizes a planner agent with structured knowledge banks and curated libraries to guide the decision process.", "result": "The empirical evaluations demonstrate that \textsf{TS-Agent} offers adaptive learning, robust debugging, and transparent auditing, which are essential for high-stakes environments like financial services. The framework consistently outperforms existing AutoML and agentic baselines in terms of accuracy, robustness, and decision traceability.", "conclusion": "\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability in financial forecasting and synthetic data generation tasks."}}
{"id": "2508.13942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13942", "abs": "https://arxiv.org/abs/2508.13942", "authors": ["Soumyadeep Dhar"], "title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management", "comment": null, "summary": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u5728\u591a\u5c42\u6b21\u4f9b\u5e94\u94fe\u4e2d\u4f7f\u7528AI\u4ee3\u7406\u7684\u884c\u4e3a\u503e\u5411\uff0c\u53d1\u73b0\u4e86\u201c\u534f\u4f5c\u6096\u8bba\u201d\u73b0\u8c61\u3002\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u7b56\u7565\uff0c\u5373\u7ed3\u5408\u9ad8\u7ea7\u522bAI\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u653f\u7b56\u8bbe\u5b9a\u548c\u4f4e\u7ea7\u522b\u534f\u4f5c\u6267\u884c\u534f\u8bae\uff0c\u5b9e\u73b0\u4f9b\u5e94\u94fe\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u7ecf\u6d4e\u73af\u5883\u4e2d\u81ea\u4e3bAI\u9a71\u52a8\u4ee3\u7406\u7684\u5174\u8d77\u5f15\u53d1\u4e86\u5173\u4e8e\u5b83\u4eec\u65b0\u5174\u6218\u7565\u884c\u4e3a\u7684\u91cd\u8981\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u591a\u5c42\u6b21\u4f9b\u5e94\u94fe\u7684\u5408\u4f5c\u73af\u5883\u4e2d\u8fd9\u4e9b\u52a8\u6001\uff0c\u4ee5\u63ed\u793a\u5408\u4f5cAI\u4ee3\u7406\u7684\u65b0\u5174\u884c\u4e3a\u3002", "method": "\u5728\u591a\u5c42\u6b21\u4f9b\u5e94\u94fe\u6a21\u62df\u4e2d\uff0c\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210AI\u4ee3\u7406\u8fdb\u884c\u8ba1\u7b97\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u534f\u4f5cAI\u4ee3\u7406\u7684\u884c\u4e3a\u503e\u5411\u3002\u7814\u7a76\u5206\u6790\u4e86\u4ee3\u7406\u5546\u56e4\u79ef\u5e93\u5b58\u5bfc\u81f4\u7cfb\u7edf\u9965\u997f\u7684\u64cd\u4f5c\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u7ea7\u522b\u9884\u6d4b\u6027\u653f\u7b56\u8bbe\u5b9a\u548c\u4f4e\u7ea7\u522b\u534f\u4f5c\u6267\u884c\u534f\u8bae\u76f8\u7ed3\u5408\u53ef\u4ee5\u5b9e\u73b0\u7cfb\u7edf\u97e7\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97\u5b9e\u9a8c\u53d1\u73b0\u4e86\u201c\u534f\u4f5c\u6096\u8bba\u201d\u73b0\u8c61\uff0c\u8868\u660e\u7ed3\u5408\u9ad8\u7ea7\u522bAI\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u653f\u7b56\u8bbe\u5b9a\u548c\u4f4e\u7ea7\u522b\u534f\u4f5c\u6267\u884c\u534f\u8bae\u53ef\u4ee5\u5b9e\u73b0\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002\u6700\u7ec8\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u751f\u6210\u3001\u8bc4\u4f30\u548c\u91cf\u5316\u591a\u79cd\u6218\u7565\u9009\u62e9\u3002", "conclusion": "\u8be5\u8bba\u6587\u53d1\u73b0\u4e86\u201c\u534f\u4f5c\u6096\u8bba\u201d\uff0c\u5373\u5408\u4f5c\u578bAI\u4ee3\u7406\u5728\u4f9b\u5e94\u94fe\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u73b0\u8c61\uff0c\u8fd9\u662f\u7531\u4e8e\u64cd\u4f5c\u6027\u7f3a\u9677\u5bfc\u81f4\u7684\u3002\u7814\u7a76\u663e\u793a\uff0c\u552f\u6709\u7ed3\u5408\u9ad8\u7ea7\u522bAI\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u653f\u7b56\u8bbe\u5b9a\u548c\u4f4e\u7ea7\u522b\u534f\u4f5c\u6267\u884c\u534f\u8bae\uff0c\u624d\u80fd\u5b9e\u73b0\u4f9b\u5e94\u94fe\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002\u6700\u7ec8\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u81ea\u4e3b\u4ea7\u751f\u3001\u8bc4\u4f30\u548c\u91cf\u5316\u4e00\u7cfb\u5217\u53ef\u884c\u7684\u6218\u7565\u9009\u62e9\uff0c\u4e3a\u8bbe\u8ba1\u7a33\u5b9a\u6709\u6548\u7684AI\u9a71\u52a8\u7684\u4f01\u4e1a\u5206\u6790\u7cfb\u7edf\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002"}}
{"id": "2508.13975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13975", "abs": "https://arxiv.org/abs/2508.13975", "authors": ["Jingquan Wang", "Andrew Negrut", "Harry Zhang", "Khailanii Slaton", "Shu Wang", "Radu Serban", "Jinlong Wu", "Dan Negrut"], "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation", "comment": null, "summary": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u88ab\u4f18\u5316\u548c\u5b9a\u5236\u5316\u6210\u4e3a\u865a\u62df\u52a9\u624b\uff0c\u534f\u52a9\u4e13\u5bb6\u6709\u6548\u4f7f\u7528\u4eff\u771f\u5de5\u5177\u3002\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u548c\u5b9a\u5236\u8bed\u8a00\u6a21\u578b\u751f\u6210PyChrono\u865a\u62df\u5b9e\u9a8c\u811a\u672c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u811a\u672c\u8d28\u91cf\uff0c\u6db5\u76d6\u7b80\u5355\u5230\u590d\u6742\u7684\u865a\u62df\u5b9e\u9a8c\u3002\u751f\u6210\u7684\u811a\u672c\u53ef\u4f5c\u4e3a\u7528\u6237\u6539\u8fdb\u7684\u8d77\u70b9\uff0c\u8bed\u8a00\u6a21\u578b\u8fd8\u53ef\u56de\u7b54\u7279\u5b9aAPI\u95ee\u9898\u6216\u63a8\u8350\u5efa\u6a21\u65b9\u6cd5\u3002\u7814\u7a76\u5efa\u7acb\u4e86\u666e\u9002\u6027\u6846\u67b6\uff0c\u964d\u4f4e\u4e86\u5176\u4ed6\u9886\u57df\u4eff\u771f\u5de5\u5177\u7684\u51c6\u5165\u95e8\u69db\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u88ab\u7cbe\u70bc\u548c\u5b9a\u5236\u5316\uff0c\u6210\u4e3a\u865a\u62df\u52a9\u624b\uff0c\u5e2e\u52a9\u4e13\u5bb6\u6709\u6548\u4f7f\u7528\u4eff\u771f\u5de5\u5177\u3002\u4f5c\u8005\u8003\u8651\u4e86PyChrono\u8fd9\u4e00\u5f00\u6e90\u591a\u7269\u7406\u52a8\u529b\u5b66\u5f15\u64ce\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u5bf9\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u4f18\u5316\u548c\u5b9a\u5236\u8bed\u8a00\u6a21\u578b\u4ee5\u751f\u6210PyChrono\u865a\u62df\u5b9e\u9a8c\u811a\u672c\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u548c\u5b9a\u5236\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u6267\u884cPyChrono\u865a\u62df\u5b9e\u9a8c\u7684\u811a\u672c\u3002", "result": "\u901a\u8fc7\u7814\u7a76\uff0c\u4f5c\u8005\u6210\u529f\u4f18\u5316\u548c\u5b9a\u5236\u4e86\u591a\u7c7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u660e\u663e\u63d0\u9ad8\u4e86\u751f\u6210\u7684PyChrono\u4eff\u771f\u811a\u672c\u7684\u8d28\u91cf\u3002\u751f\u6210\u7684\u811a\u672c\u53ef\u4ee5\u6db5\u76d6\u7b80\u5355\u5230\u590d\u6742\u7684\u865a\u62df\u5b9e\u9a8c\uff0c\u867d\u7136\u4e0d\u5b8c\u7f8e\u4f46\u53ef\u4f5c\u4e3a\u7528\u6237\u4fee\u6539\u548c\u6539\u8fdb\u7684\u8d77\u70b9\u3002\u540c\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fd8\u80fd\u56de\u7b54\u7279\u5b9a\u7684\u4eff\u771f\u5668API\u95ee\u9898\u6216\u63a8\u8350\u5efa\u6a21\u65b9\u6cd5\u3002\u7814\u7a76\u5df2\u5efa\u7acb\u4e86\u666e\u9002\u6027\u6846\u67b6\uff0c\u53ef\u5e94\u7528\u4e8e\u964d\u4f4e\u5176\u4ed6\u9886\u57df\u4eff\u771f\u5de5\u5177\u7684\u51c6\u5165\u95e8\u69db\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u7cbe\u70bc\u548c\u5b9a\u5236\u5316\u7684\u8fc7\u7a0b\u53d8\u6210\u865a\u62df\u52a9\u624b\uff0c\u5e2e\u52a9\u4e13\u5bb6\u6709\u6548\u4f7f\u7528\u4eff\u771f\u5de5\u5177\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u548c\u5b9a\u5236\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u7684\u529b\u91cf\u751f\u6210\u6267\u884cPyChrono\u865a\u62df\u5b9e\u9a8c\u7684\u811a\u672c\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8fd9\u4e00\u8fc7\u7a0b\u53ef\u4ee5\u660e\u663e\u63d0\u9ad8\u751f\u6210\u7684PyChrono\u4eff\u771f\u811a\u672c\u7684\u8d28\u91cf\uff0c\u4ece\u7b80\u5355\u7684\u5355\u6446\u6a21\u62df\u5230\u590d\u6742\u7684\u6d89\u53ca\u8f66\u8f86\u5728\u53ef\u53d8\u5f62\u5730\u5f62\u4e0a\u7684\u865a\u62df\u5b9e\u9a8c\u3002\u751f\u6210\u7684\u811a\u672c\u867d\u7136\u4e0d\u5b8c\u7f8e\uff0c\u4f46\u5e38\u5e38\u4f5c\u4e3a\u7528\u6237\u4fee\u6539\u548c\u6539\u8fdb\u7684\u826f\u597d\u8d77\u70b9\u3002\u6b64\u5916\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fd8\u53ef\u4ee5\u56de\u7b54\u5173\u4e8e\u4eff\u771f\u5668\u7684\u7279\u5b9aAPI\u95ee\u9898\u6216\u63a8\u8350\u5efa\u6a21\u65b9\u6cd5\u3002\u7814\u7a76\u7684\u6846\u67b6\u5177\u6709\u666e\u9002\u6027\uff0c\u53ef\u964d\u4f4e\u4e0e\u5176\u4ed6\u5e94\u7528\u9886\u57df\u76f8\u5173\u7684\u4eff\u771f\u5de5\u5177\u7684\u51c6\u5165\u95e8\u69db\u3002"}}
{"id": "2508.14020", "categories": ["cs.AI", "cs.DM", "68T01", "I.2.8"], "pdf": "https://arxiv.org/pdf/2508.14020", "abs": "https://arxiv.org/abs/2508.14020", "authors": ["Christian Blum", "Pedro Pinacho-Davidson"], "title": "A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem", "comment": null, "summary": "The longest run subsequence (LRS) problem is an NP-hard combinatorial\noptimization problem belonging to the class of subsequence problems from\nbioinformatics. In particular, the problem plays a role in genome reassembly.\nIn this paper, we present a solution to the LRS problem using a Biased Random\nKey Genetic Algorithm (BRKGA). Our approach places particular focus on the\ncomputational efficiency of evaluating individuals, which involves converting\nvectors of gray values into valid solutions to the problem. For comparison\npurposes, a Max-Min Ant System is developed and implemented. This is in\naddition to the application of the integer linear programming solver CPLEX for\nsolving all considered problem instances. The computation results show that the\nproposed BRKGA is currently a state-of-the-art technique for the LRS problem.\nNevertheless, the results also show that there is room for improvement,\nespecially in the context of input strings based on large alphabet sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528BRKGA\u89e3\u51b3LRS\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4e0e\u5176\u4ed6\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793aBRKGA\u662f\u76ee\u524dLRS\u95ee\u9898\u7684\u9886\u5148\u6280\u672f\uff0c\u4f46\u5728\u5904\u7406\u5927\u578b\u5b57\u6bcd\u8868\u8f93\u5165\u65f6\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "LRS\u95ee\u9898\u662fNP\u96be\u9898\uff0c\u5c5e\u4e8e\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u5b50\u5e8f\u5217\u95ee\u9898\u7684\u4e00\u79cd\u3002\u89e3\u51b3\u8be5\u95ee\u9898\u5728\u57fa\u56e0\u7ec4\u91cd\u65b0\u7ec4\u88c5\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8fdb\u884c\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7684\u6bd4\u8f83\u3002", "method": "\u672c\u6587\u91c7\u7528\u504f\u5411\u968f\u673a\u5bc6\u94a5\u9057\u4f20\u7b97\u6cd5\uff08BRKGA\uff09\u89e3\u51b3LRS\u95ee\u9898\uff0c\u5e76\u5bf9\u8ba1\u7b97\u6548\u7387\u8fdb\u884c\u91cd\u70b9\u4f18\u5316\u3002\u540c\u65f6\u5f00\u53d1\u4e86Max-Min Ant System\u7528\u4e8e\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\u5668CPLEX\u89e3\u51b3\u6240\u6709\u95ee\u9898\u5b9e\u4f8b\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0cBRKGA\u65b9\u6cd5\u662f\u76ee\u524dLRS\u95ee\u9898\u7684\u6700\u4f73\u6280\u672f\u4e4b\u4e00\uff0c\u5e76\u6307\u51fa\u5728\u5927\u578b\u5b57\u6bcd\u8868\u8f93\u5165\u60c5\u51b5\u4e0b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528\u504f\u5411\u968f\u673a\u5bc6\u94a5\u9057\u4f20\u7b97\u6cd5\uff08BRKGA\uff09\u89e3\u51b3\u6700\u957f\u8fd0\u884c\u5b50\u5e8f\u5217\uff08LRS\uff09\u95ee\u9898\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u88ab\u8bc1\u5b9e\u662f\u76ee\u524dLRS\u95ee\u9898\u7684\u6700\u65b0\u6280\u672f\u3002\u4f46\u662f\uff0c\u7ed3\u679c\u4e5f\u663e\u793a\u5728\u57fa\u4e8e\u5927\u578b\u5b57\u6bcd\u8868\u7684\u8f93\u5165\u5b57\u7b26\u4e32\u60c5\u51b5\u4e0b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2508.14040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14040", "abs": "https://arxiv.org/abs/2508.14040", "authors": ["Hanyu Lai", "Xiao Liu", "Yanxiao Zhao", "Han Xu", "Hanchen Zhang", "Bohao Jing", "Yanyu Ren", "Shuntian Yao", "Yuxiao Dong", "Jie Tang"], "title": "ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents", "comment": null, "summary": "We introduce ComputerRL, a framework for autonomous desktop intelligence that\nenables agents to operate complex digital workspaces skillfully. ComputerRL\nfeatures the API-GUI paradigm, which unifies programmatic API calls and direct\nGUI interaction to address the inherent mismatch between machine agents and\nhuman-centric desktop environments. Scaling end-to-end RL training is crucial\nfor improvement and generalization across diverse desktop tasks, yet remains\nchallenging due to environmental inefficiency and instability in extended\ntraining. To support scalable and robust training, we develop a distributed RL\ninfrastructure capable of orchestrating thousands of parallel virtual desktop\nenvironments to accelerate large-scale online RL. Furthermore, we propose\nEntropulse, a training strategy that alternates reinforcement learning with\nsupervised fine-tuning, effectively mitigating entropy collapse during extended\ntraining runs. We employ ComputerRL on open models GLM-4-9B-0414 and\nQwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B\nbased on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,\ndemonstrating significant improvements for general agents in desktop\nautomation. The algorithm and framework are adopted in building AutoGLM (Liu et\nal., 2024a)", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86ComputerRL\u6846\u67b6\uff0c\u4f7f\u7528API-GUI\u8303\u5f0f\u7edf\u4e00\u7f16\u7a0bAPI\u8c03\u7528\u4e0eGUI\u4ea4\u4e92\uff0c\u52a0\u901f\u5927\u89c4\u6a21\u5728\u7ebfRL\u8bad\u7ec3\uff0c\u63d0\u51fa\u4e86Entropulse\u8bad\u7ec3\u7b56\u7565\u7f13\u89e3\u71b5\u574d\u584c\uff0c\u53d6\u5f97\u5728\u684c\u9762\u667a\u80fd\u9886\u57df\u7684\u663e\u8457\u6539\u8fdb\uff0c\u6700\u7ec8\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9ad8\u7cbe\u5ea6", "motivation": "\u5f53\u524d\u673a\u5668\u4ee3\u7406\u548c\u684c\u9762\u73af\u5883\u4e0d\u5339\u914d\uff0c\u7ed3\u675f\u5230\u7aefRL\u8bad\u7ec3\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b", "method": "\u5f00\u53d1\u4e86\u5206\u5e03\u5f0fRL\u57fa\u7840\u8bbe\u65bd\u7528\u4e8e\u52a0\u901f\u5927\u89c4\u6a21\u5728\u7ebfRL\uff0c\u63d0\u51fa\u4e86\u8bad\u7ec3\u7b56\u7565Entropulse\u7f13\u89e3\u71b5\u574d\u584c\uff0c\u91c7\u7528API-GUI\u8303\u5f0f\u7edf\u4e00\u7f16\u7a0bAPI\u8c03\u7528\u4e0eGUI\u4ea4\u4e92", "result": "\u63d0\u51fa\u4e86ComputerRL\u6846\u67b6\uff0c\u6210\u529f\u5728\u684c\u9762\u667a\u80fd\u9886\u57df\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e86\u9ad8\u7cbe\u5ea6", "conclusion": "\u5f15\u5165\u4e86ComputerRL\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u81ea\u4e3b\u684c\u9762\u667a\u80fd\uff0c\u53ef\u4ee5\u4f7f\u4ee3\u7406\u7a0b\u5e8f\u719f\u7ec3\u64cd\u4f5c\u590d\u6742\u7684\u6570\u5b57\u5de5\u4f5c\u7a7a\u95f4\u3002\u63d0\u51fa\u4e86API-GUI\u8303\u5f0f\uff0c\u7edf\u4e00\u4e86\u7f16\u7a0bAPI\u8c03\u7528\u548c\u76f4\u63a5GUI\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4ee3\u7406\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u684c\u9762\u73af\u5883\u4e4b\u95f4\u56fa\u6709\u7684\u4e0d\u5339\u914d\u3002\u901a\u8fc7\u5f00\u53d1\u5206\u5e03\u5f0fRL\u57fa\u7840\u8bbe\u65bd\uff0c\u52a0\u901f\u5927\u89c4\u6a21\u5728\u7ebfRL\uff0c\u652f\u6301\u53ef\u6269\u5c55\u548c\u7a33\u5065\u7684\u8bad\u7ec3\u3002\u63d0\u51fa\u4e86Entropulse\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u71b5\u574d\u584c\u3002\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86GLM-4-9B-0414\u548cQwen2.5-14B\u7b49\u6a21\u578b\uff0cAutoGLM-OS-9B\u57fa\u4e8eGLM-4-9B-0414\u53d6\u5f97\u4e8648.1%\u7684\u7cbe\u5ea6\uff0c\u663e\u8457\u6539\u5584\u4e86\u684c\u9762\u81ea\u52a8\u5316\u4e2d\u7684\u901a\u7528\u4ee3\u7406\u3002\u8be5\u7b97\u6cd5\u548c\u6846\u67b6\u88ab\u91c7\u7528\u7528\u4e8e\u6784\u5efaAutoGLM\uff08Liu et al.\uff0c2024a\uff09"}}
