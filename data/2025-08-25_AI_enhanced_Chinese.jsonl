{"id": "2508.15943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15943", "abs": "https://arxiv.org/abs/2508.15943", "authors": ["Riccardo Andreoni", "Andrei Buliga", "Alessandro Daniele", "Chiara Ghidini", "Marco Montali", "Massimiliano Ronzani"], "title": "T-ILR: a Neurosymbolic Integration for LTLf", "comment": "Accepted for presentation at NeSy 2025. 10 pages", "summary": "State-of-the-art approaches for integrating symbolic knowledge with deep\nlearning architectures have demonstrated promising results in static domains.\nHowever, methods to handle temporal logic specifications remain underexplored.\nThe only existing approach relies on an explicit representation of a\nfinite-state automaton corresponding to the temporal specification. Instead, we\naim at proposing a neurosymbolic framework designed to incorporate temporal\nlogic specifications, expressed in Linear Temporal Logic over finite traces\n(LTLf), directly into deep learning architectures for sequence-based tasks. We\nextend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging\nthe recent introduction of fuzzy LTLf interpretations. We name this proposed\nmethod Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an\nexisting benchmark for temporal neurosymbolic architectures, consisting of the\nclassification of image sequences in the presence of temporal knowledge. The\nresults demonstrate improved accuracy and computational efficiency compared to\nthe state-of-the-art method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u795e\u7ecf\u7b26\u53f7\u6846\u67b6T-ILR\uff0c\u65e8\u5728\u5c06\u7ebf\u6027\u65f6\u6001\u903b\u8f91\u89c4\u8303\u76f4\u63a5\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u96c6\u6210\u7b26\u53f7\u77e5\u8bc6\u4e0e\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u65b9\u6cd5\u5728\u9759\u6001\u9886\u57df\u53d6\u5f97\u4e86\u826f\u597d\u7ed3\u679c\uff0c\u4f46\u5904\u7406\u65f6\u6001\u903b\u8f91\u89c4\u8303\u7684\u65b9\u6cd5\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\u7684\u663e\u5f0f\u8868\u793a\uff0c\u800c\u672c\u8bba\u6587\u65e8\u5728\u5c06\u7ebf\u6027\u65f6\u6001\u903b\u8f91\u89c4\u8303\u76f4\u63a5\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\uff0c\u4ee5\u5e94\u5bf9\u65f6\u5e8f\u4efb\u52a1\u3002", "method": "\u5728\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u4e2d\u6269\u5c55\u4e86\u8fed\u4ee3\u5c40\u90e8\u7ec6\u5316\uff08ILR\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u6a21\u7ccaLTLf\u89e3\u91ca\uff0c\u5c06\u7ebf\u6027\u65f6\u6001\u903b\u8f91\u89c4\u8303\u76f4\u63a5\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4ece\u800c\u63d0\u51fa\u4e86Temporal Iterative Local Refinement\uff08T-ILR\uff09\u65b9\u6cd5\u3002", "result": "T-ILR\u5728\u56fe\u50cf\u5e8f\u5217\u5206\u7c7b\u7684\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u79f0\u4e3aTemporal Iterative Local Refinement\uff08T-ILR\uff09\uff0c\u65e8\u5728\u5c06\u7ebf\u6027\u65f6\u6001\u903b\u8f91\u89c4\u8303\u76f4\u63a5\u878d\u5165\u57fa\u4e8e\u5e8f\u5217\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u3002\u901a\u8fc7\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u8bc4\u4f30T-ILR\uff0c\u5728\u5904\u7406\u5e26\u6709\u65f6\u6001\u77e5\u8bc6\u7684\u56fe\u50cf\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u65f6\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cT-ILR\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.16033", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16033", "abs": "https://arxiv.org/abs/2508.16033", "authors": ["Jong-Hwan Jang", "Junho Song", "Yong-Yeon Jo"], "title": "CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics", "comment": "Demo paper, 5 pages", "summary": "Recognizing the need for explainable AI (XAI) approaches to enable the\nsuccessful integration of AI-based ECG prediction models (AI-ECG) into clinical\npractice, we introduce a framework generating \\textbf{Co}unter\\textbf{F}actual\n\\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as\namplitudes and intervals, influence the model's predictive decisions. To\ndemonstrate the applicability of the CoFE, we present two case studies: atrial\nfibrillation classification and potassium level regression models. The CoFE\nreveals feature changes in ECG signals that align with the established clinical\nknowledge. By clarifying both \\textbf{where valid features appear} in the ECG\nand \\textbf{how they influence the model's predictions}, we anticipate that our\nframework will enhance the interpretability of AI-ECG models and support more\neffective clinical decision-making. Our demonstration video is available at:\nhttps://www.youtube.com/watch?v=YoW0bNBPglQ.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u751f\u6210\u5bf9\u6297\u6027\u5fc3\u7535\u56fe\uff08CoFE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5c55\u793a\u7279\u5f81\u5982\u4f55\u5f71\u54cdAI-ECG\u6a21\u578b\u7684\u9884\u6d4b\u51b3\u7b56\u3002\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u6846\u67b6\u7684\u9002\u7528\u6027\uff0c\u63ed\u793a\u5fc3\u7535\u56fe\u4fe1\u53f7\u4e2d\u7279\u5f81\u7684\u53d8\u5316\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u7684\u4e00\u81f4\u6027\uff0c\u9884\u671f\u63d0\u9ad8AI-ECG\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u53ca\u652f\u6301\u66f4\u6709\u6548\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5bf9AI-ECG\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u5176\u6210\u529f\u878d\u5165\u4e34\u5e8a\u5b9e\u8df5\uff0c\u4ecb\u7ecd\u4e86CoFE\u6846\u67b6\u3002", "method": "\u4ecb\u7ecd\u4e86\u751f\u6210\u5bf9\u6297\u6027\u5fc3\u7535\u56fe\uff08CoFE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210CoFE\u4ee5\u8bf4\u660e\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u9884\u6d4b\u51b3\u7b56\uff0c\u5c55\u793a\u4e86\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4ee5\u8bc1\u660e\u6846\u67b6\u7684\u9002\u7528\u6027\u3002", "result": "\u901a\u8fc7\u63ed\u793a\u5fc3\u7535\u56fe\u4fe1\u53f7\u4e2d\u7279\u5f81\u7684\u53d8\u5316\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u7684\u4e00\u81f4\u6027\uff0c\u9884\u671f\u589e\u5f3aAI-ECG\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u652f\u6301\u66f4\u6709\u6548\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u751f\u6210\u5bf9\u6297\u6027\u5fc3\u7535\u56fe\uff08CoFE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8bf4\u660e\u7279\u5b9a\u7279\u5f81\u5982\u632f\u5e45\u548c\u95f4\u9694\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u9884\u6d4b\u51b3\u7b56\u3002\u901a\u8fc7\u5c55\u793aCoFE\u7684\u9002\u7528\u6027\uff0c\u5305\u62ec\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff1a\u623f\u98a4\u5206\u7c7b\u548c\u94be\u6c34\u5e73\u56de\u5f52\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5fc3\u7535\u56fe\u4fe1\u53f7\u4e2d\u7279\u5f81\u53d8\u5316\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u7684\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u9610\u660e\u5fc3\u7535\u56fe\u4e2d\u6709\u6548\u7279\u5f81\u7684\u51fa\u73b0\u4f4d\u7f6e\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\uff0c\u9884\u671f\u8be5\u6846\u67b6\u5c06\u589e\u5f3aAI-ECG\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2508.16051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16051", "abs": "https://arxiv.org/abs/2508.16051", "authors": ["Yiheng Hu", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Qian Fu", "Wenjie Zhang", "Liming Zhu"], "title": "MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs", "comment": null, "summary": "Multimodal Multi-hop question answering requires integrating information from\ndiverse sources, such as images and texts, to derive answers. Existing methods\ntypically rely on sequential retrieval and reasoning, where each step builds on\nthe previous output. However, this single-path paradigm makes them vulnerable\nto errors due to misleading intermediate steps. Moreover, developing multimodal\nmodels can be computationally expensive, often requiring extensive training. To\naddress these limitations, we propose a training-free framework guided by an\nAdaptive Planning Graph, which consists of planning, retrieval and reasoning\nmodules. The planning module analyzes the current state of the Adaptive\nPlanning Graph, determines the next action and where to expand the graph, which\nenables dynamic and flexible exploration of reasoning paths. To handle\nretrieval of text to unspecified target modalities, we devise modality-specific\nstrategies that dynamically adapt to distinct data types. Our approach\npreserves the characteristics of multimodal information without costly\ntask-specific training, enabling seamless integration with up-to-date models.\nFinally, the experiments on MultimodalQA and WebQA show that our approach\nmatches or outperforms existing models that rely on training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u7684\u6846\u67b6\uff0c\u7531\u81ea\u9002\u5e94\u89c4\u5212\u56fe\u5f15\u5bfc\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u591a\u8df3\u95ee\u9898\u56de\u7b54\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728MultimodalQA\u548cWebQA\u4e0a\u4e0e\u4f9d\u8d56\u8bad\u7ec3\u7684\u73b0\u6709\u6a21\u578b\u76f8\u5339\u914d\u751a\u81f3\u8868\u73b0\u66f4\u597d\u3002\u6846\u67b6\u5305\u62ec\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\u6a21\u5757\uff0c\u89c4\u5212\u6a21\u5757\u5206\u6790\u81ea\u9002\u5e94\u89c4\u5212\u56fe\u7684\u5f53\u524d\u72b6\u6001\uff0c\u786e\u5b9a\u4e0b\u4e00\u6b65\u64cd\u4f5c\u548c\u56fe\u7684\u6269\u5c55\u4f4d\u7f6e\uff0c\u5b9e\u73b0\u63a8\u7406\u8def\u5f84\u7684\u52a8\u6001\u7075\u6d3b\u63a2\u7d22\u3002\u8bbe\u8ba1\u4e86\u9002\u5e94\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7684\u6a21\u6001\u7279\u5b9a\u7b56\u7565\uff0c\u4ee5\u5904\u7406\u6587\u672c\u5230\u672a\u6307\u5b9a\u76ee\u6807\u6a21\u6001\u7684\u68c0\u7d22\u3002\u4f5c\u8005\u7684\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u793a\u51fa\u4e0e\u4f9d\u8d56\u8bad\u7ec3\u7684\u73b0\u6709\u6a21\u578b\u76f8\u5339\u914d\u751a\u81f3\u4f18\u4e8e\u5176\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u987a\u5e8f\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u8fd9\u79cd\u5355\u4e00\u8def\u5f84\u8303\u5f0f\u4f7f\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u9519\u8bef\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u591a\u6a21\u6001\u6a21\u578b\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u6602\u8d35\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u6846\u67b6\u5305\u62ec\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\u6a21\u5757\uff0c\u89c4\u5212\u6a21\u5757\u5206\u6790\u81ea\u9002\u5e94\u89c4\u5212\u56fe\u7684\u5f53\u524d\u72b6\u6001\uff0c\u786e\u5b9a\u4e0b\u4e00\u6b65\u64cd\u4f5c\u548c\u56fe\u7684\u6269\u5c55\u4f4d\u7f6e\uff0c\u5b9e\u73b0\u63a8\u7406\u8def\u5f84\u7684\u52a8\u6001\u7075\u6d3b\u63a2\u7d22\u3002\u8bbe\u8ba1\u4e86\u9002\u5e94\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7684\u6a21\u6001\u7279\u5b9a\u7b56\u7565\uff0c\u4ee5\u5904\u7406\u6587\u672c\u5230\u672a\u6307\u5b9a\u76ee\u6807\u6a21\u6001\u7684\u68c0\u7d22\u3002", "result": "\u4f5c\u8005\u7684\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u793a\u51fa\u4e0e\u4f9d\u8d56\u8bad\u7ec3\u7684\u73b0\u6709\u6a21\u578b\u76f8\u5339\u914d\u751a\u81f3\u4f18\u4e8e\u5176\u8868\u73b0\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u7684\u6846\u67b6\uff0c\u7531\u81ea\u9002\u5e94\u89c4\u5212\u56fe\u5f15\u5bfc\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u591a\u8df3\u95ee\u9898\u56de\u7b54\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728MultimodalQA\u548cWebQA\u4e0a\u4e0e\u4f9d\u8d56\u8bad\u7ec3\u7684\u73b0\u6709\u6a21\u578b\u76f8\u5339\u914d\u751a\u81f3\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2508.16054", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16054", "abs": "https://arxiv.org/abs/2508.16054", "authors": ["Sonish Sivarajkumar", "Hang Zhang", "Yuelyu Ji", "Maneesh Bilalpur", "Xizhi Wu", "Chenyu Li", "Min Gu Kwak", "Shyam Visweswaran", "Yanshan Wang"], "title": "Generative Foundation Model for Structured and Unstructured Electronic Health Records", "comment": null, "summary": "Electronic health records (EHRs) are rich clinical data sources but complex\nrepositories of patient data, spanning structured elements (demographics,\nvitals, lab results, codes), unstructured clinical notes and other modalities\nof data. Harnessing this heterogeneity is critical for improving patient\noutcomes. Recent advances in large language models (LLMs) have enabled\nfoundation models that can learn from multiple data modalities and support\nclinical tasks. However, most current approaches simply serialize numeric EHR\ndata into text, which risks losing temporal and quantitative detail. We\nintroduce Generative Deep Patient (GDP), a multimodal foundation model that\nnatively encodes structured EHR time-series via a CNN-Transformer encoder and\nfuses it with unstructured EHRs through cross-modal attention into a\nLLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,\nwhere it learns to produce clinical narratives from raw patient timelines while\nalso performing masked feature prediction (MFP) and next time-step prediction\n(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for\nclinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day\nreadmission). In clinical prediction, GDP demonstrated superior performance on\nMIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and\n30-day readmission AUROC = 0.627. For narrative generation, GDP achieved\nROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,\nGDP-Instruct scored highest on faithfulness, fluency, and overall clinical\nutility, suggesting reduced hospital documentation workload without sacrificing\naccuracy. Our results demonstrate that a single multimodal foundation model can\nboth predict clinically actionable events and generate high-quality clinical\nnarratives. Furthermore, GDP's flexible architecture can be extended to\nadditional modalities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86Generative Deep Patient\uff08GDP\uff09\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u4e0e\u5f53\u524d\u5927\u591a\u6570\u65b9\u6cd5\u4e0d\u540c\uff0cGDP\u80fd\u591f\u6709\u6548\u5904\u7406\u7ed3\u6784\u5316\u7535\u5b50\u75c5\u5386\u65f6\u95f4\u5e8f\u5217\u548c\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\u3002\u8be5\u6a21\u578b\u7ecf\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\uff0c\u5728\u4e34\u5e8a\u9884\u6d4b\u548c\u4e34\u5e8a\u53d9\u8ff0\u751f\u6210\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u540c\u65f6\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002GDP\u5c55\u793a\u4e86\u7075\u6d3b\u7684\u67b6\u6784\u80fd\u591f\u6269\u5c55\u5230\u5176\u4ed6\u6570\u636e\u6a21\u6001\u3002", "motivation": "\u7535\u5b50\u75c5\u5386\uff08EHRs\uff09\u662f\u4e30\u5bcc\u7684\u4e34\u5e8a\u6570\u636e\u6e90\uff0c\u4f46\u4e5f\u662f\u590d\u6742\u7684\u60a3\u8005\u6570\u636e\u5b58\u50a8\u5e93\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u5143\u7d20\uff08\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u751f\u547d\u4f53\u5f81\u3001\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u3001\u7f16\u7801\uff09\u3001\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u548c\u5176\u4ed6\u6570\u636e\u6a21\u6001\u3002\u5229\u7528\u8fd9\u79cd\u5f02\u8d28\u6027\u5bf9\u4e8e\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\u3002\u8fd1\u671f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u5c55\u4f7f\u5f97\u53ef\u4ee5\u5b66\u4e60\u591a\u79cd\u6570\u636e\u6a21\u6001\u5e76\u652f\u6301\u4e34\u5e8a\u4efb\u52a1\u7684\u57fa\u7840\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u5f53\u524d\u65b9\u6cd5\u4ec5\u5c06\u6570\u503c\u578bEHR\u6570\u636e\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u5b58\u5728\u9057\u5931\u65f6\u95f4\u548c\u6570\u91cf\u7ec6\u8282\u7684\u98ce\u9669\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86Generative Deep Patient\uff08GDP\uff09\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7CNN-Transformer\u7f16\u7801\u5668\u5bf9\u7ed3\u6784\u5316\u7535\u5b50\u75c5\u5386\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u975e\u7ed3\u6784\u5316\u7535\u5b50\u75c5\u5386\uff0c\u91c7\u7528LLaMA-based\u89e3\u7801\u5668\u3002GDP\u8bad\u7ec3\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\uff081\uff09\u751f\u6210\u6027\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u4ece\u539f\u59cb\u60a3\u8005\u65f6\u95f4\u7ebf\u4e2d\u4ea7\u751f\u4e34\u5e8a\u53d9\u8ff0\u5e76\u6267\u884c\u63a9\u853d\u7279\u5f81\u9884\u6d4b\uff08MFP\uff09\u548c\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u9884\u6d4b\uff08NTP\uff09\u4ee5\u6355\u83b7\u65f6\u95f4\u52a8\u6001\uff1b\uff082\uff09\u591a\u4efb\u52a1\u5fae\u8c03\u7528\u4e8e\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u9884\u6d4b\uff08\u5982\u5fc3\u529b\u8870\u7aed\u30012\u578b\u7cd6\u5c3f\u75c5\u300130\u5929\u518d\u5165\u9662\uff09\u3002", "result": "\u5728MIMIC-IV\u4e0a\uff0cGDP\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5fc3\u529b\u8870\u7aedAUROC = 0.923\u30012\u578b\u7cd6\u5c3f\u75c5AUROC = 0.817\u300130\u5929\u518d\u5165\u9662AUROC = 0.627\u3002\u5728\u53d9\u8ff0\u751f\u6210\u65b9\u9762\uff0cGDP\u5b9e\u73b0\u4e86ROUGE-L = 0.135\u548cBERTScore-F1 = 0.545\u3002\u5728\u76f2\u4eba\u8bc4\u4f30\u4e2d\uff0cGDP-Instruct\u5728\u5fe0\u5b9e\u5ea6\u3001\u6d41\u7545\u5ea6\u548c\u6574\u4f53\u4e34\u5e8a\u5b9e\u7528\u6027\u65b9\u9762\u5f97\u5206\u6700\u9ad8\uff0c\u8868\u660e\u51cf\u5c11\u533b\u9662\u6587\u4ef6\u5de5\u4f5c\u91cf\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684Generative Deep Patient\uff08GDP\uff09\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u4e34\u5e8a\u9884\u6d4b\u548c\u4e34\u5e8a\u53d9\u8ff0\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002\u901a\u8fc7\u5bf9\u7ed3\u6784\u5316\u7535\u5b50\u75c5\u5386\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u7f16\u7801\u5e76\u4e0e\u975e\u7ed3\u6784\u5316\u75c5\u5386\u8fdb\u884c\u8de8\u6a21\u6001\u6ce8\u610f\u878d\u5408\uff0cGDP\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8868\u73b0\u3002\u8be5\u6a21\u578b\u4e0d\u4ec5\u53ef\u4ee5\u9884\u6d4b\u4e34\u5e8a\u53ef\u64cd\u4f5c\u4e8b\u4ef6\uff0c\u8fd8\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e34\u5e8a\u53d9\u8ff0\uff0c\u51cf\u5c11\u4e86\u533b\u9662\u6587\u4ef6\u5de5\u4f5c\u91cf\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002GDP\u8fd8\u5c55\u73b0\u4e86\u7075\u6d3b\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u6570\u636e\u6a21\u6001\u3002"}}
{"id": "2508.16057", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.16057", "abs": "https://arxiv.org/abs/2508.16057", "authors": ["Sijie Yang", "Binyu Lei", "Filip Biljecki"], "title": "Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework", "comment": "Presented at 19th International Conference on Computational Urban\n  Planning and Urban Management (CUPUM 2025)", "summary": "Ensuring liveability and comfort is one of the fundamental objectives of\nurban planning. Numerous studies have employed computational methods to assess\nand quantify factors related to urban comfort such as greenery coverage,\nthermal comfort, and walkability. However, a clear definition of urban comfort\nand its comprehensive evaluation framework remain elusive. Our research\nexplores the theoretical interpretations and methodologies for assessing urban\ncomfort within digital planning, emphasising three key dimensions:\nmultidimensional analysis, data support, and AI assistance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57ce\u5e02\u8212\u9002\u6027\u7684\u7406\u8bba\u89e3\u91ca\u548c\u65b9\u6cd5\u8bba\uff0c\u5f3a\u8c03\u4e86\u591a\u7ef4\u5206\u6790\u3001\u6570\u636e\u652f\u6301\u548c\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u7b49\u5173\u952e\u7ef4\u5ea6\u3002\u91c7\u7528\u8ba1\u7b97\u65b9\u6cd5\u8bc4\u4f30\u7eff\u5316\u8986\u76d6\u3001\u70ed\u8212\u9002\u548c\u6b65\u884c\u4fbf\u5229\u6027\u7b49\u56e0\u7d20\u3002", "motivation": "\u57ce\u5e02\u89c4\u5212\u7684\u57fa\u672c\u76ee\u6807\u4e4b\u4e00\u662f\u786e\u4fdd\u751f\u6d3b\u8d28\u91cf\u548c\u8212\u9002\u5ea6\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u4f7f\u7528\u4e86\u8ba1\u7b97\u65b9\u6cd5\u6765\u8bc4\u4f30\u4e0e\u57ce\u5e02\u8212\u9002\u6027\u76f8\u5173\u7684\u56e0\u7d20\uff0c\u4f46\u5bf9\u57ce\u5e02\u8212\u9002\u6027\u7684\u660e\u786e\u5b9a\u4e49\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u4ecd\u7136\u6a21\u7cca\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u65b9\u6cd5\u6765\u8bc4\u4f30\u57ce\u5e02\u8212\u9002\u6027\uff0c\u5305\u62ec\u7eff\u5316\u8986\u76d6\u3001\u70ed\u8212\u9002\u548c\u6b65\u884c\u4fbf\u5229\u6027\u7b49\u56e0\u7d20\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u57ce\u5e02\u8212\u9002\u6027\u7684\u7406\u8bba\u89e3\u91ca\u548c\u65b9\u6cd5\u8bba\uff0c\u7a81\u51fa\u4e86\u591a\u7ef4\u5206\u6790\u3001\u6570\u636e\u652f\u6301\u548c\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u7b49\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6570\u5b57\u89c4\u5212\u4e2d\u8bc4\u4f30\u57ce\u5e02\u8212\u9002\u6027\u7684\u7406\u8bba\u89e3\u91ca\u548c\u65b9\u6cd5\u8bba\uff0c\u5f3a\u8c03\u4e86\u591a\u7ef4\u5206\u6790\u3001\u6570\u636e\u652f\u6301\u548c\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u7b49\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002"}}
{"id": "2508.16059", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16059", "abs": "https://arxiv.org/abs/2508.16059", "authors": ["Zhuomin Chen", "Dan Li", "Jiahui Zhou", "Shunyu Wu", "Haozheng Ye", "Jian Lou", "See-Kiong Ng"], "title": "Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting", "comment": "To be published in CIKM 2025", "summary": "Time series (TS) data are ubiquitous across various application areas,\nrendering time series forecasting (TSF) a fundamental task. With the astounding\nadvances in large language models (LLMs), a variety of methods have been\ndeveloped to adapt LLMs for time series forecasting. Despite unlocking the\npotential of LLMs in comprehending TS data, existing methods are inherently\nconstrained by their shallow integration of TS information, wherein LLMs\ntypically access TS representations at shallow layers, primarily at the input\nlayer. This causes the influence of TS representations to progressively fade in\ndeeper layers and eventually leads to ineffective adaptation between textual\nembeddings and TS representations. In this paper, we propose the Multi-layer\nSteerable Embedding Fusion (MSEF), a novel framework that enables LLMs to\ndirectly access time series patterns at all depths, thereby mitigating the\nprogressive loss of TS information in deeper layers. Specifically, MSEF\nleverages off-the-shelf time series foundation models to extract semantically\nrich embeddings, which are fused with intermediate text representations across\nLLM layers via layer-specific steering vectors. These steering vectors are\ndesigned to continuously optimize the alignment between time series and textual\nmodalities and facilitate a layer-specific adaptation mechanism that ensures\nefficient few-shot learning capabilities. Experimental results on seven\nbenchmarks demonstrate significant performance improvements by MSEF compared\nwith baselines, with an average reduction of 31.8% in terms of MSE. The code is\navailable at https://github.com/One1sAll/MSEF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Multi-layer Steerable Embedding Fusion (MSEF)\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86LLMs\u5728\u6240\u6709\u6df1\u5ea6\u76f4\u63a5\u8bbf\u95ee\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\uff0c\u4ece\u800c\u6539\u5584\u4e86\u5728\u6df1\u5c42\u4e2d\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u7684\u6e10\u8fdb\u4e22\u5931\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMSEF\u5728\u4e03\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u8f83\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5747\u65b9\u8bef\u5dee\u5e73\u5747\u964d\u4f4e31.8%\u3002", "motivation": "\u5728\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0cLLMs\u901a\u5e38\u4ec5\u5728\u8f93\u5165\u5c42\u6d45\u5c42\u6b21\u8bbf\u95ee\u65f6\u95f4\u5e8f\u5217\u8868\u793a\uff0c\u8fd9\u5bfc\u81f4\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u5728\u66f4\u6df1\u5c42\u6b21\u9010\u6e10\u51cf\u5f31\uff0c\u6700\u7ec8\u5bfc\u81f4\u6587\u672c\u5d4c\u5165\u548c\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e4b\u95f4\u7684\u6709\u6548\u9002\u5e94\u6027\u964d\u4f4e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4ee5\u5229\u7528LLMs\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86Multi-layer Steerable Embedding Fusion (MSEF)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u73b0\u6210\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u4e30\u5bcc\u7684\u5d4c\u5165\uff0c\u901a\u8fc7\u5c42\u7279\u5b9a\u7684\u5f15\u5bfc\u5411\u91cf\u5c06\u8fd9\u4e9b\u5d4c\u5165\u4e0eLLM\u5c42\u4e4b\u95f4\u7684\u4e2d\u95f4\u6587\u672c\u8868\u793a\u8fdb\u884c\u878d\u5408\u3002\u8fd9\u4e9b\u5f15\u5bfc\u5411\u91cf\u65e8\u5728\u4e0d\u65ad\u4f18\u5316\u65f6\u95f4\u5e8f\u5217\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4fc3\u8fdb\u5c42\u7279\u5b9a\u7684\u9002\u5e94\u673a\u5236\uff0c\u786e\u4fdd\u9ad8\u6548\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5728\u4e03\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMSEF\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u660e\u663e\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u5747\u65b9\u8bef\u5dee\u5e73\u5747\u964d\u4f4e\u4e8631.8%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6Multi-layer Steerable Embedding Fusion (MSEF)\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u80fd\u591f\u76f4\u63a5\u5728\u6240\u6709\u6df1\u5ea6\u8bbf\u95ee\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u5728\u6df1\u5c42\u4e2d\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u7684\u6e10\u8fdb\u4e22\u5931\u3002\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5728\u4e03\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMSEF \u76f8\u5bf9\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u5747\u65b9\u8bef\u5dee\u5e73\u5747\u964d\u4f4e\u4e8631.8%\u3002"}}
{"id": "2508.16072", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16072", "abs": "https://arxiv.org/abs/2508.16072", "authors": ["Zizhen Li", "Chuanhao Li", "Yibin Wang", "Qi Chen", "Diping Song", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Fanrui Zhang", "Mingzhu Sun", "Kaipeng Zhang"], "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles", "comment": "EMNLP 2025 MainConference", "summary": "LLMs have shown strong performance on human-centric reasoning tasks. While\nprevious evaluations have explored whether LLMs can infer intentions or detect\ndeception, they often overlook the individualized reasoning styles that\ninfluence how people interpret and act in social contexts. Social deduction\ngames (SDGs) provide a natural testbed for evaluating individualized reasoning\nstyles, where different players may adopt diverse but contextually valid\nreasoning strategies under identical conditions. To address this, we introduce\nInMind, a cognitively grounded evaluation framework designed to assess whether\nLLMs can capture and apply personalized reasoning styles in SDGs. InMind\nenhances structured gameplay data with round-level strategy traces and\npost-game reflections, collected under both Observer and Participant modes. It\nsupports four cognitively motivated tasks that jointly evaluate both static\nalignment and dynamic adaptation. As a case study, we apply InMind to the game\nAvalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o\nfrequently rely on lexical cues, struggling to anchor reflections in temporal\ngameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs\nlike DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These\nfindings reveal key limitations in current LLMs' capacity for individualized,\nadaptive reasoning, and position InMind as a step toward cognitively aligned\nhuman-AI interaction.", "AI": {"tldr": "The paper introduces InMind, a framework to evaluate whether LLMs can adopt personalized reasoning styles in social deduction games, highlighting the limitations of general-purpose LLMs like GPT-4o in individualized reasoning. Reasoning-enhanced LLMs, such as DeepSeek-R1, show promise in style-sensitive reasoning. InMind paves the way for cognitively aligned human-AI interaction.", "motivation": "Previous evaluations of LLMs focused on human-centric reasoning tasks but neglected individualized reasoning styles. Social deduction games offer a testbed for evaluating diverse reasoning strategies. The motivation is to understand if LLMs can adapt to different players' reasoning styles in identical conditions.", "method": "The paper introduces InMind, a framework for evaluating whether LLMs can capture and apply personalized reasoning styles in social deduction games. InMind collects round-level strategy traces and post-game reflections in Observer and Participant modes to assess static alignment and dynamic adaptation. The framework supports four cognitively motivated tasks for evaluation.", "result": "The study applied InMind to evaluate 11 LLMs in the game Avalon. General-purpose LLMs, including GPT-4o, struggled with temporal anchoring and evolving strategies, relying on lexical cues. Reasoning-enhanced LLMs like DeepSeek-R1 exhibited style-sensitive reasoning early on. These results reveal the limitations of current LLMs in personalized, adaptive reasoning.", "conclusion": "LLMs, including state-of-the-art ones such as GPT-4o, struggle with capturing and applying personalized reasoning styles in social deduction games. Reasoning-enhanced LLMs, like DeepSeek-R1, show potential in style-sensitive reasoning. The study highlights the limitations of current LLMs in individualized, adaptive reasoning and proposes InMind as a framework for cognitively aligned human-AI interaction."}}
{"id": "2508.16112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16112", "abs": "https://arxiv.org/abs/2508.16112", "authors": ["Heewoong Noh", "Namkyeong Lee", "Gyoung S. Na", "Kibum Kim", "Chanyoung Park"], "title": "IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra", "comment": null, "summary": "Spectral analysis provides crucial clues for the elucidation of unknown\nmaterials. Among various techniques, infrared spectroscopy (IR) plays an\nimportant role in laboratory settings due to its high accessibility and low\ncost. However, existing approaches often fail to reflect expert analytical\nprocesses and lack flexibility in incorporating diverse types of chemical\nknowledge, which is essential in real-world analytical scenarios. In this\npaper, we propose IR-Agent, a novel multi-agent framework for molecular\nstructure elucidation from IR spectra. The framework is designed to emulate\nexpert-driven IR analysis procedures and is inherently extensible. Each agent\nspecializes in a specific aspect of IR interpretation, and their complementary\nroles enable integrated reasoning, thereby improving the overall accuracy of\nstructure elucidation. Through extensive experiments, we demonstrate that\nIR-Agent not only improves baseline performance on experimental IR spectra but\nalso shows strong adaptability to various forms of chemical information.", "AI": {"tldr": "IR-Agent is a multi-agent framework for molecular structure elucidation from IR spectra, designed to emulate expert-driven IR analysis and improve accuracy. It enhances performance on experimental IR spectra and shows adaptability to diverse chemical information.", "motivation": "Existing approaches in IR spectroscopy lack flexibility in incorporating diverse types of chemical knowledge essential in real-world analytical scenarios. The paper aims to address this gap by introducing a framework that enhances performance and adaptability in molecular structure elucidation from IR spectra.", "method": "The paper proposes the IR-Agent framework designed to emulate expert-driven IR analysis procedures with a multi-agent approach. Each agent specializes in a specific aspect of IR interpretation, enabling integrated reasoning for improved accuracy of structure elucidation.", "result": "Extensive experiments demonstrate that IR-Agent enhances baseline performance on experimental IR spectra and exhibits strong adaptability to various forms of chemical information.", "conclusion": "IR-Agent is a novel multi-agent framework for molecular structure elucidation from IR spectra that improves baseline performance and shows strong adaptability to various forms of chemical information."}}
{"id": "2508.16117", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16117", "abs": "https://arxiv.org/abs/2508.16117", "authors": ["Saransh Kumar Gupta", "Rizwan Gulzar Mir", "Lipika Dey", "Partha Pratim Das", "Anirban Sen", "Ramesh Jain"], "title": "Extending FKG.in: Towards a Food Claim Traceability Network", "comment": "10 pages, 3 figures, 1 table, 45 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop", "summary": "The global food landscape is rife with scientific, cultural, and commercial\nclaims about what foods are, what they do, what they should not do, or should\nnot do. These range from rigorously studied health benefits (probiotics improve\ngut health) and misrepresentations (soaked almonds make one smarter) to vague\npromises (superfoods boost immunity) and culturally rooted beliefs (cold foods\ncause coughs). Despite their widespread influence, the infrastructure for\ntracing, verifying, and contextualizing these claims remains fragmented and\nunderdeveloped. In this paper, we propose a Food Claim-Traceability Network\n(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have\nbeen incrementally building. We also present the ontology design and the\nsemi-automated knowledge curation workflow that we used to develop a proof of\nconcept of FKG.in-FCN using Reddit data and Large Language Models. FCN\nintegrates curated data inputs, structured schemas, and provenance-aware\npipelines for food-related claim extraction and validation. While directly\nlinked to the Indian food knowledge graph as an application, our methodology\nremains application-agnostic and adaptable to other geographic, culinary, or\nregulatory settings. By modeling food claims and their traceability in a\nstructured, verifiable, and explainable way, we aim to contribute to more\ntransparent and accountable food knowledge ecosystems, supporting researchers,\npolicymakers, and most importantly, everyday consumers in navigating a world\nsaturated with dietary assertions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFood Claim-Traceability Network\uff08FCN\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7FKG.in\u77e5\u8bc6\u56fe\u8c31\u548cReddit\u6570\u636e\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u98df\u54c1\u58f0\u79f0\u53ca\u5176\u53ef\u8ffd\u6eaf\u6027\u7684\u5efa\u6a21\u3002\u4f5c\u8005\u91c7\u7528\u672c\u4f53\u8bbe\u8ba1\u548c\u534a\u81ea\u52a8\u5316\u77e5\u8bc6\u6574\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6210\u529f\u5f00\u53d1\u4e86FCN\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5370\u5ea6\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u5e94\u7528\u3002\u8be5\u65b9\u6cd5\u7684\u5e94\u7528\u4e0d\u53ef\u77e5\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u5730\u7406\u3001\u70f9\u996a\u6216\u76d1\u7ba1\u73af\u5883\uff0c\u65e8\u5728\u4e3a\u98df\u54c1\u77e5\u8bc6\u751f\u6001\u7cfb\u7edf\u7684\u900f\u660e\u6027\u548c\u53ef\u9760\u6027\u505a\u51fa\u8d21\u732e\u3002", "motivation": "\u5168\u7403\u98df\u54c1\u9886\u57df\u5145\u65a5\u7740\u5173\u4e8e\u98df\u7269\u7684\u79d1\u5b66\u3001\u6587\u5316\u548c\u5546\u4e1a\u4e3b\u5f20\uff0c\u5305\u62ec\u5bf9\u5176\u5c5e\u6027\u3001\u529f\u6548\u4ee5\u53ca\u4e0d\u5e94\u8be5\u6216\u4e0d\u80fd\u505a\u7684\u4e8b\u60c5\u7684\u5404\u79cd\u4e3b\u5f20\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u4e3b\u5f20\u5bf9\u4eba\u4eec\u5177\u6709\u5e7f\u6cdb\u5f71\u54cd\uff0c\u4f46\u8ffd\u8e2a\u3001\u9a8c\u8bc1\u548c\u80cc\u666f\u5316\u8fd9\u4e9b\u4e3b\u5f20\u7684\u57fa\u7840\u8bbe\u65bd\u4ecd\u7136\u96f6\u6563\u548c\u4e0d\u5b8c\u5584\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u7684\u52a8\u673a\u662f\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548c\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u7ed3\u6784\u5316\u3001\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u5efa\u6a21\u98df\u54c1\u58f0\u79f0\u53ca\u5176\u53ef\u8ffd\u6eaf\u6027\uff0c\u4ece\u800c\u652f\u6301\u66f4\u900f\u660e\u548c\u8d1f\u8d23\u4efb\u7684\u98df\u54c1\u77e5\u8bc6\u751f\u6001\u7cfb\u7edf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Food Claim-Traceability Network\uff08FCN\uff09\u7684\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528FKG.in\u77e5\u8bc6\u56fe\u8c31\u548cReddit\u6570\u636e\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\u3002\u4ed6\u4eec\u91c7\u7528\u672c\u4f53\u8bbe\u8ba1\u548c\u534a\u81ea\u52a8\u5316\u77e5\u8bc6\u6574\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u67b6\u6784\u548c\u6eaf\u6e90\u610f\u8bc6\u7ba1\u9053\uff0c\u5b9e\u73b0\u98df\u54c1\u76f8\u5173\u58f0\u660e\u7684\u63d0\u53d6\u548c\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u662f\u5e94\u7528\u4e0d\u53ef\u77e5\u7684\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u5730\u7406\u3001\u70f9\u996a\u6216\u76d1\u7ba1\u73af\u5883\u3002", "result": "\u901a\u8fc7\u672c\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u4f5c\u8005\u6210\u529f\u5730\u5f00\u53d1\u4e86Food Claim-Traceability Network\uff08FCN\uff09\u4f5c\u4e3a\u5370\u5ea6\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31FKG.in\u7684\u6269\u5c55\uff0c\u5e76\u5229\u7528Reddit\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u6982\u5ff5\u9a8c\u8bc1\u3002\u4ed6\u4eec\u5c55\u793a\u4e86\u672c\u4f53\u8bbe\u8ba1\u548c\u534a\u81ea\u52a8\u5316\u77e5\u8bc6\u6574\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u98df\u54c1\u76f8\u5173\u58f0\u660e\u7684\u63d0\u53d6\u548c\u9a8c\u8bc1\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u7528\u4e0d\u53ef\u77e5\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u5176\u4ed6\u5730\u7406\u3001\u70f9\u996a\u6216\u76d1\u7ba1\u73af\u5883\u3002", "conclusion": "\u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFood Claim-Traceability Network\uff08FCN\uff09\u7684\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u5370\u5ea6\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31FKG.in\u7684\u6269\u5c55\u3002\u4ed6\u4eec\u5c55\u793a\u4e86\u672c\u4f53\u8bbe\u8ba1\u548c\u534a\u81ea\u52a8\u5316\u77e5\u8bc6\u6574\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528Reddit\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e86FKG.in-FCN\u7684\u6982\u5ff5\u9a8c\u8bc1\u3002FCN\u6574\u5408\u4e86\u7b56\u5212\u6570\u636e\u8f93\u5165\u3001\u7ed3\u6784\u5316\u67b6\u6784\u548c\u5177\u6709\u6eaf\u6e90\u610f\u8bc6\u7684\u98df\u54c1\u76f8\u5173\u58f0\u660e\u62bd\u53d6\u548c\u9a8c\u8bc1\u6d41\u7a0b\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u4ee5\u7ed3\u6784\u5316\u3001\u53ef\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u5efa\u6a21\u98df\u54c1\u58f0\u660e\u53ca\u5176\u53ef\u8ffd\u6eaf\u6027\uff0c\u4e3a\u66f4\u900f\u660e\u548c\u8d1f\u8d23\u4efb\u7684\u98df\u54c1\u77e5\u8bc6\u751f\u6001\u7cfb\u7edf\u505a\u51fa\u8d21\u732e\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u3001\u653f\u7b56\u5236\u5b9a\u8005\u4ee5\u53ca\u65e5\u5e38\u6d88\u8d39\u8005\u3002"}}
{"id": "2508.16129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16129", "abs": "https://arxiv.org/abs/2508.16129", "authors": ["Ruiqi Wu", "Yuang Yao", "Tengfei Ma", "Chenran Zhang", "Na Su", "Tao Zhou", "Geng Chen", "Wen Fan", "Yi Zhou"], "title": "Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning", "comment": null, "summary": "Multimodal large language models (MLLMs) have recently demonstrated\nremarkable reasoning abilities with reinforcement learning paradigm. Although\nseveral multimodal reasoning models have been explored in the medical domain,\nmost of them focus exclusively on basic reasoning, which refers to shallow\ninference based on visual feature matching. However, real-world clinical\ndiagnosis extends beyond basic reasoning, demanding reasoning processes that\nintegrate heterogeneous clinical information (such as chief complaints and\nmedical history) with multimodal medical imaging data. To bridge this gap, we\nintroduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the\nfull spectrum of perception and reasoning. It encompasses both basic reasoning\ntasks and complex reasoning tasks, aiming to enhance visual-centric fundamental\nreasoning capabilities and emulate realistic clinical thinking patterns.\nBuilding upon MM-Retinal-Reason, we propose OphthaReason, the first\nophthalmology-specific multimodal reasoning model with step-by-step reasoning\ntraces. To enable flexible adaptation to both basic and complex reasoning\ntasks, we specifically design a novel method called Uncertainty-Aware Dynamic\nThinking (UADT), which estimates sample-level uncertainty via entropy and\ndynamically modulates the model's exploration depth using a shaped advantage\nmechanism. Comprehensive experiments demonstrate that our model achieves\nstate-of-the-art performance on both basic and complex reasoning tasks,\noutperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and\nophthalmic MLLMs by at least 24.92\\%, 15.00\\%, 21.20\\%, and 17.66\\%. Project\nPage: \\href{https://github.com/lxirich/OphthaReason}{link}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5f15\u5165\u4e86 MM-Retinal-Reason \u6570\u636e\u96c6\u548c OphthaReason \u6a21\u578b\uff0c\u901a\u8fc7 Uncertainty-Aware Dynamic Thinking\uff08UADT\uff09\u65b9\u6cd5\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u8868\u73b0\u3002\u63d0\u51fa\u7684\u6a21\u578b\u5728\u57fa\u672c\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5176\u4ed6\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u533b\u5b66\u9886\u57df\u7684\u6a21\u578b\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u533b\u5b66\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4ec5\u4e13\u6ce8\u4e8e\u57fa\u672c\u63a8\u7406\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f02\u8d28\u4e34\u5e8a\u4fe1\u606f\u548c\u591a\u6a21\u6001\u533b\u5b66\u6210\u50cf\u6570\u636e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5e0c\u671b\u901a\u8fc7 MM-Retinal-Reason \u6570\u636e\u96c6\u4ee5\u53ca OphthaReason \u6a21\u578b\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86 OphthaReason \u6a21\u578b\uff0c\u5e76\u4ecb\u7ecd\u4e86 Uncertainty-Aware Dynamic Thinking\uff08UADT\uff09\u65b9\u6cd5\u7528\u4e8e\u4f30\u8ba1\u6837\u672c\u7ea7\u4e0d\u786e\u5b9a\u6027\u548c\u52a8\u6001\u8c03\u8282\u6a21\u578b\u7684\u63a2\u7d22\u6df1\u5ea6\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8be5\u6a21\u578b\u5728\u57fa\u672c\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u901a\u7528 MLLMs\u3001\u533b\u5b66 MLLMs\u3001RL-based \u533b\u5b66 MLLMs \u548c\u773c\u79d1 MLLMs \u81f3\u5c11 24.92\uff05\u300115.00\uff05\u300121.20\uff05 \u548c 17.66\uff05\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86 MM-Retinal-Reason \u6570\u636e\u96c6\u548c OphthaReason \u6a21\u578b\uff0c\u901a\u8fc7 Uncertainty-Aware Dynamic Thinking\uff08UADT\uff09\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u57fa\u672c\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u7075\u6d3b\u9002\u5e94\uff0c\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.16172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16172", "abs": "https://arxiv.org/abs/2508.16172", "authors": ["Kai Hu", "Parfait Atchade-Adelomou", "Carlo Adornetto", "Adrian Mora-Carrero", "Luis Alonso-Pastor", "Ariel Noyman", "Yubo Liu", "Kent Larson"], "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain", "comment": null, "summary": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability.", "AI": {"tldr": "This paper introduces the Preference Chain method to improve simulation of human behavior in transportation systems using Graph Retrieval-Augmented Generation and Large Language Models. Experiments show its superiority over standard LLM, with applications in urban mobility modeling and personalized travel behavior analysis.", "motivation": "Collecting accurate behavioral data in newly developed areas is challenging. Existing generative agents struggle with generating consistent, context-sensitive, and realistic behavioral outputs. This paper aims to address these limitations by introducing a novel method for simulating human behavior in data-scarce environments.", "method": "Introducing the Preference Chain method, which integrates Graph Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) to enhance context-aware simulation of human behavior in transportation systems.", "result": "Experiments on the Replica dataset show that the Preference Chain method aligns better with real-world transportation mode choices compared to standard LLM. The Mobility Agent developed based on this method demonstrates potential applications in urban mobility modeling, personalized travel behavior analysis, and dynamic traffic forecasting.", "conclusion": "Preference Chain method outperforms standard LLM in simulating human behavior in transportation systems, with potential applications in urban mobility modeling and personalized travel behavior analysis."}}
{"id": "2508.16204", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.16204", "abs": "https://arxiv.org/abs/2508.16204", "authors": ["Jo\u00e3o Abrantes", "Robert Tjarko Lange", "Yujin Tang"], "title": "Competition and Attraction Improve Model Fusion", "comment": "Accepted at GECCO 2025 as a full paper", "summary": "Model merging is a powerful technique for integrating the specialized\nknowledge of multiple machine learning models into a single model. However,\nexisting methods require manually partitioning model parameters into fixed\ngroups for merging, which restricts the exploration of potential combinations\nand limits performance. To overcome these limitations, we propose Model Merging\nof Natural Niches (M2N2), an evolutionary algorithm with three key features:\n(1) dynamic adjustment of merging boundaries to progressively explore a broader\nrange of parameter combinations; (2) a diversity preservation mechanism\ninspired by the competition for resources in nature, to maintain a population\nof diverse, high-performing models that are particularly well-suited for\nmerging; and (3) a heuristicbased attraction metric to identify the most\npromising pairs of models for fusion. Our experimental results demonstrate, for\nthe first time, that model merging can be used to evolve models entirely from\nscratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch\nand achieve performance comparable to CMA-ES, while being computationally more\nefficient. Furthermore, M2N2 scales to merge specialized language and image\ngeneration models, achieving state-of-the-art performance. Notably, it\npreserves crucial model capabilities beyond those explicitly optimized by the\nfitness function, highlighting its robustness and versatility. Our code is\navailable at https://github.com/SakanaAI/natural_niches", "AI": {"tldr": "M2N2 is an evolutionary algorithm designed to enhance model merging by dynamically adjusting boundaries, preserving model diversity, and identifying promising model pairs for fusion. It evolves models from scratch, achieves comparable performance to CMA-ES with greater computational efficiency, and scales to merge specialized language and image generation models, maintaining crucial model capabilities. The code is available on GitHub at https://github.com/SakanaAI/natural_niches.", "motivation": "Existing model merging methods require manually partitioning model parameters into fixed groups, limiting the exploration of potential combinations and overall performance. To overcome these limitations, M2N2 is introduced to enable the evolution of models entirely from scratch, improve computational efficiency, and merge specialized language and image generation models.", "method": "The proposed method, Model Merging of Natural Niches (M2N2), is an evolutionary algorithm with three key features: dynamic adjustment of merging boundaries, diversity preservation mechanism, and a heuristic-based attraction metric. These features enable exploring a broader range of parameter combinations, maintaining a diverse population of high-performing models, and identifying promising model pairs for fusion.", "result": "Experimental results demonstrate the effectiveness of M2N2 in evolving models from scratch, achieving performance comparable to CMA-ES but with higher computational efficiency. M2N2 is also successful in merging specialized language and image generation models, showcasing state-of-the-art performance and preserving essential model capabilities beyond those explicitly optimized by the fitness function.", "conclusion": "Model Merging of Natural Niches (M2N2) is proposed as an evolutionary algorithm to address the limitations of existing model merging methods. It dynamically adjusts merging boundaries, preserves diversity in the model population, and uses a heuristic-based attraction metric to identify promising model pairs for fusion. Experimental results show that M2N2 can evolve models from scratch with performance comparable to CMA-ES, with greater computational efficiency. It also scales to merge specialized language and image generation models, achieving state-of-the-art performance and preserving crucial model capabilities."}}
{"id": "2508.16277", "categories": ["cs.AI", "cs.HC", "68T01, 68T05, 68T42, 91A80", "I.2; K.4"], "pdf": "https://arxiv.org/pdf/2508.16277", "abs": "https://arxiv.org/abs/2508.16277", "authors": ["Alexandru Tugui"], "title": "The next question after Turing's question: Introducing the Grow-AI test", "comment": "9th International Conference on Inventive Systems and Control ICISC\n  2025", "summary": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7GROW-AI\u6846\u67b6\u6269\u5c55\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\uff0c\u63a2\u8ba8\u673a\u5668\u662f\u5426\u53ef\u4ee5\u201c\u6210\u957f\u201d\u3002\u65b9\u6cd5\u57fa\u4e8e\u516d\u4e2a\u4e3b\u8981\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u201c\u6e38\u620f\u201d\u63a2\u7d22\u4eba\u7c7b\u7ef4\u5ea6\u548c\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u8f6c\u79fb\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u5bf9\u4e0d\u540c\u7c7b\u578b\uff08\u673a\u5668\u4eba\u3001\u8f6f\u4ef6\u4ee3\u7406\u3001LLMs\uff09\u7684AI\u5b9e\u4f53\u7684\u201c\u6210\u957f\u201d\u6c34\u5e73\u8fdb\u884c\u4e00\u81f4\u4e14\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u4f18\u52bf\u548c\u8106\u5f31\u9886\u57df\uff0c\u4fdd\u8bc1\u4e86\u8bc4\u4f30\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u590d\u5236\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u6269\u5c55\u5bf9\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u901a\u8fc7GROW-AI\u6765\u63a2\u8ba8\u673a\u5668\u662f\u5426\u53ef\u4ee5\u201c\u6210\u957f\u201d\uff0c\u5e76\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u800c\u8bbe\u8ba1\u3002\u65e8\u5728\u5c06\u201c\u6210\u957f\u201d\u7684\u6982\u5ff5\u4ece\u4eba\u7c7b\u4e16\u754c\u8f6c\u79fb\u5230\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u7ed3\u5408\u8de8\u5b66\u79d1\u7684\u89c6\u89d2\u8fdb\u884c\u6574\u5408\u6d4b\u8bd5\uff0c\u4ee5\u6d4b\u91cfAI\u5b9e\u4f53\u7684\u6210\u719f\u6c34\u5e73\u548c\u8fdb\u5316\u8def\u5f84\u3002", "method": "\u8be5\u7814\u7a76\u65b9\u6cd5\u57fa\u4e8eGROW-AI\u6846\u67b6\uff0c\u5e94\u7528\u516d\u4e2a\u4e3b\u8981\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u201c\u6e38\u620f\u201d\u63a2\u7d22\u4eba\u7c7b\u7ef4\u5ea6\u548c\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u8f6c\u79fb\u3002\u5b9e\u4f53\u7684\u51b3\u7b56\u548c\u884c\u52a8\u5728AI\u65e5\u5fd7\u4e2d\u8bb0\u5f55\uff0c\u8bc4\u4f30\u4f7f\u7528\u5148\u524d\u4e13\u5bb6\u65b9\u6cd5\u786e\u5b9a\u521d\u59cb\u6743\u91cd\uff0c\u5e76\u8ba1\u7b97\u5168\u7403\u5206\u6570\uff08\u6210\u957f\u6307\u6570\uff09\u4e3a\u516d\u4e2a\u5206\u6570\u7684\u7b97\u672f\u5e73\u5747\u503c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u5bf9AI\u5b9e\u4f53\u7684\u201c\u6210\u957f\u201d\u6c34\u5e73\u8fdb\u884c\u4e00\u81f4\u4e14\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\uff0c\u65e0\u8bba\u5176\u7c7b\u578b\u662f\u673a\u5668\u4eba\u3001\u8f6f\u4ef6\u4ee3\u7406\u8fd8\u662fLLMs\u3002\u591a\u6e38\u620f\u7ed3\u6784\u7a81\u51fa\u4e86\u4f18\u52bf\u548c\u8106\u5f31\u9886\u57df\uff0c\u4f7f\u7528\u7edf\u4e00\u65e5\u5fd7\u4fdd\u8bc1\u4e86\u8bc4\u4f30\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u590d\u5236\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u62d3\u5c55\u4e86\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7684\u6846\u67b6\uff0c\u79f0\u4e3aGROW-AI\uff08\u6210\u957f\u4e0e\u5b9e\u73b0\u81ea\u4e3b\u667a\u6167\uff09\uff0c\u65e8\u5728\u56de\u7b54\u201c\u673a\u5668\u53ef\u4ee5\u6210\u957f\u5417\uff1f\u201d\u8fd9\u4e00\u81ea\u7136\u5ef6\u4f38\u56fe\u7075\u6d4b\u8bd5\u7684\u95ee\u9898\u3002\u65b9\u6cd5\u5e94\u7528\u57fa\u4e8e\u516d\u4e2a\u4e3b\u8981\u6807\u51c6\uff08C1-C6\uff09\u7684\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u6807\u51c6\u901a\u8fc7\u7279\u5b9a\u7684\u201c\u6e38\u620f\u201d\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u4e3a\u63a2\u7d22\u4eba\u7c7b\u7ef4\u5ea6\u53ca\u5176\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u8f6c\u79fb\u7684\u56db\u4e2a\u7ade\u6280\u573a\u3002\u5b9e\u4f53\u7684\u6240\u6709\u51b3\u7b56\u548c\u884c\u52a8\u90fd\u8bb0\u5f55\u5728\u6807\u51c6\u5316\u7684AI\u65e5\u5fd7\u4e2d\uff0c\u8fd9\u662f\u8ba1\u7b97\u590d\u5408\u5206\u6570\u7684\u4e3b\u8981\u6765\u6e90\u3002\u8bc4\u4f30\u4f7f\u7528\u5148\u524d\u4e13\u5bb6\u65b9\u6cd5\u5efa\u7acb\u521d\u59cb\u6743\u91cd\uff0c\u5168\u7403\u5206\u6570\u2014\u2014\u6210\u957f\u6307\u6570\u2014\u2014\u8ba1\u7b97\u4e3a\u516d\u4e2a\u5206\u6570\u7684\u7b97\u672f\u5e73\u5747\u503c\uff0c\u5e76\u5bf9\u6210\u719f\u9608\u503c\u8fdb\u884c\u89e3\u91ca\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u5bf9AI\u5b9e\u4f53\u7684\u201c\u6210\u957f\u201d\u6c34\u5e73\u8fdb\u884c\u4e00\u81f4\u4e14\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\uff0c\u65e0\u8bba\u5176\u7c7b\u578b\u662f\u673a\u5668\u4eba\u3001\u8f6f\u4ef6\u4ee3\u7406\u8fd8\u662fLLMs\u3002\u591a\u6e38\u620f\u7ed3\u6784\u7a81\u51fa\u4e86\u4f18\u52bf\u548c\u8106\u5f31\u9886\u57df\uff0c\u4f7f\u7528\u7edf\u4e00\u65e5\u5fd7\u4fdd\u8bc1\u4e86\u8bc4\u4f30\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u590d\u5236\u6027\u3002\u7814\u7a76\u7684\u72ec\u521b\u6027\u5728\u4e8e\u5c06\u4ece\u4eba\u7c7b\u4e16\u754c\u5230\u4eba\u5de5\u667a\u80fd\u4e16\u754c\u7684\u201c\u6210\u957f\u201d\u8fc7\u7a0b\u6982\u5ff5\u8f6c\u79fb\uff0c\u4ee5\u7ed3\u5408\u5fc3\u7406\u5b66\u3001\u673a\u5668\u4eba\u6280\u672f\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4f26\u7406\u5b66\u7684\u89c6\u89d2\u8fdb\u884c\u7efc\u5408\u6d4b\u8bd5\u683c\u5f0f\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0cGROW-AI\u4e0d\u4ec5\u8861\u91cf\u6027\u80fd\uff0c\u8fd8\u6355\u6349\u4e86AI\u5b9e\u4f53\u5411\u6210\u719f\u53d1\u5c55\u7684\u8fdb\u5316\u8def\u5f84\u3002"}}
{"id": "2508.16279", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16279", "abs": "https://arxiv.org/abs/2508.16279", "authors": ["Dawei Gao", "Zitao Li", "Yuexiang Xie", "Weirui Kuang", "Liuyi Yao", "Bingchen Qian", "Zhijian Ma", "Yue Cui", "Haohao Luo", "Shen Li", "Lu Yi", "Yi Yu", "Shiqi He", "Zhiling Luo", "Wenmeng Zhou", "Zhicheng Zhang", "Xuguang He", "Ziqian Chen", "Weikai Liao", "Farruh Isakulovich Kushnazarov", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications", "comment": null, "summary": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.", "AI": {"tldr": "AgentScope enhances tool-based interactions for building agentic applications by introducing foundational components, unified interfaces, and extensible modules. It grounds agent behaviors in the ReAct paradigm, integrates built-in agents, and offers robust engineering support. The system includes a scalable evaluation module and a runtime sandbox for safe execution, providing a practical foundation for scalable and effective agentic applications.", "motivation": "Driven by the advancements of Large Language Models (LLMs) empowering agents to combine intrinsic knowledge with dynamic tool use. Aim to enhance agent capacity in addressing real-world tasks and improving human-agent and agent-agent interaction patterns. Provide developer-friendly experiences and enable the development of long-trajectory agentic applications in a manageable way.", "method": "Introducing foundational components, unified interfaces, and extensible modules to facilitate leveraging new models and functionalities. Grounding agent behaviors in the ReAct paradigm and design systematic asynchronous processes for improved interactions and execution efficiency. Integrating built-in agents tailored for specific scenarios and providing robust engineering support. Offering a scalable evaluation module with visual studio interface and a runtime sandbox for safe agent execution and rapid deployment.", "result": "AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications with major improvements in tool-based interactions, foundational components, advanced infrastructure, and safe execution environment.", "conclusion": "AgentScope introduces major improvements in supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. It provides foundational components, unified interfaces, and extensible modules to leverage the latest progress. The ReAct paradigm grounds agent behaviors and advanced agent-level infrastructure enhances interaction patterns and execution efficiency. AgentScope includes built-in agents for practical scenarios, robust engineering support, scalable evaluation module, and a runtime sandbox for safe agent execution and rapid deployment."}}
{"id": "2508.16292", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16292", "abs": "https://arxiv.org/abs/2508.16292", "authors": ["Wen-Han Hsieh", "Elvis Hsieh", "Dantong Niu", "Trevor Darrell", "Roei Herzig", "David M. Chan"], "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible", "comment": "9 pages, 2 figures, 1 table", "summary": "Recently, Vision-Language-Action (VLA) models have demonstrated strong\nperformance on a range of robotic tasks. These models rely on multimodal\ninputs, with language instructions playing a crucial role -- not only in\npredicting actions, but also in robustly interpreting user intent, even when\nthe requests are impossible to fulfill. In this work, we investigate how VLAs\ncan recognize, interpret, and respond to false-premise instructions: natural\nlanguage commands that reference objects or conditions absent from the\nenvironment. We propose Instruct-Verify-and-Act (IVA), a unified framework that\n(i) detects when an instruction cannot be executed due to a false premise, (ii)\nengages in language-based clarification or correction, and (iii) grounds\nplausible alternatives in perception and action. Towards this end, we construct\na large-scale instruction tuning setup with structured language prompts and\ntrain a VLA model capable of handling both accurate and erroneous requests. Our\napproach leverages a contextually augmented, semi-synthetic dataset containing\npaired positive and false-premise instructions, enabling robust detection and\nnatural language correction. Our experiments show that IVA improves false\npremise detection accuracy by 97.56% over baselines, while increasing\nsuccessful responses in false-premise scenarios by 50.78%.", "AI": {"tldr": "IVA framework proposed to enhance VLAs' handling of false-premise instructions, improving detection accuracy and successful response rates in such scenarios.", "motivation": "Investigating how VLAs can handle false-premise instructions in robotic tasks, aiming to improve the robustness of interpreting user intent in the presence of inaccurate requests.", "method": "Proposed Instruct-Verify-and-Act (IVA) framework to address false-premise instructions in Vision-Language-Action (VLA) models, which includes detection of unexecutable instructions, language-based clarification, and grounding plausible alternatives. Constructed a large-scale instruction tuning setup and trained a VLA model with structured language prompts. Leveraged a contextually augmented, semi-synthetic dataset for robust detection and natural language correction.", "result": "IVA framework significantly improves false premise detection accuracy and successful responses in false-premise scenarios.", "conclusion": "IVA framework improves false premise detection accuracy by 97.56% over baselines and increases successful responses in false-premise scenarios by 50.78%."}}
{"id": "2508.16352", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16352", "abs": "https://arxiv.org/abs/2508.16352", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management", "comment": null, "summary": "Efficient and reliable beam alignment is a critical requirement for mmWave\nmultiple-input multiple-output (MIMO) systems, especially in 6G and beyond,\nwhere communication must be fast, adaptive, and resilient to real-world\nuncertainties. Existing deep learning (DL)-based beam alignment methods often\nneglect the underlying causal relationships between inputs and outputs, leading\nto limited interpretability, poor generalization, and unnecessary beam sweeping\noverhead. In this work, we propose a causally-aware DL framework that\nintegrates causal discovery into beam management pipeline. Particularly, we\npropose a novel two-stage causal beam selection algorithm to identify a minimal\nset of relevant inputs for beam prediction. First, causal discovery learns a\nBayesian graph capturing dependencies between received power inputs and the\noptimal beam. Then, this graph guides causal feature selection for the DL-based\nclassifier. Simulation results reveal that the proposed causal beam selection\nmatches the performance of conventional methods while drastically reducing\ninput selection time by 94.4% and beam sweeping overhead by 59.4% by focusing\nonly on causally relevant features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8emmWave\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7cfb\u7edf\u7684\u6ce2\u675f\u5bf9\u51c6\u3002\u901a\u8fc7\u56e0\u679c\u53d1\u73b0\u6765\u4f18\u5316\u6ce2\u675f\u9009\u62e9\u8fc7\u7a0b\uff0c\u51cf\u5c11\u8f93\u5165\u9009\u62e9\u65f6\u95f4\u548c\u6ce2\u675f\u626b\u63cf\u5f00\u9500\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6ce2\u675f\u5bf9\u51c6\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u6f5c\u5728\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u89e3\u91ca\u6027\u6709\u9650\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u4e0d\u5fc5\u8981\u7684\u6ce2\u675f\u626b\u63cf\u5f00\u9500\u3002\u57286G\u53ca\u4ee5\u540e\u7684\u65f6\u4ee3\uff0c\u901a\u4fe1\u9700\u5feb\u901f\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u97e7\u6027\uff0c\u9ad8\u6548\u53ef\u9760\u7684\u6ce2\u675f\u5bf9\u51c6\u5bf9\u4e8emmWave\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u56e0\u679c\u53d1\u73b0\u5b66\u4e60\u8d1d\u53f6\u65af\u56fe\u6765\u6355\u83b7\u63a5\u6536\u529f\u7387\u8f93\u5165\u548c\u6700\u4f73\u6ce2\u675f\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u6307\u5bfc\u56e0\u679c\u7279\u5f81\u9009\u62e9\u4ee5\u4f9b\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u4f7f\u7528\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u56e0\u679c\u6ce2\u675f\u9009\u62e9\u65b9\u6cd5\u5728\u51cf\u5c11\u8f93\u5165\u9009\u62e9\u65f6\u95f4\u548c\u6ce2\u675f\u626b\u63cf\u5f00\u9500\u7684\u540c\u65f6\uff0c\u80fd\u591f\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5b9e\u73b0\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u56e0\u679c\u53d1\u73b0\u96c6\u6210\u5230\u6ce2\u675f\u7ba1\u7406\u6d41\u7a0b\u4e2d\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u56e0\u679c\u6ce2\u675f\u9009\u62e9\u7b97\u6cd5\uff0c\u8bc6\u522b\u51fa\u6ce2\u675f\u9884\u6d4b\u7684\u76f8\u5173\u8f93\u5165\u7684\u6700\u5c0f\u96c6\u5408\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5c06\u8f93\u5165\u9009\u62e9\u65f6\u95f4\u548c\u6ce2\u675f\u626b\u63cf\u5f00\u9500\u5206\u522b\u51cf\u5c11\u4e8694.4%\u548c59.4%\u3002"}}
{"id": "2508.16383", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.16383", "abs": "https://arxiv.org/abs/2508.16383", "authors": ["Xinyu Yang", "Chenlong Deng", "Zhicheng Dou"], "title": "GLARE: Agentic Reasoning for Legal Judgment Prediction", "comment": null, "summary": "Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications.", "AI": {"tldr": "GLARE is a legal reasoning framework that dynamically acquires legal knowledge to enhance reasoning in Legal Judgment Prediction tasks. Experiments validate its effectiveness on real-world datasets, offering improved interpretability and practical applications.", "motivation": "Existing large language models (LLMs) lack legal knowledge, leading to insufficient reasoning in Legal Judgment Prediction tasks. The paper aims to address this issue by developing GLARE to enhance reasoning depth and breadth in LJP.", "method": "Introduce GLARE, an agentic legal reasoning framework that acquires key legal knowledge dynamically by invoking different modules for improved reasoning. Conduct experiments on real-world dataset to validate the method's effectiveness.", "result": "The experiments verify the effectiveness of GLARE in improving reasoning in Legal Judgment Prediction tasks. The reasoning chain generated adds interpretability and opens possibilities for practical applications.", "conclusion": "GLARE is an agentic legal reasoning framework that dynamically acquires legal knowledge to enhance reasoning in Legal Judgment Prediction (LJP) tasks. The experiments demonstrate the effectiveness of GLARE on real-world datasets, improving interpretability and offering practical applications."}}
{"id": "2508.16463", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16463", "abs": "https://arxiv.org/abs/2508.16463", "authors": ["Aniello Panariello", "Emanuele Frascaroli", "Pietro Buzzega", "Lorenzo Bonicelli", "Angelo Porrello", "Simone Calderara"], "title": "Modular Embedding Recomposition for Incremental Learning", "comment": "Accepted to the 36th British Machine Vision Conference (BMVC 2025),\n  Sheffield, UK", "summary": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMoDER\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u6587\u672c\u4e13\u5bb6\u5e76\u5229\u7528\u6a21\u5757\u5316\u6846\u67b6\u5b58\u50a8\u5b83\u4eec\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u8fd9\u4e9b\u4e13\u5bb6\u7ec4\u5408\u4ece\u800c\u6539\u8fdbVision-Language Models\u7684\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6548\u679c\uff0c\u63d0\u9ad8\u4e86VLMs\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u5148\u524d\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4fdd\u7559VLMs\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u672c\u7814\u7a76\u5219\u65e8\u5728\u63d0\u9ad8\u8fd9\u79cd\u80fd\u529b\u3002\u901a\u8fc7MoDER\u65b9\u6cd5\uff0c\u8bd5\u56fe\u5c06\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\u7684\u4fdd\u6301\u8f6c\u53d8\u4e3a\u589e\u5f3a\uff0c\u4ee5\u63d0\u9ad8VLMs\u5728\u672a\u89c1\u7c7b\u522b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u7814\u7a76\u5229\u7528MoDular Embedding Recomposition\uff08MoDER\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u6587\u672c\u4e13\u5bb6\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4ece\u5b58\u50a8\u7684\u4e13\u5bb6\u4e2d\u68c0\u7d22\u6765\u63d0\u9ad8\u9884\u8bad\u7ec3\u7684Vision-Language Models\uff08VLMs\uff09\u7684\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660eMoDER\u65b9\u6cd5\u5728\u4e24\u79cd\u96f6\u6837\u672c\u589e\u91cf\u5b66\u4e60\u534f\u8bae\u4e0b\u5c55\u73b0\u51fa\u4e86\u6709\u6548\u6027\uff0c\u5e76\u572814\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoDular Embedding Recomposition\uff08MoDER\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u8bad\u7ec3\u591a\u4e2a\u6587\u672c\u4e13\u5bb6\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u4e00\u4e2a\u5355\u72ec\u7684\u5df2\u89c1\u7c7b\uff0c\u5e76\u5c06\u5b83\u4eec\u5b58\u50a8\u5728\u4e00\u4e2a\u57fa\u7840\u67a2\u7ebd\u4e2d\u3002\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\uff0c\u9488\u5bf9\u6bcf\u4e2a\u672a\u89c1\u7c7b\u522b\u4ece\u67a2\u7ebd\u4e2d\u67e5\u8be2\u548c\u7ec4\u5408\u68c0\u7d22\u7684\u4e13\u5bb6\uff0c\u4ee5\u5408\u6210\u4e00\u4e2a\u6539\u8fdb\u7684\u539f\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002\u7814\u7a76\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e24\u79cd\u5e38\u7528\u7684\u96f6\u6837\u672c\u589e\u91cf\u5b66\u4e60\u534f\u8baeClass-IL\u548cMTIL\u4e0a\u8868\u73b0\u51fa\u4e86\u6709\u6548\u6027\uff0c\u6db5\u76d6\u4e86\u603b\u517114\u4e2a\u6570\u636e\u96c6\u3002"}}
{"id": "2508.16524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16524", "abs": "https://arxiv.org/abs/2508.16524", "authors": ["Xuan Zhang", "Zhijian Zhou", "Weidi Xu", "Yanting Miao", "Chao Qu", "Yuan Qi"], "title": "Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning", "comment": null, "summary": "Enabling neural networks to learn complex logical constraints and fulfill\nsymbolic reasoning is a critical challenge. Bridging this gap often requires\nguiding the neural network's output distribution to move closer to the symbolic\nconstraints. While diffusion models have shown remarkable generative capability\nacross various domains, we employ the powerful architecture to perform\nneuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline\nadopts a two-stage training strategy: the first stage focuses on cultivating\nbasic reasoning abilities, while the second emphasizes systematic learning of\nlogical constraints. To impose hard constraints on neural outputs in the second\nstage, we formulate the diffusion reasoner as a Markov decision process and\ninnovatively fine-tune it with an improved proximal policy optimization\nalgorithm. We utilize a rule-based reward signal derived from the logical\nconsistency of neural outputs and adopt a flexible strategy to optimize the\ndiffusion reasoner's policy. We evaluate our methodology on some classical\nsymbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and\npreference learning. Experimental results demonstrate that our approach\nachieves outstanding accuracy and logical consistency among neural networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u57f9\u517b\u57fa\u672c\u63a8\u7406\u80fd\u529b\u548c\u7cfb\u7edf\u5b66\u4e60\u903b\u8f91\u7ea6\u675f\uff0c\u53d6\u5f97\u4e86\u5728\u7b26\u53f7\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u590d\u6742\u7684\u903b\u8f91\u7ea6\u675f\u548c\u5b9e\u73b0\u7b26\u53f7\u63a8\u7406\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u5206\u5e03\u5f15\u5bfc\u5230\u7b26\u53f7\u7ea6\u675f\u9644\u8fd1\uff0c\u6b64\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u95f4\u9699\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u67b6\u6784\u8fdb\u884c\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7b2c\u4e00\u9636\u6bb5\u57f9\u517b\u57fa\u672c\u63a8\u7406\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7740\u91cd\u4e8e\u7cfb\u7edf\u5b66\u4e60\u903b\u8f91\u7ea6\u675f\uff0c\u5229\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5236\u5b9a\u6269\u6563\u63a8\u7406\u5668\uff0c\u5229\u7528\u6539\u8fdb\u7684\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u521b\u65b0\u5fae\u8c03\uff0c\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u4fe1\u53f7\u5e76\u7075\u6d3b\u4f18\u5316\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e00\u4e9b\u7ecf\u5178\u7b26\u53f7\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u51c6\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u8ba9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u590d\u6742\u7684\u903b\u8f91\u7ea6\u675f\u548c\u5b9e\u73b0\u7b26\u53f7\u63a8\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u6765\u5b9e\u73b0\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\uff0c\u5e76\u5728\u7b26\u53f7\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u51c6\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u8868\u73b0\u3002"}}
{"id": "2508.16571", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.16571", "abs": "https://arxiv.org/abs/2508.16571", "authors": ["Alisa Vinogradova", "Vlad Vinogradov", "Dmitrii Radkevich", "Ilya Yasny", "Dmitry Kobyzev", "Ivan Izmailov", "Katsiaryna Yanchanka", "Andrey Doronichev"], "title": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence", "comment": null, "summary": "In this paper, we describe and benchmark a competitor-discovery component\nused within an agentic AI system for fast drug asset due diligence. A\ncompetitor-discovery AI agent, given an indication, retrieves all drugs\ncomprising the competitive landscape of that indication and extracts canonical\nattributes for these drugs. The competitor definition is investor-specific, and\ndata is paywalled/licensed, fragmented across registries, ontology-mismatched\nby indication, alias-heavy for drug names, multimodal, and rapidly changing.\nAlthough considered the best tool for this problem, the current LLM-based AI\nsystems aren't capable of reliably retrieving all competing drug names, and\nthere is no accepted public benchmark for this task. To address the lack of\nevaluation, we use LLM-based agents to transform five years of multi-modal,\nunstructured diligence memos from a private biotech VC fund into a structured\nevaluation corpus mapping indications to competitor drugs with normalized\nattributes. We also introduce a competitor validating LLM-as-a-judge agent that\nfilters out false positives from the list of predicted competitors to maximize\nprecision and suppress hallucinations. On this benchmark, our\ncompetitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research\n(65%) and Perplexity Labs (60%). The system is deployed in production with\nenterprise users; in a case study with a biotech VC investment fund, analyst\nturnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the\ncompetitive analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7ade\u4e89\u5bf9\u624b\u53d1\u73b0\u7ec4\u4ef6\u5728\u4ee3\u7406AI\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528LLM-based AI\u7cfb\u7edf\u5904\u7406\u6570\u636e\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u9a8c\u8bc1\u4ee3\u7406\u3002\u4f5c\u8005\u6210\u529f\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\uff0c\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\uff0c\u5e76\u5728\u751f\u4ea7\u4e2d\u53d6\u5f97\u6210\u529f\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u7684LLM-based AI\u7cfb\u7edf\u65e0\u6cd5\u53ef\u9760\u5730\u68c0\u7d22\u6240\u6709\u7ade\u4e89\u6027\u836f\u7269\u540d\u79f0\uff0c\u5e76\u4e14\u9488\u5bf9\u6b64\u4efb\u52a1\u6ca1\u6709\u516c\u8ba4\u7684\u516c\u5f00\u57fa\u51c6\uff0c\u4f5c\u8005\u53d7\u5230\u8bc4\u4f30\u7f3a\u5931\u7684\u542f\u53d1\u3002\u4ed6\u4eec\u5e0c\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u6539\u8fdb\u7ade\u4e89\u5bf9\u624b\u53d1\u73b0\u7684\u6548\u7387\u3002", "method": "\u4f5c\u8005\u4f7f\u7528LLM-based AI\u7cfb\u7edf\u6765\u5904\u7406\u591a\u6a21\u5f0f\u3001\u975e\u7ed3\u6784\u5316\u7684\u5c3d\u804c\u8c03\u67e5\u5907\u5fd8\u5f55\u6570\u636e\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u8bed\u6599\u5e93\u3002\u4ed6\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u9a8c\u8bc1\u7ade\u4e89\u5bf9\u624b\u7684LLM-as-a-judge\u4ee3\u7406\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u6700\u5927\u5316\u7cbe\u5ea6\u548c\u6291\u5236\u865a\u5047\u6210\u679c\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u9a8c\u8bc1\u7ade\u4e89\u5bf9\u624b\u7684\u4ee3\u7406\uff0c\u4f5c\u8005\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5f53\u524dLLM-based AI\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u5e76\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u3002\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u4ed6\u4eec\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u5e76\u52a0\u5feb\u4e86\u7ade\u4e89\u5206\u6790\u7684\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86\u4e00\u4e2a\u5728\u4ee3\u7406AI\u7cfb\u7edf\u4e2d\u7528\u4e8e\u5feb\u901f\u836f\u7269\u8d44\u4ea7\u5c3d\u804c\u8c03\u67e5\u7684\u7ade\u4e89\u5bf9\u624b\u53d1\u73b0\u7ec4\u4ef6\uff0c\u5e76\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u4ed6\u4eec\u4f7f\u7528LLM-based AI\u7cfb\u7edf\u6765\u89e3\u51b3\u5f53\u524d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u8be5\u7cfb\u7edf\u53ef\u9760\u5730\u68c0\u7d22\u6240\u6709\u7ade\u4e89\u6027\u836f\u7269\u540d\u79f0\uff0c\u5e76\u4e3a\u6b64\u4efb\u52a1\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u4ed6\u4eec\u7684\u7ade\u4e89\u5bf9\u624b\u53d1\u73b0\u4ee3\u7406\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8683%\u7684\u53ec\u56de\u7387\uff0c\u8d85\u8fc7\u4e86OpenAI Deep Research\u768465%\u548cPerplexity Labs\u768460%\u3002\u7cfb\u7edf\u5df2\u90e8\u7f72\u5728\u4f01\u4e1a\u7528\u6237\u4e2d\uff0c\u5e76\u5728\u751f\u4ea7\u4e2d\u53d6\u5f97\u6210\u529f\u3002\u5728\u4e0e\u751f\u7269\u6280\u672f\u98ce\u9669\u6295\u8d44\u57fa\u91d1\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5206\u6790\u5e08\u7684\u5904\u7406\u65f6\u95f4\u4ece2.5\u5929\u964d\u4f4e\u5230\u7ea63\u5c0f\u65f6\uff0c\u63d0\u9ad8\u4e8620\u500d\u3002"}}
