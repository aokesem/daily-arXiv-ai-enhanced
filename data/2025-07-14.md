<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: This paper explores whether AI can exhibit creativity by analyzing historical views on creativity, psychological advancements' impact, different definitions of creativity, and responses from naturalism and cognitive neuroscience.


<details>
  <summary>Details</summary>
Motivation: With the evolution of science and technology, the understanding of creativity has evolved significantly. This paper aims to investigate how AI can demonstrate creativity by delving into research in psychology, cognitive neuroscience, and the philosophy of creativity.

Method: The paper reviews historical perspectives on the philosophy of creativity, analyzes psychological advancements' influence on the study of creativity, examines various definitions of creativity, and explores the responses of naturalism and cognitive neuroscience to the concept of creativity.

Result: The paper provides insights into the possibility of AI exhibiting creativity and offers a comprehensive analysis of the interactions between AI techniques and the study of creativity from various perspectives.

Conclusion: AI has the potential to exhibit creativity, and this paper explores the intersection of AI techniques with psychology, cognitive neuroscience, and the philosophy of creativity to address this question.

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [2] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 该论文提出了TableReasoner框架，结合大型语言模型和编程，用于解决表格问答任务中的挑战。通过模式化表格建模和迭代思维架构，系统在SemEval-2025 Task 8的两个子任务中取得了第一名的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决表格问答任务面临的挑战，如大型数据、不完整的列语义和实体的歧义。通过综合使用大型语言模型和编程，设计能够高效处理大表格并消除歧义的TableReasoner框架。

Method: 使用大型语言模型和编程相结合的方法，设计了TableReasoner框架，并实现了结构和语义表示的模式化表格建模。通过多步骤的模式链接计划提取关注特定查询的表格模式，集成推理工作流到迭代思维架构中。

Result: 该系统在SemEval-2025 Task 8的两个子任务中获得第一名，表明所提出的TableReasoner框架在表格问答任务中取得了显著的成果。

Conclusion: 该论文提出了一种基于大型语言模型和编程的表格推理框架TableReasoner，用于表格问答任务。通过结构和语义表示相结合的模式建模表格，实现对大表格的整体理解和高效处理。设计了多步骤的模式链接计划，提取关注特定查询的表格模式，消除歧义和减轻错觉。通过将推理工作流集成到迭代思维架构中，允许思考、推理和反思的增量循环。该系统在SemEval-2025 Task 8的两个子任务中取得第一名的成绩。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [3] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出了一个动态Stackelberg博弈框架，用于建模LLM jailbreaking的攻击者和防御者互动。设计了一种名为“紫色特工”的AI解决方案，通过RRT结合敌对探索和防御策略，有效预防恶意输出，为分析对抗动态和降低越狱风险提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在关键应用中的部署越来越多，越狱挑战成为一个重要问题。有必要建立一个模型来分析和预防对抗动态，减轻LLM被攻击的风险。

Method: 使用动态Stackelberg博弈框架建模LLM jailbreaking的攻击者和防御者互动，设计了“紫色特工”AI解决方案，利用Rapidly-exploring Random Trees（RRT）结合敌对探索和防御策略，主动模拟潜在攻击路径并采取预防措施。

Result: 提出了一种用于分析LLM越狱攻击的新颖方法，通过“紫色特工”AI解决方案有效预防恶意输出，并为降低越狱风险提供了基础。

Conclusion: 本文提出了一种动态Stackelberg博弈框架，用于建模LLM越狱攻击中攻击者和防御者之间的互动。通过引入新型AI解决方案“紫色特工”，结合敌对探索和防御策略，有效地预防恶意输出，为分析对抗动态提供了基础，从而减轻越狱风险。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [4] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: The paper introduces the LLM-Nash framework, a game-theoretic model that explores how agents' choice of reasoning prompts in Large Language Models can affect decision-making. It addresses bounded rationality, models reasoning explicitly, and shows divergent outcomes from classical Nash equilibria through examples, offering insights into strategic interactions in LLM-enabled systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional game models by considering bounded rationality and explicitly modeling reasoning processes in LLMs.

Method: Proposing a game-theoretic model where agents choose reasoning prompts to influence decision-making in Large Language Models (LLMs) to study cognitive constraints, mindset expressiveness, and epistemic learning.

Result: Demonstrating through examples how reasoning equilibria in the LLM-Nash framework can differ from classical Nash outcomes, providing a new basis for analyzing strategic interactions in systems utilizing LLMs.

Conclusion: Introducing the LLM-Nash framework that incorporates bounded rationality and explicit modeling of the reasoning process, leading to divergent outcomes from classical Nash equilibria.

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [5] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 论文探讨了智能Agent在探索和控制环境时如何平衡好奇心和能力，研究了内部表征对探索中好奇心和能力的影响，比较了使用不同方法的Agent，并发现了探索与表示学习之间的双向互动。研究结果为认知理论和强化学习提供了启示。


<details>
  <summary>Details</summary>
Motivation: 论文旨在将认知内在动机理论与强化学习相结合，研究内部表征如何调节好奇心和能力的平衡，从而形式化适应性探索为追求未知和可控性之间的平衡。

Method: 论文比较了使用手工状态抽象（Tabular）和学习内部世界模型（Dreamer）的两种基于模型的Agent。Tabular Agent显示好奇心和能力在不同模式下引导探索，而同时优先考虑二者可改善探索。Dreamer Agent揭示了探索和表示学习之间的双向互动。

Result: 论文的研究结果为认知理论和强化学习提供了洞见，并阐明了探索的平衡机制。

Conclusion: 该论文探讨了智能Agent在探索世界的同时保持对环境的控制，研究了好奇心与能力之间的平衡关系，以及内部表征如何影响好奇心和能力的权衡。研究发现使用内部世界模型的Agent表现出探索和表示学习之间的双向互动，反映了好奇心和能力的发展共同演化。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [6] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 本文探討了Neural-Symbolic方法中存在的grounding問題，提出了一個新的parametrized grounding方法家族，通過不同選擇可以控制推理器的表達能力和可擴展性之間的平衡。實驗結果強調了grounding標準的選擇對於NeSy方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 現有的Neural-Symbolic方法中，存在一些grounding方法會導致推理樹的組合爆炸，嚴重限制了其可擴展性。本文靈感來源於多跳符號推理，提出一個新的grounding方法家族，以解決表達能力和可擴展性之間的折衷。

Method: 本文提出了一個參數化的grounding方法家族，通過對這個家族中不同選擇的使用，可以控制推理器的表達能力和可擴展性之間的平衡。

Result: 通過實驗結果顯示，grounding標準的選擇對於NeSy方法的效果同樣重要。

Conclusion: 這篇論文提出一個參數化的grounding方法家族，可以泛化經典的Backward Chaining方法，通過不同的選擇在這個家族中可以獲得常用的grounding方法，並且控制推理器的表達能力和可擴展性之間的折衷。實驗結果表明，grounding標準的選擇通常與NeSy方法本身同等重要。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [7] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 本文针对QFL领域的研究现状和问题，提出了首个多模态方法及Missing Modality Agnostic（MMA）机制，通过量子纠缠实现中间融合，解决了多模态QFL中的性能瓶颈，提高了模型准确度。仿真结果表明，该方法在IID和非IID数据分布下均取得了显著的准确度提升。


<details>
  <summary>Details</summary>
Motivation: 由于现有QFL框架主要集中在单模态系统上，对于现实任务中涉及多种模态的应用有限，本文旨在填补这一重要空白。通过新颖的多模态方法和MMA机制，解决了多模态QFL中的主要瓶颈，提高了模型性能。

Method: 针对QFL设置，引入了多模态方法和MMA机制。使用量子纠缠实现中间融合，隔离未训练的量子电路以确保训练过程稳定。通过仿真实验证明方法的有效性。

Result: 通过仿真实验证明了提出的多模态QFL方法结合MMA机制在IID和非IID数据分布下比现有最先进方法分别提高了6.84%和7.25%的准确度。

Conclusion: 提出了首个针对量子联邦学习（QFL）的多模态方法，通过量子纠缠进行中间融合，提高了模型准确度。引入了Missing Modality Agnostic（MMA）机制，确保在多模态QFL中训练过程稳定，提高了模型性能。仿真结果显示，与现有最先进方法相比，在IID和非IID数据分布下，提出的多模态QFL方法结合MMA可以使准确度分别提高了6.84%和7.25%。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [8] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 本文对给予AI代理访问加密货币和智能合约可能带来的新风险进行了探讨，描述了这些新风险并呼吁加强技术研究以降低风险。


<details>
  <summary>Details</summary>
Motivation: 探讨给予AI代理访问加密货币和智能合约的增长兴趣，提出这可能导致新的AI伤害渠道。针对这一观点，呼吁加强对潜在风险的研究。

Method: 首先研究加密货币和智能合约的独特属性，探讨可能导致新风险的因素，然后详细描述这些新风险，并呼吁进行更多技术研究以预防和减轻这些风险。

Result: 通过分析加密货币和智能合约的特性，并描述新的风险渠道，加强了对AI代理与加密货币和智能合约潜在风险的认识。

Conclusion: 呼吁进行更多的技术研究，以防止和减轻AI代理与加密货币和智能合约产生风险，从而使AI代理具有加密货币和智能合约更加安全。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [9] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 本论文综述了Abductive Reasoning在认识论、科学和设计领域的讨论情况，分析了计算系统应用Abductive Reasoning的方式。研究发现现有的理论框架和计算系统并未有效解决生成创造性假设的问题，提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于解决Abductive Reasoning在不同领域中理解的差异问题，以及现有理论框架和计算系统在生成创造性假设方面的不足。研究旨在促进计算系统中创造性Abductive Reasoning的发展。

Method: 该论文通过文献综述和分析研究了Abductive Reasoning在认识论、科学和设计领域的讨论状况，并调查了计算系统如何应用Abductive Reasoning。研究采用分析方法，对现有的理论框架和计算系统进行评估，指出它们在生成创造性假设方面存在的不足。最后，提出了未来研究的方向。

Result: 研究发现当前的Abductive Reasoning理论框架和计算系统缺乏对创造性假设生成的有效方法。理论框架不能直接支持创造性Abductive假设的生成，而计算系统主要采用演绎形式的Abductive Reasoning。通过对计算系统进行拆解，为未来研究提供了具体方向。

Conclusion: 该论文总结了目前关于Abductive Reasoning在不同领域（认识论、科学和设计）的讨论情况，并分析了各种计算系统如何使用Abductive Reasoning。研究结果显示，现有的理论框架和计算系统对于生成创造性假设并不足够。理论框架没有提供直接的生成创造性Abductive假设的模型，计算系统主要实现了演绎形式的Abductive Reasoning。研究将Abductive计算系统分解为不同组件，并指出未来研究的具体方向，以推动计算系统中创造性Abductive Reasoning的发展。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [10] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 本文提出了面向工具使用代理的统一安全对齐框架，引入了三模态分类法，通过自定义设计的沙盒环境模拟实际工具执行，展示了安全对齐代理在安全威胁抵抗能力方面的显著改进。结果表明安全性和有效性可以同时得到优化，为自主LLM代理的可信部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 自主大型语言模型代理的出现引入了新的安全风险，需要超越传统对话误用的考虑。现有代理可以执行外部功能，因此容易受到用户发起的威胁和工具发起的威胁的影响。因此，本文的动机是应对这种新型威胁，提出统一的安全对齐框架，以保证模型在面对威胁时能够做出正确的决策。

Method: 本文采用了自定义设计的沙盒环境模拟实际工具执行，允许精细化奖励塑造。引入了三模态分类法，包括良性、恶意和敏感，为用户提示和工具响应进行定义，建立基于策略的决策模型。

Result: 通过广泛评估，展示了安全对齐代理在安全威胁抵抗能力方面的显著改进，同时在良性任务上保持了强大的效用。研究结果表明安全性和有效性可以同时得到优化。

Conclusion: 本文提出了面向工具使用代理的统一安全对齐框架，利用结构化推理和沙箱强化学习帮助模型处理用户发起的威胁和工具发起的威胁。通过在公共和自建基准测试上进行广泛评估，展示了安全对齐代理显著提高了抵抗安全威胁的能力，同时在良性任务上保持强大的效用。研究结果表明安全性和有效性可以同时优化，为自主LLM代理的可信部署奠定基础。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [11] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: Recent advancements in Multimodal Large Language Models have led to the development of M2-Reasoning-7B, which excels in general and spatial reasoning. The model integrates innovative data pipeline and dynamic multi-task training strategy to enhance reasoning abilities across 8 benchmarks, setting a new state-of-the-art in both domains.


<details>
  <summary>Details</summary>
Motivation: Existing Multimodal Large Language Models struggle with dynamic spatial interactions, crucial for real-world applications. The goal is to enhance reasoning abilities in both general and spatial domains.

Method: Integrates a novel data pipeline to generate high-quality data samples for cold-start fine-tuning and RLVR, and implements a dynamic multi-task training strategy with step-wise optimization to address conflicts between data and provide task-specific rewards for tailored incentive signals.

Result: The model M2-Reasoning-7B outperforms previous models, achieving state-of-the-art results in 8 benchmarks for general and spatial reasoning.

Conclusion: M2-Reasoning-7B sets a new state-of-the-art across 8 benchmarks, demonstrating superior performance in both general and spatial reasoning domains.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [12] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 本研究提出了一个框架，通过引入道德倡导者代理在多智能体LLM设置中，对系统描述的道德问题进行批判和提供意见，生成道德要求草案。该框架通过两个案例研究进行了评估，发现其有效捕捉了大部分研究人员在短时间内识别的道德要求，并引入了一些其他相关要求。然而，也指出了在生成道德要求方面的可靠性问题，强调了在这一敏感领域中需要人类反馈的必要性。研究认为这项工作可以促进道德在需求工程过程中的更广泛采用，最终导致更符合道德的产品。


<details>
  <summary>Details</summary>
Motivation: 将道德纳入需求引出过程对于创建符合道德的系统至关重要。传统的手动道德要求引出虽然有效，但需要来自多个利益相关者的多样化输入，由于时间和资源限制可能存在挑战。此外，在需求引出过程中通常将其优先级降低。本研究旨在解决这些问题，并提出了一个新的框架。

Method: 引入一个道德倡导者代理在多智能体LLM设置中，对系统描述的道德问题进行批判和提供意见，生成道德要求草案。通过两个不同背景的案例研究对提出的框架进行评估。

Result: 通过案例研究评估了提出的框架，发现其捕捉了大部分研究人员在30分钟访谈中识别的道德要求，并引入了一些其他相关要求。然而，也突出了在生成道德要求方面的可靠性问题，强调了在这一敏感领域中需要人类反馈的必要性。

Conclusion: 研究提出了一个在多智能体LLM环境中引入道德倡导者代理的框架，用于生成道德要求草案。该框架通过两个不同背景的案例研究进行了评估，展示了在30分钟访谈中识别的大部分道德要求，并引入了一些其他相关要求。然而，它也强调了在生成道德要求时存在的可靠性问题，强调了在这一敏感领域需要人类反馈的必要性。研究认为这项工作可以促进道德在需求工程过程中的更广泛采用，最终导致更符合道德的产品。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [13] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 本文探讨了对比解释的经典问题，比如“为什么是P而不是Q？”利用命题逻辑框架定义了这些问题，展示了其基本属性和计算复杂性分析。通过答案集编程实现了问题的解决方案，并呈现了实际示例。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于探索涉及对比解释的经典问题，并提供了一种计算方法和实现这些问题的方式。通过比较P和Q的原因，探索了对比解释的新视角。

Method: 研究了对比解释的几个经典问题，并在命题逻辑框架下定义了这些问题。展示了计算对问题的基本属性和计算复杂性进行了分析。通过答案集编程实现了对问题的解决方案，并呈现了实际示例。

Result: 通过命题逻辑的定义，论文提供了对比解释的框架，并展示了捕捉现有对比解释版本的能力以及问题的计算复杂性分析。此外，通过答案集编程实现了对问题的解决方案，并通过示例展示了其实际应用。

Conclusion: 该论文定义了几个涉及对比解释的经典问题，回答了“为什么是P而不是Q？”这种形式的问题。他们计算了P和Q的原因，并明确比较了它们之间的差异。在命题逻辑设置下研究了定义的基本属性，并展示了该框架捕捉到了现有文献中存在的对比解释的基数最小版本。此外，对问题的计算复杂性进行了广泛分析。还使用答案集编程为CNF公式实现了这些问题，并展示了几个示例来演示它们在实践中的运作。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [14] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 本文提出了一种双层框架，通过高级任务抽象和低级逻辑生成将自然语言映射到逻辑表示。使用双层优化方法优化框架，在多个现实推理基准测试中，精度明显优于现有基线，精度提高达到40%。双层设计增强了透明性和错误可追溯性，为与大语言模型进行可信赖和系统推理迈出了有益的一步。


<details>
  <summary>Details</summary>
Motivation: 在人工智能领域中，结构化推理仍然是一个核心挑战，需要建立自然语言表达与形式逻辑表示之间的桥梁。该研究的动机是提出一种框架，以更好地将自然语言映射到逻辑表示，支持多领域推理，并在精度、透明性和错误追溯性上优于现有方法。

Method: 提出了一个双层框架，包括高级任务抽象和低级逻辑生成阶段，通过使用大语言模型解析自然语言查询并生成结构化表示，进而生成符号工作流或可执行推理程序。优化框架采用双层优化方法，同时完善高级抽象和低级逻辑生成阶段。

Result: 通过实验在多个现实推理基准测试中显示，该方法在精度方面大幅超越了现有基线，精度提高达到40%。双层设计增强了透明性和错误可追溯性，为与大语言模型进行可信赖和系统推理迈出了有益的一步。

Conclusion: 提出了一种新颖的双层框架，通过高级任务抽象和低级逻辑生成将自然语言映射到逻辑表示，支持模块化推理，强制显式约束，并在数学问题解决、问答和逻辑推理等领域具有泛化能力。使用双层优化方法优化框架，实验结果显示在多个现实推理基准测试中，精度明显优于现有基线，精度提高达到40%。双层设计增强了透明性和错误可追溯性，为与大语言模型进行可信赖和系统推理迈出了有益的一步。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [15] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 本研究提出了一种框架，耦合多粒度稀疏激活医学概念和分层知识图，通过多种算法实现精确概念激活。实验证明在罕见疾病诊断方面取得了显著提升，专家评估证实了信息质量和推理能力的改进，有助于缩短罕见疾病患者的诊断时间。


<details>
  <summary>Details</summary>
Motivation: 医疗大型语言模型在医疗保健领域取得了进展，但罕见疾病诊断仍受到知识表示深度不足、概念理解有限和临床推理受限的影响。为了解决这一问题，提出了本研究的方法。

Method: 耦合多粒度稀疏激活医学概念和分层知识图的框架，四种匹配算法、多样性控制和五级备用策略实现精确概念激活。三层知识图（分类、临床特征、实例）提供结构化、最新的上下文。

Result: 在BioASQ罕见疾病QA数据集上，实验证明该方法取得了可观的提升，包括BLEU增益为0.09，ROUGE增益为0.05，准确度增益为0.12，且准确度接近临床阈值。专家评估确认了信息质量、推理和专业表达等方面的改进。

Conclusion: 提出了一种耦合多粒度稀疏激活医学概念和分层知识图的框架，通过四种互补的匹配算法、多样性控制和五级备用策略实现精确概念激活。三层知识图（分类、临床特征、实例）提供结构化、最新的上下文。实验证明，在BioASQ罕见疾病QA数据集上，BLEU增益为0.09，ROUGE增益为0.05，准确度增益为0.12，准确度达到0.89，接近0.90的临床阈值。专家评估证实了信息质量、推理和专业表达方面的改进，表明该方法缩短了罕见疾病患者的“诊断奥德赛”。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [16] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 该论文介绍了一种利用大型多模态模型(LMM)进行地理参考的新方法。采用了基于网格的自适应自回归模型，在零样本设定下取得了令人瞩目的成果($	ilda1$ km平均距离误差)。实验结果显示该方法优越于传统的单模态地理参考方法和现有工具。论文还提出了将该方法整合到地理参考工作流程的实用框架。


<details>
  <summary>Details</summary>
Motivation: 论文指出自然历史收藏中的几百万个生物样本记录未进行地理编码，地理编码与样本采集相关的复杂地点描述是收集机构难以处理的劳动密集任务。现有的自动化方法未利用地图作为地理编码复杂关系的基本工具。因此，作者受到这一挑战的启发，提出利用最近大型多模态模型的多模态能力，通过新颖的方法解决这一问题。

Method: 论文采用了基于网格的方法，使用自回归模型在零样本设定下进行自适应，以实现地理参考复杂地点描述的任务。

Result: 实验结果显示，该方法在处理地理参考任务上表现出色，平均距离误差约为1公里。相比于使用大型语言模型和现有地理参考工具进行单模态地理参考，该方法显著优越。

Conclusion: 该论文介绍了一种新颖的方法，利用最近大型多模态模型(LMM)的多模态能力，实现对复杂地点描述的自动地理参考。实验结果显示该方法在处理地理参考任务上取得了显著的成果，与单模态地理参考方法和现有地理参考工具相比表现出色。论文还讨论了实验结果，并提出了将该方法融入地理参考工作流程的实用框架。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [17] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 本文提出了一种通过查询重写框架和多个大型语言模型知识融合的方法，成功将文本指令转换为更适合语音合成的分布，提高数据可用性并展现在复杂任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管现代文本转语音(TTS)模型已经实现了接近人类水平的合成质量，但由于TTS模型训练数据分布的限制，将分布外文本指令适当转换为语音仍具有挑战性。通过人工合成语音指令数据集的高成本，提出使用语音合成构建大规模语音指令数据集作为替代方案。

Method: 提出了查询重写框架，利用多个大型语言模型进行知识融合，使用多个代理进行注释和验证合成语音，以构建高质量的语音指令数据集。通过实验证明，该方法能够通过零样本重写，将文本指令转换为更适合用于语音合成的分布。

Result: 实验证明，该方法可以通过零样本重写，将文本指令转换为更适合用于语音合成的分布，将数据可用性从72%提高到93%。在需要复杂知识和上下文相关能力的重写任务中表现出独特优势。

Conclusion: 本文提出了一种查询重写框架，利用多个大型语言模型进行知识融合，通过零样本重写，将文本指令转换为更适合用于语音合成的分布，从而将数据可用性从72%提高到93%，在复杂知识和上下文相关能力方面具有独特优势。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [18] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 本研究比较了多代理系统和双代理系统在工程设计中的效果。多代理系统提高了设计细节和任务管理，但编码兼容性较差。深度推理的大型语言模型提高了任务完成率，但需求和编码方面存在差距。


<details>
  <summary>Details</summary>
Motivation: 针对早期工程设计中的复杂迭代推理过程，尝试解决现有大型语言模型工作流在任务连续性和可执行模型生成方面的困难。

Method: 引入了Design-State Graph (DSG)，利用九种角色的多代理系统迭代构建和完善DSG，与2AS系统进行比较。运行了60个实验，对多种指标进行评估，如JSON有效性、需求覆盖、代码兼容性、工作流程完成率等。

Result: 多代理系统在设计细节和任务管理方面表现优异，但在编码兼容性上表现不如双代理系统。深度推理的大型语言模型提高了任务完成率，但存在需求和编码忠实度方面的差距。

Conclusion: 多代理系统在管理需求提取、功能分解和模拟器代码生成方面比简单的双代理系统更有效。多代理系统提高了设计细节，但对编码方面要求较低。深度推理的大型语言模型提高了完成率，但在编码方面存在需求和忠实度差距。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [19] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: Leanabell-Prover-V2是一个7B大型语言模型，通过与Lean 4验证器的反馈结合使用强化学习（RL）来生成正式定理证明。实验结果表明，在MiniF2F测试集上，它通过不同证明模型分别提高了3.2%和2.0%的性能。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于通过Leanabell-Prover-V2提高LLM的性能，使其能够自我意识地纠正错误并生成正式定理证明。

Method: Leanabell-Prover-V2通过升级强化学习（RL）结合Lean 4验证器的反馈来生成正式定理证明。它直接优化LLM的推理轨迹，实现多轮验证器交互，采用反馈令牌掩盖进行稳定的RL训练并采用简单的奖励策略。

Result: 实验结果表明，Leanabell-Prover-V2在MiniF2F测试集上分别通过Kimina-Prover-Preview-Distill-7B和DeepSeek-Prover-V2-7B提高了3.2%和2.0%，展示了其性能提升。

Conclusion: Leanabell-Prover-V2 是一个7B大型语言模型，可以在Lean 4中生成正式定理证明，具有验证器集成的Long Chain-of-Thoughts（CoT）。通过强调Leanabell-Prover-V1的先前工作，我们选择进一步后训练现有强大的证明模型以提高性能。在V2版本中，我们主要通过Lean 4验证器提供的反馈来升级强化学习（RL）。关键的是，验证器的反馈，例如指示成功或详细说明特定错误，使LLM能够“自我意识”地了解其推理过程的正确性，并学会自我修正错误。Leanabell-Prover-V2直接优化LLM的推理轨迹，具有多轮验证器交互以及反馈令牌掩盖以进行稳定的RL训练和简单的奖励策略。实验证明，Leanabell-Prover-V2在MiniF2F测试集上通过Kimina-Prover-Preview-Distill-7B提高了3.2%（pass@128），通过DeepSeek-Prover-V2-7B提高了2.0%（pass@128）。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [20] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: 研究提出了一种名为INoT的新的AI Agent推理框架，通过在提示中设计新的LLM-Read代码，使LLM执行程序化对话推理过程。在六项基准测试上的实验证实了INoT的有效性，平均性能提高了7.95p并超过了基线，同时标记成本低于基线上最佳方法的58.3p。还证明了INoT在图像解释和推理中的多功能性。


<details>
  <summary>Details</summary>
Motivation: AI Agents依赖大型语言模型（LLMs）和多模式LLMs（MLLMs）执行文本和图像任务中的解释和推理，但受限于LLM在理解自然语言方面的固有限制以及迭代推理过程可能带来的大量推理成本。因此，需要提出一种新的框架来解决这些问题。

Method: 设计了一个新的AI Agent推理框架，INoT，其中包括LLM-Read代码，以执行程序化对话推理过程。通过实验在六项基准测试上验证了INoT的有效性，并探讨了其在图像解释和推理中的适用性。

Result: 提出的INoT框架在实验中显示了其在改善AI Agent性能方面的有效性，并降低了标记成本。此外，INoT还展示了在图像解释和推理中的多功能性。

Conclusion: 提出了一种新的AI Agent推理框架，名为“思维内省（INoT）”，通过在提示中设计新的LLM-Read代码，实现LLM执行程序化对话推理过程。通过在六项基准测试上的实验验证了INoT的有效性，在性能方面平均提高了7.95p，并超过了基线。此外，INoT的标记成本平均低于基线上最佳性能方法的58.3p。通过验证实验展示了INoT在图像解释和推理中的多功能性。

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [21] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: elsciRL是一个Python库，用于在强化学习问题上应用语言解决方案。他们扩展了Language Adapter with Self-Completing Instruction框架，结合LLMs，并提供新的GUI。实证结果表明生成的指令可以改善强化学习代理的性能，该工作旨在加速评估语言解决方案的应用。


<details>
  <summary>Details</summary>
Motivation: 加速评估语言解决方案在奖励驱动环境中的应用，为科学发现提供新机会。

Method: 扩展了Language Adapter with Self-Completing Instruction框架，结合LLMs，并提供新颖的GUI进行文本输入，生成指令并自我完成，以改善强化学习代理的性能。

Result: 通过实证结果表明，生成的指令可以提高强化学习代理的性能。

Conclusion: 介绍了elsciRL，一个开源的Python库，用于在强化学习问题上应用语言解决方案。通过将Language Adapter with Self-Completing Instruction框架与LLMs相结合，展示了软件的潜力。他们提供了一种新颖的GUI，允许用户提供文本输入，以生成指令并进行自我完成。实证结果表明，这些指令可以提高强化学习代理的性能。该工作旨在加速评估语言解决方案在基于奖励的环境中的应用，从而为科学发现提供新机会。

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [22] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: The paper discusses the importance of modeling and optimization techniques in developing innovative systems architectures, particularly for system-of-systems. It highlights the use of dedicated approaches like physics-based simulations and the emergence of surrogate-based optimization algorithms, such as Bayesian optimization with Gaussian process models, to tackle challenges in exploring novel architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to reduce the computational complexity of targeted applications when exploring novel architectures. It aims to overcome challenges faced by optimization algorithms, including increased evaluation costs and potential failures.

Method: The paper focuses on modeling and optimization techniques for developing innovative systems architectures. It emphasizes the use of efficient dedicated approaches, such as physics-based simulations, for system-of-systems.

Result: The use of surrogate-based optimization algorithms, particularly Bayesian optimization with Gaussian process models, has shown promise in addressing the challenges associated with exploring novel architectures for system-of-systems.

Conclusion: Surrogate-based optimization algorithms, such as Bayesian optimization using Gaussian process models, have emerged to address challenges in exploring novel architectures with dedicated approaches for system-of-systems.

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>


### [23] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: The paper explores whether AI can exhibit creativity by reviewing contemporary research in psychology, cognitive neuroscience, and philosophy of creativity. It analyzes historical perspectives, different definitions of creativity, and responses from naturalism and cognitive neuroscience.


<details>
  <summary>Details</summary>
Motivation: Investigate the reinterpretation of creativity philosophy in the digital age and the potential of AI to exhibit creativity. Address the central question regarding AI's creative capabilities.

Method: Review of contemporary research in psychology, cognitive neuroscience, and philosophy of creativity. Analysis of historical perspectives, definitions of creativity, and responses from naturalism and cognitive neuroscience.

Result: Reinterpretation of creativity philosophy in the digital age. Exploration of AI's potential for creativity. Analysis of historical perspectives and definitions of creativity.

Conclusion: AI can exhibit creativity with the advancement of technology and insights from psychology and cognitive neuroscience. The paper examines historical perspectives, definitions of creativity, and responses from naturalism and cognitive neuroscience.

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [24] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 该论文介绍了TableReasoner框架，结合大型语言模型和编程，解决了表格问答任务中的挑战，并在SemEval-2025 Task 8中获得了首位。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界的表格数据特性，如大规模、不完整的列语义和实体模糊性，表格问答任务面临挑战，因此需要解决这些问题。

Method: 采用大型语言模型和编程相结合的方式，设计了TableReasoner框架，提出了多步骤模式链接计划以获得专注的表格模式，并将推理工作流程整合到迭代思维架构中。

Result: 在SemEval-2025 Task 8的两个子任务中取得了第一名的成绩。

Conclusion: 该论文提出了基于大型语言模型和基于编程的表格推理框架TableReasoner，用于解决表格问答任务面临的挑战，取得了SemEval-2025 Task 8两个子任务的第一名。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [25] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 该论文介绍了如何利用动态Stackelberg博弈框架模拟大型语言模型的破解攻击和防御互动，提出了一种名为“紫色Agent”的AI解决方案，通过RRT集成对抗性探索和防御策略，积极干预潜在的攻击轨迹。该方法为分析对抗动态和减轻破解风险提供了基础和原则性方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在关键应用中的部署越来越多，破解挑战已经成为一个重要关注点。作者希望通过提出动态Stackelberg博弈框架和紫色Agent解决方案来应对这一挑战，为分析对抗动态和减轻破解风险提供方法和基础。

Method: 将动态Stackelberg博弈框架应用于模拟攻击者和防御者之间的互动，提出“紫色Agent”解决方案，利用RRT集成对抗性探索和防御策略，以积极干预潜在的攻击轨迹。

Result: 通过紫色Agent解决方案，在大型语言模型的破解攻击和防御方面取得了积极成果，为减轻破解风险提供了原则性方法。

Conclusion: 该论文提出了一个动态Stackelberg博弈框架，用于模拟大型语言模型的破解攻击和防御之间的互动。他们提出了一种名为“紫色Agent”的创新AI解决方案，通过快速探索随机树（RRT）来集成对抗性探索和防御策略，从而积极干预潜在的攻击轨迹，防止有害输出。这种方法为分析对抗动态提供了一个原则性方法，并为减轻破解风险奠定了基础。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [26] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: The paper presents the LLM-Nash framework, which accounts for bounded rationality in decision-making processes, deviating from classical Nash outcomes. It explores cognitive constraints and offers a new perspective on strategic interactions in LLM-enabled systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical games assuming utility-maximizing agents with full rationality and to explore decision-making in Large Language Models (LLMs) by considering cognitive constraints and reasoning processes explicitly.

Method: Introducing the LLM-Nash framework, modeling bounded rationality, defining equilibrium over the prompt space, and studying cognitive constraints, mindset expressiveness, and epistemic learning through illustrative examples.

Result: The paper demonstrates the potential of the LLM-Nash framework to offer insights into strategic interactions in LLM-enabled systems and how reasoning equilibria can differ from classical Nash outcomes.

Conclusion: The paper introduces the LLM-Nash framework, which captures bounded rationality in decision-making by modeling the reasoning process explicitly. It shows how reasoning equilibria can diverge from classical Nash outcomes, providing a new foundation for strategic interaction in LLM-enabled systems.

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [27] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 本研究通过比较两种代理模型，研究了好奇心和能力在探索中的作用。结果显示优先考虑好奇心和能力可以改善探索，内部表征的演化影响好奇心和能力之间的平衡。研究揭示了探索的关键概念，即在追求未知和可控性之间取得平衡，为认知理论和强化学习提供了启示。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于探讨智能代理如何在探索世界的同时保持对环境的控制，如何平衡好奇心（追求知识）和能力（掌握和控制环境）之间的关系。通过将认知理论和强化学习相结合，研究内部表征如何调节好奇心和能力之间的权衡，揭示了探索中追求未知和可控性之间的平衡。

Method: 本研究比较了两种基于模型的代理（Tabular和Dreamer），通过手工制作状态抽象或学习内部世界模型，分析好奇心和能力在探索中的角色。研究通过实验证明了优先考虑好奇心和能力可以改善探索，并揭示了探索和表征学习之间的双向交互作用。

Result: 研究结果表明，优先考虑好奇心和能力可以改善探索过程，内部表征的演化在好奇心和能力之间有着相互影响。通过比较基于Tabular和Dreamer的代理模型，揭示了好奇心和能力在探索中的不同模式，并且探索和表征学习之间存在双向交互作用。

Conclusion: 本研究通过比较基于手工状态抽象和学习内部世界模型的两种模型，探讨了好奇心和能力在探索过程中的作用。结果显示，优先考虑好奇心和能力可以改善探索过程，并且内部表征的演化在好奇心和能力之间有着相互影响。研究结果形式化了自适应探索的核心概念，即在追求未知和可控性之间取得平衡，为认知理论和有效的强化学习提供了启示。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [28] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 本文提出了一种参数化的逻辑基础方法家族，泛化了经典的反向链接方法，可以控制推理器的表达能力和可扩展性之间的权衡。实验结果显示，逻辑基础准则的选择通常与NeSy方法本身一样重要。


<details>
  <summary>Details</summary>
Motivation: 许多神经符号方法依赖机器学习处理输入实体，同时依赖基于一阶逻辑的推理器来表示和处理实体之间更复杂的关系。逻辑基础的过程对这些方法起着关键作用，确定逻辑规则的相关替换，使用一组实体。一些NeSy方法使用了所有可能替换的详尽推导，保留了逻辑知识的完整表达能力。但这导致了需要考虑的地面公式数量的组合爆炸，因此严重限制了可扩展性。其他方法依赖基于启发式的选择推导，通常在计算上更高效，但缺乏理由，并且无法保证保留提供给推理器和返回的信息。

Method: 本文基于多跳符号推理的启发，提出了一种参数化的逻辑基础方法家族，泛化了经典的反向链接。通过该家族内的不同选择，可以得到常用的逻辑基础方法，并控制推理器的表达能力和可扩展性之间的权衡。

Result: 实验结果表明，逻辑基础准则的选择通常同样重要如NeSy方法本身。

Conclusion: 本文提出了一种参数化的逻辑基础方法家族，泛化了经典的反向链接方法，可以控制推理器的表达能力和可扩展性之间的权衡。实验结果显示，逻辑基础准则的选择通常与NeSy方法本身一样重要。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [29] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 该论文解决了现有QFL框架对多模态系统的局限性，提出了一种用于QFL的新型多模态方法，通过引入量子纠缠和Missing Modality Agnostic（MMA）机制，实现了更好的准确率表现。


<details>
  <summary>Details</summary>
Motivation: 现有的QFL框架主要专注于单一模态系统，忽略了现实任务中涉及多模态的情况，本论文旨在填补这一重要空白，提出适用于QFL设置的多模态方法，并解决了多模态QFL中的主要瓶颈。

Method: 论文通过引入量子纠缠和Missing Modality Agnostic（MMA）机制，实现了多模态量子联邦学习，并进行了仿真实验验证方法的有效性。

Result: 提出的多模态QFL方法结合了量子纠缠和MMA机制，在IID和非IID数据分布下准确率分别提升了6.84%和7.25%，优于现有方法。

Conclusion: 该论文提出了一种针对量子联邦学习的新型多模态方法，使用量子纠缠进行中间融合，解决了传统QFL框架对单模态系统的局限性，同时引入了Missing Modality Agnostic（MMA）机制以确保稳定训练。仿真结果显示，该方法在IID和非IID数据分布下相比现有方法分别提升了6.84%和7.25%的准确率。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [30] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 本文探讨将AI代理与加密货币和智能合约结合可能带来的新威胁，呼吁进行更多技术研究以确保这种结合更加安全。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理访问加密货币和智能合约的兴趣增加，探讨这种结合可能会带来新的AI伤害。

Method: 首先分析加密货币和智能合约的独特特性，指出可能导致新危害的路径，然后详细描述每种新危害。

Result: 发现AI代理与加密货币和智能合约结合可能导致新的危害，并提出需要更多技术研究来预防和减轻这些潜在的伤害。

Conclusion: 呼吁进行更多的技术研究，以预防和减轻AI代理与加密货币和智能合约相结合可能带来的新危害，从而使赋予AI代理在加密货币和智能合约中的功能更安全。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [31] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 该论文回顾了适缘推理在认识论、科学和设计中的讨论，分析了计算系统如何使用适缘推理，发现现有理论和计算系统未能满足生成创造性假设的需求，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 作者发现当前的适缘推理研究在创造性假设生成方面存在挑战，认为有必要改进理论和计算系统。

Method: 对适缘推理在认识论、科学和设计中的讨论进行回顾，分析计算系统如何使用适缘推理。将适缘计算系统分解为组件，识别未来研究方向。

Result: 研究表明目前的理论和计算系统未能充分解决生成创造性适缘假设的问题。

Conclusion: 该论文指出当前的理论和计算系统在创造性适缘推理方面存在不足，需要进一步研究。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [32] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 该论文提出了第一个针对工具使用代理的统一安全对齐框架，通过结构化推理和沙箱强化学习处理用户和工具发起的威胁。引入三模态分类法，定义基于策略驱动的决策模型，使用自定义设计的沙盒环境模拟真实世界的工具执行并允许细粒度奖励塑造。广泛评估结果表明，安全对齐代理显著提高了抵抗安全威胁的能力，同时在良性任务上保持强大的效用，为自主LLM代理的可信赖部署提供基础。


<details>
  <summary>Details</summary>
Motivation: 自主LLM代理的出现引入了新的安全风险，需要处理用户和工具发起的威胁，传统的对话误用已经无法涵盖这些风险。作者提出了统一的安全对齐框架，旨在通过结构化推理和沙箱强化学习处理这些威胁，为自主LLM代理的可信赖部署奠定基础。

Method: 论文提出了一个统一的安全对齐框架，采用结构化推理和沙箱强化学习来处理用户和工具发起的威胁。引入了三模态分类法，定义了基于策略驱动的决策模型，使用自定义设计的沙盒环境模拟真实世界的工具执行并允许细粒度奖励塑造。通过对公共和自建基准的广泛评估，包括Agent SafetyBench、InjecAgent和BFCL，展示了安全对齐代理显著提高了抵抗安全威胁的能力同时保持在良性任务上的强大效用。

Result: 通过广泛评估，包括Agent SafetyBench、InjecAgent和BFCL等基准，展示了安全对齐代理显著提高了抵抗安全威胁的能力同时保持在良性任务上的强大效用。结果表明安全性和效用可以共同优化，为可信赖部署自主LLM代理奠定了基础。

Conclusion: 该论文提出了针对工具使用代理的统一安全对齐框架，通过结构化推理和沙箱强化学习来处理用户和工具发起的威胁，通过对公共和自建基准进行广泛评估，展示了安全对齐代理显著提高了抵抗安全威胁的能力，同时在良性任务上保持强大的效用。结果表明，安全性和效用可以共同优化，为可信赖部署自主LLM代理奠定了基础。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [33] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: M2-Reasoning-7B model is introduced to address the gap in reasoning abilities of Multimodal Large Language Models. It combines curated data samples and advanced training strategies to achieve state-of-the-art performance in both general and spatial reasoning domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical gap in current Multimodal Large Language Models (MLLMs) related to dynamic spatial interactions, which are crucial for real-world applications.

Method: Introduces M2-Reasoning-7B model designed for general and spatial reasoning by integrating a novel data pipeline and dynamic multi-task training strategy with step-wise optimization.

Result: The result includes the development of 294.2K high-quality data samples, out of which 168K are for cold-start fine-tuning and 126.2K for Reinforcement Learning with Verifiable Rewards (RLVR). The model outperforms existing models and achieves state-of-the-art performance across 8 benchmarks.

Conclusion: M2-Reasoning-7B achieves state-of-the-art performance in both general and spatial reasoning domains across 8 benchmarks, showcasing superior reasoning abilities.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [34] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 本研究提出了一个框架，在多智能体LLM设置中引入伦理倡导代理人生成伦理要求草案。通过两个案例研究评估，该框架有效捕捉大部分伦理要求，但也存在可靠性问题，需要人类反馈。最终目标是促进更符合伦理的产品设计。


<details>
  <summary>Details</summary>
Motivation: 将伦理纳入需求引出过程对于创建合乎伦理的系统至关重要，然而手动引出伦理要求需要多方利益相关者的多样输入，受限于时间和资源约束，同时在需求引出过程中常被较低的优先级对待。因此，本研究旨在解决这一问题，提出一个框架来通过引入伦理倡导代理人来生成伦理要求草案。

Method: 该研究提出了引入伦理倡导代理人在多智能体LLM设置中生成伦理要求草案的框架，并通过两个案例研究评估了该框架的效果。

Result: 根据两个不同背景的案例研究评估，框架能够捕捉大部分伦理要求并引入额外的相关要求，同时也凸显了可靠性问题，强调了人类反馈的必要性。

Conclusion: 该研究提出了一个在多智能体LLM设置中引入伦理倡导代理人的框架，用于生成伦理要求草案，通过两个不同背景的案例研究对该框架进行评估，展示了它可以捕捉研究人员在30分钟采访中确定的大部分伦理要求，并引入了几个额外的相关要求。然而，该框架也凸显了在生成伦理要求方面的可靠性问题，强调了在这个敏感领域需要人类反馈的必要性。最终，这项工作有助于促进伦理在需求工程过程中的广泛应用，从而最终实现更符合伦理的产品。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [35] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 该论文定义了多个经典问题，涉及对比解释中的“为什么是P而不是Q？”问题，包括计算P和Q的原因并比较它们的差异。通过命题逻辑设置，实现了对CNF公式的问题，分析了计算复杂性，并提供了实际示例。研究表明框架捕捉了现有文献中的最小基数版本。


<details>
  <summary>Details</summary>
Motivation: 探索“为什么是P而不是Q？”问题的多个经典问题，研究其基本属性，并通过命题逻辑设置刻画已有对比解释的最小基数版本。还对问题的计算复杂性进行了分析，并展示了实际应用示例。

Method: 在命题逻辑设置中定义了多个与对比解释相关的经典问题，计算P和Q的原因并明确比较它们的差异。实现了针对CNF公式的问题，运用答案集编程。

Result: 研究表明，论文框架捕捉到了现有对比解释文献中的最小基数版本。此外，对问题的计算复杂性进行了广泛分析，并利用答案集编程实现了针对CNF公式的问题。

Conclusion: 该论文定义了与对比解释相关的多个经典问题，探讨了问题形式为“为什么是P而不是Q？”的问题。研究了在命题逻辑设置中对定义的基本属性。展示了该框架捕捉到了文献中现有对比解释的最小基数版本。此外，对这些问题的计算复杂性进行了广泛分析。利用答案集编程实现了针对CNF公式的问题，并提供了几个示例来演示实际应用情况。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [36] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 本论文提出了一种新颖的双层框架，通过将语言映射到逻辑，实现了结构化推理。实验结果显示，该方法在多个实际推理基准测试中显著优于现有基线，准确度提高了40%。双层设计增强了透明度和错误可追溯性，为实现可信度高和系统性推理迈出了重要一步。


<details>
  <summary>Details</summary>
Motivation: 本论文的动机在于解决人工智能中结构化推理的核心挑战，即如何将非结构化的语言表达与形式化逻辑表示之间的鸿沟。通过提出双层框架，旨在实现从语言到逻辑的映射，为准确、可解释的决策制定提供新的方法。

Method: 该论文提出的方法是使用双层框架，分为高级任务抽象和低级逻辑生成两个阶段的过程。在上层，大型语言模型(LLM)将自然语言查询解析为中间结构化表示，指定问题类型、目标、决策变量和符号约束。在下层，LLM使用这些表示来生成符号工作流或可执行的推理程序，以进行准确且可解释的决策制定。该框架支持模块化推理，强制执行显式约束，并在数学问题解决、问答和逻辑推理等不同领域中具有泛化能力。同时，利用端到端的双层优化方法进一步优化了该框架，联合完善了高级抽象和低级逻辑生成阶段。

Result: 实验结果表明，作者的方法在多个实际推理基准测试中取得了显著的准确性优势，准确度提高达到40%。双层设计加强了透明度和错误可追溯性，为LLM的可信度高和系统性推理迈出了重要一步。

Conclusion: 该论文提出了一种新颖的双层框架，通过将语言映射到逻辑，实现了结构化推理。在多个实际的推理基准测试中，该方法在准确性方面明显优于现有基线，准确度提高达到40%。双层设计增强了透明度和错误可追溯性，为与LLM的可信度高和系统性推理迈出了重要一步。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [37] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 本文提出了一种框架，结合了多粒度稀疏激活的医学概念和分层知识图，通过四种互补匹配算法、多样性控制和五级回退策略实现精确的概念激活。在实验中取得了显著性能提升，专家评估证实改进，并缩短了罕见疾病患者的“诊断奥德赛”。


<details>
  <summary>Details</summary>
Motivation: 医学大型语言模型在医疗保健领域取得了进展，但罕见疾病的诊断仍受制于知识表示深度不足、概念理解有限和临床推理受限的问题。因此，有必要提出一种能够克服这些问题的方法。

Method: 提出的框架结合了多粒度稀疏激活的医学概念和分层知识图，并使用四种互补匹配算法、多样性控制和五级回退策略，构建了三层知识图（分类、临床特征、实例）以提供结构化、最新的上下文。在实验中，提出的方法在罕见疾病QA数据集上取得了显著的性能提升。

Result: 实验结果表明，该方法在罕见疾病诊断方面取得了显著的提升，其中BLEU增益为0.09，ROUGE增益为0.05，准确度增益为0.12。达到了接近临床阈值的峰值准确度，并得到专家评估的确认。

Conclusion: 本文提出的框架结合了多粒度稀疏激活的医学概念和分层知识图，通过四种互补匹配算法、多样性控制和五级回退策略实现精确的概念激活。在BioASQ罕见疾病QA数据集上的实验显示，BLEU增益为0.09，ROUGE增益为0.05，准确度增益为0.12，峰值准确度达到了0.89，接近0.90的临床阈值。专家评估证实信息质量、推理和专业表达的改进，表明本方法缩短了罕见疾病患者的“诊断奥德赛”。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [38] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本论文提出了一种利用大型多模态模型（LMM）的新方法来地理参考生物样本记录。实验结果显示，该方法在小规模手动注释的数据集上取得了令人印象深刻的成果，平均距离误差约为1公里，优于传统的地理参考方法。该研究探讨了LMM理解地图的能力，并提出了将此方法整合到地理参考工作流程中的实用框架。


<details>
  <summary>Details</summary>
Motivation: 自然历史收藏品中存档的数百万条生物样本记录在过去几个世纪中收集而来，但这些记录都没有地理参考信息。处理与这些收集样本相关的复杂地点描述是一个高度劳动密集型的任务，收藏机构很难应对。现有的自动化方法没有利用到地理参考复杂关系的重要工具——地图。本研究的动机在于填补这一空白，探索如何利用最近大型多模态模型的能力来解决这一问题。

Method: 通过利用最近的大型多模态模型（LMM）的多模态能力，提出了一种新方法来地理参考历史收集的未地理参考的生物样本记录。该方法利用自回归模型，并采用基于网格的方法在零-shot设置下对这些模型进行调整，使模型能够在视觉上理解读取的位置描述与空间关系。实验结果基于一个小规模手动注释的数据集，展示了该方法在地理参考任务中的优异表现。

Result: 通过在小规模手动注释的数据集上进行实验，展示了新方法相比于传统的地理参考方法取得了令人印象深刻的成果。在本研究中，实验结果表明该方法的平均距离误差约为1公里，优于使用大语言模型和现有地理参考工具的单模态方法。

Conclusion: 提出了一种利用大型多模态模型（LMM）的新方法来地理参考生物样本记录的初步实验结果，结果显示相比于使用大语言模型和现有地理参考工具的单模态地理参考方法，在小规模手动注释的数据集上取得了令人印象深刻的成果（约1公里的平均距离误差）。此外，探讨了实验证明LMM能够理解精细地图的能力，并提出了一个实用框架来将这种方法整合到地理参考工作流程中。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [39] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 提出了一种查询重写框架，利用多LLM知识融合，通过多代理人验证生成的语音指令，构建高质量语音指令数据集。实验证明，该方法能够将文本指令转化为适合TTS模型的分布，将数据可用性提升至93%，在重写任务中展现独特优势。


<details>
  <summary>Details</summary>
Motivation: 鉴于人工收集和注释语音指令数据集的高成本，提出了使用语音合成构建大规模语音指令数据集的平衡而稳固的替代方案。现代TTS模型虽然达到了接近人类水平的合成质量，但由于TTS模型训练数据分布的限制，将分布外文本指令适当转换为语音仍然具有挑战性。

Method: 提出了一个基于查询重写框架的方法，利用多个LLM进行知识融合，并通过多个代理人注释和验证生成的语音指令，以构建高质量语音指令数据集。通过零样本重写，将文本指令转化为适合TTS模型的分布，实现数据可用性的提高。

Result: 实验证明，通过多LLM知识融合的查询重写框架，能够将文本指令转化为更适合TTS模型的分布，将数据可用性提升至93%，并在需要复杂知识和上下文相关能力的重写任务中展示了独特优势。

Conclusion: 提出了一种基于多LLM知识融合的查询重写框架，通过多个代理人注释和验证综合语音，从而构建高质量的语音指令数据集，实现了文本指令向适合TTS模型的分布转化。实验证明，这种方法能够通过零样本重写，将文本指令转化为更适合语音合成的分布，将数据可用性从72%提高到93%，在需要复杂知识和上下文相关能力的重写任务中展示了独特优势。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [40] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 本研究评估了MAS和2AS系统在工程设计中的表现，发现MAS在设计细节方面更好，但在代码兼容性方面不如2AS。推理提炼的LLM模型提高了完成率，但在编码需求和准确性方面有待改进。


<details>
  <summary>Details</summary>
Motivation: 早期工程设计涉及复杂的迭代推理，现有的大型语言模型（LLM）工作流在维持任务连续性和生成可执行模型方面存在困难。因此，研究动机在于探讨采用MAS系统是否能更好地管理工程设计过程。

Method: 研究采用九种角色的MAS和2AS系统进行实验，比较它们在生成可执行模型方面的表现。使用Design-State Graph（DSG）作为表示来捆绑需求、物理实体和基于Python的物理模型。通过运行60个实验，评估了JSON有效性、需求覆盖率、代码兼容性等指标。

Result: 研究结果显示，MAS在设计细节方面表现更好，但在代码兼容性方面表现不如2AS。推理提炼的LLM模型提高了完成率，但在编码需求和准确性方面仍存在差距。

Conclusion: 在本研究中，作者评估了一个结构化多Agent系统（MAS）是否能比简单的双Agent系统（2AS）更有效地管理需求提取、功能分解和仿真器代码生成。研究结果显示，MAS在设计细节方面表现更好，但在代码兼容性方面表现不如2AS。推理提炼的LLM模型提高了完成率，但在编码需求和准确性方面仍存在差距。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [41] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: Leanabell-Prover-V2 是一款7B大型语言模型，可在 Lean 4 中生成形式化定理证明，通过强化学习算法和 Lean 4 验证器的反馈实现自我纠错，提高性能。实验表明在 MiniF2F 测试集上，性能相较于其他模型提升了3.2%和2.0%（通过率@128）.


<details>
  <summary>Details</summary>
Motivation: 基于之前的工作 Leanabell-Prover-V1，Leanabell-Prover-V2 不断选择对现有强大证明模型进行进一步的后训练，以实现性能的提升。通过结合 RL 算法和 Lean 4 验证器的反馈，使得 LLMS 能够自我纠错，提高自身推理过程的准确性。

Method: Leanabell-Prover-V2 采用了强化学习算法优化 LLMS 的推理过程，通过与验证器的交互和反馈进行训练以提高性能。对算法进行重要的升级，包括 Lean 4 验证器提供的反馈信息、多轮验证器交互、反馈 Token 掩码以稳定强化学习训练，以及简单的奖励策略。

Result: Leanabell-Prover-V2 通过与验证器的交互和反馈优化 LLMS 的推理轨迹，加强稳定的 RL 训练，取得了性能提升。在实验中，Leanabell-Prover-V2 在 MiniF2F 测试集上相较于 Kimina-Prover-Preview-Distill-7B 提高了3.2%（通过率@128），相较于 DeepSeek-Prover-V2-7B 提高了2.0%（通过率@128）。

Conclusion: Leanabell-Prover-V2 是一款7B大型语言模型，能够在 Lean 4 中生成形式化定理证明，并具有内置的 Long Chain-of-Thoughts (CoT) 验证器。通过升级强化学习（RL）算法并结合 Lean 4 验证器提供的反馈，使得这个模型能够自我纠错，进而提高性能。在实验中，Leanabell-Prover-V2 在 MiniF2F 测试集上相较于 Kimina-Prover-Preview-Distill-7B 提高了3.2%（通过率@128），相较于 DeepSeek-Prover-V2-7B 提高了2.0%（通过率@128）。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [42] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: 提出了一种名为Introspection of Thought（INoT）的AI Agent推理框架，通过设计新的LLM-Read代码在提示中，使LLM能够执行编程对话推理过程。经实验证实了INoT在三个不同任务的六项基准测试中的有效性，性能提高了7.95％，超过基线效果，并且token成本比最佳基线方法低58.3％。INoT还在图像解释和推理方面具有多功能性。


<details>
  <summary>Details</summary>
Motivation: AI Agents在文本和图像任务中执行解释和推理时主要依赖大型语言模型（LLMs）和多模态LLMs（MLLMs）。当前AI Agents与LLMs的交互需要复杂的提示工程和外部推理框架，然而仍受限于LLM在理解自然语言方面的固有限制和推理过程中的推理成本。因此，提出INoT框架以实现更有效的推理过程。

Method: 设计了一种新的AI Agent推理框架INoT，其中包括LLM-Read代码，使得LLM能够执行编程对话推理过程。通过在六个基准测试上进行实验来验证INoT的有效性，并比较其性能和token成本与基线方法。此外，还进行了图像解释和推理方面的验证实验。

Result: 通过实验验证了INoT在提高AI Agent性能方面的有效性，平均性能提高了7.95％，token成本平均比基线方法低58.3％。还展示了INoT在图像解释和推理方面的多功能性。

Conclusion: 提出了一种新颖的AI Agent推理框架，名为Introspection of Thought (INoT)，通过在提示中设计新的LLM-Read代码，使LLM能够按照提示中的代码执行编程对话推理过程。INoT的有效性在三个不同任务的六项基准测试中得到验证，性能平均提高了7.95％，超过基线效果。此外，INoT的token成本平均比基线中表现最好的方法低58.3％。还通过验证实验展示了INoT在图像解释和推理方面的多功能性。

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [43] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: elsciRL是一个开源的Python库，用于在强化学习问题上应用语言解决方案。通过扩展语言适配器和自完成指令框架，结合LLMs技术，提供了新的GUI界面。实验证明指令可以改善代理的性能，加速语言解决方案在奖励环境中的评估，为科学研究带来新机遇。


<details>
  <summary>Details</summary>
Motivation: 为了加快语言解决方案在强化学习问题上的应用，通过LLMs技术改善强化学习代理性能，探索科学发现的新机遇。

Method: 使用elsciRL Python库，扩展语言适配器和自完成指令框架，结合LLMs技术进行研究。提供了图形用户界面(GUI)以便用户提供文本输入，生成指令并自动完成。

Result: 实证结果显示指令可以提高强化学习代理的性能。

Conclusion: 该论文提出了elsciRL，一个开源的Python库，旨在促进语言解决方案在强化学习问题上的应用。通过使用LLMs扩展语言适配器和自完成指令框架，展示了软件的潜力。他们提供了一种新颖的GUI，允许用户提供文本输入，以便LLM生成指令并自动完成。实证结果表明，这些指令可以改善强化学习代理的性能。因此，他们希望通过这项工作加速语言解决方案在基于奖励的环境中的评估，为科学发现开辟新机遇。

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [44] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: The paper emphasizes the use of modeling and optimization techniques in developing innovative systems architectures. It discusses challenges in exploring novel architectures with dedicated approaches and introduces surrogate-based optimization algorithms like Bayesian optimization with Gaussian process models as solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to highlight the importance of framing the architecting process and defining optimization and modeling problems for innovative systems architectures. It also aims to address challenges faced in exploring novel architectures using dedicated approaches.

Method: The paper discusses the central role of modeling and optimization techniques in developing innovative systems architectures, emphasizing the use of efficient dedicated approaches like physics-based simulations for system-of-systems.

Result: The result of the paper is the emergence of surrogate-based optimization algorithms, specifically Bayesian optimization with Gaussian process models, as solutions to the challenges posed by using dedicated approaches for system-of-systems.

Conclusion: Surrogate-based optimization algorithms, such as Bayesian optimization utilizing Gaussian process models, have emerged to address challenges in exploring novel architectures using dedicated approaches for system-of-systems.

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>
