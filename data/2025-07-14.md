<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: This paper explores whether AI can exhibit creativity by analyzing historical views on creativity, psychological advancements' impact, different definitions of creativity, and responses from naturalism and cognitive neuroscience.


<details>
  <summary>Details</summary>
Motivation: With the evolution of science and technology, the understanding of creativity has evolved significantly. This paper aims to investigate how AI can demonstrate creativity by delving into research in psychology, cognitive neuroscience, and the philosophy of creativity.

Method: The paper reviews historical perspectives on the philosophy of creativity, analyzes psychological advancements' influence on the study of creativity, examines various definitions of creativity, and explores the responses of naturalism and cognitive neuroscience to the concept of creativity.

Result: The paper provides insights into the possibility of AI exhibiting creativity and offers a comprehensive analysis of the interactions between AI techniques and the study of creativity from various perspectives.

Conclusion: AI has the potential to exhibit creativity, and this paper explores the intersection of AI techniques with psychology, cognitive neuroscience, and the philosophy of creativity to address this question.

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [2] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 该论文提出了TableReasoner框架，结合大型语言模型和编程，用于解决表格问答任务中的挑战。通过模式化表格建模和迭代思维架构，系统在SemEval-2025 Task 8的两个子任务中取得了第一名的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决表格问答任务面临的挑战，如大型数据、不完整的列语义和实体的歧义。通过综合使用大型语言模型和编程，设计能够高效处理大表格并消除歧义的TableReasoner框架。

Method: 使用大型语言模型和编程相结合的方法，设计了TableReasoner框架，并实现了结构和语义表示的模式化表格建模。通过多步骤的模式链接计划提取关注特定查询的表格模式，集成推理工作流到迭代思维架构中。

Result: 该系统在SemEval-2025 Task 8的两个子任务中获得第一名，表明所提出的TableReasoner框架在表格问答任务中取得了显著的成果。

Conclusion: 该论文提出了一种基于大型语言模型和编程的表格推理框架TableReasoner，用于表格问答任务。通过结构和语义表示相结合的模式建模表格，实现对大表格的整体理解和高效处理。设计了多步骤的模式链接计划，提取关注特定查询的表格模式，消除歧义和减轻错觉。通过将推理工作流集成到迭代思维架构中，允许思考、推理和反思的增量循环。该系统在SemEval-2025 Task 8的两个子任务中取得第一名的成绩。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [3] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出了一个动态Stackelberg博弈框架，用于建模LLM jailbreaking的攻击者和防御者互动。设计了一种名为“紫色特工”的AI解决方案，通过RRT结合敌对探索和防御策略，有效预防恶意输出，为分析对抗动态和降低越狱风险提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在关键应用中的部署越来越多，越狱挑战成为一个重要问题。有必要建立一个模型来分析和预防对抗动态，减轻LLM被攻击的风险。

Method: 使用动态Stackelberg博弈框架建模LLM jailbreaking的攻击者和防御者互动，设计了“紫色特工”AI解决方案，利用Rapidly-exploring Random Trees（RRT）结合敌对探索和防御策略，主动模拟潜在攻击路径并采取预防措施。

Result: 提出了一种用于分析LLM越狱攻击的新颖方法，通过“紫色特工”AI解决方案有效预防恶意输出，并为降低越狱风险提供了基础。

Conclusion: 本文提出了一种动态Stackelberg博弈框架，用于建模LLM越狱攻击中攻击者和防御者之间的互动。通过引入新型AI解决方案“紫色特工”，结合敌对探索和防御策略，有效地预防恶意输出，为分析对抗动态提供了基础，从而减轻越狱风险。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [4] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: The paper introduces the LLM-Nash framework, a game-theoretic model that explores how agents' choice of reasoning prompts in Large Language Models can affect decision-making. It addresses bounded rationality, models reasoning explicitly, and shows divergent outcomes from classical Nash equilibria through examples, offering insights into strategic interactions in LLM-enabled systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional game models by considering bounded rationality and explicitly modeling reasoning processes in LLMs.

Method: Proposing a game-theoretic model where agents choose reasoning prompts to influence decision-making in Large Language Models (LLMs) to study cognitive constraints, mindset expressiveness, and epistemic learning.

Result: Demonstrating through examples how reasoning equilibria in the LLM-Nash framework can differ from classical Nash outcomes, providing a new basis for analyzing strategic interactions in systems utilizing LLMs.

Conclusion: Introducing the LLM-Nash framework that incorporates bounded rationality and explicit modeling of the reasoning process, leading to divergent outcomes from classical Nash equilibria.

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [5] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 论文探讨了智能Agent在探索和控制环境时如何平衡好奇心和能力，研究了内部表征对探索中好奇心和能力的影响，比较了使用不同方法的Agent，并发现了探索与表示学习之间的双向互动。研究结果为认知理论和强化学习提供了启示。


<details>
  <summary>Details</summary>
Motivation: 论文旨在将认知内在动机理论与强化学习相结合，研究内部表征如何调节好奇心和能力的平衡，从而形式化适应性探索为追求未知和可控性之间的平衡。

Method: 论文比较了使用手工状态抽象（Tabular）和学习内部世界模型（Dreamer）的两种基于模型的Agent。Tabular Agent显示好奇心和能力在不同模式下引导探索，而同时优先考虑二者可改善探索。Dreamer Agent揭示了探索和表示学习之间的双向互动。

Result: 论文的研究结果为认知理论和强化学习提供了洞见，并阐明了探索的平衡机制。

Conclusion: 该论文探讨了智能Agent在探索世界的同时保持对环境的控制，研究了好奇心与能力之间的平衡关系，以及内部表征如何影响好奇心和能力的权衡。研究发现使用内部世界模型的Agent表现出探索和表示学习之间的双向互动，反映了好奇心和能力的发展共同演化。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [6] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 本文探討了Neural-Symbolic方法中存在的grounding問題，提出了一個新的parametrized grounding方法家族，通過不同選擇可以控制推理器的表達能力和可擴展性之間的平衡。實驗結果強調了grounding標準的選擇對於NeSy方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 現有的Neural-Symbolic方法中，存在一些grounding方法會導致推理樹的組合爆炸，嚴重限制了其可擴展性。本文靈感來源於多跳符號推理，提出一個新的grounding方法家族，以解決表達能力和可擴展性之間的折衷。

Method: 本文提出了一個參數化的grounding方法家族，通過對這個家族中不同選擇的使用，可以控制推理器的表達能力和可擴展性之間的平衡。

Result: 通過實驗結果顯示，grounding標準的選擇對於NeSy方法的效果同樣重要。

Conclusion: 這篇論文提出一個參數化的grounding方法家族，可以泛化經典的Backward Chaining方法，通過不同的選擇在這個家族中可以獲得常用的grounding方法，並且控制推理器的表達能力和可擴展性之間的折衷。實驗結果表明，grounding標準的選擇通常與NeSy方法本身同等重要。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [7] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 本文针对QFL领域的研究现状和问题，提出了首个多模态方法及Missing Modality Agnostic（MMA）机制，通过量子纠缠实现中间融合，解决了多模态QFL中的性能瓶颈，提高了模型准确度。仿真结果表明，该方法在IID和非IID数据分布下均取得了显著的准确度提升。


<details>
  <summary>Details</summary>
Motivation: 由于现有QFL框架主要集中在单模态系统上，对于现实任务中涉及多种模态的应用有限，本文旨在填补这一重要空白。通过新颖的多模态方法和MMA机制，解决了多模态QFL中的主要瓶颈，提高了模型性能。

Method: 针对QFL设置，引入了多模态方法和MMA机制。使用量子纠缠实现中间融合，隔离未训练的量子电路以确保训练过程稳定。通过仿真实验证明方法的有效性。

Result: 通过仿真实验证明了提出的多模态QFL方法结合MMA机制在IID和非IID数据分布下比现有最先进方法分别提高了6.84%和7.25%的准确度。

Conclusion: 提出了首个针对量子联邦学习（QFL）的多模态方法，通过量子纠缠进行中间融合，提高了模型准确度。引入了Missing Modality Agnostic（MMA）机制，确保在多模态QFL中训练过程稳定，提高了模型性能。仿真结果显示，与现有最先进方法相比，在IID和非IID数据分布下，提出的多模态QFL方法结合MMA可以使准确度分别提高了6.84%和7.25%。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [8] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 本文对给予AI代理访问加密货币和智能合约可能带来的新风险进行了探讨，描述了这些新风险并呼吁加强技术研究以降低风险。


<details>
  <summary>Details</summary>
Motivation: 探讨给予AI代理访问加密货币和智能合约的增长兴趣，提出这可能导致新的AI伤害渠道。针对这一观点，呼吁加强对潜在风险的研究。

Method: 首先研究加密货币和智能合约的独特属性，探讨可能导致新风险的因素，然后详细描述这些新风险，并呼吁进行更多技术研究以预防和减轻这些风险。

Result: 通过分析加密货币和智能合约的特性，并描述新的风险渠道，加强了对AI代理与加密货币和智能合约潜在风险的认识。

Conclusion: 呼吁进行更多的技术研究，以防止和减轻AI代理与加密货币和智能合约产生风险，从而使AI代理具有加密货币和智能合约更加安全。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [9] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 本论文综述了Abductive Reasoning在认识论、科学和设计领域的讨论情况，分析了计算系统应用Abductive Reasoning的方式。研究发现现有的理论框架和计算系统并未有效解决生成创造性假设的问题，提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于解决Abductive Reasoning在不同领域中理解的差异问题，以及现有理论框架和计算系统在生成创造性假设方面的不足。研究旨在促进计算系统中创造性Abductive Reasoning的发展。

Method: 该论文通过文献综述和分析研究了Abductive Reasoning在认识论、科学和设计领域的讨论状况，并调查了计算系统如何应用Abductive Reasoning。研究采用分析方法，对现有的理论框架和计算系统进行评估，指出它们在生成创造性假设方面存在的不足。最后，提出了未来研究的方向。

Result: 研究发现当前的Abductive Reasoning理论框架和计算系统缺乏对创造性假设生成的有效方法。理论框架不能直接支持创造性Abductive假设的生成，而计算系统主要采用演绎形式的Abductive Reasoning。通过对计算系统进行拆解，为未来研究提供了具体方向。

Conclusion: 该论文总结了目前关于Abductive Reasoning在不同领域（认识论、科学和设计）的讨论情况，并分析了各种计算系统如何使用Abductive Reasoning。研究结果显示，现有的理论框架和计算系统对于生成创造性假设并不足够。理论框架没有提供直接的生成创造性Abductive假设的模型，计算系统主要实现了演绎形式的Abductive Reasoning。研究将Abductive计算系统分解为不同组件，并指出未来研究的具体方向，以推动计算系统中创造性Abductive Reasoning的发展。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [10] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 本文提出了面向工具使用代理的统一安全对齐框架，引入了三模态分类法，通过自定义设计的沙盒环境模拟实际工具执行，展示了安全对齐代理在安全威胁抵抗能力方面的显著改进。结果表明安全性和有效性可以同时得到优化，为自主LLM代理的可信部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 自主大型语言模型代理的出现引入了新的安全风险，需要超越传统对话误用的考虑。现有代理可以执行外部功能，因此容易受到用户发起的威胁和工具发起的威胁的影响。因此，本文的动机是应对这种新型威胁，提出统一的安全对齐框架，以保证模型在面对威胁时能够做出正确的决策。

Method: 本文采用了自定义设计的沙盒环境模拟实际工具执行，允许精细化奖励塑造。引入了三模态分类法，包括良性、恶意和敏感，为用户提示和工具响应进行定义，建立基于策略的决策模型。

Result: 通过广泛评估，展示了安全对齐代理在安全威胁抵抗能力方面的显著改进，同时在良性任务上保持了强大的效用。研究结果表明安全性和有效性可以同时得到优化。

Conclusion: 本文提出了面向工具使用代理的统一安全对齐框架，利用结构化推理和沙箱强化学习帮助模型处理用户发起的威胁和工具发起的威胁。通过在公共和自建基准测试上进行广泛评估，展示了安全对齐代理显著提高了抵抗安全威胁的能力，同时在良性任务上保持强大的效用。研究结果表明安全性和有效性可以同时优化，为自主LLM代理的可信部署奠定基础。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [11] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: Recent advancements in Multimodal Large Language Models have led to the development of M2-Reasoning-7B, which excels in general and spatial reasoning. The model integrates innovative data pipeline and dynamic multi-task training strategy to enhance reasoning abilities across 8 benchmarks, setting a new state-of-the-art in both domains.


<details>
  <summary>Details</summary>
Motivation: Existing Multimodal Large Language Models struggle with dynamic spatial interactions, crucial for real-world applications. The goal is to enhance reasoning abilities in both general and spatial domains.

Method: Integrates a novel data pipeline to generate high-quality data samples for cold-start fine-tuning and RLVR, and implements a dynamic multi-task training strategy with step-wise optimization to address conflicts between data and provide task-specific rewards for tailored incentive signals.

Result: The model M2-Reasoning-7B outperforms previous models, achieving state-of-the-art results in 8 benchmarks for general and spatial reasoning.

Conclusion: M2-Reasoning-7B sets a new state-of-the-art across 8 benchmarks, demonstrating superior performance in both general and spatial reasoning domains.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [12] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 本研究提出了一个框架，通过引入道德倡导者代理在多智能体LLM设置中，对系统描述的道德问题进行批判和提供意见，生成道德要求草案。该框架通过两个案例研究进行了评估，发现其有效捕捉了大部分研究人员在短时间内识别的道德要求，并引入了一些其他相关要求。然而，也指出了在生成道德要求方面的可靠性问题，强调了在这一敏感领域中需要人类反馈的必要性。研究认为这项工作可以促进道德在需求工程过程中的更广泛采用，最终导致更符合道德的产品。


<details>
  <summary>Details</summary>
Motivation: 将道德纳入需求引出过程对于创建符合道德的系统至关重要。传统的手动道德要求引出虽然有效，但需要来自多个利益相关者的多样化输入，由于时间和资源限制可能存在挑战。此外，在需求引出过程中通常将其优先级降低。本研究旨在解决这些问题，并提出了一个新的框架。

Method: 引入一个道德倡导者代理在多智能体LLM设置中，对系统描述的道德问题进行批判和提供意见，生成道德要求草案。通过两个不同背景的案例研究对提出的框架进行评估。

Result: 通过案例研究评估了提出的框架，发现其捕捉了大部分研究人员在30分钟访谈中识别的道德要求，并引入了一些其他相关要求。然而，也突出了在生成道德要求方面的可靠性问题，强调了在这一敏感领域中需要人类反馈的必要性。

Conclusion: 研究提出了一个在多智能体LLM环境中引入道德倡导者代理的框架，用于生成道德要求草案。该框架通过两个不同背景的案例研究进行了评估，展示了在30分钟访谈中识别的大部分道德要求，并引入了一些其他相关要求。然而，它也强调了在生成道德要求时存在的可靠性问题，强调了在这一敏感领域需要人类反馈的必要性。研究认为这项工作可以促进道德在需求工程过程中的更广泛采用，最终导致更符合道德的产品。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [13] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 本文探讨了对比解释的经典问题，比如“为什么是P而不是Q？”利用命题逻辑框架定义了这些问题，展示了其基本属性和计算复杂性分析。通过答案集编程实现了问题的解决方案，并呈现了实际示例。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于探索涉及对比解释的经典问题，并提供了一种计算方法和实现这些问题的方式。通过比较P和Q的原因，探索了对比解释的新视角。

Method: 研究了对比解释的几个经典问题，并在命题逻辑框架下定义了这些问题。展示了计算对问题的基本属性和计算复杂性进行了分析。通过答案集编程实现了对问题的解决方案，并呈现了实际示例。

Result: 通过命题逻辑的定义，论文提供了对比解释的框架，并展示了捕捉现有对比解释版本的能力以及问题的计算复杂性分析。此外，通过答案集编程实现了对问题的解决方案，并通过示例展示了其实际应用。

Conclusion: 该论文定义了几个涉及对比解释的经典问题，回答了“为什么是P而不是Q？”这种形式的问题。他们计算了P和Q的原因，并明确比较了它们之间的差异。在命题逻辑设置下研究了定义的基本属性，并展示了该框架捕捉到了现有文献中存在的对比解释的基数最小版本。此外，对问题的计算复杂性进行了广泛分析。还使用答案集编程为CNF公式实现了这些问题，并展示了几个示例来演示它们在实践中的运作。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [14] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 本文提出了一种双层框架，通过高级任务抽象和低级逻辑生成将自然语言映射到逻辑表示。使用双层优化方法优化框架，在多个现实推理基准测试中，精度明显优于现有基线，精度提高达到40%。双层设计增强了透明性和错误可追溯性，为与大语言模型进行可信赖和系统推理迈出了有益的一步。


<details>
  <summary>Details</summary>
Motivation: 在人工智能领域中，结构化推理仍然是一个核心挑战，需要建立自然语言表达与形式逻辑表示之间的桥梁。该研究的动机是提出一种框架，以更好地将自然语言映射到逻辑表示，支持多领域推理，并在精度、透明性和错误追溯性上优于现有方法。

Method: 提出了一个双层框架，包括高级任务抽象和低级逻辑生成阶段，通过使用大语言模型解析自然语言查询并生成结构化表示，进而生成符号工作流或可执行推理程序。优化框架采用双层优化方法，同时完善高级抽象和低级逻辑生成阶段。

Result: 通过实验在多个现实推理基准测试中显示，该方法在精度方面大幅超越了现有基线，精度提高达到40%。双层设计增强了透明性和错误可追溯性，为与大语言模型进行可信赖和系统推理迈出了有益的一步。

Conclusion: 提出了一种新颖的双层框架，通过高级任务抽象和低级逻辑生成将自然语言映射到逻辑表示，支持模块化推理，强制显式约束，并在数学问题解决、问答和逻辑推理等领域具有泛化能力。使用双层优化方法优化框架，实验结果显示在多个现实推理基准测试中，精度明显优于现有基线，精度提高达到40%。双层设计增强了透明性和错误可追溯性，为与大语言模型进行可信赖和系统推理迈出了有益的一步。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [15] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 本研究提出了一种框架，耦合多粒度稀疏激活医学概念和分层知识图，通过多种算法实现精确概念激活。实验证明在罕见疾病诊断方面取得了显著提升，专家评估证实了信息质量和推理能力的改进，有助于缩短罕见疾病患者的诊断时间。


<details>
  <summary>Details</summary>
Motivation: 医疗大型语言模型在医疗保健领域取得了进展，但罕见疾病诊断仍受到知识表示深度不足、概念理解有限和临床推理受限的影响。为了解决这一问题，提出了本研究的方法。

Method: 耦合多粒度稀疏激活医学概念和分层知识图的框架，四种匹配算法、多样性控制和五级备用策略实现精确概念激活。三层知识图（分类、临床特征、实例）提供结构化、最新的上下文。

Result: 在BioASQ罕见疾病QA数据集上，实验证明该方法取得了可观的提升，包括BLEU增益为0.09，ROUGE增益为0.05，准确度增益为0.12，且准确度接近临床阈值。专家评估确认了信息质量、推理和专业表达等方面的改进。

Conclusion: 提出了一种耦合多粒度稀疏激活医学概念和分层知识图的框架，通过四种互补的匹配算法、多样性控制和五级备用策略实现精确概念激活。三层知识图（分类、临床特征、实例）提供结构化、最新的上下文。实验证明，在BioASQ罕见疾病QA数据集上，BLEU增益为0.09，ROUGE增益为0.05，准确度增益为0.12，准确度达到0.89，接近0.90的临床阈值。专家评估证实了信息质量、推理和专业表达方面的改进，表明该方法缩短了罕见疾病患者的“诊断奥德赛”。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [16] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 该论文介绍了一种利用大型多模态模型(LMM)进行地理参考的新方法。采用了基于网格的自适应自回归模型，在零样本设定下取得了令人瞩目的成果($	ilda1$ km平均距离误差)。实验结果显示该方法优越于传统的单模态地理参考方法和现有工具。论文还提出了将该方法整合到地理参考工作流程的实用框架。


<details>
  <summary>Details</summary>
Motivation: 论文指出自然历史收藏中的几百万个生物样本记录未进行地理编码，地理编码与样本采集相关的复杂地点描述是收集机构难以处理的劳动密集任务。现有的自动化方法未利用地图作为地理编码复杂关系的基本工具。因此，作者受到这一挑战的启发，提出利用最近大型多模态模型的多模态能力，通过新颖的方法解决这一问题。

Method: 论文采用了基于网格的方法，使用自回归模型在零样本设定下进行自适应，以实现地理参考复杂地点描述的任务。

Result: 实验结果显示，该方法在处理地理参考任务上表现出色，平均距离误差约为1公里。相比于使用大型语言模型和现有地理参考工具进行单模态地理参考，该方法显著优越。

Conclusion: 该论文介绍了一种新颖的方法，利用最近大型多模态模型(LMM)的多模态能力，实现对复杂地点描述的自动地理参考。实验结果显示该方法在处理地理参考任务上取得了显著的成果，与单模态地理参考方法和现有地理参考工具相比表现出色。论文还讨论了实验结果，并提出了将该方法融入地理参考工作流程的实用框架。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [17] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 本文提出了一种通过查询重写框架和多个大型语言模型知识融合的方法，成功将文本指令转换为更适合语音合成的分布，提高数据可用性并展现在复杂任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管现代文本转语音(TTS)模型已经实现了接近人类水平的合成质量，但由于TTS模型训练数据分布的限制，将分布外文本指令适当转换为语音仍具有挑战性。通过人工合成语音指令数据集的高成本，提出使用语音合成构建大规模语音指令数据集作为替代方案。

Method: 提出了查询重写框架，利用多个大型语言模型进行知识融合，使用多个代理进行注释和验证合成语音，以构建高质量的语音指令数据集。通过实验证明，该方法能够通过零样本重写，将文本指令转换为更适合用于语音合成的分布。

Result: 实验证明，该方法可以通过零样本重写，将文本指令转换为更适合用于语音合成的分布，将数据可用性从72%提高到93%。在需要复杂知识和上下文相关能力的重写任务中表现出独特优势。

Conclusion: 本文提出了一种查询重写框架，利用多个大型语言模型进行知识融合，通过零样本重写，将文本指令转换为更适合用于语音合成的分布，从而将数据可用性从72%提高到93%，在复杂知识和上下文相关能力方面具有独特优势。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [18] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 本研究比较了多代理系统和双代理系统在工程设计中的效果。多代理系统提高了设计细节和任务管理，但编码兼容性较差。深度推理的大型语言模型提高了任务完成率，但需求和编码方面存在差距。


<details>
  <summary>Details</summary>
Motivation: 针对早期工程设计中的复杂迭代推理过程，尝试解决现有大型语言模型工作流在任务连续性和可执行模型生成方面的困难。

Method: 引入了Design-State Graph (DSG)，利用九种角色的多代理系统迭代构建和完善DSG，与2AS系统进行比较。运行了60个实验，对多种指标进行评估，如JSON有效性、需求覆盖、代码兼容性、工作流程完成率等。

Result: 多代理系统在设计细节和任务管理方面表现优异，但在编码兼容性上表现不如双代理系统。深度推理的大型语言模型提高了任务完成率，但存在需求和编码忠实度方面的差距。

Conclusion: 多代理系统在管理需求提取、功能分解和模拟器代码生成方面比简单的双代理系统更有效。多代理系统提高了设计细节，但对编码方面要求较低。深度推理的大型语言模型提高了完成率，但在编码方面存在需求和忠实度差距。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [19] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: Leanabell-Prover-V2是一个7B大型语言模型，通过与Lean 4验证器的反馈结合使用强化学习（RL）来生成正式定理证明。实验结果表明，在MiniF2F测试集上，它通过不同证明模型分别提高了3.2%和2.0%的性能。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于通过Leanabell-Prover-V2提高LLM的性能，使其能够自我意识地纠正错误并生成正式定理证明。

Method: Leanabell-Prover-V2通过升级强化学习（RL）结合Lean 4验证器的反馈来生成正式定理证明。它直接优化LLM的推理轨迹，实现多轮验证器交互，采用反馈令牌掩盖进行稳定的RL训练并采用简单的奖励策略。

Result: 实验结果表明，Leanabell-Prover-V2在MiniF2F测试集上分别通过Kimina-Prover-Preview-Distill-7B和DeepSeek-Prover-V2-7B提高了3.2%和2.0%，展示了其性能提升。

Conclusion: Leanabell-Prover-V2 是一个7B大型语言模型，可以在Lean 4中生成正式定理证明，具有验证器集成的Long Chain-of-Thoughts（CoT）。通过强调Leanabell-Prover-V1的先前工作，我们选择进一步后训练现有强大的证明模型以提高性能。在V2版本中，我们主要通过Lean 4验证器提供的反馈来升级强化学习（RL）。关键的是，验证器的反馈，例如指示成功或详细说明特定错误，使LLM能够“自我意识”地了解其推理过程的正确性，并学会自我修正错误。Leanabell-Prover-V2直接优化LLM的推理轨迹，具有多轮验证器交互以及反馈令牌掩盖以进行稳定的RL训练和简单的奖励策略。实验证明，Leanabell-Prover-V2在MiniF2F测试集上通过Kimina-Prover-Preview-Distill-7B提高了3.2%（pass@128），通过DeepSeek-Prover-V2-7B提高了2.0%（pass@128）。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [20] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: 研究提出了一种名为INoT的新的AI Agent推理框架，通过在提示中设计新的LLM-Read代码，使LLM执行程序化对话推理过程。在六项基准测试上的实验证实了INoT的有效性，平均性能提高了7.95p并超过了基线，同时标记成本低于基线上最佳方法的58.3p。还证明了INoT在图像解释和推理中的多功能性。


<details>
  <summary>Details</summary>
Motivation: AI Agents依赖大型语言模型（LLMs）和多模式LLMs（MLLMs）执行文本和图像任务中的解释和推理，但受限于LLM在理解自然语言方面的固有限制以及迭代推理过程可能带来的大量推理成本。因此，需要提出一种新的框架来解决这些问题。

Method: 设计了一个新的AI Agent推理框架，INoT，其中包括LLM-Read代码，以执行程序化对话推理过程。通过实验在六项基准测试上验证了INoT的有效性，并探讨了其在图像解释和推理中的适用性。

Result: 提出的INoT框架在实验中显示了其在改善AI Agent性能方面的有效性，并降低了标记成本。此外，INoT还展示了在图像解释和推理中的多功能性。

Conclusion: 提出了一种新的AI Agent推理框架，名为“思维内省（INoT）”，通过在提示中设计新的LLM-Read代码，实现LLM执行程序化对话推理过程。通过在六项基准测试上的实验验证了INoT的有效性，在性能方面平均提高了7.95p，并超过了基线。此外，INoT的标记成本平均低于基线上最佳性能方法的58.3p。通过验证实验展示了INoT在图像解释和推理中的多功能性。

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [21] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: elsciRL是一个Python库，用于在强化学习问题上应用语言解决方案。他们扩展了Language Adapter with Self-Completing Instruction框架，结合LLMs，并提供新的GUI。实证结果表明生成的指令可以改善强化学习代理的性能，该工作旨在加速评估语言解决方案的应用。


<details>
  <summary>Details</summary>
Motivation: 加速评估语言解决方案在奖励驱动环境中的应用，为科学发现提供新机会。

Method: 扩展了Language Adapter with Self-Completing Instruction框架，结合LLMs，并提供新颖的GUI进行文本输入，生成指令并自我完成，以改善强化学习代理的性能。

Result: 通过实证结果表明，生成的指令可以提高强化学习代理的性能。

Conclusion: 介绍了elsciRL，一个开源的Python库，用于在强化学习问题上应用语言解决方案。通过将Language Adapter with Self-Completing Instruction框架与LLMs相结合，展示了软件的潜力。他们提供了一种新颖的GUI，允许用户提供文本输入，以生成指令并进行自我完成。实证结果表明，这些指令可以提高强化学习代理的性能。该工作旨在加速评估语言解决方案在基于奖励的环境中的应用，从而为科学发现提供新机会。

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [22] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: The paper discusses the importance of modeling and optimization techniques in developing innovative systems architectures, particularly for system-of-systems. It highlights the use of dedicated approaches like physics-based simulations and the emergence of surrogate-based optimization algorithms, such as Bayesian optimization with Gaussian process models, to tackle challenges in exploring novel architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to reduce the computational complexity of targeted applications when exploring novel architectures. It aims to overcome challenges faced by optimization algorithms, including increased evaluation costs and potential failures.

Method: The paper focuses on modeling and optimization techniques for developing innovative systems architectures. It emphasizes the use of efficient dedicated approaches, such as physics-based simulations, for system-of-systems.

Result: The use of surrogate-based optimization algorithms, particularly Bayesian optimization with Gaussian process models, has shown promise in addressing the challenges associated with exploring novel architectures for system-of-systems.

Conclusion: Surrogate-based optimization algorithms, such as Bayesian optimization using Gaussian process models, have emerged to address challenges in exploring novel architectures with dedicated approaches for system-of-systems.

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>
