{"id": "2508.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10047", "abs": "https://arxiv.org/abs/2508.10047", "authors": ["Ziyang Xiao", "Jingrong Xie", "Lilin Xu", "Shisi Guan", "Jingyan Zhu", "Xiongwei Han", "Xiaojin Fu", "WingYin Yu", "Han Wu", "Wei Shi", "Qingcan Kang", "Jiahui Duan", "Tao Zhong", "Mingxuan Yuan", "Jia Zeng", "Yuan Wang", "Gang Chen", "Dongxiang Zhang"], "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions", "comment": null, "summary": "By virtue of its great utility in solving real-world problems, optimization\nmodeling has been widely employed for optimal decision-making across various\nsectors, but it requires substantial expertise from operations research\nprofessionals. With the advent of large language models (LLMs), new\nopportunities have emerged to automate the procedure of mathematical modeling.\nThis survey presents a comprehensive and timely review of recent advancements\nthat cover the entire technical stack, including data synthesis and fine-tuning\nfor the base model, inference frameworks, benchmark datasets, and performance\nevaluation. In addition, we conducted an in-depth analysis on the quality of\nbenchmark datasets, which was found to have a surprisingly high error rate. We\ncleaned the datasets and constructed a new leaderboard with fair performance\nevaluation in terms of base LLM model and datasets. We also build an online\nportal that integrates resources of cleaned datasets, code and paper repository\nto benefit the community. Finally, we identify limitations in current\nmethodologies and outline future research opportunities.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u6db5\u76d6\u4e86\u4f18\u5316\u5efa\u6a21\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4ecb\u7ecd\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u7684\u673a\u4f1a\u3002\u7814\u7a76\u5bf9\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u53d1\u73b0\u4e86\u9ad8\u8bef\u5dee\u7387\uff0c\u6e05\u6d17\u6570\u636e\u96c6\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u8bc4\u4f30\u6392\u884c\u699c\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u7ebf\u95e8\u6237\u6574\u5408\u8d44\u6e90\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u5e2e\u52a9\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u673a\u4f1a\u3002", "motivation": "\u7531\u4e8e\u4f18\u5316\u5efa\u6a21\u5728\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9700\u8981\u8fd0\u7b79\u5b66\u4e13\u4e1a\u4eba\u5458\u7684\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u8fc7\u7a0b\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u5728\u6574\u4e2a\u6280\u672f\u6808\u4e0a\u8fdb\u884c\u4e86\u7efc\u5408\u6027\u7684\u5ba1\u67e5\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u548c\u7cbe\u8c03\u3001\u63a8\u7406\u6846\u67b6\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6027\u80fd\u8bc4\u4f30\u3002\u5bf9\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u5e76\u53d1\u73b0\u4e86\u60ca\u4eba\u7684\u9ad8\u8bef\u5dee\u7387\u3002", "result": "\u6e05\u6d17\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u6392\u884c\u699c\uff0c\u5efa\u7acb\u4e86\u5728\u7ebf\u95e8\u6237\u6574\u5408\u8d44\u6e90\uff0c\u53d1\u73b0\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u673a\u4f1a\u3002", "conclusion": "\u8be5\u7814\u7a76\u7efc\u8ff0\u4e86\u6700\u8fd1\u5728\u4f18\u5316\u5efa\u6a21\u9886\u57df\u7684\u65b0\u8fdb\u5c55\uff0c\u4ecb\u7ecd\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u8fc7\u7a0b\u7684\u673a\u4f1a\u3002\u7814\u7a76\u6e05\u6d17\u4e86\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u516c\u5e73\u8bc4\u4f30\u6027\u80fd\u7684\u65b0\u6392\u884c\u699c\u3002\u901a\u8fc7\u5728\u7ebf\u95e8\u6237\u6574\u5408\u4e86\u6e05\u6d17\u6570\u636e\u96c6\u8d44\u6e90\u3001\u4ee3\u7801\u548c\u8bba\u6587\u5e93\uff0c\u9020\u798f\u793e\u533a\u3002\u6700\u540e\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u52fe\u52d2\u4e86\u672a\u6765\u7684\u7814\u7a76\u673a\u4f1a\u3002"}}
{"id": "2508.10108", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; E.0"], "pdf": "https://arxiv.org/pdf/2508.10108", "abs": "https://arxiv.org/abs/2508.10108", "authors": ["Sattvik Sahai", "Prasoon Goyal", "Michael Johnston", "Anna Gottardi", "Yao Lu", "Lucy Hu", "Luke Dai", "Shaohua Liu", "Samyuth Sagi", "Hangjie Shi", "Desheng Zhang", "Lavina Vaz", "Leslie Ball", "Maureen Murray", "Rahul Gupta", "Shankar Ananthakrishna"], "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development", "comment": "18 pages, 1st Proceedings of Amazon Nova AI Challenge (Trusted AI\n  2025)", "summary": "AI systems for software development are rapidly gaining prominence, yet\nsignificant challenges remain in ensuring their safety. To address this, Amazon\nlaunched the Trusted AI track of the Amazon Nova AI Challenge, a global\ncompetition among 10 university teams to drive advances in secure AI. In the\nchallenge, five teams focus on developing automated red teaming bots, while the\nother five create safe AI assistants. This challenge provides teams with a\nunique platform to evaluate automated red-teaming and safety alignment methods\nthrough head-to-head adversarial tournaments where red teams have multi-turn\nconversations with the competing AI coding assistants to test their safety\nalignment. Along with this, the challenge provides teams with a feed of high\nquality annotated data to fuel iterative improvement. Throughout the challenge,\nteams developed state-of-the-art techniques, introducing novel approaches in\nreasoning-based safety alignment, robust model guardrails, multi-turn\njail-breaking, and efficient probing of large language models (LLMs). To\nsupport these efforts, the Amazon Nova AI Challenge team made substantial\nscientific and engineering investments, including building a custom baseline\ncoding specialist model for the challenge from scratch, developing a tournament\norchestration service, and creating an evaluation harness. This paper outlines\nthe advancements made by university teams and the Amazon Nova AI Challenge team\nin addressing the safety challenges of AI for software development,\nhighlighting this collaborative effort to raise the bar for AI safety.", "AI": {"tldr": "Amazon Nova AI Challenge focused on driving advances in secure AI for software development through a global competition among university teams. Teams developed automated red teaming bots and safe AI assistants, introducing novel techniques in safety alignment methods. The challenge led to significant advancements in AI safety, highlighting a collaborative effort to raise the bar for AI safety in software development.", "motivation": "To address the safety challenges of AI systems for software development and ensure their secure implementation. The goal was to drive advances in secure AI by exploring novel approaches in safety alignment methods through a global competition among university teams.", "method": "The Amazon Nova AI Challenge involved 10 university teams focusing on developing automated red teaming bots and safe AI assistants. Teams participated in head-to-head adversarial tournaments to test safety alignment methods. High-quality annotated data was provided to fuel iterative improvement. The challenge also included building a custom baseline coding specialist model, developing a tournament orchestration service, and creating an evaluation harness to support teams' efforts.", "result": "University teams and the Amazon Nova AI Challenge team made significant advancements in addressing the safety challenges of AI for software development. The challenge led to the introduction of state-of-the-art techniques in safety alignment, model guardrails, jail-breaking, and probing of large language models. The collaborative effort aimed to raise the bar for AI safety in software development.", "conclusion": "Amazon launched the Trusted AI track of the Amazon Nova AI Challenge, a global competition among university teams to drive advances in secure AI for software development. The challenge involved developing automated red teaming bots and safe AI assistants, providing a platform to evaluate safety alignment methods through adversarial tournaments. Teams introduced novel techniques in reasoning-based safety alignment, model guardrails, jail-breaking, and probing of large language models. The paper highlights the collaborative effort to address the safety challenges of AI in software development."}}
{"id": "2508.10143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10143", "abs": "https://arxiv.org/abs/2508.10143", "authors": ["Alexandru-Andrei Avram", "Adrian Groza", "Alexandru Lecu"], "title": "MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection", "comment": "8 pages + 1 page references, 5 figures, 4 tables, Registered for the\n  27th International Symposium on Symbolic and Numeric Algorithms for\n  Scientific Computing, 2025, Timisoara", "summary": "The large spread of disinformation across digital platforms creates\nsignificant challenges to information integrity. This paper presents a\nmulti-agent system that uses relation extraction to detect disinformation in\nnews articles, focusing on titles and short text snippets. The proposed Agentic\nAI system combines four agents: (i) a machine learning agent (logistic\nregression), (ii) a Wikipedia knowledge check agent (which relies on named\nentity recognition), (iii) a coherence detection agent (using LLM prompt\nengineering), and (iv) a web-scraped data analyzer that extracts relational\ntriplets for fact checking. The system is orchestrated via the Model Context\nProtocol (MCP), offering shared context and live learning across components.\nResults demonstrate that the multi-agent ensemble achieves 95.3% accuracy with\nan F1 score of 0.964, significantly outperforming individual agents and\ntraditional approaches. The weighted aggregation method, mathematically derived\nfrom individual agent misclassification rates, proves superior to algorithmic\nthreshold optimization. The modular architecture makes the system easily\nscalable, while also maintaining details of the decision processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5173\u7cfb\u62bd\u53d6\u6280\u672f\u68c0\u6d4b\u65b0\u95fb\u6587\u7ae0\u4e2d\u865a\u5047\u4fe1\u606f\u7684\u591aAgent\u7cfb\u7edf\uff0c\u5305\u62ec\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u3001\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\u68c0\u67e5\u4ee3\u7406\u3001\u8fde\u8d2f\u6027\u68c0\u6d4b\u4ee3\u7406\u548c\u7f51\u7edc\u6293\u53d6\u6570\u636e\u5206\u6790\u4ee3\u7406\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e8695.3%\u7684\u51c6\u786e\u5ea6\u548cF1\u5206\u6570\u4e3a0.964\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5355\u4e2a\u4ee3\u7406\u3002\u7cfb\u7edf\u901a\u8fc7\u52a0\u6743\u805a\u5408\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u5177\u6709\u6613\u6269\u5c55\u7684\u6a21\u5757\u5316\u67b6\u6784\u3002", "motivation": "\u6570\u5b57\u5e73\u53f0\u4e0a\u865a\u5047\u4fe1\u606f\u7684\u5927\u91cf\u4f20\u64ad\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u9020\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u8bba\u6587\u7684\u52a8\u673a\u5728\u4e8e\u5229\u7528\u591aAgent\u7cfb\u7edf\u68c0\u6d4b\u65b0\u95fb\u6587\u7ae0\u4e2d\u7684\u865a\u5047\u4fe1\u606f\uff0c\u9488\u5bf9\u6807\u9898\u548c\u77ed\u6587\u672c\u7247\u6bb5\u8fdb\u884c\u68c0\u6d4b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAgentic AI\u7cfb\u7edf\uff0c\u5229\u7528\u5173\u7cfb\u62bd\u53d6\u6280\u672f\u68c0\u6d4b\u65b0\u95fb\u6587\u7ae0\u4e2d\u7684\u865a\u5047\u4fe1\u606f\uff0c\u7ed3\u5408\u4e86\u673a\u5668\u5b66\u4e60\u4ee3\u7406\uff08\u903b\u8f91\u56de\u5f52\uff09\u3001\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\u68c0\u67e5\u4ee3\u7406\uff08\u4f9d\u8d56\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff09\u3001\u8fde\u8d2f\u6027\u68c0\u6d4b\u4ee3\u7406\uff08\u4f7f\u7528LLM\u63d0\u793a\u5de5\u7a0b\uff09\u548c\u7f51\u7edc\u6293\u53d6\u6570\u636e\u5206\u6790\u4ee3\u7406\u63d0\u51fa\u7684\u56db\u4e2aAgent\u3002\u7cfb\u7edf\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u534f\u8c03\u5404\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u5171\u4eab\u4e0a\u4e0b\u6587\u548c\u52a8\u6001\u5b66\u4e60\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u591aAgent\u5408\u594f\u7cfb\u7edf\u5728F1\u5206\u6570\u4e3a0.964\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e8695.3%\u7684\u51c6\u786e\u5ea6\uff0c\u660e\u663e\u4f18\u4e8e\u5355\u4e2a\u4ee3\u7406\u548c\u4f20\u7edf\u65b9\u6cd5\u3002\u901a\u8fc7\u52a0\u6743\u805a\u5408\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u4e14\u6a21\u5757\u5316\u67b6\u6784\u4f7f\u5176\u6613\u4e8e\u6269\u5c55\u548c\u4fdd\u6301\u51b3\u7b56\u8fc7\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u591aAgent\u7cfb\u7edf\uff0c\u5229\u7528\u5173\u7cfb\u62bd\u53d6\u6280\u672f\u5728\u65b0\u95fb\u6587\u7ae0\u4e2d\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\uff0c\u5728\u6807\u9898\u548c\u77ed\u6587\u672c\u7247\u6bb5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u5ea6\u3002\u901a\u8fc7\u6574\u5408\u56db\u4e2aAgent\uff08\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u3001\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\u68c0\u67e5\u4ee3\u7406\u3001\u8fde\u8d2f\u6027\u68c0\u6d4b\u4ee3\u7406\u548c\u7f51\u7edc\u6293\u53d6\u6570\u636e\u5206\u6790\u4ee3\u7406\uff09\uff0c\u7cfb\u7edf\u5229\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u534f\u8c03\u5404\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u5171\u4eab\u4e0a\u4e0b\u6587\u548c\u52a8\u6001\u5b66\u4e60\u3002\u7ed3\u679c\u8868\u660e\uff0c\u591aAgent\u5408\u594f\u7cfb\u7edf\u5b9e\u73b0\u4e8695.3%\u7684\u51c6\u786e\u5ea6\uff0cF1\u5206\u6570\u8fbe\u52300.964\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e2aAgent\u548c\u4f20\u7edf\u65b9\u6cd5\u3002\u52a0\u6743\u805a\u5408\u65b9\u6cd5\u4f18\u4e8e\u7b97\u6cd5\u9608\u503c\u4f18\u5316\uff0c\u6a21\u5757\u5316\u67b6\u6784\u4f7f\u7cfb\u7edf\u6613\u4e8e\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u51b3\u7b56\u8fc7\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\u3002"}}
{"id": "2508.10146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10146", "abs": "https://arxiv.org/abs/2508.10146", "authors": ["Hana Derouiche", "Zaki Brahmi", "Haithem Mazeni"], "title": "Agentic AI Frameworks: Architectures, Protocols, and Design Challenges", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has ushered in a transformative\nparadigm in artificial intelligence, Agentic AI, where intelligent agents\nexhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent\ncoordination. This paper provides a systematic review and comparative analysis\nof leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,\nSemantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural\nprinciples, communication mechanisms, memory management, safety guardrails, and\nalignment with service-oriented computing paradigms. Furthermore, we identify\nkey limitations, emerging trends, and open challenges in the field. To address\nthe issue of agent communication, we conduct an in-depth analysis of protocols\nsuch as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network\nProtocol (ANP), and Agora. Our findings not only establish a foundational\ntaxonomy for Agentic AI systems but also propose future research directions to\nenhance scalability, robustness, and interoperability. This work serves as a\ncomprehensive reference for researchers and practitioners working to advance\nthe next generation of autonomous AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u9886\u5148\u7684\u4e3b\u4f53\u5316\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5ba1\u67e5\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u67b6\u6784\u539f\u7406\u3001\u901a\u4fe1\u673a\u5236\u7b49\u5404\u65b9\u9762\uff0c\u5e76\u5bf9\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002\u7814\u7a76\u7ed3\u679c\u5efa\u7acb\u4e86\u8fd9\u4e9b\u6846\u67b6\u7684\u57fa\u7840\u5206\u7c7b\uff0c\u53d1\u73b0\u4e86\u4e3b\u8981\u9650\u5236\u3001\u65b0\u5174\u8d8b\u52bf\u548c\u672a\u89e3\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u9886\u7684\u4e3b\u4f53\u5316\u4eba\u5de5\u667a\u80fd\u8303\u5f0f\u7684\u5174\u8d77\uff0c\u610f\u56fe\u5efa\u7acb\u5bf9Agentic AI\u7cfb\u7edf\u7684\u57fa\u7840\u5206\u7c7b\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5ba1\u67e5\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u9886\u5148Agentic AI\u6846\u67b6\u7684\u5404\u65b9\u9762\uff0c\u8fd8\u8fdb\u884c\u4e86\u5bf9\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\u7684\u6df1\u5165\u5206\u6790\u3002", "result": "\u5efa\u7acb\u4e86\u9886\u5148Agentic AI\u6846\u67b6\u7684\u57fa\u7840\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u7684\u4e3b\u8981\u9650\u5236\u3001\u65b0\u5174\u8d8b\u52bf\u548c\u672a\u89e3\u6311\u6218\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u9886\u5148\u7684\u201c\u4e3b\u4f53\u5316\u4eba\u5de5\u667a\u80fd\u201d\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u6027\u5ba1\u67e5\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u5b83\u4eec\u7684\u67b6\u6784\u539f\u7406\u3001\u901a\u4fe1\u673a\u5236\u3001\u5185\u5b58\u7ba1\u7406\u3001\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u4ee5\u53ca\u4e0e\u9762\u5411\u670d\u52a1\u8ba1\u7b97\u8303\u5f0f\u7684\u4e00\u81f4\u6027\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u8be5\u9886\u57df\u7684\u4e3b\u8981\u9650\u5236\u3001\u65b0\u5174\u8d8b\u52bf\u548c\u672a\u89e3\u6311\u6218\u3002\u7814\u7a76\u5bf9\u8c61\u6d89\u53ca\u7684\u6846\u67b6\u5305\u62ecCrewAI\u3001LangGraph\u3001AutoGen\u3001Semantic Kernel\u3001Agno\u3001Google ADK\u548cMetaGPT\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u89e3\u51b3\u4ee3\u7406\u901a\u4fe1\u95ee\u9898\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u589e\u5f3a\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u81f4\u529b\u4e8e\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\u3002"}}
{"id": "2508.10152", "categories": ["cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.10152", "abs": "https://arxiv.org/abs/2508.10152", "authors": ["Doaa Allabadi", "Kyle Bradbury", "Jordan M. Malof"], "title": "Improving and Evaluating Open Deep Research Agents", "comment": "8 pages, 2 figures, 2 tables", "summary": "We focus here on Deep Research Agents (DRAs), which are systems that can take\na natural language prompt from a user, and then autonomously search for, and\nutilize, internet-based content to address the prompt. Recent DRAs have\ndemonstrated impressive capabilities on public benchmarks however, recent\nresearch largely involves proprietary closed-source systems. At the time of\nthis work, we only found one open-source DRA, termed Open Deep Research (ODR).\nIn this work we adapt the challenging recent BrowseComp benchmark to compare\nODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),\ncomprising a subset of BrowseComp, as a more computationally-tractable DRA\nbenchmark for academic labs. We benchmark ODR and two other proprietary systems\non BC-Small: one system from Anthropic and one system from Google. We find that\nall three systems achieve 0% accuracy on the test set of 60 questions. We\nintroduce three strategic improvements to ODR, resulting in the ODR+ model,\nwhich achieves a state-of-the-art 10% success rate on BC-Small among both\nclosed-source and open-source systems. We report ablation studies indicating\nthat all three of our improvements contributed to the success of ODR+.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86ODR\u548c\u4e24\u4e2a\u4e13\u6709\u7cfb\u7edf\u5728BC-Small\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e09\u4e2a\u7cfb\u7edf\u572860\u4e2a\u95ee\u9898\u7684\u6d4b\u8bd5\u96c6\u4e0a\u5747\u672a\u83b7\u5f97\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5f15\u5165\u4e09\u9879\u6218\u7565\u6539\u8fdb\uff0c\u63d0\u51fa\u4e86ODR+\u6a21\u578b\uff0c\u5176\u53d6\u5f97\u4e8610%\u7684\u6210\u529f\u7387\uff0c\u662f\u9886\u5148\u7684\u7cfb\u7edf\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u8fd9\u4e09\u9879\u6539\u8fdb\u5bf9ODR+\u7684\u6210\u529f\u5f71\u54cd\u91cd\u5927\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRAs\uff09\u5728\u5904\u7406\u7528\u6237\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u6d89\u53ca\u4e13\u6709\u95ed\u6e90\u7cfb\u7edf\u3002\u9274\u4e8e\u76ee\u524d\u4ec5\u6709\u4e00\u4e2a\u5f00\u6e90\u7684DRA\u7cfb\u7edf\uff08ODR\uff09\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83ODR\u548c\u73b0\u6709\u4e13\u6709\u7cfb\u7edf\u5728BrowseComp-Small\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u63a8\u52a8\u5b66\u672f\u754c\u5728\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5728BrowseComp-Small\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83ODR\u548c\u4e24\u4e2a\u4e13\u6709\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86ODR+\u6a21\u578b\uff0c\u5e76\u9488\u5bf9\u6539\u8fdb\u6548\u679c\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\u3002", "result": "\u672c\u7814\u7a76\u53d1\u73b0\u5728BC-Small\u6d4b\u8bd5\u96c6\u4e0a\uff0cODR\u3001Anthropic\u548cGoogle\u8fd9\u4e09\u4e2a\u7cfb\u7edf\u572860\u4e2a\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u6027\u5747\u4e3a0%\u3002\u901a\u8fc7\u5f15\u5165\u4e09\u9879\u6218\u7565\u6539\u8fdb\uff0cODR+\u6a21\u578b\u5728BC-Small\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e8610%\u7684\u6210\u529f\u7387\uff0c\u662f\u5f53\u524d\u95ed\u6e90\u548c\u5f00\u6e90\u7cfb\u7edf\u4e2d\u7684\u9886\u5148\u8005\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u8fd9\u4e09\u9879\u6539\u8fdb\u5bf9ODR+\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5728BrowseComp-Small\u57fa\u51c6\u4e0a\u5bf9\u6bd4ODR\u548c\u4e24\u4e2a\u4e13\u6709\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u4e09\u4e2a\u7cfb\u7edf\u572860\u4e2a\u95ee\u9898\u7684\u6d4b\u8bd5\u96c6\u4e0a\u5747\u672a\u83b7\u5f97\u51c6\u786e\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u9879\u6218\u7565\u6539\u8fdb\u63aa\u65bd\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u5230ODR\u6a21\u578b\u4e2d\uff0c\u5f97\u5230\u4e86ODR+\u6a21\u578b\uff0c\u5176\u5728\u5b66\u672f\u5b9e\u9a8c\u5ba4\u4e2d\u8fbe\u5230\u4e8610%\u7684\u6210\u529f\u7387\uff0c\u5728\u95ed\u6e90\u548c\u5f00\u6e90\u7cfb\u7edf\u4e2d\u5747\u5904\u4e8e\u9886\u5148\u5730\u4f4d\u3002\u6211\u4eec\u7684\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e09\u9879\u6539\u8fdb\u5747\u5bf9ODR+\u7684\u6210\u529f\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2508.10164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10164", "abs": "https://arxiv.org/abs/2508.10164", "authors": ["Bin Hong", "Jiayu Liu", "Zhenya Huang", "Kai Zhang", "Mengdi Zhang"], "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization", "comment": "19 pages, 5 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated strong\nperformance on complex tasks through long Chain-of-Thought (CoT) reasoning.\nHowever, their lengthy outputs increase computational costs and may lead to\noverthinking, raising challenges in balancing reasoning effectiveness and\nefficiency. Current methods for efficient reasoning often compromise reasoning\nquality or require extensive resources. This paper investigates efficient\nmethods to reduce the generation length of LRMs. We analyze generation path\ndistributions and filter generated trajectories through difficulty estimation.\nSubsequently, we analyze the convergence behaviors of the objectives of various\npreference optimization methods under a Bradley-Terry loss based framework.\nBased on the analysis, we propose Length Controlled Preference Optimization\n(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can\neffectively learn length preference with limited data and training. Extensive\nexperiments demonstrate that our approach significantly reduces the average\noutput length by over 50\\% across multiple benchmarks while maintaining the\nreasoning performance. Our work highlights the potential for computationally\nefficient approaches in guiding LRMs toward efficient reasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLength Controlled Preference Optimization (LCPO)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u751f\u6210\u8def\u5f84\u5206\u5e03\u548c\u8fc7\u6ee4\u901a\u8fc7\u96be\u5ea6\u4f30\u8ba1\u7b5b\u9009\u751f\u6210\u8f68\u8ff9\uff0c\u51cf\u5c11LRM\u7684\u751f\u6210\u957f\u5ea6\u3002LCPO\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861NLL\u635f\u5931\u76f8\u5173\u7684\u9690\u5f0f\u5956\u52b1\uff0c\u6709\u6548\u5b66\u4e60\u957f\u5ea6\u504f\u597d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u4f7fLRM\u7684\u5e73\u5747\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u8d85\u8fc750%\u3002", "motivation": "LRM\u7684\u957f\u8f93\u51fa\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6709\u6548\u65b9\u6cd5\u4ee5\u51cf\u5c11LRM\u7684\u751f\u6210\u957f\u5ea6\uff0c\u5f53\u524d\u7684\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u901a\u5e38\u8981\u4e48\u964d\u4f4e\u63a8\u7406\u8d28\u91cf\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u8d44\u6e90\uff0c\u5b58\u5728\u5e73\u8861\u63a8\u7406\u6548\u679c\u548c\u6548\u7387\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u6790\u751f\u6210\u8def\u5f84\u5206\u5e03\u5e76\u901a\u8fc7\u96be\u5ea6\u4f30\u8ba1\u7b5b\u9009\u751f\u6210\u7684\u8f68\u8ff9\uff0c\u5206\u6790\u4e0d\u540c\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5728\u57fa\u4e8eBradley-Terry\u635f\u5931\u6846\u67b6\u4e0b\u7684\u76ee\u6807\u6536\u655b\u884c\u4e3a\uff0c\u63d0\u51faLength Controlled Preference Optimization (LCPO)\u65b9\u6cd5\uff0c\u76f4\u63a5\u5e73\u8861\u4e0eNLL\u635f\u5931\u76f8\u5173\u7684\u9690\u5f0f\u5956\u52b1\uff0c\u4ece\u800c\u964d\u4f4eLRMs\u7684\u751f\u6210\u957f\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u9a8c\u8bc1\u660eLCPO\u663e\u8457\u51cf\u5c11LRM\u7684\u5e73\u5747\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0Length Controlled Preference Optimization (LCPO) \u53ef\u4ee5\u5728\u6709\u9650\u7684\u6570\u636e\u548c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u6709\u6548\u5b66\u4e60\u957f\u5ea6\u504f\u597d\uff0c\u663e\u8457\u51cf\u5c11LRM\u7684\u5e73\u5747\u8f93\u51fa\u957f\u5ea6\u8d85\u8fc750%\u5e76\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2508.10177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10177", "abs": "https://arxiv.org/abs/2508.10177", "authors": ["Stepan Kulibaba", "Artem Dzhalilov", "Roman Pakhomov", "Oleg Svidchenko", "Alexander Gasnikov", "Aleksei Shpilman"], "title": "KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems", "comment": null, "summary": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive\ncapabilities but face significant limitations such as constrained exploration\nstrategies and a severe execution bottleneck. Exploration is hindered by\none-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)\napproaches that fail to recombine strong partial solutions. The execution\nbottleneck arises from lengthy code validation cycles that stifle iterative\nrefinement. To overcome these challenges, we introduce KompeteAI, a novel\nAutoML framework with dynamic solution space exploration. Unlike previous MCTS\nmethods that treat ideas in isolation, KompeteAI introduces a merging stage\nthat composes top candidates. We further expand the hypothesis space by\nintegrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle\nnotebooks and arXiv papers to incorporate real-world strategies. KompeteAI also\naddresses the execution bottleneck via a predictive scoring model and an\naccelerated debugging method, assessing solution potential using early stage\nmetrics to avoid costly full-code execution. This approach accelerates pipeline\nevaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,\nAIDE, and Ml-Master) by an average of 3\\% on the primary AutoML benchmark,\nMLE-Bench. Additionally, we propose Kompete-bench to address limitations in\nMLE-Bench, where KompeteAI also achieves state-of-the-art results", "AI": {"tldr": "KompeteAI is a novel AutoML framework that enhances solution space exploration, integrates Retrieval-Augmented Generation, and improves pipeline evaluation speed. It outperforms existing methods by 3% on the primary AutoML benchmark and introduces a new benchmark, Kompete-bench, achieving state-of-the-art results.", "motivation": "To overcome limitations in existing Large Language Model (LLM)-based AutoML systems, including constrained exploration strategies and severe execution bottlenecks. The goal is to enhance exploration diversity, improve recombination of partial solutions, and accelerate pipeline evaluation.", "method": "Introducing KompeteAI, a novel AutoML framework with dynamic solution space exploration, a merging stage for composing top candidates, integration of Retrieval-Augmented Generation (RAG), and addressing the execution bottleneck with a predictive scoring model and accelerated debugging method.", "result": "KompeteAI accelerates pipeline evaluation 6.9 times and outperforms leading methods by an average of 3% on the primary AutoML benchmark, MLE-Bench. Additionally, a new benchmark, Kompete-bench, is proposed to address limitations in MLE-Bench, where KompeteAI achieves state-of-the-art results.", "conclusion": "KompeteAI is introduced as a novel AutoML framework with dynamic solution space exploration, integrating Retrieval-Augmented Generation (RAG) and addressing the execution bottleneck through a predictive scoring model and accelerated debugging method, outperforming leading methods by an average of 3% on the primary AutoML benchmark."}}
{"id": "2508.10241", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10241", "abs": "https://arxiv.org/abs/2508.10241", "authors": ["Mark Zilberman"], "title": "Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence", "comment": "10 pages", "summary": "This work demonstrates how the concept of the entropic potential of events --\na parameter quantifying the influence of discrete events on the expected future\nentropy of a system -- can enhance uncertainty quantification, decision-making,\nand interpretability in artificial intelligence (AI). Building on its original\nformulation in physics, the framework is adapted for AI by introducing an\nevent-centric measure that captures how actions, observations, or other\ndiscrete occurrences impact uncertainty at future time horizons. Both the\noriginal and AI-adjusted definitions of entropic potential are formalized, with\nthe latter emphasizing conditional expectations to account for counterfactual\nscenarios. Applications are explored in policy evaluation, intrinsic reward\ndesign, explainable AI, and anomaly detection, highlighting the metric's\npotential to unify and strengthen uncertainty modeling in intelligent systems.\nConceptual examples illustrate its use in reinforcement learning, Bayesian\ninference, and anomaly detection, while practical considerations for\ncomputation in complex AI models are discussed. The entropic potential\nframework offers a theoretically grounded, interpretable, and versatile\napproach to managing uncertainty in AI, bridging principles from\nthermodynamics, information theory, and machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5c06\u4e8b\u4ef6\u7684\u71b5\u52bf\u6982\u5ff5\u5e94\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\u4e2d\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u5236\u5b9a\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u71b5\u52bf\u6846\u67b6\u5728\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u63a8\u65ad\u3001\u5f02\u5e38\u68c0\u6d4b\u7b49\u65b9\u9762\u5c55\u793a\u4e86\u5176\u6f5c\u529b\uff0c\u4e3a\u5904\u7406\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u57fa\u7840\u3001\u53ef\u89e3\u91ca\u548c\u591a\u529f\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u8be5\u8bba\u6587\u7684\u52a8\u673a\u5728\u4e8e\u5229\u7528\u71b5\u52bf\u6982\u5ff5\u6765\u52a0\u5f3a\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u5236\u5b9a\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5f15\u5165\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u4f7f\u5176\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4e2d\u5f97\u4ee5\u9002\u7528\u548c\u53d1\u5c55\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u5bf9\u4e8b\u4ef6\u7684\u71b5\u52bf\u6982\u5ff5\u8fdb\u884c\u9002\u5e94\uff0c\u5f15\u5165\u4e86\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5f62\u5f0f\u5316\u4e86\u71b5\u52bf\u7684\u539f\u59cb\u5b9a\u4e49\u548c\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u8c03\u6574\u5b9a\u4e49\uff0c\u5f3a\u8c03\u6761\u4ef6\u671f\u671b\u4ee5\u8003\u8651\u53cd\u4e8b\u5b9e\u60c5\u666f\u3002\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u590d\u6742\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u4e2d\u8fdb\u884c\u8ba1\u7b97\u7684\u5b9e\u9645\u8003\u8651\uff0c\u6db5\u76d6\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u65b9\u9762\u7684\u6982\u5ff5\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u71b5\u52bf\u6846\u67b6\u5728\u7ba1\u7406\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u71b5\u52bf\u6846\u67b6\u5728\u667a\u80fd\u7cfb\u7edf\u4e2d\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5177\u6709\u7406\u8bba\u4e0a\u7684\u57fa\u7840\u3001\u53ef\u89e3\u91ca\u548c\u591a\u529f\u80fd\u7684\u4f18\u70b9\uff0c\u53ef\u4ee5\u7edf\u4e00\u5e76\u52a0\u5f3a\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u4e8b\u4ef6\u7684\u71b5\u52bf\u6982\u5ff5\u6765\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u5236\u5b9a\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u5ea6\u91cf\u6807\u51c6\u6765\u9002\u5e94\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u5f62\u5f0f\u5316\u4e86\u71b5\u52bf\u7684\u539f\u59cb\u5b9a\u4e49\u548c\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u8c03\u6574\u5b9a\u4e49\uff0c\u5e94\u7528\u4e8e\u653f\u7b56\u8bc4\u4f30\u3001\u5185\u5728\u5956\u52b1\u8bbe\u8ba1\u3001\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u65b9\u9762\uff0c\u7a81\u51fa\u4e86\u8be5\u5ea6\u91cf\u6807\u51c6\u5728\u667a\u80fd\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u57fa\u7840\u3001\u53ef\u89e3\u91ca\u548c\u591a\u529f\u80fd\u7684\u7ba1\u7406\u4eba\u5de5\u667a\u80fd\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.10265", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.10265", "abs": "https://arxiv.org/abs/2508.10265", "authors": ["Jingde Cheng"], "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?", "comment": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2412.12408", "summary": "Recently, with the application progress of AIGC tools based on large language\nmodels (LLMs), led by ChatGPT, many AI experts and more non-professionals are\ntrumpeting the \"understanding ability\" and \"reasoning ability\" of the LLMs. The\npresent author considers that the so-called \"understanding ability\" and\n\"reasoning ability\" of LLMs are just illusions of those people who with vague\nconcepts. In fact, the LLMs can never have the true understanding ability and\ntrue reasoning ability. This paper intents to explain that, because the\nessential limitations of their working principle, the LLMs can never have the\nability of true correct reasoning.", "AI": {"tldr": "The paper argues that the understanding and reasoning abilities attributed to LLMs are illusory and based on vague concepts, stating that these models cannot attain true understanding and reasoning due to essential limitations in their working principles.", "motivation": "To clarify that the so-called \"understanding ability\" and \"reasoning ability\" of LLMs are illusions based on vague concepts.", "method": "Not specified.", "result": "The paper explains that LLMs cannot possess the ability of true correct reasoning.", "conclusion": "LLMs cannot have true understanding and reasoning ability due to their essential limitations in working principles."}}
{"id": "2508.10293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10293", "abs": "https://arxiv.org/abs/2508.10293", "authors": ["Chuhuai Yue", "Chengqi Dong", "Yinan Gao", "Hang He", "Jiajun Chai", "Guojun Yin", "Wei Lin"], "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward", "comment": null, "summary": "Large reasoning models (LRMs) have recently achieved significant progress in\ncomplex reasoning tasks, aided by reinforcement learning with verifiable\nrewards. However, LRMs often suffer from overthinking, expending excessive\ncomputation on simple problems and reducing efficiency. Existing efficient\nreasoning methods typically require accurate task assessment to preset token\nbudgets or select reasoning modes, which limits their flexibility and\nreliability. In this work, we revisit the essence of overthinking and identify\nthat encouraging effective steps while penalizing ineffective ones is key to\nits solution. To this end, we propose a novel rule-based verifiable stepwise\nreward mechanism (VSRM), which assigns rewards based on the performance of\nintermediate states in the reasoning trajectory. This approach is intuitive and\nnaturally fits the step-by-step nature of reasoning tasks. We conduct extensive\nexperiments on standard mathematical reasoning benchmarks, including AIME24 and\nAIME25, by integrating VSRM with PPO and Reinforce++. Results show that our\nmethod achieves substantial output length reduction while maintaining original\nreasoning performance, striking an optimal balance between efficiency and\naccuracy. Further analysis of overthinking frequency and pass@k score before\nand after training demonstrates that our approach in deed effectively\nsuppresses ineffective steps and encourages effective reasoning, fundamentally\nalleviating the overthinking problem. All code will be released upon\nacceptance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u9a8c\u8bc1\u9010\u6b65\u5956\u52b1\u673a\u5236\uff08VSRM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002VSRM\u901a\u8fc7\u5206\u914d\u57fa\u4e8e\u63a8\u7406\u8f68\u8ff9\u4e2d\u4e2d\u95f4\u72b6\u6001\u8868\u73b0\u7684\u5956\u52b1\uff0c\u5b9e\u73b0\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u5e76\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cVSRM\u7ed3\u5408PPO\u548cReinforce++\u53d6\u5f97\u4e86\u663e\u8457\u7ed3\u679c\uff0c\u6709\u6548\u7f13\u89e3\u4e86LRMs\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "motivation": "LRMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u7ecf\u5e38\u9762\u4e34\u8fc7\u5ea6\u601d\u8003\u7684\u95ee\u9898\uff0c\u6d6a\u8d39\u8fc7\u591a\u8ba1\u7b97\u8d44\u6e90\u3002\u73b0\u6709\u7684\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u51c6\u786e\u7684\u4efb\u52a1\u8bc4\u4f30\u6765\u9884\u8bbe\u4ee3\u5e01\u9884\u7b97\u6216\u9009\u62e9\u63a8\u7406\u6a21\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u91cd\u65b0\u601d\u8003\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u7684\u672c\u8d28\u5bf9\u4e8e\u89e3\u51b3LRMs\u7684\u6548\u7387\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86\u8fc7\u5ea6\u601d\u8003\u7684\u672c\u8d28\uff0c\u63d0\u51fa\u9f13\u52b1\u6709\u6548\u6b65\u9aa4\u5e76\u60e9\u7f5a\u65e0\u6548\u6b65\u9aa4\u7684\u65b9\u6848\u3002\u901a\u8fc7\u5f15\u5165VSRM\u673a\u5236\uff0c\u6839\u636e\u63a8\u7406\u8f68\u8ff9\u4e2d\u4e2d\u95f4\u72b6\u6001\u7684\u8868\u73b0\u5206\u914d\u5956\u52b1\uff0c\u4ee5\u5b9e\u73b0\u8be5\u76ee\u6807\u3002\u7814\u7a76\u91c7\u7528\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86VSRM\u4e0ePPO\u548cReinforce++\u7684\u96c6\u6210\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cVSRM\u5728\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u79ef\u6781\u6210\u679c\uff0c\u5b9e\u73b0\u4e86\u8f93\u51fa\u957f\u5ea6\u7684\u663e\u8457\u7f29\u51cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u63a8\u7406\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u65e0\u6548\u6b65\u9aa4\u7684\u540c\u65f6\u9f13\u52b1\u6709\u6548\u63a8\u7406\uff0c\u4ece\u6839\u672c\u4e0a\u7f13\u89e3\u4e86\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u9a8c\u8bc1\u9010\u6b65\u5956\u52b1\u673a\u5236\uff08VSRM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u63a8\u7406\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cVSRM\u7ed3\u5408PPO\u548cReinforce++\u5728\u6807\u51c6\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2508.10337", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10337", "abs": "https://arxiv.org/abs/2508.10337", "authors": ["Chenliang Zhang", "Lin Wang", "Yuanyuan Lu", "Yusheng Qi", "Kexin Wang", "Peixu Hou", "Wenshi Chen"], "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering", "comment": null, "summary": "This paper describes the solutions of the Dianping-Trust-Safety team for the\nMETA CRAG-MM challenge. The challenge requires building a comprehensive\nretrieval-augmented generation system capable for multi-modal multi-turn\nquestion answering. The competition consists of three tasks: (1) answering\nquestions using structured data retrieved from an image-based mock knowledge\ngraph, (2) synthesizing information from both knowledge graphs and web search\nresults, and (3) handling multi-turn conversations that require context\nunderstanding and information aggregation from multiple sources. For Task 1,\nour solution is based on the vision large language model, enhanced by\nsupervised fine-tuning with knowledge distilled from GPT-4.1. We further\napplied curriculum learning strategies to guide reinforcement learning,\nresulting in improved answer accuracy and reduced hallucination. For Task 2 and\nTask 3, we additionally leveraged web search APIs to incorporate external\nknowledge, enabling the system to better handle complex queries and multi-turn\nconversations. Our approach achieved 1st place in Task 1 with a significant\nlead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of\nthe integration of curriculum learning with reinforcement learning in our\ntraining pipeline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86Dianping-Trust-Safety\u56e2\u961f\u5728META CRAG-MM\u6311\u6218\u4e2d\u53d6\u5f97\u7684\u6210\u5c31\u3002\u4ed6\u4eec\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u3001\u8bfe\u7a0b\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u865a\u6784\u5185\u5bb9\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8868\u73b0\u3002", "motivation": "\u7ade\u8d5b\u8981\u6c42\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u5177\u6709\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u7684\u80fd\u529b\u3002\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u7684\u5173\u952e\u5728\u4e8e\u6574\u5408\u7ed3\u6784\u5316\u6570\u636e\u548c\u5916\u90e8\u77e5\u8bc6\u6e90\uff0c\u4ee5\u53ca\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u4fe1\u606f\u6574\u5408\u3002\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u7efc\u5408\u5229\u7528\u89c6\u89c9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u8bfe\u7a0b\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u3002", "method": "\u56e2\u961f\u89e3\u51b3\u65b9\u6848\u57fa\u4e8e\u89c6\u89c9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6765\u81eaGPT-4.1\u7684\u77e5\u8bc6\u63d0\u70bc\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002\u4ed6\u4eec\u8fd8\u5e94\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u865a\u6784\u5185\u5bb9\u3002\u5728\u4efb\u52a12\u548c\u4efb\u52a13\u4e2d\uff0c\u4ed6\u4eec\u8fd8\u5229\u7528\u7f51\u7edc\u641c\u7d22API\uff0c\u878d\u5165\u5916\u90e8\u77e5\u8bc6\uff0c\u4f7f\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u591a\u8f6e\u5bf9\u8bdd\u3002", "result": "\u4ed6\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u5728\u4efb\u52a11\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u7684\u663e\u8457\u9886\u5148\u4f18\u52bf\uff0c\u5e76\u5728\u4efb\u52a13\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e09\u540d\u3002\u4ed6\u4eec\u5c55\u793a\u4e86\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u96c6\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86Dianping-Trust-Safety\u56e2\u961f\u9488\u5bf9META CRAG-MM\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4ed6\u4eec\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u7684\u80fd\u529b\u3002\u56e2\u961f\u5728\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u548c\u7b2c\u4e09\u540d\u7684\u663e\u7740\u6210\u7ee9\uff0c\u5c55\u793a\u4e86\u4ed6\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.10340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10340", "abs": "https://arxiv.org/abs/2508.10340", "authors": ["Chak Lam Shek", "Guangyao Shi", "Pratap Tokekar"], "title": "Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) requires coordinated and stable\npolicy updates among interacting agents. Heterogeneous-Agent Trust Region\nPolicy Optimization (HATRPO) enforces per-agent trust region constraints using\nKullback-Leibler (KL) divergence to stabilize training. However, assigning each\nagent the same KL threshold can lead to slow and locally optimal updates,\nespecially in heterogeneous settings. To address this limitation, we propose\ntwo approaches for allocating the KL divergence threshold across agents:\nHATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes\nthreshold assignment under global KL constraints, and HATRPO-G, a greedy\nalgorithm that prioritizes agents based on improvement-to-divergence ratio. By\nconnecting sequential policy optimization with constrained threshold\nscheduling, our approach enables more flexible and effective learning in\nheterogeneous-agent settings. Experimental results demonstrate that our methods\nsignificantly boost the performance of HATRPO, achieving faster convergence and\nhigher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and\nHATRPO-G achieve comparable improvements in final performance, each exceeding\n22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as\nreflected by its lower variance.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1aHATRPO-W \u548c HATRPO-G\uff0c\u7528\u4e8e\u6539\u8fdb HATRPO \u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u52a0\u5feb\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6700\u7ec8\u5956\u52b1\uff0c\u5728\u5e7f\u6cdb\u7684 MARL \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "MARL \u9700\u8981\u5728\u4e92\u52a8\u667a\u80fd\u4f53\u4e4b\u95f4\u5b9e\u73b0\u534f\u8c03\u548c\u7a33\u5b9a\u7684\u7b56\u7565\u66f4\u65b0\u3002\u73b0\u6709\u7684 HATRPO \u65b9\u6cd5\u5728\u5f02\u6784\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u4f1a\u5bfc\u81f4\u66f4\u65b0\u7f13\u6162\u548c\u5c40\u90e8\u6700\u4f18\u89e3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63d0\u51fa\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1aHATRPO-W \u548c HATRPO-G\uff0c\u7528\u4e8e\u5728\u5168\u5c40 KL \u7ea6\u675f\u4e0b\u4f18\u5316\u9608\u503c\u5206\u914d\u4ee5\u53ca\u57fa\u4e8e\u6539\u8fdb-\u6563\u5ea6\u6bd4\u7387\u5bf9\u4ee3\u7406\u8fdb\u884c\u4f18\u5148\u6392\u5e8f\u3002\u901a\u8fc7\u8fde\u63a5\u987a\u5e8f\u7b56\u7565\u4f18\u5316\u548c\u7ea6\u675f\u9608\u503c\u8c03\u5ea6\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u5f02\u6784\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cHATRPO-W \u548c HATRPO-G \u663e\u8457\u63d0\u5347\u4e86 HATRPO \u7684\u6027\u80fd\uff0c\u5728\u591a\u6837\u7684 MARL \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u5956\u52b1\u3002\u5177\u4f53\u5730\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u8d85\u8fc7\u4e86 22.5% \u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14 HATRPO-W \u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u52a8\u6001\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u5206\u914d\u591a\u667a\u80fd\u4f53\u95f4\u7684 KL \u6563\u5ea6\u9608\u503c\uff1aHATRPO-W \u548c HATRPO-G\uff0c\u5b83\u4eec\u663e\u7740\u63d0\u9ad8\u4e86 HATRPO \u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u5956\u52b1\u3002\u5728\u591a\u6837\u7684 MARL \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHATRPO-W \u548c HATRPO-G\u5206\u522b\u63d0\u5347\u8d85\u8fc7 22.5%\uff0c\u5176\u4e2d HATRPO-W \u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u52a8\u6001\u3002"}}
{"id": "2508.10358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10358", "abs": "https://arxiv.org/abs/2508.10358", "authors": ["Mengtao Zhou", "Sifan Wu", "Huan Zhang", "Qi Sima", "Bang Liu"], "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles", "comment": null, "summary": "We investigate the capacity of Large Language Models (LLMs) for imaginative\nreasoning--the proactive construction, testing, and revision of hypotheses in\ninformation-sparse environments. Existing benchmarks, often static or focused\non social deduction, fail to capture the dynamic, exploratory nature of this\nreasoning process. To address this gap, we introduce a comprehensive research\nframework based on the classic \"Turtle Soup\" game, integrating a benchmark, an\nagent, and an evaluation protocol. We present TurtleSoup-Bench, the first\nlarge-scale, bilingual, interactive benchmark for imaginative reasoning,\ncomprising 800 turtle soup puzzles sourced from both the Internet and expert\nauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'\nperformance in this setting. To evaluate reasoning quality, we develop a\nmulti-dimensional protocol measuring logical consistency, detail completion,\nand conclusion alignment. Experiments with leading LLMs reveal clear capability\nlimits, common failure patterns, and a significant performance gap compared to\nhumans. Our work offers new insights into LLMs' imaginative reasoning and\nestablishes a foundation for future research on exploratory agent behavior.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60f3\u8c61\u6027\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u901a\u8fc7\u4f7f\u7528\u201cTurtle Soup\u201d\u6e38\u620f\u4e3a\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u7efc\u5408\u7814\u7a76\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u51c6\u6d4b\u8bd5\u3001\u667a\u80fd\u4f53\u548c\u8bc4\u4f30\u534f\u8bae\u3002\u5f15\u5165\u4e86TurtleSoup-Bench\u4f5c\u4e3a\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u53cc\u8bed\u3001\u4e92\u52a8\u7684\u60f3\u8c61\u6027\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53caMosaic-Agent\u6765\u8bc4\u4f30LLMs\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0LLMs\u5b58\u5728\u660e\u663e\u7684\u80fd\u529b\u9650\u5236\u548c\u4e0e\u4eba\u7c7b\u6027\u80fd\u7684\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u9759\u6001\u6216\u4e13\u6ce8\u4e8e\u793e\u4ea4\u63a8\u7406\uff0c\u65e0\u6cd5\u6355\u6349\u5230\u60f3\u8c61\u6027\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u3001\u63a2\u7d22\u6027\u8d28\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u201cTurtle Soup\u201d\u6e38\u620f\u7684\u7efc\u5408\u7814\u7a76\u6846\u67b6\uff0c\u65e8\u5728\u63a2\u8ba8LLMs\u5728\u60f3\u8c61\u6027\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u7ecf\u5178\u7684\u201cTurtle Soup\u201d\u6e38\u620f\u4f5c\u4e3a\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u7efc\u5408\u7684\u7814\u7a76\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u51c6\u6d4b\u8bd5\u3001\u667a\u80fd\u4f53\u548c\u8bc4\u4f30\u534f\u8bae\u3002\u5f15\u5165\u4e86TurtleSoup-Bench\u4f5c\u4e3a\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u53cc\u8bed\u3001\u4e92\u52a8\u7684\u60f3\u8c61\u6027\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53caMosaic-Agent\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u667a\u80fd\u4f53\u6765\u8bc4\u4f30LLMs\u5728\u6b64\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002\u4e3a\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\uff0c\u5f00\u53d1\u4e86\u591a\u7ef4\u5ea6\u534f\u8bae\uff0c\u5305\u62ec\u903b\u8f91\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u5b8c\u6574\u6027\u548c\u7ed3\u8bba\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u9886\u5148\u7684LLMs\u5b58\u5728\u660e\u663e\u7684\u80fd\u529b\u9650\u5236\u548c\u5e38\u89c1\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5728\u60f3\u8c61\u6027\u63a8\u7406\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u60f3\u8c61\u6027\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\uff0c\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u4e0a\u9650\u548c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2508.10391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10391", "abs": "https://arxiv.org/abs/2508.10391", "authors": ["Yaoze Zhang", "Rong Wu", "Pinlong Cai", "Xiaoman Wang", "Guohang Yan", "Song Mao", "Ding Wang", "Botian Shi"], "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large\nLanguage Models by leveraging external knowledge, whereas the effectiveness is\noften compromised by the retrieval of contextually flawed or incomplete\ninformation. To address this, knowledge graph-based RAG methods have evolved\ntowards hierarchical structures, organizing knowledge into multi-level\nsummaries. However, these approaches still suffer from two critical,\nunaddressed challenges: high-level conceptual summaries exist as disconnected\n``semantic islands'', lacking the explicit relations needed for cross-community\nreasoning; and the retrieval process itself remains structurally unaware, often\ndegenerating into an inefficient flat search that fails to exploit the graph's\nrich topology. To overcome these limitations, we introduce LeanRAG, a framework\nthat features a deeply collaborative design combining knowledge aggregation and\nretrieval strategies. LeanRAG first employs a novel semantic aggregation\nalgorithm that forms entity clusters and constructs new explicit relations\namong aggregation-level summaries, creating a fully navigable semantic network.\nThen, a bottom-up, structure-guided retrieval strategy anchors queries to the\nmost relevant fine-grained entities and then systematically traverses the\ngraph's semantic pathways to gather concise yet contextually comprehensive\nevidence sets. The LeanRAG can mitigate the substantial overhead associated\nwith path retrieval on graphs and minimizes redundant information retrieval.\nExtensive experiments on four challenging QA benchmarks with different domains\ndemonstrate that LeanRAG significantly outperforming existing methods in\nresponse quality while reducing 46\\% retrieval redundancy. Code is available\nat: https://github.com/RaZzzyz/LeanRAG", "AI": {"tldr": "LeanRAG addresses limitations of existing knowledge graph-based RAG methods by introducing a collaborative framework for knowledge aggregation and retrieval strategies. It significantly improves response quality and reduces retrieval redundancy by 46% in experiments across different QA benchmarks.", "motivation": "Existing knowledge graph-based RAG methods face challenges like disconnected conceptual summaries and inefficient retrieval processes. LeanRAG aims to overcome these limitations by introducing a collaborative framework for knowledge aggregation and retrieval to enhance response quality and reduce redundancy in information retrieval.", "method": "LeanRAG employs a semantic aggregation algorithm to form entity clusters and create explicit relations among aggregation-level summaries, establishing a navigable semantic network. It also utilizes a structure-guided retrieval strategy that anchors queries to relevant fine-grained entities and traverses the graph's semantic pathways efficiently.", "result": "LeanRAG demonstrates superior performance in response quality compared to existing methods while reducing retrieval redundancy by 46%. The framework is evaluated on four QA benchmarks with different domains, showcasing its effectiveness across various tasks.", "conclusion": "LeanRAG is introduced to address the limitations of knowledge graph-based RAG methods by incorporating a collaborative design for knowledge aggregation and retrieval strategies. It significantly outperforms existing methods in response quality and reduces retrieval redundancy by 46%."}}
{"id": "2508.10425", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10425", "abs": "https://arxiv.org/abs/2508.10425", "authors": ["Yan Ting Chok", "Soyon Park", "Seungheun Baek", "Hajung Kim", "Junhyun Lee", "Jaewoo Kang"], "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation", "comment": null, "summary": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings.", "AI": {"tldr": "HiRef\u6846\u67b6\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u548c\u771f\u5b9eEHR\u6570\u636e\uff0c\u7528\u4e8e\u7a33\u5065\u7684\u836f\u7269\u63a8\u8350\u3002\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u672a\u77e5\u4ee3\u7801\u73af\u5883\u4e0b\u4e5f\u7ef4\u6301\u9ad8\u51c6\u786e\u6027\u3002\u5f15\u5165\u4e86\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6848\u548c\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5b58\u5728\u7f55\u89c1\u7684\u533b\u7597\u5b9e\u4f53\u548c\u4e0d\u5b8c\u6574\u8bb0\u5f55\uff0c\u4f7f\u5f97\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u7f3a\u5931\u6216\u65b0\u9896\u6761\u4ef6\u4e0b\u6cdb\u5316\u56f0\u96be\u3002\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4e8e\u89c2\u5bdf\u5230\u7684\u5171\u73b0\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u548c\u771f\u5b9eEHR\u6570\u636e\u7684\u6846\u67b6HiRef\uff0c\u4ee5\u63d0\u9ad8\u836f\u7269\u63a8\u8350\u7684\u7a33\u5065\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6HiRef\uff0c\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u7684\u5c42\u6b21\u8bed\u4e49\u548c\u771f\u5b9eEHR\u6570\u636e\u63a8\u5bfc\u7684\u7cbe\u7ec6\u5316\u5171\u73b0\u6a21\u5f0f\u3002\u901a\u8fc7\u5c06\u672c\u4f53\u5b9e\u4f53\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u4ee3\u7801\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5f15\u5165\u4e86\u57fa\u4e8e\u5148\u9a8c\u5f15\u5bfc\u7684\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6848\uff0c\u7528\u4e8e\u7ec6\u5316EHR\u5171\u73b0\u56fe\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7a33\u5065\u6027\u3002\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u5168\u9762\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86HiRef\u5bf9\u672a\u77e5\u533b\u5b66\u4ee3\u7801\u7684\u9c81\u68d2\u6027\u3002", "result": "\u6a21\u578b\u5728EHR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u5bf9\u672a\u77e5\u4ee3\u7801\u73af\u5883\u4e0b\u7684\u51c6\u786e\u6027\u4e5f\u5f88\u9ad8\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u5bf9\u5b66\u4e60\u5230\u7684\u7a00\u758f\u56fe\u7ed3\u6784\u548c\u533b\u5b66\u4ee3\u7801\u5d4c\u5165\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiRef\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7a33\u5065\u7684\u836f\u7269\u63a8\u8350\uff0c\u901a\u8fc7\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u548c\u771f\u5b9eEHR\u6570\u636e\u4e2d\u63a8\u5bfc\u7684\u7cbe\u7ec6\u5316\u5171\u73b0\u6a21\u5f0f\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u3002\u6a21\u578b\u5728EHR\u57fa\u51c6\u6d4b\u8bd5\uff08MIMIC-III\u548cMIMIC-IV\uff09\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u6a21\u62df\u7684\u672a\u77e5\u4ee3\u7801\u73af\u5883\u4e0b\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2508.10429", "categories": ["cs.AI", "cs.CR", "cs.CV", "I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.10429", "abs": "https://arxiv.org/abs/2508.10429", "authors": ["Yi Dong", "Yusuke Muraoka", "Scott Shi", "Yi Zhang"], "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance", "comment": "10 pages, 5 figures, 6 tables. The dataset is available at\n  https://huggingface.co/datasets/Codatta/MM-Food-100K", "summary": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence\ndataset with verifiable provenance. It is a curated approximately 10% open\nsubset of an original 1.2 million, quality-accepted corpus of food images\nannotated for a wide range of information (such as dish name, region of\ncreation). The corpus was collected over six weeks from over 87,000\ncontributors using the Codatta contribution model, which combines community\nsourcing with configurable AI-assisted quality checks; each submission is\nlinked to a wallet address in a secure off-chain ledger for traceability, with\na full on-chain protocol on the roadmap. We describe the schema, pipeline, and\nQA, and validate utility by fine-tuning large vision-language models (ChatGPT\n5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning\nyields consistent gains over out-of-box baselines across standard metrics; we\nreport results primarily on the MM-Food-100K subset. We release MM-Food-100K\nfor publicly free access and retain approximately 90% for potential commercial\naccess with revenue sharing to contributors.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86MM-Food-100K\uff0c\u4e00\u4e2a\u5305\u542b10\u4e07\u6837\u672c\u7684\u591a\u6a21\u6001\u98df\u54c1\u667a\u80fd\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u6765\u6e90\uff1b\u4f7f\u7528Codatta\u8d21\u732e\u6a21\u578b\u548cAI\u8f85\u52a9\u8d28\u91cf\u68c0\u67e5\u6536\u96c61.2\u767e\u4e07\u8d28\u91cf\u63a5\u53d7\u7684\u98df\u54c1\u56fe\u50cf\u8bed\u6599\u5e93\uff1b\u5bf9ChatGPT\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7528\u4e8e\u56fe\u50cf\u7684\u8425\u517b\u9884\u6d4b\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff1b\u53d1\u5e03\u514d\u8d39\u8bbf\u95ee\uff0c\u4fdd\u7559\u4e00\u90e8\u5206\u7528\u4e8e\u5546\u7528\u5e76\u4e0e\u8d21\u732e\u8005\u5206\u4eab\u6536\u5165\u3002", "motivation": "\u4e3a\u4e86\u521b\u5efa\u4e00\u4e2a\u591a\u6a21\u6001\u98df\u54c1\u667a\u80fd\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u6765\u6e90\u548c\u5e7f\u6cdb\u7684\u4fe1\u606f\u6ce8\u91ca\uff0c\u7528\u4e8e\u56fe\u50cf\u7684\u8425\u517b\u9884\u6d4b\u3002", "method": "\u4f7f\u7528Codatta\u8d21\u732e\u6a21\u578b\uff0c\u7ed3\u5408\u793e\u533a\u5408\u4f5c\u548c\u53ef\u914d\u7f6e\u7684AI\u8f85\u52a9\u8d28\u91cf\u68c0\u67e5\uff0c\u6536\u96c6\u4e861.2\u767e\u4e07\u8d28\u91cf\u63a5\u53d7\u7684\u98df\u54c1\u56fe\u50cf\u8bed\u6599\u5e93\u3002\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08ChatGPT 5\u3001ChatGPT OSS\u3001Qwen-Max\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8fdb\u884c\u56fe\u50cf\u7684\u8425\u517b\u9884\u6d4b\u3002", "result": "\u5fae\u8c03\u5728\u6807\u51c6\u6307\u6807\u4e0a\u6bd4\u5f00\u7bb1\u5373\u7528\u7684\u57fa\u51c6\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4e3b\u8981\u7ed3\u679c\u62a5\u544a\u5728MM-Food-100K\u5b50\u96c6\u4e0a\u3002\u53d1\u5e03MM-Food-100K\u4ee5\u4f9b\u516c\u5f00\u514d\u8d39\u8bbf\u95ee\uff0c\u5e76\u4fdd\u7559\u4e00\u90e8\u5206\u7528\u4e8e\u5546\u7528\u8bbf\u95ee\u4e0e\u8d21\u732e\u8005\u5206\u4eab\u6536\u5165\u3002", "conclusion": "\u4ecb\u7ecd\u4e86MM-Food-100K\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b10\u4e07\u6837\u672c\u7684\u591a\u6a21\u6001\u98df\u54c1\u667a\u80fd\u6570\u636e\u96c6\u3002\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u6765\u6e90\u3002\u75311.2\u767e\u4e07\u63a5\u53d7\u8d28\u91cf\u7684\u98df\u54c1\u56fe\u50cf\u8bed\u6599\u5e93\u7cbe\u5fc3\u7b5b\u9009\u51fa\u6765\uff0c\u7528\u4e8e\u5404\u79cd\u4fe1\u606f\u7684\u6ce8\u91ca\uff08\u4f8b\u5982\u83dc\u540d\uff0c\u521b\u5efa\u5730\u533a\uff09\u3002\u8bed\u6599\u5e93\u662f\u5728\u516d\u5468\u5185\u4ece\u8d85\u8fc787000\u4f4d\u8d21\u732e\u8005\u90a3\u91cc\u6536\u96c6\u800c\u6765\uff0c\u4f7f\u7528Codatta\u8d21\u732e\u6a21\u578b\uff0c\u7ed3\u5408\u793e\u533a\u5408\u4f5c\u548c\u53ef\u914d\u7f6e\u7684AI\u8f85\u52a9\u8d28\u91cf\u68c0\u67e5\uff1b\u6bcf\u4e2a\u63d0\u4ea4\u90fd\u94fe\u63a5\u5230\u4e00\u4e2a\u5b89\u5168\u7684\u79bb\u94fe\u8d26\u672c\u4e2d\u7684\u94b1\u5305\u5730\u5740\uff0c\u4ee5\u5b9e\u73b0\u53ef\u8ffd\u6eaf\u6027\uff0c\u5728\u8def\u7ebf\u56fe\u4e0a\u8fd8\u6709\u4e00\u4e2a\u5b8c\u6574\u7684\u5728\u94fe\u534f\u8bae\u3002\u63cf\u8ff0\u4e86\u6a21\u5f0f\u3001\u7ba1\u9053\u548c\u8d28\u91cf\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u8425\u517b\u9884\u6d4b\u4e0a\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08ChatGPT 5\u3001ChatGPT OSS\u3001Qwen-Max\uff09\u8fdb\u884c\u5fae\u8c03\u6765\u9a8c\u8bc1\u6548\u7528\u3002\u5fae\u8c03\u5728\u6807\u51c6\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f00\u7bb1\u5373\u7528\u7684\u57fa\u51c6\u6a21\u578b\uff1b\u6211\u4eec\u4e3b\u8981\u5728MM-Food-100K\u5b50\u96c6\u4e0a\u62a5\u544a\u7ed3\u679c\u3002\u6211\u4eec\u53d1\u5e03MM-Food-100K\u4ee5\u4f9b\u516c\u5f00\u514d\u8d39\u8bbf\u95ee\uff0c\u4fdd\u7559\u7ea690%\u4f9b\u6f5c\u5728\u7684\u5546\u7528\u8bbf\u95ee\uff0c\u5e76\u4e0e\u8d21\u732e\u8005\u5206\u4eab\u6536\u5165\u3002"}}
{"id": "2508.10433", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10433", "abs": "https://arxiv.org/abs/2508.10433", "authors": ["Runqi Qiao", "Qiuna Tan", "Peiqing Yang", "Yanzi Wang", "Xiaowan Wang", "Enhui Wan", "Sitong Zhou", "Guanting Dong", "Yuchen Zeng", "Yida Xu", "Jie Wang", "Chong Sun", "Chen Li", "Honggang Zhang"], "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning", "comment": "Working in progress", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86We-Math 2.0\u7cfb\u7edf\uff0c\u65e8\u5728\u5168\u9762\u63d0\u5347MLLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMathBook-RL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728MathBookEval\u4e0a\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u7ed3\u679c\uff0c\u8868\u660e\u5176\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u96c6\u5efa\u8bbe\u548c\u65b9\u6cd5\u4f18\u5316\uff0c\u7ecf\u5e38\u5ffd\u7565\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u5168\u9762\u7684\u77e5\u8bc6\u9a71\u52a8\u8bbe\u8ba1\u548c\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u7a7a\u95f4\u5efa\u6a21\u3002\u4e3a\u4e86\u514b\u670dMLLMs\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u56f0\u96be\uff0c\u672c\u6587\u5f15\u5165\u4e86We-Math 2.0\u7cfb\u7edf\uff0c\u65e8\u5728\u5168\u9762\u63d0\u5347MLLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86We-Math 2.0\u7cfb\u7edf\uff0c\u5176\u5173\u952e\u8d21\u732e\u6709\u56db\u4e2a\u65b9\u9762\uff1a\uff081\uff09\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\uff1a\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b491\u4e2a\u77e5\u8bc6\u70b9\u548c1819\u4e2a\u57fa\u672c\u539f\u5219\u7684\u4e94\u7ea7\u5c42\u6b21\u7cfb\u7edf\uff1b\uff082\uff09MathBook-Standard\u548cPro\uff1a\u5f00\u53d1\u4e86MathBook-Standard\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u53cc\u91cd\u6269\u5c55\u786e\u4fdd\u4e86\u5e7f\u6cdb\u7684\u6982\u5ff5\u8986\u76d6\u548c\u7075\u6d3b\u6027\u3002\u6b64\u5916\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u4e09\u7ef4\u96be\u5ea6\u7a7a\u95f4\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u95ee\u9898\u751f\u62107\u4e2a\u6e10\u8fdb\u6027\u53d8\u4f53\uff0c\u6784\u5efa\u4e86MathBook-Pro\uff0c\u7528\u4e8e\u8fdb\u884c\u5f3a\u5927\u7684\u8bad\u7ec3\uff1b\uff083\uff09MathBook-RL\uff1a\u63d0\u51fa\u4e86\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\u7684RL\u6846\u67b6\uff0c\u5305\u62ec\u51b7\u542f\u52a8\u5fae\u8c03\uff0c\u4f7f\u6a21\u578b\u4e0e\u4ee5\u77e5\u8bc6\u4e3a\u5bfc\u5411\u7684\u601d\u7ef4\u94fe\u5bf9\u9f50\uff0c\u4ee5\u53ca\u6e10\u8fdb\u5bf9\u9f50RL\uff0c\u5229\u7528\u5e73\u5747\u5956\u52b1\u5b66\u4e60\u548c\u52a8\u6001\u6570\u636e\u8c03\u5ea6\u5b9e\u73b0\u8de8\u96be\u5ea6\u7ea7\u522b\u7684\u6e10\u8fdb\u5bf9\u9f50\uff1b\uff084\uff09MathBookEval\uff1a\u5f15\u5165\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u4e86\u6240\u6709491\u4e2a\u77e5\u8bc6\u70b9\uff0c\u5177\u6709\u591a\u6837\u5316\u7684\u63a8\u7406\u6b65\u9aa4\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMathBook-RL\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728MathBookEval\u4e0a\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u7ed3\u679c\uff0c\u8868\u660e\u5176\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86We-Math 2.0\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u7ed3\u6784\u5316\u7684\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\u3001\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u7a7a\u95f4\u5efa\u6a21\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5168\u9762\u589e\u5f3a\u4e86MLLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMathBook-RL\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728MathBookEval\u4e0a\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u7ed3\u679c\uff0c\u8868\u660e\u5176\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.10467", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.10467", "abs": "https://arxiv.org/abs/2508.10467", "authors": ["Xueli Pan", "Victor de Boer", "Jacco van Ossenbruggen"], "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs", "comment": "Accepted at 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management (IC3K)", "summary": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 FIRESPARQL \u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3 NLQ \u8f6c\u4e3a SPARQL \u67e5\u8be2\u4e2d\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u9519\u8bef\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u53d1\u73b0\uff0cfine-tuning \u53ef\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u9ad8\u51c6\u786e\u5ea6\u3002", "motivation": "\u7531\u4e8e LLM \u65b9\u6cd5\u5728\u751f\u6210 SKG \u7279\u5b9a\u5185\u5bb9\u548c\u5e95\u5c42\u67b6\u6784\u7684 SPARQL \u67e5\u8be2\u65f6\u9047\u5230\u56f0\u96be\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3 NLQ \u8f6c\u4e3a SPARQL \u67e5\u8be2\u8fc7\u7a0b\u4e2d\u9519\u8bef\u7684\u6846\u67b6\u3002\u9488\u5bf9 LLM \u751f\u6210\u7684\u9519\u8bef\uff0c\u5982\u7ed3\u6784\u4e0d\u4e00\u81f4\u548c\u8bed\u4e49\u4e0d\u51c6\u786e\uff0c\u63d0\u51fa\u4e86 FIRESPARQL \u65b9\u6848\u3002", "method": "\u4f7f\u7528 FIRESPARQL \u6846\u67b6\uff0c\u652f\u6301\u7ec6\u8c03\u7684 LLM \u4f5c\u4e3a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u53ef\u901a\u8fc7 RAG \u63d0\u4f9b\u4e0a\u4e0b\u6587\uff0c\u5e76\u589e\u52a0\u4e00\u4e2a SPARQL \u67e5\u8be2\u6821\u6b63\u5c42\u3002\u5728 SciQA \u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u591a\u79cd\u914d\u7f6e\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u6027\u80fd\u4e0e\u57fa\u7ebf\u548c\u6700\u65b0\u65b9\u6cd5\u3002\u4f7f\u7528 BLEU \u548c ROUGE \u5ea6\u91cf\u67e5\u8be2\u51c6\u786e\u5ea6\uff0c\u4f7f\u7528 RelaxedEM \u5ea6\u91cf\u67e5\u8be2\u7ed3\u679c\u51c6\u786e\u5ea6\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u5305\u542b NLQ\u3001SPARQL \u67e5\u8be2\u548c\u5176\u7ed3\u679c\u7684\u91d1\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86 fine-tuning \u5728\u67e5\u8be2\u51c6\u786e\u5ea6\u548c\u7ed3\u679c\u51c6\u786e\u5ea6\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u8fdc\u8d85\u57fa\u7ebf\u548c\u6700\u65b0\u65b9\u6cd5\u3002\u6700\u7ec8\u7684\u6a21\u5757\u5316\u6846\u67b6 FIRESPARQL \u80fd\u6709\u6548\u89e3\u51b3 NLQ \u8f6c\u4e3a SPARQL \u67e5\u8be2\u7684\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFIRESPARQL\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684 NLQ \u8f6c\u4e3a SPARQL \u67e5\u8be2\u65f6\u5b58\u5728\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u9519\u8bef\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7 fine-tuning \u53ef\u5b9e\u73b0\u6700\u9ad8\u6027\u80fd\uff0c\u6d4b\u8bd5\u96c6\u4e0a\u67e5\u8be2\u51c6\u786e\u5ea6\u8fbe\u5230 0.90 ROUGE-L\uff0c\u7ed3\u679c\u51c6\u786e\u5ea6\u8fbe\u5230 0.85 RelaxedEM\u3002"}}
{"id": "2508.10486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10486", "abs": "https://arxiv.org/abs/2508.10486", "authors": ["Ivan Khai Ze Lim", "Ningyi Liao", "Yiming Yang", "Gerald Wei Yong Yip", "Siqiang Luo"], "title": "SEQ-GPT: LLM-assisted Spatial Query via Example", "comment": null, "summary": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios.", "AI": {"tldr": "This study explores the Spatial Exemplar Query (SEQ) scenario to enhance spatial search experiences, introducing SEQ-GPT, a spatial query system powered by Large Language Models (LLMs). The study aims to improve user experiences in spatial searches by enabling versatile searches based on user-specified examples through natural language interactions. Additionally, a tailored LLM adaptation pipeline is proposed to align natural language with structured spatial data and queries, offering insights into enhancing spatial search experiences.", "motivation": "The motivation behind this study is to address the limitations of contemporary spatial services, such as online maps, in performing complex tasks like searching for a group of locations simultaneously. By exploring the Spatial Exemplar Query (SEQ) scenario, the study aims to improve the user experience in spatial searches by enabling more versatile searches based on user-specified examples using natural language capabilities of Large Language Models (LLMs).", "method": "The study introduced SEQ-GPT, a spatial query system powered by Large Language Models (LLMs), and proposed a tailored LLM adaptation pipeline. The system utilizes natural language capabilities of LLMs to enhance spatial search experiences, allowing for interactive operations, query clarification, and dynamic search adjustments based on user feedback. The adaptation pipeline aligns natural language with structured spatial data and queries through dialogue synthesis and multi-model cooperation.", "result": "The study provides an end-to-end demonstration of SEQ-GPT, showcasing how the system can broaden spatial search capabilities with realistic data and application scenarios. By introducing SEQ-GPT and the LLM adaptation pipeline, the study offers insights into enhancing spatial search experiences through natural language interactions and tailored adaptation processes.", "conclusion": "Spatial Exemplar Query (SEQ) is explored in this study to enhance spatial search experiences by allowing users to search for multiple relevant locations simultaneously based on user-specified examples. SEQ-GPT, a spatial query system powered by Large Language Models (LLMs), was introduced to facilitate more versatile SEQ searches using natural language. Additionally, a tailored LLM adaptation pipeline was proposed to align natural language with structured spatial data and queries, enabling unique interactive operations in the SEQ process."}}
{"id": "2508.10492", "categories": ["cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10492", "abs": "https://arxiv.org/abs/2508.10492", "authors": ["Shicheng Xu", "Xin Huang", "Zihao Wei", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model", "comment": "39 pages", "summary": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.", "AI": {"tldr": "AI, specifically DxDirector-7B, is proposed to lead the full diagnosis process with minimal physician involvement, marking a shift where AI drives diagnosis to reduce physician workload significantly and enhance efficiency. It demonstrates superior accuracy and potential to serve as a substitute for specialists.", "motivation": "The motivation is to address the limitation of AI in fully reducing physicians' workload and driving the entire diagnostic process from an ambiguous complaint. The aim is to enhance diagnostic efficiency by repositioning AI as the main driver of diagnosis.", "method": "The paper proposes DxDirector-7B, an LLM with deep thinking capabilities, to act as the primary director in the diagnostic process while physicians serve as assistants. It establishes an accountability framework for misdiagnoses and evaluates its performance across rare, complex, and real-world cases.", "result": "DxDirector-7B demonstrates superior diagnostic accuracy and reduces physician workload compared to existing medical LLMs and general-purpose LLMs. Fine-grained analyses validate its efficacy in multiple clinical departments, and expert evaluations suggest its potential as a substitute for medical specialists.", "conclusion": "AI, particularly DxDirector-7B, can drive the full-process clinical diagnosis with minimal physician involvement and significantly reduce physician workload and enhance diagnostic efficiency. It marks a paradigm shift in the relationship between physicians and AI, positioning AI as the primary director."}}
{"id": "2508.10501", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10501", "abs": "https://arxiv.org/abs/2508.10501", "authors": ["Yushi Feng", "Junye Du", "Yingying Hong", "Qifan Wang", "Lequan Yu"], "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning", "comment": null, "summary": "Existing tool-augmented agentic systems are limited in the real world by (i)\nblack-box reasoning steps that undermine trust of decision-making and pose\nsafety risks, (ii) poor multimodal integration, which is inherently critical\nfor healthcare tasks, and (iii) rigid and computationally inefficient agentic\npipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the\nfirst multimodal framework to address these challenges in the context of Chest\nX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a\nmulti-tool graph, yielding decision paths annotated with interpretable\nprobabilities. Given the complex CXR reasoning task with multimodal medical\ndata, PASS leverages its learned task-conditioned distribution over the agentic\nsupernet. Thus, it adaptively selects the most suitable tool at each supernet\nlayer, offering probability-annotated trajectories for post-hoc audits and\ndirectly enhancing medical AI safety. PASS also continuously compresses salient\nfindings into an evolving personalized memory, while dynamically deciding\nwhether to deepen its reasoning path or invoke an early exit for efficiency. To\noptimize a Pareto frontier balancing performance and cost, we design a novel\nthree-stage training procedure, including expert knowledge warm-up, contrastive\npath-ranking, and cost-aware reinforcement learning. To facilitate rigorous\nevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,\nsafety-critical, free-form CXR reasoning. Experiments across various benchmarks\nvalidate that PASS significantly outperforms strong baselines in multiple\nmetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,\npushing a new paradigm shift towards interpretable, adaptive, and multimodal\nmedical agentic systems.", "AI": {"tldr": "PASS is a multimodal framework that improves Chest X-Ray reasoning and medical AI safety. It outperforms baselines in accuracy, AUC, and LLM-J while managing computational costs. The method includes adaptive tool selection and probability-annotated decision paths, leading to a new paradigm in medical agentic systems.", "motivation": "Existing tool-augmented agentic systems face limitations like black-box reasoning, poor multimodal integration, and inefficiency. PASS aims to overcome these challenges and enhance medical AI safety through adaptive tool selection and probability-annotated decision paths.", "method": "Introducing PASS (Probabilistic Agentic Supernet Sampling) as a multimodal framework for Chest X-Ray reasoning. It adaptively samples agentic workflows over a multi-tool graph, leveraging learned task-conditioned distribution to select suitable tools for each layer. A three-stage training procedure is designed, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning.", "result": "PASS significantly outperforms strong baselines in accuracy, AUC, and LLM-J metrics while balancing computational costs. It introduces a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.", "conclusion": "PASS is a multimodal framework that addresses the challenges in Chest X-Ray reasoning, offering probability-annotated decision paths and enhancing medical AI safety. It outperforms strong baselines in accuracy, AUC, and LLM-J metrics while balancing computational costs, leading to a new paradigm shift in medical agentic systems."}}
{"id": "2508.10530", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10530", "abs": "https://arxiv.org/abs/2508.10530", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu"], "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment", "comment": null, "summary": "The alignment of language models (LMs) with human preferences is critical for\nbuilding reliable AI systems. The problem is typically framed as optimizing an\nLM policy to maximize the expected reward that reflects human preferences.\nRecently, Direct Preference Optimization (DPO) was proposed as a LM alignment\nmethod that directly optimize the policy from static preference data, and\nfurther improved by incorporating on-policy sampling (i.e., preference\ncandidates generated during the training loop) for better LM alignment.\nHowever, we show on-policy data is not always optimal, with systematic\neffectiveness difference emerging between static and on-policy preference\ncandidates. For example, on-policy data can result in a 3$\\times$ effectiveness\ncompared with static data for Llama-3, and a 0.4$\\times$ effectiveness for\nZephyr. To explain the phenomenon, we propose the alignment stage assumption,\nwhich divides the alignment process into two distinct stages: the preference\ninjection stage, which benefits from diverse data, and the preference\nfine-tuning stage, which favors high-quality data. Through theoretical and\nempirical analysis, we characterize these stages and propose an effective\nalgorithm to identify the boundaries between them. We perform experiments on 5\nmodels (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,\nSLiC-HF) to show the generalizability of alignment stage assumption and\nboundary measurement.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6539\u8fdb\u7684LM\u5bf9\u9f50\u65b9\u6cd5 DPO\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u548c\u5728\u7ebf\u504f\u597d\u6570\u636e\u6765\u4f18\u5316LM\u7b56\u7565\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\uff0c\u5c06\u5bf9\u9f50\u8fc7\u7a0b\u5206\u4e3a\u504f\u597d\u6ce8\u5165\u548c\u504f\u597d\u5fae\u8c03\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc6\u522b\u8fd9\u4e24\u4e2a\u9636\u6bb5\u8fb9\u754c\u7684\u6709\u6548\u7b97\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u7b97\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8LM\u5bf9\u9f50\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002", "motivation": "\u672c\u6587\u5f3a\u8c03\u4e86\u5c06LM\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u8fdb\u884c\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u9759\u6001\u548c\u5728\u7ebf\u504f\u597d\u6570\u636e\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u6548\u679c\u5dee\u5f02\u3002\u4f5c\u8005\u901a\u8fc7\u7814\u7a76\u53d1\u73b0\uff0conline\u6570\u636e\u4e0d\u4e00\u5b9a\u603b\u662f\u6700\u4f18\u7684\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\u6765\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u672c\u6587\u4e3b\u8981\u901a\u8fc7\u5bf9\u9759\u6001\u504f\u597d\u6570\u636e\u8fdb\u884c\u76f4\u63a5\u4f18\u5316LM\u7b56\u7565\u7684DPO\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u91c7\u6837\u6765\u6539\u8fdbLM\u5bf9\u9f50\u6548\u679c\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\uff0c\u5c06\u5bf9\u9f50\u8fc7\u7a0b\u5206\u4e3a\u504f\u597d\u6ce8\u5165\u9636\u6bb5\u548c\u504f\u597d\u5fae\u8c03\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u6765\u8868\u5f81\u8fd9\u4e24\u4e2a\u9636\u6bb5\u3002\u53e6\u5916\uff0c\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b97\u6cd5\u6765\u8bc6\u522b\u8fd9\u4e24\u4e2a\u9636\u6bb5\u4e4b\u95f4\u7684\u8fb9\u754c\u3002", "result": "\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u5728 5 \u4e2aLM\u6a21\u578b\u548c 2 \u79cd\u5bf9\u9f50\u65b9\u6cd5\u4e0a\u9a8c\u8bc1\u4e86\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\u548c\u8fb9\u754c\u6d4b\u91cf\u7684\u666e\u9002\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8LM\u5bf9\u9f50\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7b97\u6cd5\u8bc6\u522bLM\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u4e0d\u540c\u9636\u6bb5\u4e4b\u95f4\u7684\u8fb9\u754c\uff0c\u5e76\u5728 Llama\uff0cZephyr\uff0cPhi-2\uff0cQwen \u548c Pythia \u7b49 5 \u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u8be5\u7b97\u6cd5\u6709\u52a9\u4e8e\u533a\u5206\u504f\u597d\u6ce8\u5165\u9636\u6bb5\u548c\u504f\u597d\u5fae\u8c03\u9636\u6bb5\uff0c\u4ece\u800c\u63d0\u9ad8LM\u5bf9\u9f50\u7684\u8d28\u91cf\u3002"}}
{"id": "2508.10539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10539", "abs": "https://arxiv.org/abs/2508.10539", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu", "Min Zhang"], "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ComMCS\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u7b49\u590d\u6742\u9886\u57df\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6311\u6218\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aComMCS\u5728MATH-500\u548cGSM8K\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u5176\u4ed6\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u7b49\u590d\u6742\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u4ef7\u503c\u7684\u8fc7\u7a0b\u9a8c\u8bc1\u5668\u6548\u679c\u53d7\u9650\u4e8e\u8bad\u7ec3\u6ce8\u91ca\u7684\u4f30\u8ba1\u8bef\u5dee\u3002\u9700\u8981\u89e3\u51b3\u9ad8\u65b9\u5dee\u9020\u6210\u7684\u4f30\u8ba1\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ComMCS\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u5f53\u524d\u548c\u540e\u7eed\u6b65\u9aa4\u7684MC\u4f30\u8ba1\u91cf\u6784\u5efa\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u964d\u4f4e\u65b9\u5dee\u5e76\u4fdd\u6301\u65e0\u504f\u4f30\u8ba1\u3002\u5728MATH-500\u548cGSM8K\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684ComMCS\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5728MATH-500\u548cGSM8K\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aComMCS\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u7b49\u590d\u6742\u9886\u57df\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6311\u6218\u3002\u901a\u8fc7\u7ed3\u5408\u5f53\u524d\u548c\u540e\u7eed\u6b65\u9aa4\u7684MC\u4f30\u8ba1\u91cf\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u4ece\u800c\u6709\u6548\u964d\u4f4e\u4e86\u65b9\u5dee\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u989d\u5916\u7684LLM\u63a8\u65ad\u6210\u672c\u3002\u5b9e\u8bc1\u5b9e\u9a8c\u8bc1\u660eComMCS\u5728MATH-500\u548cGSM8K\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u56de\u5f52\u4f18\u5316\u65b9\u6cd5\u548c\u57fa\u51c6\u65b9\u6cd5\u5206\u522b\u63d0\u9ad8\u4e862.8\u70b9\u548c2.2\u70b9\u3002"}}
{"id": "2508.10599", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10599", "abs": "https://arxiv.org/abs/2508.10599", "authors": ["Xinyan Jiang", "Lin Zhang", "Jiayi Zhang", "Qingsong Yang", "Guimin Hu", "Di Wang", "Lijie Hu"], "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models", "comment": null, "summary": "Activation steering offers a promising approach to controlling the behavior\nof Large Language Models by directly manipulating their internal activations.\nHowever, most existing methods struggle to jointly steer multiple attributes,\noften resulting in interference and undesirable trade-offs. To address this\nchallenge, we propose Multi-Subspace Representation Steering (MSRS), a novel\nframework for effective multi-attribute steering via subspace representation\nfine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal\nsubspaces to each attribute, isolating their influence within the model's\nrepresentation space. MSRS also incorporates a hybrid subspace composition\nstrategy: it combines attribute-specific subspaces for unique steering\ndirections with a shared subspace for common steering directions. A dynamic\nweighting function learns to efficiently integrate these components for precise\ncontrol. During inference, MSRS introduces a token-level steering mechanism\nthat dynamically identifies and intervenes on the most semantically relevant\ntokens, enabling fine-grained behavioral modulation. Experimental results show\nthat MSRS significantly reduces attribute conflicts, surpasses existing methods\nacross a range of attributes, and generalizes effectively to diverse downstream\ntasks.", "AI": {"tldr": "MSRS proposes a novel framework for multi-attribute steering in Large Language Models, reducing interference and achieving better control. It combines orthogonal subspaces for each attribute, a hybrid subspace composition strategy, and a dynamic weighting function for precise control. The token-level steering mechanism enables fine-grained behavioral modulation. MSRS outperforms existing methods and generalizes well to diverse tasks.", "motivation": "Address the challenge of jointly steering multiple attributes in Large Language Models, reducing interference and undesirable trade-offs. Aim to provide effective multi-attribute steering by isolating attribute influence within the model's representation space.", "method": "Multi-Subspace Representation Steering (MSRS) is proposed to allocate orthogonal subspaces to each attribute, reducing inter-attribute interference. A hybrid subspace composition strategy combines attribute-specific and shared subspaces. A dynamic weighting function integrates these components for precise control. Token-level steering mechanism intervenes on semantically relevant tokens for fine-grained behavioral modulation.", "result": "MSRS significantly reduces attribute conflicts, outperforms existing methods across various attributes, and generalizes effectively to diverse downstream tasks.", "conclusion": "MSRS is a novel framework for multi-attribute steering in Large Language Models, reducing interference and achieving better control over various attributes. It outperforms existing methods and generalizes well to different tasks."}}
{"id": "2508.10669", "categories": ["cs.AI", "cs.IR", "H.3.3; I.2.7; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.10669", "abs": "https://arxiv.org/abs/2508.10669", "authors": ["Zhenye Yang", "Jinpeng Chen", "Huan Li", "Xiongnan Jin", "Xuanyang Li", "Junwei Zhang", "Hongbo Gao", "Kaimin Wei", "Senzhang Wang"], "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation", "comment": "10 pages; 4 figures; 6 tables; code available at\n  https://github.com/Alex-bupt/STEP", "summary": "Conversational recommender systems (CRSs) aim to proactively capture user\npreferences through natural language dialogue and recommend high-quality items.\nTo achieve this, CRS gathers user preferences via a dialog module and builds\nuser profiles through a recommendation module to generate appropriate\nrecommendations. However, existing CRS faces challenges in capturing the deep\nsemantics of user preferences and dialogue context. In particular, the\nefficient integration of external knowledge graph (KG) information into\ndialogue generation and recommendation remains a pressing issue. Traditional\napproaches typically combine KG information directly with dialogue content,\nwhich often struggles with complex semantic relationships, resulting in\nrecommendations that may not align with user expectations.\n  To address these challenges, we introduce STEP, a conversational recommender\ncentered on pre-trained language models that combines curriculum-guided\ncontext-knowledge fusion with lightweight task-specific prompt tuning. At its\nheart, an F-Former progressively aligns the dialogue context with\nknowledge-graph entities through a three-stage curriculum, thus resolving\nfine-grained semantic mismatches. The fused representation is then injected\ninto the frozen language model via two minimal yet adaptive prefix prompts: a\nconversation prefix that steers response generation toward user intent and a\nrecommendation prefix that biases item ranking toward knowledge-consistent\ncandidates. This dual-prompt scheme allows the model to share cross-task\nsemantics while respecting the distinct objectives of dialogue and\nrecommendation. Experimental results show that STEP outperforms mainstream\nmethods in the precision of recommendation and dialogue quality in two public\ndatasets.", "AI": {"tldr": "The paper introduces STEP, a conversational recommender system that effectively integrates external knowledge graph information and outperforms existing methods in recommendation precision and dialogue quality on public datasets.", "motivation": "Existing conversational recommender systems struggle with capturing deep user preferences and integrating external knowledge graph information efficiently. Traditional approaches face challenges in integrating KG information into dialogue generation effectively, resulting in recommendations that may not meet user expectations.", "method": "The paper introduces STEP, a conversational recommender system that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. It uses an F-Former to align dialogues with knowledge-graph entities through a three-stage curriculum and injects the fused representation into a pre-trained language model via two adaptive prefix prompts.", "result": "Experimental results demonstrate that STEP achieves better precision in recommendation and higher dialogue quality compared to mainstream methods on two public datasets.", "conclusion": "STEP, a conversational recommender system, outperforms existing methods in recommendation precision and dialogue quality on two public datasets."}}
{"id": "2508.10703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10703", "abs": "https://arxiv.org/abs/2508.10703", "authors": ["Yiping Song", "Jiaoyan Chen", "Renate A. Schmidt"], "title": "GenOM: Ontology Matching with Description Generation and Large Language Model", "comment": null, "summary": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.", "AI": {"tldr": "GenOM is a language model-based ontology alignment framework that enhances semantic representations and achieves competitive performance in ontology matching, particularly in the biomedical domain. It outperforms traditional systems and recent methods, as evidenced in experiments on the OAEI Bio-ML track.", "motivation": "The paper aims to address the challenge of ontology matching in the biomedical domain by introducing GenOM, emphasizing its ability to enhance semantic interoperability and integration across heterogeneous knowledge sources.", "method": "GenOM enriches the semantic representations of ontology concepts through generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments were conducted on the OAEI Bio-ML track to demonstrate GenOM's performance.", "result": "The results of experiments on the OAEI Bio-ML track show that GenOM outperforms traditional ontology matching systems and recent LLM-based methods, confirming its competitive performance. Ablation studies further validate the effectiveness of semantic enrichment and few-shot prompting in GenOM.", "conclusion": "GenOM is a large language model-based ontology alignment framework that enhances semantic representations and achieves competitive performance in ontology matching, surpassing traditional systems and recent LLM-based methods."}}
{"id": "2508.10745", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10745", "abs": "https://arxiv.org/abs/2508.10745", "authors": ["Sayan Nag", "K J Joseph", "Koustava Goswami", "Vlad I Morariu", "Balaji Vasan Srinivasan"], "title": "Agentic Design Review System", "comment": null, "summary": "Evaluating graphic designs involves assessing it from multiple facets like\nalignment, composition, aesthetics and color choices. Evaluating designs in a\nholistic way involves aggregating feedback from individual expert reviewers.\nTowards this, we propose an Agentic Design Review System (AgenticDRS), where\nmultiple agents collaboratively analyze a design, orchestrated by a meta-agent.\nA novel in-context exemplar selection approach based on graph matching and a\nunique prompt expansion method plays central role towards making each agent\ndesign aware. Towards evaluating this framework, we propose DRS-BENCH\nbenchmark. Thorough experimental evaluation against state-of-the-art baselines\nadapted to the problem setup, backed-up with critical ablation experiments\nbrings out the efficacy of Agentic-DRS in evaluating graphic designs and\ngenerating actionable feedback. We hope that this work will attract attention\nto this pragmatic, yet under-explored research direction.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86Agentic Design Review System\uff08AgenticDRS\uff09\uff0c\u901a\u8fc7\u591a\u4e2a\u4ee3\u7406\u534f\u4f5c\u5206\u6790\u8bbe\u8ba1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bc4\u4f30\u56fe\u5f62\u8bbe\u8ba1\u548c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u53cd\u9988\u65b9\u9762\u7684\u529f\u6548\u3002\u7814\u7a76\u8fd8\u6784\u5efa\u4e86DRS-BENCH\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u8be5\u6846\u67b6\uff0c\u7ed3\u679c\u8868\u660eAgentic-DRS\u5728\u8be5\u9886\u57df\u5177\u6709\u6709\u6548\u6027\u3002", "motivation": "\u56fe\u5f62\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6d89\u53ca\u591a\u4e2a\u65b9\u9762\uff0c\u5982\u5bf9\u9f50\u3001\u6784\u56fe\u3001\u7f8e\u5b66\u548c\u989c\u8272\u9009\u62e9\u3002\u8bc4\u4f30\u8bbe\u8ba1\u9700\u8981\u7efc\u5408\u4e2a\u4eba\u4e13\u5bb6\u8bc4\u5ba1\u5458\u7684\u53cd\u9988\u610f\u89c1\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f15\u8d77\u5bf9\u8fd9\u4e00\u52a1\u5b9e\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u7814\u7a76\u65b9\u5411\u7684\u6ce8\u610f\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdAgentic Design Review System\uff08AgenticDRS\uff09\uff0c\u5176\u4e2d\u591a\u4e2a\u4ee3\u7406\u901a\u8fc7\u56fe\u5339\u914d\u548c\u72ec\u7279\u7684\u63d0\u793a\u6269\u5c55\u65b9\u6cd5\u534f\u4f5c\u5206\u6790\u8bbe\u8ba1\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u7528\u4e8e\u8bc4\u4f30\u8be5\u6846\u67b6\u7684DRS-BENCH\u57fa\u51c6\u3002\u901a\u8fc7\u4e0e\u95ee\u9898\u8bbe\u7f6e\u76f8\u9002\u5e94\u7684\u6700\u65b0\u57fa\u7ebf\u7684\u5f7b\u5e95\u5bf9\u6bd4\u5b9e\u9a8c\u8bc4\u4f30\u4ee5\u53ca\u5173\u952e\u6027\u6d88\u878d\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86Agentic-DRS\u5728\u8bc4\u4f30\u56fe\u5f62\u8bbe\u8ba1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u548c\u4e0e\u6700\u65b0\u57fa\u7ebf\u7684\u5bf9\u6bd4\uff0c\u4ee5\u53ca\u5173\u952e\u6027\u6d88\u878d\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86Agentic-DRS\u5728\u8bc4\u4f30\u56fe\u5f62\u8bbe\u8ba1\u548c\u751f\u6210\u53ef\u64cd\u4f5c\u53cd\u9988\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2aAgentic Design Review System\uff08AgenticDRS\uff09\uff0c\u901a\u8fc7\u591a\u4e2a\u4ee3\u7406\u534f\u4f5c\u5206\u6790\u8bbe\u8ba1\uff0c\u4e3a\u4ece\u6574\u4f53\u4e0a\u8bc4\u4f30\u56fe\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u5176\u5728\u8bc4\u4f30\u56fe\u5f62\u8bbe\u8ba1\u548c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u53cd\u9988\u65b9\u9762\u7684\u529f\u6548\u3002"}}
{"id": "2508.10747", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10747", "abs": "https://arxiv.org/abs/2508.10747", "authors": ["Sangwoo Jeon", "Juchul Shin", "Gyeong-Tae Kim", "YeonJe Cho", "Seongwoo Kim"], "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning", "comment": "16 pages, 10 figures", "summary": "Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u3001\u76ee\u6807\u611f\u77e5\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u5927\u89c4\u6a21\u7f51\u683c\u73af\u5883\u4e2d\u89c4\u5212\u95ee\u9898\u65f6\u666e\u904d\u5b58\u5728\u7684\u7a00\u758f\u6027\u548c\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8\u7b49\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u63d0\u9ad8\u7b56\u7565\u6cdb\u5316\u548c\u6210\u529f\u7387\uff0c\u4e3a\u89e3\u51b3\u5b9e\u9645\u7684\u5927\u89c4\u6a21\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u5b58\u5728\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8\u3001\u5b66\u4e60\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8ePDDL\u7684\u65b0\u578b\u65e0\u4eba\u673a\u4efb\u52a1\u573a\u666f\uff0c\u5728\u7f51\u683c\u4e16\u754c\u4e2d\u6a21\u62df\u771f\u5b9e\u4efb\u52a1\u6267\u884c\u73af\u5883\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u7684\u3001\u76ee\u6807\u611f\u77e5\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7f51\u683c\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u7b56\u7565\u6cdb\u5316\u548c\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u7684\u3001\u76ee\u6807\u611f\u77e5\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5728\u7b26\u53f7\u89c4\u5212\u9886\u57df\u4e2d\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u65f6\u666e\u904d\u5b58\u5728\u7684\u7a00\u758f\u6027\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7f51\u683c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6539\u5584\u7b56\u7565\u6cdb\u5316\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2508.10769", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10769", "abs": "https://arxiv.org/abs/2508.10769", "authors": ["Zhiqi Shen", "Shaojing Fan", "Danni Xu", "Terence Sim", "Mohan Kankanhalli"], "title": "Modeling Human Responses to Multimodal AI Content", "comment": null, "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5173\u6ce8AI\u751f\u6210\u5185\u5bb9\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5f15\u5165\u4e86MhAIM\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5ea6\u91cf\u65b9\u5f0f\u4ee5\u8bc4\u4f30\u7528\u6237\u5bf9\u5728\u7ebf\u5185\u5bb9\u7684\u5224\u65ad\u548c\u4e92\u52a8\u3002\u8bbe\u8ba1\u4e86T-Lens\u7cfb\u7edf\uff0c\u57fa\u4e8eLLM\uff0c\u80fd\u591f\u9884\u6d4b\u4eba\u7c7b\u5bf9\u591a\u6a21\u6001\u4fe1\u606f\u7684\u53cd\u5e94\uff0c\u589e\u5f3a\u89e3\u91ca\u6027\u548c\u4ea4\u4e92\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86AI\u3001\u4eba\u7c7b\u8ba4\u77e5\u548c\u4fe1\u606f\u63a5\u6536\u4e4b\u95f4\u7684\u590d\u6742\u4e92\u52a8\uff0c\u4e3a\u51cf\u8f7bAI\u9a71\u52a8\u7684\u9519\u8bef\u4fe1\u606f\u98ce\u9669\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7b56\u7565\u3002", "motivation": "\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u7814\u7a76AI\u751f\u6210\u5185\u5bb9\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5185\u5bb9\u662f\u5426\u771f\u5b9e\u7684\u7f3a\u9677\u3002\u5728\u4ea4\u6613\u6216\u80a1\u5e02\u7b49\u9886\u57df\uff0c\u9884\u6d4b\u4eba\u4eec\u7684\u53cd\u5e94\u53ef\u80fd\u6bd4\u9a8c\u8bc1\u5185\u5bb9\u7684\u51c6\u786e\u6027\u66f4\u52a0\u5173\u952e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5f15\u5165\u4e86MhAIM\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5ea6\u91cf\u65b9\u5f0f\u548cT-Lens\u7cfb\u7edf\uff0c\u65e8\u5728\u589e\u5f3aLLMs\u7684\u4eba\u7c7b\u610f\u8bc6\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86MhAIM\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u4eba\u7c7b\u7814\u7a76\u4ee5\u8bc4\u4f30\u7528\u6237\u5bf9AI\u751f\u6210\u5185\u5bb9\u7684\u53cd\u5e94\u3002\u63d0\u51fa\u4e86\u65b0\u7684\u5ea6\u91cf\u65b9\u5f0f\uff1a\u4fe1\u4efb\u5ea6\u3001\u5f71\u54cd\u529b\u548c\u516c\u5f00\u6027\uff0c\u7528\u4e8e\u91cf\u5316\u7528\u6237\u5bf9\u5728\u7ebf\u5185\u5bb9\u7684\u8bc4\u5224\u548c\u4e92\u52a8\u3002\u8bbe\u8ba1\u4e86T-Lens\u7cfb\u7edf\uff0c\u57fa\u4e8eLLM\uff0c\u7ed3\u5408\u4e86\u4eba\u7c7b\u53cd\u5e94\u9884\u6d4b\uff0c\u901a\u8fc7HR-MCP\uff08Human Response Model Context Protocol\uff09\u5b9e\u73b0\u4e0e\u4efb\u4f55LLM\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u53d1\u73b0\uff0c\u4eba\u4eec\u5728\u5305\u542b\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u65f6\u66f4\u5bb9\u6613\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\uff0c\u7279\u522b\u662f\u5728\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u65f6\u3002\u63d0\u51fa\u4e86\u4e09\u4e2a\u65b0\u7684\u5ea6\u91cf\u65b9\u5f0f\uff1a\u4fe1\u4efb\u5ea6\u3001\u5f71\u54cd\u529b\u548c\u516c\u5f00\u6027\uff0c\u4ee5\u91cf\u5316\u7528\u6237\u5bf9\u5728\u7ebf\u5185\u5bb9\u7684\u8bc4\u5224\u548c\u4e92\u52a8\u3002\u8bbe\u8ba1\u4e86T-Lens\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86LLM\u548c\u4eba\u7c7b\u53cd\u5e94\u9884\u6d4b\uff0c\u901a\u8fc7HR-MCP\u5b9e\u73b0\u4e86\u4e0e\u4efb\u4f55LLM\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "\u7814\u7a76\u96c6\u4e2d\u5728AI\u751f\u6210\u5185\u5bb9\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u548c\u884c\u4e3a\u7684\u5f71\u54cd\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5ea6\u91cf\u65b9\u5f0f\u4ee5\u8bc4\u4f30\u7528\u6237\u5bf9\u5728\u7ebf\u5185\u5bb9\u7684\u5224\u65ad\u548c\u4e92\u52a8\u3002\u5f15\u5165\u4e86MhAIM\u6570\u636e\u96c6\uff0c\u7814\u7a76\u8868\u660e\u4eba\u4eec\u5728\u5305\u542b\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u65f6\u66f4\u5bb9\u6613\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\uff0c\u7279\u522b\u662f\u5728\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u65f6\u3002\u63d0\u51fa\u4e86T-Lens\u7cfb\u7edf\uff0c\u57fa\u4e8eLLM\u8bbe\u8ba1\uff0c\u80fd\u591f\u9884\u6d4b\u4eba\u7c7b\u5bf9\u591a\u6a21\u6001\u4fe1\u606f\u7684\u53cd\u5e94\uff0c\u589e\u5f3a\u89e3\u91ca\u6027\u548c\u4ea4\u4e92\u80fd\u529b\u3002\u7814\u7a76\u4e3aLLMs\u63d0\u4f9b\u4e86\u4eba\u7c7b\u610f\u8bc6\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86AI\u3001\u4eba\u7c7b\u8ba4\u77e5\u548c\u4fe1\u606f\u63a5\u6536\u4e4b\u95f4\u7684\u590d\u6742\u4e92\u52a8\uff0c\u4e3a\u51cf\u8f7bAI\u9a71\u52a8\u7684\u9519\u8bef\u4fe1\u606f\u98ce\u9669\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7b56\u7565\u3002"}}
{"id": "2508.10777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10777", "abs": "https://arxiv.org/abs/2508.10777", "authors": ["Ma\u00ebl Jullien", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference", "comment": "19 pages", "summary": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.", "AI": {"tldr": "\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e34\u5e8a\u8bd5\u9a8c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u57fa\u51c6\u6d4b\u8bd5\u4e0b\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728GKMRV\u4e0a\u51c6\u786e\u7387\u8f83\u9ad8\uff0c\u4f46\u5728\u4e3b\u8981\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u66b4\u9732\u51fa\u7ed3\u6784\u548c\u8868\u5f81\u9650\u5236\u3002\u8be5\u7814\u7a76\u8d28\u7591\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ec5\u901a\u8fc7\u6269\u5c55\u6570\u636e\u548c\u53c2\u6570\u6765\u83b7\u5f97\u6709\u7ed3\u6784\u3001\u53ef\u63a8\u5e7f\u5185\u90e8\u8868\u5f81\u7684\u5047\u8bbe\u3002\u9700\u8981\u63ed\u793aLLM\u5728\u9ad8\u98ce\u9669\u9886\u57df\u53ef\u9760\u6027\u7684\u6846\u67b6\u3002", "motivation": "\u8d28\u7591\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ec5\u901a\u8fc7\u6269\u5c55\u6570\u636e\u548c\u53c2\u6570\u6765\u83b7\u5f97\u8d8a\u6765\u8d8a\u6709\u7ed3\u6784\u3001\u53ef\u63a8\u5e7f\u7684\u5185\u90e8\u8868\u5f81\u7684\u5047\u8bbe\u3002\u4e3b\u8981\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u4f4e\u51c6\u786e\u6027\u8868\u660e\u5f53\u524dLLM\u867d\u7136\u5177\u5907\u76f8\u5173\u4e34\u5e8a\u77e5\u8bc6\uff0c\u4f46\u5728\u90e8\u7f72\u8fd9\u4e9b\u77e5\u8bc6\u65f6\u7f3a\u4e4f\u7ed3\u6784\u5316\u3001\u53ef\u7ec4\u5408\u7684\u5185\u90e8\u8868\u5f81\u3002\u9700\u8981\u63ed\u793aLLM\u5728\u9ad8\u98ce\u9669\u9886\u57df\u53ef\u9760\u6027\u7684\u6846\u67b6\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e34\u5e8a\u8bd5\u9a8c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u56db\u4e2a\u63a8\u7406\u7c7b\u522b\uff1a\u56e0\u679c\u5f52\u56e0\u3001\u7ec4\u6210\u6027\u57fa\u7840\u3001\u8ba4\u8bc6\u9a8c\u8bc1\u548c\u98ce\u9669\u72b6\u6001\u62bd\u8c61\u3002\u6bcf\u4e2a\u9879\u76ee\u4e0e\u6709\u9488\u5bf9\u6027\u7684\u5730\u9762\u77e5\u8bc6\u548c\u5143\u7ea7\u522b\u63a8\u7406\u9a8c\u8bc1\u63a2\u6d4b\uff08GKMRV\uff09\u76f8\u914d\u5bf9\uff0c\u8ba9\u6211\u4eec\u80fd\u591f\u533a\u5206\u4e8b\u5b9e\u8bbf\u95ee\u7684\u5931\u8d25\u548c\u63a8\u7406\u7684\u5931\u8d25\u3002\u8bc4\u4f30\u4e86\u516d\u79cd\u5f53\u4ee3LLM\u6a21\u578b\uff0c\u5305\u62ec\u76f4\u63a5\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728GKMRV\u4e0a\u51c6\u786e\u7387\u63a5\u8fd1\u6700\u9ad8\uff08\u5e73\u5747\u51c6\u786e\u7387\u4e3a0.918\uff09\uff0c\u4f46\u5728\u4e3b\u8981\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u5e73\u5747\u51c6\u786e\u7387\u4e3a0.25\uff09\u3002\u5c3d\u7ba1\u51c6\u786e\u7387\u4f4e\uff0c\u8f93\u51fa\u63a8\u65ad\u5728\u6837\u672c\u95f4\u9ad8\u5ea6\u4e00\u81f4\uff08\u5e73\u5747\u4e3a0.87\uff09\uff0c\u8868\u660e\u5b58\u5728\u57fa\u7840\u542f\u53d1\u5f0f\u548c\u6377\u5f84\u7684\u7cfb\u7edf\u6027\u5e94\u7528\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u8bd5\u9a8c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8GKMRV\u51c6\u786e\u6027\uff0c\u4f46\u5728\u4e3b\u8981\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\u3002\u5c3d\u7ba1\u51c6\u786e\u6027\u8f83\u4f4e\uff0c\u8f93\u51fa\u63a8\u65ad\u5728\u6837\u672c\u95f4\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8868\u660e\u5b58\u5728\u57fa\u7840\u542f\u53d1\u5f0f\u548c\u6377\u5f84\u7684\u7cfb\u7edf\u6027\u5e94\u7528\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLM\u5b58\u5728\u7684\u57fa\u672c\u7ed3\u6784\u548c\u8868\u5f81\u9650\u5236\uff1a\u867d\u7136\u5177\u5907\u76f8\u5173\u4e34\u5e8a\u77e5\u8bc6\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u5730\u90e8\u7f72\u7684\u7ed3\u6784\u5316\u3001\u53ef\u7ec4\u5408\u5185\u90e8\u8868\u5f81\uff08\u4f8b\u5982\uff0c\u6574\u5408\u9650\u5236\u3001\u6743\u8861\u8bc1\u636e\u6216\u6a21\u62df\u53cd\u4e8b\u5b9e\uff09\u3002\u901a\u8fc7\u4f7f\u7528GKMRV\u5c06\u77e5\u8bc6\u4e0e\u63a8\u7406\u5206\u79bb\uff0c\u4f7f\u8fd9\u79cd\u533a\u5206\u53d8\u5f97\u660e\u786e\u548c\u53ef\u6d4b\u91cf\uff0c\u4e3a\u63a2\u7a76LLM\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2508.10806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10806", "abs": "https://arxiv.org/abs/2508.10806", "authors": ["Maria J. P. Peixoto", "Akriti Pandey", "Ahsan Zaman", "Peter R. Lewis"], "title": "Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems", "comment": "Paper accepted for the IJCAI 2025 Workshop on Explainable Artificial\n  Intelligence (XAI): https://sites.google.com/view/xai2025/proceedings", "summary": "As AI systems are increasingly deployed to support decision-making in\ncritical domains, explainability has become a means to enhance the\nunderstandability of these outputs and enable users to make more informed and\nconscious choices. However, despite growing interest in the usability of\neXplainable AI (XAI), the accessibility of these methods, particularly for\nusers with vision impairments, remains underexplored. This paper investigates\naccessibility gaps in XAI through a two-pronged approach. First, a literature\nreview of 79 studies reveals that evaluations of XAI techniques rarely include\ndisabled users, with most explanations relying on inherently visual formats.\nSecond, we present a four-part methodological proof of concept that\noperationalizes inclusive XAI design: (1) categorization of AI systems, (2)\npersona definition and contextualization, (3) prototype design and\nimplementation, and (4) expert and user assessment of XAI techniques for\naccessibility. Preliminary findings suggest that simplified explanations are\nmore comprehensible for non-visual users than detailed ones, and that\nmultimodal presentation is required for more equitable interpretability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86XAI\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u6b8b\u969c\u7528\u6237\u5728XAI\u6280\u672f\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\u3002\u7b80\u5316\u89e3\u91ca\u66f4\u6613\u7406\u89e3\uff0c\u591a\u6a21\u5f0f\u5448\u73b0\u6709\u52a9\u4e8e\u66f4\u516c\u5e73\u7684\u53ef\u89e3\u91ca\u6027\u3002\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u548c\u65b9\u6cd5\u8bba\u6982\u5ff5\u9a8c\u8bc1\u4f5c\u4e3a\u7814\u7a76\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1XAI\u7684\u53ef\u89e3\u91ca\u6027\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5bf9\u6b8b\u969c\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u5f71\u54cd\u5374\u9c9c\u6709\u63a2\u8ba8\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5305\u5bb9\u548c\u516c\u5e73\u7684XAI\u8bbe\u8ba1\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u6587\u732e\u7efc\u8ff0\u548c\u65b9\u6cd5\u8bba\u6982\u5ff5\u9a8c\u8bc1\u4f5c\u4e3a\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u522b\u4ece\u5df2\u6709\u7814\u7a76\u548c\u5b9e\u8bc1\u7814\u7a76\u7684\u89d2\u5ea6\u63a2\u8ba8\u4e86XAI\u7684\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7b80\u5316\u7684\u89e3\u91ca\u5bf9\u975e\u89c6\u89c9\u7528\u6237\u66f4\u6613\u7406\u89e3\uff0c\u800c\u591a\u6a21\u5f0f\u5448\u73b0\u5bf9\u66f4\u516c\u5e73\u7684\u53ef\u89e3\u91ca\u6027\u662f\u5fc5\u8981\u7684\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u5305\u62ec\u6b8b\u969c\u7528\u6237\u5728\u5185\u7684\u7528\u6237\u5728\u8bc4\u4f30XAI\u6280\u672f\u4e2d\u7684\u5fc5\u8981\u6027\u3002\u901a\u8fc7\u4e00\u9879\u6587\u732e\u7efc\u8ff0\u548c\u4e00\u4e2a\u56db\u90e8\u5206\u7684\u65b9\u6cd5\u8bba\u6982\u5ff5\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u7b80\u5316\u89e3\u91ca\u5bf9\u4e8e\u975e\u89c6\u89c9\u7528\u6237\u66f4\u6613\u7406\u89e3\uff0c\u800c\u591a\u6a21\u5f0f\u5448\u73b0\u5bf9\u4e8e\u66f4\u516c\u5e73\u7684\u53ef\u89e3\u91ca\u6027\u662f\u5fc5\u8981\u7684\u3002"}}
