<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 39]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 本研究设计自动化流程将GUI代理的顺序事件转换为页面图，引入RAG技术检索感知指南，提出PG-Agent框架以增强泛化能力。实验证实了PG-Agent在不同基准测试中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理利用顺序事件来处理多步操作，在面对新场景时泛化能力有限，无法深度感知GUI环境。提出页面图转换和RAG技术结合的新方法，旨在增强泛化能力。

Method: 设计自动化流程将顺序事件转换为页面图，引入RAG技术从页面图中检索感知指南，提出PG-Agent框架并结合任务分解策略，进行实验验证。

Result: 通过实验表明，PG-Agent在各种基准测试中表现出良好的有效性，即使在有限的页面图构建事件下也能成功泛化。

Conclusion: 通过设计自动化流程将顺序事件转换为页面图，以增强GUI代理的深度感知和泛化能力。引入RAG技术从页面图中检索可靠的感知指南，结合任务分解策略的PG-Agent框架，使其能够泛化到未知场景。通过大量实验证明PG-Agent的有效性，在有限的页面图构建事件下也能取得良好效果。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [2] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 研究了一类因果模型中部分可识别的查询问题，提出了一种新算法来计算概率边界，采用多线性编程和线性编程解决了这一问题。使用列生成技术进行概率边界计算，证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究部分可识别的查询在特定因果模型中的情况。解决因果模型中外生变量非完全指定的情况下计算概率边界的问题。简化这些程序的构建过程，提出新算法。证明外生变量存在多项式基数的表示方式。

Method: 通过多线性编程和线性编程解决概率边界的计算问题。利用输入概率简化程序的构建。采用列生成技术通过一系列辅助线性整数规划计算概率边界。

Result: 提出了一种新的算法，通过多线性编程和线性编程解决概率边界的计算问题。采用列生成技术计算概率边界，证明了其优于现有方法。

Conclusion: 研究了在一类因果模型中部分可识别的查询。通过对无环结构因果模型进行研究，其中每个内生变量最多连接一个外生混淆因素。在内生变量被观察到的情况下，但外生变量并非完全指定的情况下，我们探讨了概率边界的计算。通过多线性编程和线性编程解决概率边界的计算问题。提出了一个新算法，简化了这些程序的构建，并利用了内生变量上的概率。通过计算辅助线性整数规划的序列来计算概率边界，展示了对于外生变量存在多项式基数的表示方式。实验证明列生成技术优于现有方法。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [3] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Diffusion-AC的自动化冲突解决框架，通过将扩散概率模型集成到冲突检测和解决任务中，实现了富含多模态动作分布的政策建模，并引入了DPSC的训练机制，确保代理在学习过程中稳定有效。实验结果表明，该方法在性能上显著优于最先进的深度强化学习基准，在高密度场景下取得了显著的安全性能提升。


<details>
  <summary>Details</summary>
Motivation: 在全球空中交通持续增长的背景下，提高冲突检测和解决的效率和安全性至关重要。现有的深度强化学习方法存在单一模态偏见，导致在复杂和动态约束条件下决策灵活性不足，常导致决策僵局，本研究旨在克服这一局限性。

Method: 将扩散概率模型集成到冲突检测和解决任务中，提出了一种自动化冲突解决框架Diffusion-AC，并引入了Density-Progressive Safety Curriculum（DPSC）的训练机制，确保代理在从稀疏到高密度交通环境的学习过程中稳定有效。

Result: 通过广泛的模拟实验，证明了提出的方法在性能上显著优于一系列最先进的深度强化学习基准。在挑战性较大的高密度场景下，Diffusion-AC不仅保持了94.1%的高成功率，还相对于次佳基线减少了约59%的几乎中空中碰撞事件数量，大幅增强了系统的安全边际。

Conclusion: 提出了一种名为Diffusion-AC的自动化冲突解决框架，通过将扩散概率模型集成到冲突检测和解决任务中，实现了富含多模态动作分布的政策建模，大幅提升了系统的安全边际，证明在高密度场景下成功率达到94.1%，将近中空中碰撞事件数量降低了约59%。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [4] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 本文通过强化学习提高大型语言模型解决问题的能力。介绍了动态规划的概念框架，让LLM代理能够灵活决定何时分配测试时间计算。提出了两阶段训练流程：在多样化数据上进行监督微调，然后使用RL在长期环境中完善。实验证明，这种方法训练的动态规划代理更高效，并能被人类计划有效引导。


<details>
  <summary>Details</summary>
Motivation: 现有方法在代理设置中促使LLM在每次行动之前明确计划，但总是规划在计算上昂贵且降低对长期任务的性能，而永远不规划则限制性能。为解决这一问题，引入了一种概念框架，形式化了LLM代理的动态规划，使它们能够灵活决定何时为规划分配测试时间计算。

Method: 介绍了一个概念框架，形式化了LLM代理的动态规划，让它们灵活地决定何时为规划分配测试时间计算。提出了简单的两阶段训练流程：（1）在多样化合成数据上进行监督微调，为动态规划准备模型；（2）使用RL在长期环境中完善这种能力。

Result: 实验结果表明，使用该方法训练的动态规划代理在Crafter环境中更具样本效率，不断实现更复杂的目标。此外，展示这些代理可以通过人类编写的计划有效地引导，超越其独立能力。

Conclusion: 训练大型语言模型（LLM）通过强化学习（RL）推进问题解决能力。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [5] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 研究开发了一种名为KG-SMILE的框架，通过对图形RAG应用受控摄动、计算相似性并训练加权线性替代物，识别生成输出中具有影响力的图形实体和关系，从而提高了RAG模型的透明度和可信度。KG-SMILE框架通过全面的归因度量评估，展示了稳定且与人类对齐的解释，提高了模型的解释性和有效性。


<details>
  <summary>Details</summary>
Motivation: Addressing the opacity and reliance on data quality of Retrieval-Augmented Generation (RAG) models, especially in sensitive domains like healthcare, to enhance model transparency and trustworthiness.

Method: Developed a method-agnostic, perturbation-based framework named KG-SMILE for Graph RAG. Applied controlled perturbations, computed similarities, and trained weighted linear surrogates to identify influential graph entities and relations in generated outputs.

Result: KG-SMILE framework evaluated using comprehensive attribution metrics, showing stable and human-aligned explanations with improved model interpretability and effectiveness.

Conclusion: KG-SMILE method enhances transparency and trust in Retrieval-Augmented Generation (RAG) models by providing token and component-level interoperability for Graph RAG, leading to stable and human-aligned explanations.

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [6] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC is introduced as a testbed for AI reasoning in low-data and out-of-distribution scenarios. It provides feedback through data augmentations and is demonstrated in various language model evaluation settings.


<details>
  <summary>Details</summary>
Motivation: Reasoning in novel problem settings with limited data and distribution shift is crucial. The motivation behind this work is to address this challenge by introducing CausalARC as a tool for AI reasoning in such scenarios.

Method: The paper utilizes CausalARC, a testbed modeled after the Abstraction and Reasoning Corpus (ARC), with reasoning tasks sampled from a specified causal world model expressed as a structural causal model. It employs data augmentations to provide observational, interventional, and counterfactual feedback for in-context learning demonstrations.

Result: The paper showcases the use of CausalARC in four language model evaluation settings: abstract reasoning with test-time training, counterfactual reasoning with in-context learning, program synthesis, and causal discovery with logical reasoning.

Conclusion: CausalARC is introduced as an experimental testbed for AI reasoning in low-data and out-of-distribution scenarios, demonstrating its utility in various language model evaluation settings.

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [7] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 提出了Embodied-LM系统，通过图像模式的图解和声明性空间推理，在逻辑推理问题上指导LLMs进行场景理解，形式化认知结构为可执行程序，有效支持逻辑推理并提高可解释性。系统初步实现聚焦于空间基元，为整合更复杂和动态表示奠定了计算基础。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言理解取得了显著进展，但LLMs在执行逻辑推理时仍存在错误，缺乏人类理解所需的健壮心理表征。本研究的动力在于引入Embodied-LM系统，旨在通过图像模式的图解和声明性空间推理来建立理解和逻辑推理的基础，从而解决LLMs的不足。

Method: 通过Embodied-Lm系统，利用基于图像模式的图解和声明性空间推理，在逻辑推理问题上引导LLMs进行场景理解，形式化认知结构为可执行程序，实现了逻辑推理的有效支持，并提高了可解释性。

Result: 通过在逻辑演绎问题上的评估，表明LLMs可以通过具身认知结构解释场景，这些结构形式化为可执行程序，支持有效的逻辑推理并提高可解释性。虽然当前系统主要集中于空间原型，但为整合更复杂和动态表示打下了计算基础。

Conclusion: 提出了一种原型神经符号系统 Embodied-LM，通过基于图像模式的图解来对理解和逻辑推理进行根本性设定，系统利用答案集编程内的声明性空间推理来操作化这些认知结构的空间基础。通过在逻辑演绎问题上的评估，展示了LLMs可以通过具身认知结构来解释场景，这些结构可以形式化为可执行程序，结果表示支持有效的逻辑推理并提升可解释性。尽管当前实现侧重于空间基元，但为整合更复杂和动态表示打下了计算基础。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [8] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 研究揭示了强化学习在增强大型语言模型推理能力时的成功机制，提出了HICRA算法，集中优化努力在高影响力计划token上，显著优于基线算法。验证了语义熵作为衡量战略探索的更优指南。


<details>
  <summary>Details</summary>
Motivation: 鉴于强化学习（RL）在增强大型语言模型（LLMs）的复杂推理能力方面的高效性，但驱动此成功的基本机制仍然大多不透明，研究揭示出许多令人困惑的现象不是不相关的事件，而是新兴推理层次的标志。

Method: 揭示了两阶段动态：模型首先受限于程序的正确性，必须改善低级技能。学习瓶颈随后决定性转移，性能增益是由高水平战略规划的探索和掌握驱动的。提出了HIerarchy-Aware Credit Assignment (HICRA)算法，集中优化努力在高影响力计划token上。

Result: HICRA明显优于强基线，突出了专注于战略瓶颈是解锁先进推理的关键。

Conclusion: 提出了一种新的算法HICRA，专注于高影响力的计划性token，显著优于基线算法，表明专注于这一策略瓶颈是解锁先进推理的关键。验证了语义熵作为衡量战略探索的更优指南，而不是像token级熵这样的误导性指标。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [9] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: 本研究调查了八种时间序列分割算法，发现分段组合对解释质量的影响较大，而段数对解释质量的影响大于特定分段方法。等长分段一直优于自定义时间序列分割算法，新的归一化技术通过加权分段长度持续改善了解释质量。


<details>
  <summary>Details</summary>
Motivation: 对于长时间序列，SHAP方法的计算复杂度随特征数量呈指数级增长，限制其实用性。先前的研究表明，通过分段聚合特征来计算一组相邻时间点的单个归因值可以显著减少SHAP的运行时间，但选择最佳分段策略仍是一个未解决的问题。

Method: 评估了两种建立的XAI评估方法：InterpretTime和AUC Difference，使用多元时间序列（MTS）和单一时间序列（UTS）进行实验证明，段数对解释质量的影响大于特定分段方法。提出了新的归一化技术以改善解释质量。

Result: 发现段数对解释质量的影响大于特定分段方法，等长分段比大多数自定义时间序列分割算法表现更好。引入的归一化技术通过加权分段长度持续改善了归因质量。

Conclusion: 通过对八种不同的时间序列分割算法进行研究，发现分段组合对解释质量的影响较大。特别是等长分段一直优于大多数自定义时间序列分割算法。此外，引入一种新的归一化技术，通过按长度加权分段，持续提高了解释质量。

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [10] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: 研究介绍了一种名为PersonaTeaming的新方法，通过在对抗提示生成过程中引入角色，改进了自动化红队测试。实验证明PersonaTeaming通过角色突变在攻击成功率方面取得了显著进展，最高可提高至144.1%，同时保持了提示的多样性。


<details>
  <summary>Details</summary>
Motivation: 最近的AI治理和安全研究发展要求有效地提出可能由AI模型构成的潜在风险。现有的方法中缺乏考虑身份角色的因素，而红队测试的策略和发现的风险可能受红队员身份和背景的影响。基于这一背景，研究初步做出了尝试，引入人们的背景和身份，开发了PersonaTeaming方法，旨在自动化红队测试中更广泛地探索对抗策略。

Method: 开发了一种名为PersonaTeaming的新方法，通过在对抗式提示生成过程中引入角色来改进自动化红队测试。首先介绍了一种基于“红队专家”角色或“常规AI用户”角色的提示突变方法学。然后开发了一种动态生成算法，自动生成适应不同种子提示的各种角色类型。此外，还开发了一组新的度量标准，明确衡量了“突变距离”，以补充对抗性提示的多样性度量。

Result: 通过角色突变，实验证明PersonaTeaming在攻击成功率方面取得了显著进展，最高可提高至144.1%，同时保持了提示的多样性。相较于现有的自动化红队方法RainbowPlus，PersonaTeaming表现出良好的效果。

Conclusion: 研究开发了一种名为PersonaTeaming的新方法，旨在在自动化的红队测试过程中引入人们的背景和身份，从而探索更广泛的对抗策略。实验证明通过角色突变，在对抗性提示的攻击成功率有了显著提升（最高达到144.1%），同时保持了提示的多样性。与RainbowPlus相比，PersonaTeaming在红队测试中取得了很好的效果。研究探讨了不同角色类型和突变方法的优势和局限性，为未来探索自动化和人工红队测试方法之间的互补性提供了启示。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [11] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 本研究系统性分析了LLM的人格特征，发现指导调整可以稳定特质表达并增强特质相关性，但自我报告的特质不能可靠预测行为。角色投入可以引导自我报告的方向，但对行为影响较小。研究挑战了对LLM人格的假设，强调了对调整和可解释性更深入评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 近期LLM的发展显示了与人类特质类似的一致行为倾向，然而先前的研究主要依赖简化的自我报告和启发式提示，很少有行为验证。因此，对于LLM人格模式的理解至关重要。

Method: 本研究通过对LLM的三个维度进行系统性特征分析：(1)特质配置在训练阶段的动态出现和演变；(2)自我报告特征在行为任务中的预测有效性；(3)针对性干预的影响，如角色投入，对自我报告和行为的影响。

Result: 研究发现指导调整显著稳定了特质表达并增强了特质相关性，与人类数据呈镜像关系。然而，这些自我报告的特质并不能可靠地预测行为，且观察到的关联经常与人类模式不同。角色投入成功引导了自我报告朝预期方向发展，但对实际行为的影响较小或不一致。研究发现挑战了关于LLM人格的假设，强调了对于调整和可解释性更深入评估的必要性。

Conclusion: 在本研究中，通过对LLM的人格特征进行系统性表征，揭示了指导调整对于稳定特质表达及加强特质相关性的重要性。然而，自我报告的特质并不能可靠地预测行为，且观察到的关联经常与人类模式不同。研究挑战了关于LLM人格的假设，并强调了对于调整和可解释性更深入评估的必要性。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [12] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: The study reveals that Largest Language Models (LLMs) lack internal consistency, posing a critical limitation in their ability to replace real participants in human-subject research. The research design focused on exploring agent behavior and internal state, showing significant inconsistencies in LLMs across models and sizes, despite their ability to generate responses similar to humans.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the fundamental question of whether agents maintain internal consistency across different experimental settings, rather than just focusing on the correspondence of LLM-generated survey data with human counterparts.

Method: The study developed a research design to reveal the internal state of agents and examine their behavior in a basic dialogue setting. Behavioral hypotheses were explored to assess the consistency of an agent's conversation behavior with their internal state.

Result: The findings show significant internal inconsistencies in LLMs across different model families and sizes. While agents can generate responses similar to humans, they lack internal consistency. The study highlights the limitation of LLMs in substituting for real participants in human-subject research.

Conclusion: Largest Language Models (LLMs) are not internally consistent, indicating a critical gap in their ability to substitute for real participants in human-subject research.

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [13] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard and SafetyClamp extend the capabilities of Large Language Models in Offshore Wind maintenance by improving safety coverage and technical accuracy. They significantly enhance Safety Recall while maintaining high Technical Recall, indicating a potential new standard for safety assurance in critical maintenance contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of conventional Large Language Models in specialized and unexpected scenarios in Offshore Wind maintenance. The goal is to improve safety coverage and technical accuracy in critical maintenance tasks.

Method: The paper introduces RAGuard, a Retrieval-Augmented Generation framework that integrates safety-critical documents and technical manuals. It utilizes parallel queries to two indices with separate retrieval budgets for knowledge and safety. The SafetyClamp extension expands the candidate pool and enforces safety guarantees.

Result: RAGuard and SafetyClamp extensions improved Safety Recall@K to over 50% from nearly 0% in traditional RAG, while maintaining Technical Recall above 60%. The results suggest a significant enhancement in safety coverage and technical depth in LLM-powered decision support systems for critical maintenance tasks.

Conclusion: RAGuard and SafetyClamp enhance safety assurance and technical depth in Offshore Wind maintenance tasks, setting a new standard for LLM-powered decision support. Safety Recall significantly improved with RAGuard compared to traditional RAG.

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [14] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: This paper presents an SCPA framework that utilizes AI technologies, particularly large language models, to address supply chain challenges. Implemented at JD.com, it resulted in enhanced efficiency, accuracy, and key metric performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to tackle the practical and challenging problem of collecting relevant data from e-commerce platforms, formulating long-term plans, and dynamically adjusting them based on environmental changes in the supply chain domain. The authors aim to ensure interpretability, efficiency, and reliability in the planning process.

Method: The paper constructs a Supply Chain Planning Agent (SCPA) framework that understands domain knowledge, decomposes tasks, and utilizes or creates tools based on AI technologies. The framework is deployed in JD.com's real-world scenario to showcase its feasibility and impact on supply chain operations.

Result: The SCPA framework effectively reduced labor, improved accuracy, and enhanced key metrics such as stock availability in JD.com's real-world supply chain scenario.

Conclusion: This paper introduces a Supply Chain Planning Agent (SCPA) framework that leverages AI technologies, specifically large language models, to address supply chain management challenges. The framework demonstrated improved efficiency, accuracy, and key metric performance in a real-world scenario at JD.com.

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [15] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 研究引入了Meta-Policy Deliberation Framework (MPDF)和SoftRankPO强化学习算法来提高多智能体语言模型(LLMs)的推理能力。实验证明，这些方法在多个基准测试中表现更优，准确率提高了4-5%。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统框架通常集中于宏观层面的协调，而忽略了代理的内部思考能力。作者认为这种元认知盲点将代理视为 passvie 执行者，无法根据内部认知状态如不确定性或信心来调整策略。因此，作者引入了MPDF和SoftRankPO，以学习分布式策略，提高系统对不确定性的适应能力。

Method: 研究引入了Meta-Policy Deliberation Framework (MPDF)和SoftRankPO强化学习算法，通过开发分布式策略来提高多智能体语言模型(LLMs)的推理能力。

Result: 实验证明，MPDF与SoftRankPO相比于现有算法，在多个基准测试中表现更优。在数学和一般推理基准测试中，准确率提高了4-5%。

Conclusion: 本研究引入了Meta-Policy Deliberation Framework (MPDF)来提高多智能体语言模型(LLMs)的复杂推理效果。通过开发SoftRankPO这一新颖的强化学习算法，解决了传统模型梯度不稳定的问题。实验证明，MPDF与SoftRankPO相比于六种最先进的启发式算法和学习型多智能体推理算法，在五个数学和一般推理基准测试中，平均准确率提高了4-5%。这项工作为多智能体LLM系统学习自适应的元认知策略提供了一个范式，将重点从设计固定协议转向学习动态的思考策略。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [16] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: The paper assesses the alignment of Large Language Models (LLMs) with domain experts in informing social policymaking on homelessness alleviation. By developing a benchmark of decision scenarios and an automated pipeline connecting policies to an agent-based model, the study shows promising potential for LLMs in providing valuable insights for alternative policies at scale when collaborated with local domain experts.


<details>
  <summary>Details</summary>
Motivation: To evaluate if Large Language Models (LLMs) can align with domain experts and inform social policymaking on homelessness alleviation, a global challenge affecting over 150 million people worldwide.

Method: Developed a novel benchmark of decision scenarios across four geographies grounded in the Capability Approach for human development. Presented an automated pipeline connecting benchmarked policies to an agent-based model and explored the social impact of recommended policies through simulated social scenarios.

Result: The paper demonstrates promising potential to leverage LLMs for social policy making. Collaborating with local domain experts and introducing responsible guardrails and contextual calibrations are crucial for maximizing the value of LLMs in providing valuable insights for alternative policies at scale.

Conclusion: LLMs have promising potential in informing social policymaking on homelessness alleviation when collaborated with domain experts and contextual calibrations. The automated pipeline connecting benchmarked policies to an agent-based model shows valuable insights for alternative policies at scale.

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [17] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 本论文开发了一种基于MCP的零训练、防幻觉映射系统，提高了数据标准化过程中映射的效率和准确性。该系统能够实现实时词汇查找和结构化推理输出，适用于探索性和生产环境。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于解决使用OMOP CDM进行数据标准化时的挑战，特别是在将源医学术语映射到OMOP标准概念时所面临的资源密集和易出错的问题。大语言模型存在幻觉问题，需要专业训练和验证，因此需要一种零训练、防幻觉的映射系统来加强数据标准化过程。

Method: 论文开发了基于模型上下文协议（MCP）的映射系统，利用该系统与外部资源和工具的交互，实现了“零训练、防幻觉”的特性。系统实现了实时词汇查找和结构化推理输出，提高了映射的效率和准确性。

Result: 通过开发基于MCP的映射系统，论文实现了实时词汇查找和结构化推理输出，显著提高了映射的效率和准确性。该系统为数据标准化提供了可解释的映射，适用于探索性和生产环境。

Conclusion: 本论文开发了一种基于模型上下文协议（MCP）的零训练、防幻觉映射系统，有效解决了在使用OMOP CDM进行数据标准化过程中遇到的资源密集、容易出错的问题。该系统提供了可解释的映射，显著提高了效率和准确性，同时用最小的努力实现了实时词汇查找和结构化推理输出。

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [18] [A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai](https://arxiv.org/abs/2509.03830)
*Kaizhen Tan,Yufan Wu,Yuxuan Liu,Haoran Zeng*

Main category: cs.AI

TL;DR: 这项研究提出了一个AI框架，结合社交媒体数据分析历史城区中游客的感知。通过对游客照片的分析，揭示了游客对历史城区的美学偏好和情感反应的空间变化。研究结果显示游客满意度在不同维度上存在空间变化，还发现了社交媒体照片和真实街景之间的颜色主题差异。


<details>
  <summary>Details</summary>
Motivation: 历史城区在保护文化遗产的同时为旅游和日常生活提供了活力空间。了解游客如何感知这些环境对于可持续的、以人为本的城市规划至关重要。因此，本研究的动机在于提出一种新颖的、多维的分析框架，通过AI技术和社交媒体数据来揭示游客在历史城区中的感知。

Method: 本研究提出了一个基于AI的框架，结合焦点点提取、颜色主题分析和情感挖掘等方法，通过社交媒体图片的分析来研究历史城区中游客的感知。框架从游客分享的照片中识别视觉焦点区域，提取主要颜色并分析它们在城区的空间分布。同时，通过比较社交媒体照片和真实街景的颜色主题，揭示了显著差异。此外，还采用了混合情感分析方法评估游客评论。通过结合基于规则的方法和多任务BERT模型，评估了游客对四个维度的满意度：旅游活动、建筑环境、服务设施和商业形式。

Result: 通过研究，揭示了游客对历史城区的美学偏好和情感反应的空间变化，并评估了游客对不同维度的满意度。研究结果还显示了社交媒体照片和真实街景之间的颜色主题差异，突出了对视觉期望和建筑环境之间的潜在差距。

Conclusion: 该研究提出了一个多维AI框架，通过社交媒体的多模态数据来分析历史城区中游客的感知。研究结果揭示了游客对历史城区的美学偏好和情感反应之间的空间变化。框架为解码游客感知提供了整合的、数据驱动的方法，并有助于旅游、文化遗产保护以及设计具有美学吸引力的公共空间时做出明智的决策。

Abstract: Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.

</details>


### [19] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: GEN AI模型在不同应用领域取得成功，但存在可靠性问题。该研究提出一种方法，使用确定性和LLM生成的知识图谱来连续监测评估GEN AI的可靠性。研究构建了两个并行知识图谱：确定性知识图谱和LLM生成的知识图谱，并采用多个已建立的知识图谱度量标准。研究结果表明，该方法能够及时发现语义异常或幻觉，提供稳健和可扩展的评估框架。


<details>
  <summary>Details</summary>
Motivation: GEN AI模型在应用领域取得了巨大的成功，但存在诸多可靠性问题，包括幻觉、语义漂移和固有偏见。当前的评估方法主要依赖于主观的人类评估，限制了可扩展性、透明性和有效性。因此，本研究的动机在于提出一种系统方法来监测和评估GEN AI的可靠性，通过构建确定性和LLM生成的知识图谱，进行连续的评估，以增强可靠性评估的透明度和客观性。

Method: 该研究使用确定性和大型语言模型生成的知识图谱来监测和评估GEN AI的可靠性。通过构建两个并行的知识图谱，利用实时新闻数据流生成LLM知识图谱，并采用多个已建立的知识图谱度量标准，如实例化类比率（ICR）、实例化属性比率（IPR）和类实例化（CI）。同时，建立自动化实时监测框架，连续计算确定性和LLM生成的知识图谱之间的偏差，并基于历史结构度量分布建立动态异常阈值。

Result: 研究采用确定性和LLM生成的知识图谱进行了实时监测和评估GEN AI的可靠性，通过度量结构偏差和语义差异，以及建立自动化实时监测框架来识别和标记重大偏差，从而及时发现语义异常或幻觉，为GEN AI的可靠性评估提供了一个稳健和可扩展的框架。

Conclusion: 该研究提出了一种系统方法，利用确定性和大型语言模型生成的知识图谱来持续监测和评估GEN AI的可靠性。通过建立两个并行的知识图谱：（i）使用显式基于规则的方法建立的确定性知识图谱，预定义的本体论，领域特定词典和结构化实体关系提取规则，以及（ii）根据实时文本数据流（例如实时新闻文章）动态生成的LLM生成的知识图谱，利用实时新闻流确保真实性，减轻重复训练的偏见，并防止自适应LLM通过反馈记忆绕过预定义的基准。该研究提出的方法通过建立动态异常阈值，并基于历史结构度量分布，及时识别和标记显著偏差，从而及时检测语义异常或幻觉。通过确定性和动态生成的知识图谱之间的结构度量驱动比较，提供了一个强大且可扩展的评估框架。

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [20] [Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata](https://arxiv.org/abs/2509.03863)
*Sina Khajehabdollahi,Gautier Hamon,Marko Cvjetko,Pierre-Yves Oudeyer,Clément Moulin-Frier,Cédric Colas*

Main category: cs.AI

TL;DR: 本文介绍了Expedition and Expansion (E&E)混合策略，通过在连续细胞自动机中的探索，提高了多样性解决方案的发现。E&E利用语言化目标和语义空间操作，增强了解决方案的可解释性。在Flow Lenia上的实验结果显示，E&E相比现有方法发现更多多样化的解决方案，同时基因谱分析揭示了探险解决方案对长期探索的影响。


<details>
  <summary>Details</summary>
Motivation: 连续细胞自动机中发现多样化的视觉模式具有挑战性，传统的探索方法在本地新颖性耗尽时往往停滞不前，无法探索未知领域。因此，本文旨在引入一种混合策略，提高探索的多样性并增加解决方案的可解释性和相关性。

Method: 本文采用Expedition and Expansion (E&E)混合策略，在连续细胞自动机中进行探索。E&E通过本地新颖性驱动的扩展和目标导向的探险交替进行，利用Vision-Language Model (VLM)生成语言化目标，评估新颖性并生成有意义的目标，提高探索行为的可解释性和相关性。研究采用Flow Lenia进行测试，利用基因谱分析揭示了探险解决方案对长期探索的影响。

Result: Expedition and Expansion (E&E)混合策略成功在连续细胞自动机中发现更多多样化的解决方案，比现有探索方法表现更好。基因谱分析显示，来自探险的解决方案影响长期探索，解锁新的行为领域。

Conclusion: 本文引入了Expedition and Expansion (E&E)混合策略，通过在本地新颖性驱动的扩展和目标导向的探险之间交替，成功地在连续细胞自动机中发现更多多样化的解决方案。E&E在语义空间中操作，评估新颖性并以概念上有意义的方式生成目标，提高了发现行为的可解释性和相关性。研究结果显示，E&E比现有的探索方法在Flow Lenia上持续发现更多多样的解决方案。基因谱分析进一步揭示，来自探险的解决方案不成比例地影响长期探索，解锁了新的行为领域，为后续搜索铺平道路。这些发现突显了E&E突破本地新颖性边界的能力，以人类对齐、可解释的方式探索行为景观，为人工生命等领域的无尽探索提供了一个有前途的模板。

Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.

</details>


### [21] [FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace](https://arxiv.org/abs/2509.03890)
*Yineng Yan,Xidong Wang,Jin Seng Cheng,Ran Hu,Wentao Guan,Nahid Farahmand,Hengte Lin,Yue Li*

Main category: cs.AI

TL;DR: 该论文介绍了FaMA，一款基于LLMs的智能助手，旨在简化C2C电子商务平台上的用户交互过程。实验证明FaMA在任务成功率和交互时间上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 针对C2C电子商务平台上用户面临的复杂GUI导致交互体验耗时的问题，提出了使用智能助手的解决方案。通过引入LLMs和新型交互模式，试图提高市场活动管理的效率和易用性。

Method: 介绍了论文提出的新方法，即使用LLMs构建基于自然语言命令的智能助手，用于简化C2C电子商务平台上的用户交互过程。详细描述了Facebook Marketplace Assistant（FaMA）的架构，并通过实验证明了其有效性。

Result: 通过实验验证，FaMA在处理市场上复杂任务和优化交互时间方面取得了显著成果。

Conclusion: 该论文介绍了一种基于Large Language Models（LLMs）的智能助手，名为Facebook Marketplace Assistant（FaMA），用于简化在C2C电子商务平台上的复杂交互过程。研究表明FaMA在解决市场上的复杂任务方面达到了98%的成功率，并可使交互时间加快2倍。

Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

</details>


### [22] [A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning](https://arxiv.org/abs/2509.03906)
*Qika Lin,Yifan Zhu,Bin Pu,Ling Huang,Haoran Luo,Jingying Ma,Zhen Peng,Tianzhe Zhao,Fangzhi Xu,Jian Zhang,Kai He,Zhonghong Ou,Swapnil Mishra,Mengling Feng*

Main category: cs.AI

TL;DR: 本文介绍了DeepMedix-R1，一种用于CXR解释的全面医学基金会模型。通过顺序训练流程，包括微调、合成推理样本暴露和在线强化学习，提高了模型的推理质量和生成性能。Quantitative evaluation展示了在报告生成和视觉问答任务中与其他模型相比的显著改进。通过新的评估框架Report Arena和专家审查，显示了DeepMedix-R1具有更高的可解释性和临床合理性。


<details>
  <summary>Details</summary>
Motivation: 目前医学基金会模型存在缺乏透明推理过程和局部可解释性的问题，限制了它们在临床中的实际应用。为了解决这一问题，作者引入了DeepMedix-R1，旨在提供全面、透明和临床可行的CXR解释模型。

Method: 该论文介绍了DeepMedix-R1的训练流程，包括初步微调、合成推理样本暴露和在线强化学习精化的顺序流程。还提出了Report Arena作为评估答案质量的基准框架，并通过专家审查比较推理步骤的可解释性和临床合理性。

Result: 在报告生成和视觉问答任务中，DeepMedix-R1相较于现有模型取得了显著改进。专家审查显示其推理步骤具有更高的可解释性和临床合理性。

Conclusion: 该论文引入了DeepMedix-R1，一种用于胸部X射线（CXR）解释的全面医学基金会模型。通过顺序训练流程，在基本CXR解释能力的基础上进行微调，然后暴露于高质量的合成推理样本以启用冷启动推理，最后通过在线强化学习进行精化以提高扎实推理质量和生成性能。模型不仅产生答案，还为每个查询的图像局部区域生成相关的推理步骤。定量评估显示在报告生成和视觉问答任务中取得了实质性改进。作者提出Report Arena作为评估答案质量的基准框架，并通过专家审查显示生成的推理步骤具有更高的可解释性和临床合理性。总体而言，该工作推动了医学基金会模型向全面、透明和临床可行的CXR解释建模发展。

Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.

</details>


### [23] [Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions](https://arxiv.org/abs/2509.03953)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的搜索算法，将控制参数作为真正的决策点处理，证明了在一定条件下对其决策空间的完备性。结果显示该算法是解决涉及控制参数的计划问题的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有的最先进方法主要将控制参数作为嵌入约束处理，而不是作为搜索空间中的决策点，本文旨在提出一种更高效的替代方案，将控制参数明确处理为搜索空间中真正的决策点。

Method: 开发了基于最佳优先的启发式搜索算法，处理控制参数作为真正的决策点，在决策空间中进行系统搜索。利用延迟部分扩展的概念，对状态的一部分后继状态进行增量展开。

Result: 研究结果表明，提出的搜索算法是解决涉及控制参数的计划问题的竞争性替代方案，证明了在一定条件下对控制参数决策空间的完备性。

Conclusion: 本文提出了一种有效的替代方案，将控制参数作为真正的决策点处理，开发了一个基于最佳优先的启发式搜索算法，证明了在一定条件下对控制参数的决策空间的完备性。采用延迟部分扩展的概念，而不是完全展开状态，结果表明这种新颖的搜索算法是解决涉及控制参数的计划问题的竞争性替代方案。

Abstract: In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.

</details>


### [24] [World Model Implanting for Test-time Adaptation of Embodied Agents](https://arxiv.org/abs/2509.03956)
*Minjong Yoo,Jinwoo Jang,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: 该论文介绍了一种名为WorMI的世界模型植入框架，结合了大型语言模型（LLMs）的推理能力与独立学习的领域特定世界模型，实现了跨领域适应性。WorMI框架有效整合了多个世界模型的知识，确保了对未知领域的鲁棒适应性，在VirtualHome和ALFWorld基准上表现出卓越的性能，具有潜力在现实世界的具体代理情景中进行可伸缩且有效的部署。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于解决在具体AI领域中，使代理能够在新领域中稳健适应而无需进行大量数据收集或重新训练的持久性挑战。

Method: 该论文提出的WorMI框架采用基于原型的世界模型检索方法，利用高效的基于轨迹的抽象表示匹配，将相关模型纳入测试时组合中。同时还开发了一种世界化复合注意力方法，不仅整合了从检索的世界模型中获得的知识，还将它们的中间表示与推理模型的表示在体现代理策略中进行了对齐。这一框架设计有效地融合了多个世界模型的领域特定知识，确保了对未知领域的鲁棒适应。

Result: 通过在VirtualHome和ALFWorld基准上评估WorMI，得到在一系列未知领域中具有卓越零样本和少样本性能的结果，相比于几种基于LLMs的方法，WorMI表现出更优异的性能。这些结果突显了该框架在具体代理情景中具有可伸缩性、现实世界部署所需的适应性和数据效率的潜力。

Conclusion: 该论文提出了一种名为WorMI的世界模型植入框架，结合了大型语言模型（LLMs）的推理能力与独立学习的领域特定世界模型，实现了跨领域适应性。通过在测试时组合允许无缝植入和移除世界模型，该框架有效促使具有鲁棒跨领域适应性的体现代理策略。

Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.

</details>


### [25] [Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent](https://arxiv.org/abs/2509.03990)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: 本论文介绍了Meta-Policy Reflexion（MPR）框架，通过整合LLM生成的反思为结构化的元策略记忆，在推理时应用该记忆，并提高单任务性能、改进探索效率和跨任务适应性。实验证明MPR相比Reflexion基准在执行精确度和稳健性上有提升；规则的适应性进一步提高了稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的反思策略改进了每集行为，但通常产生不可重复、任务特定的痕迹；强化学习的替代方案可以产生可转移的策略，但需要大量参数更新和计算；本研究旨在通过MPR框架外部化可重复使用的纠正知识，以减少不安全或无效的动作，并保留基于语言的反思的适应性。

Method: 引入了Meta-Policy Reflexion（MPR）框架，通过将LLM生成的反思整合为结构化的元策略记忆（MPM）并在推理时应用该记忆，采用软性记忆引导解码和硬性规则可接受性检查（HAC）两种机制。

Result: 实验结果显示，MPR相比Reflexion基准在执行精确度和稳健性上有提升；规则的适应性进一步提高了稳定性。

Conclusion: 该论文介绍了一种新的元策略反思（Meta-Policy Reflexion，MPR）框架，通过将LLM生成的反思整合为结构化的元策略记忆（Meta-Policy Memory，MPM）并在推理时应用该记忆，以提高单任务性能、改进探索效率和跨任务适应性。实验证明MPR相比Reflexion基准在执行精确度和稳健性上有一致的提升，规则的适应性进一步提高了稳定性。论文讨论了这些提升的机制，讨论了可扩展性和故障模式，并概述了未来面向多模态和多代理扩展的方向。

Abstract: Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.

</details>


### [26] [AutoPBO: LLM-powered Optimization for Local Search PBO Solvers](https://arxiv.org/abs/2509.04007)
*Jinyuan Li,Yi Chu,Yiwen Sun,Mengchuan Zou,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoPBO is introduced as a novel LLM-powered framework to enhance PBO local search solvers automatically. It outperforms previous local search approaches and competes well with state-of-the-art competitors, demonstrating promising potential in automating local search solver design.


<details>
  <summary>Details</summary>
Motivation: While Large Language Models (LLMs) have demonstrated potential in automating algorithm design, their application to optimizing PBO solvers remains unexplored. Local search solvers' design often requires significant expert effort and manual tuning. This paper aims to automate the enhancement of PBO local search solvers using an LLM-powered framework.

Method: Introducing AutoPBO, a novel LLM-powered framework to automatically enhance PBO local search solvers. Conducting experiments on multiple public benchmarks to evaluate the performance improvement achieved by AutoPBO and comparing it with six state-of-the-art competitors.

Result: AutoPBO demonstrates significant performance improvements over previous local search approaches and competitive performance with state-of-the-art competitors in optimizing PBO solvers.

Conclusion: AutoPBO demonstrates significant improvements over previous local search approaches, while maintaining competitive performance compared to state-of-the-art competitors. The results suggest that AutoPBO offers a promising approach to automating local search solver design.

Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.

</details>


### [27] [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
*Zeyu Gan,Hao Yi,Yong Liu*

Main category: cs.AI

TL;DR: 本文介绍了CoT-Space框架，重新构建了LLM推理过程，证明了在欠拟合和过拟合之间的基本权衡导致对最佳CoT长度的收敛是一个自然结果。提供了实验证据来验证理论发现，为未来更有效和可推广的推理代理开发奠定了坚实的理论基础，并解释了超思考等经验现象。


<details>
  <summary>Details</summary>
Motivation: 传统的标记级RL框架与复杂的多步思考过程（如CoT）的推理级性质不一致，存在理论上的差距。为解决这一挑战，引入了CoT-Space框架将推理框架从标记预测转变为连续的推理级语义空间中的优化过程。

Method: 介绍了CoT-Space框架，重新构建了LLM推理过程，从噪声和风险角度分析，进行了大量实验以验证理论发现。

Result: 通过对过拟合和欠拟合之间的权衡进行分析，证明了对最佳CoT长度的收敛是一个自然结果。通过大量实验证明了理论发现的可靠性。

Conclusion: 引入了一种名为CoT-Space的新理论框架，将LLM推理从离散的标记预测任务转化为连续的推理级语义空间内的优化过程，证明了在欠拟合和过拟合之间的基本权衡导致对最佳CoT长度的收敛是一个自然结果。通过噪声和风险的分析，提供了实验证据来验证理论发现，为未来更有效和可推广的推理代理开发奠定了坚实的理论基础，并解释了超思考等经验现象。

Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.

</details>


### [28] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: 本文介绍了Oruga项目，通过结构传输方法实现了对表示的结构转换，实现了跨不同领域的创新类比。该项目包括核心数据结构、交流语言和转换引擎，实现了RST的各个方面。


<details>
  <summary>Details</summary>
Motivation: 作者希望将人类灵活运用表示的能力赋予机器，使机器更符合人类使用的需求。之前他们开发了Representational Systems Theory（RST）来研究表示的结构和变换。

Method: 文章使用结构传输方法实现了Oruga项目，该项目包括数据结构核心、用于与核心交流的语言以及用于生成转换的引擎。

Result: Oruga项目实现了RST的各个方面，包括核心数据结构、与核心交流的语言以及使用结构传输方法执行转换的引擎。作者展示了Oruga的核心和语言的概览，并提供了结构转换可执行的一个示例。

Conclusion: 本文介绍了Oruga项目，它是基于Representational Systems Theory（RST）开发的实现。利用Oruga，可以对表示进行结构转换，以实现跨不同领域的创新类比。

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [29] [Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning](https://arxiv.org/abs/2509.04083)
*Alexander Beiser,David Penz,Nysret Musliu*

Main category: cs.AI

TL;DR: 本论文研究了神经符号LLM推理的成功因素之一是选择合适的形式语言，比较了四种形式语言在不同LLMs上的表现，发现形式语言选择会影响句法和语义推理能力。


<details>
  <summary>Details</summary>
Motivation: 神经符号LLM推理是一个有前途的方法，但对其成功因素仍不清楚，本文关注了先前被忽视的形式语言选择因素，提出了中间语言挑战来研究神经符号推理中选择合适形式语言的重要性。

Method: 通过比较四种形式语言在三个数据集和七个LLMs上的表现，探讨选择形式语言对神经符号LLM推理能力的影响。

Result: 通过实验证明形式语言选择对神经符号LLM推理能力的影响，探讨了不同LLMs之间的差异效应。

Conclusion: 该论文表明神经符号LLM推理的成功因素之一是选择合适的形式语言，对选择的形式语言进行比较实验，揭示了不同形式语言对句法和语义推理能力的影响。

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.

</details>


### [30] [Hybrid Reinforcement Learning and Search for Flight Trajectory Planning](https://arxiv.org/abs/2509.04100)
*Alberto Luise,Michele Lombardi,Florent Teichteil Koenigsbuch*

Main category: cs.AI

TL;DR: 本文探讨了强化学习和搜索路径规划器相结合的方法，用于加速航线路径的优化过程，特别是在紧急情况下快速重新计算路径至关重要。通过训练RL代理程序预先计算近似最优路径，并在运行时约束路径规划求解器，可以显著减小求解器的搜索空间，加快路径优化过程。实验结果显示，在空客飞机性能模型下，该方法在保持燃料消耗不变的情况下，相对于传统求解器提高了计算速度。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于提高航线路径规划的速度，并在紧急情况下提供快速路径重新计算的能力。通过结合强化学习和基于搜索的路径规划器，减小了求解器的搜索空间，以实现更快的路径优化。

Method: 方法：训练强化学习代理程序预先计算基于位置和大气数据的近似最优路径，并在运行时使用这些路径来约束基础路径规划求解器，从而在距离初始猜测一定距离范围内找到解决方案。该方法有效地减小了求解器的搜索空间的大小，显著加快了路径优化过程。

Result: 结果：实验结果表明，通过使用空客飞机性能模型进行实验，本方法在保持燃料消耗几乎不变的情况下，相对于仅使用传统求解器，计算速度可以提高多达50%。尽管不能保证全局最优性，但仍有效地加快了路径优化过程。

Conclusion: 结论：本文探讨了强化学习（RL）和基于搜索的路径规划器的结合，以加快航班路径的优化过程，特别是在紧急情况下快速重新计算路径可能至关重要。尽管不能保证全局最优性，但实证结果表明，使用空客飞机性能模型进行的实验显示燃料消耗几乎与无约束求解器的相同，偏差通常在1%以内。与仅使用传统求解器相比，计算速度可以提高多达50%。

Abstract: This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.

</details>


### [31] [Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker](https://arxiv.org/abs/2509.04125)
*Tarik Zaciragic,Aske Plaat,K. Joost Batenburg*

Main category: cs.AI

TL;DR: 本文研究了计算机扑克中DQN和CFR算法在Leduc Hold'em扑克游戏中的虚张行为。实验发现两者都展示了虚张行为，但方式不同。成功的虚张比例相近，表明虚张是游戏的重要方面，而非算法特性。未来研究可以探讨不同的虚张风格和完整扑克游戏。


<details>
  <summary>Details</summary>
Motivation: 在扑克游戏中，虚张声势是一项重要技能，但大多数计算机扑克游戏的研究都集中在诸如胜率之类的性能度量上，而虚张却被忽略了。

Method: 设计实验让DQN和CFR代理之间互相对抗，在记录它们的行为的过程中研究它们是否表现出虚张行为。

Result: 实验结果表明，DQN和CFR算法都表现出虚张行为，但它们的方式有所不同。尽管它们尝试以不同比率进行虚张，成功虚张的比例大致相同。

Conclusion: 研究发现DQN和CFR算法在Leduc Hold'em扑克游戏中展示出模糊行为，虽然它们尝试以不同方式进行虚张声势，但成功的虚张声势的比例大致相同。这表明虚张声势是游戏的重要方面，而不是算法的特性。未来的工作应该考虑不同的虚张风格以及完整的扑克游戏。

Abstract: In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.

</details>


### [32] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: Recent advances in AI suggest the potential for artificial general intelligence (AGI) to surpass human capabilities. However, the paper argues that the emotional understanding and integration with physical reality provided by the central nervous system (CNS) in humans, allowing for the development of sustainable ethical systems, gives DNA-based beings an edge in leadership over silicon-based AI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to explore the potential future where AI systems may surpass humans in capabilities and whether they could become superior leaders of the universe. It highlights the importance of emotional understanding, which is unique to biological beings with a central nervous system, in developing ethical systems and leadership qualities.

Method: The paper discusses the differentiation between human beings and AI, focusing on the role of the central nervous system (CNS) in providing emotional understanding and integration with physical reality. It argues that the CNS, responsible for experiencing emotions like pain, joy, suffering, and love, is a crucial factor in developing sustainable ethical systems and leadership qualities.

Result: The paper concludes that while AI systems may excel in various measures and transform society, true leadership that requires emotional understanding of consequences will remain better suited for DNA-based beings with a central nervous system (CNS) than silicon-based AI systems. The development of consciousness alone will not be sufficient for AI systems to be superior to humans.

Conclusion: AI systems, even with artificial general intelligence (AGI), may surpass humans in capabilities but will lack the emotional understanding and integration with physical reality provided by the central nervous system (CNS) in humans. The development of sustainable ethical systems and true leadership requires this emotional understanding, leading to the assertion that DNA-based beings will always be better suited for leadership than silicon-based AI systems.

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


### [33] [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
*Aarush Kumbhakern,Saransh Kumar Gupta,Lipika Dey,Partha Pratim Das*

Main category: cs.AI

TL;DR: 本文介绍了一种新颖的领域特定语言（DSL），用于精确表示并自动化烹饪程序。通过引入DSL和时间图的方法，实现了对复杂烹饪工作流的精确建模。通过初步手动评估展示了DSL的潜力，为未来自动化食谱分析和执行打下基础。


<details>
  <summary>Details</summary>
Motivation: 由于烹饪程序的复杂性和模糊性，形式化烹饪程序一直是具有挑战性的任务。为实现对复杂烹饪工作流的精确建模，并为自动化食谱分析和执行打下基础，需要引入一种新的语言和方法。这些举措旨在将烹饪过程标准化，促进烹饪流程的机器理解和自动化。

Method: 引入了一种可扩展的领域特定语言，将食谱表示为指导性操作图，捕捉了过程、转移、环境、并发性和组成结构。通过DSL实现对复杂烹饪工作流的精确、模块化建模。进行了初步手动评估，展示了DSL的表达性和适用性，为未来自动化食谱分析和执行打下基础。利用时间图实现结构化机器理解、精确解释和可扩展的烹饪流程自动化。

Result: 通过引入DSL和时间图的方法，实现了对复杂烹饪工作流的精确建模，并展示了DSL在食谱表示和自动化烹饪过程方面的潜力。初步手动评估表明DSL的表达性和适用性，为未来自动化食谱分析和执行奠定了基础。这项工作为烹饪流程的机器理解和自动化提供了新的可能性。

Conclusion: 在烹饪过程中，由于其固有的复杂性和模糊性，形式化烹饪程序仍然是一项具有挑战性的任务。本文引入了一种可扩展的领域特定语言，将食谱表示为指导性操作图，捕捉了过程、转移、环境、并发性和组成结构。我们的方法实现了对复杂烹饪工作流的精确、模块化建模。对完整的英式早餐食谱进行的初始手动评估显示出DSL的表达性和适用性，可用于未来自动化食谱分析和执行。这项工作是朝着以行动为中心的烹饪本体论迈出的首要步骤，利用时间图实现了结构化机器理解、精确解释和可扩展的烹饪流程自动化，无论是在家庭厨房中还是专业烹饪场所。

Abstract: Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.

</details>


### [34] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 本文研究了Markov Logic Network（MLN）在无限域上的概率分布特性，包括不同类型的具体MLN示例的性质。通过研究随机结构的极限行为，发现MLN中的软约束权重可能影响极限行为，不同类型的MLN示例可能导致不同的极限行为。提出了“δ-近似0-1定律”和MLN与提升贝叶斯网络在大域上的不可比较性质。


<details>
  <summary>Details</summary>
Motivation: 研究MLN在无限域上的概率分布行为有助于理解随机结构的极限行为。研究不同类型的MLN示例以及它们的属性可以帮助揭示MLN在不同约束条件下的性质。比较无量词自由的MLN和提升贝叶斯网络的渐近比较也有助于深入了解这两种方法的异同。

Method: 研究了MLN在无限域上的概率分布特性，考虑了不同类型的具体MLN示例，并进行了随机结构的性质研究。通过(1)的分析，表明无量词自由的MLN和广义上的提升贝叶斯网络在渐近意义上不可比较。还在一般情况下表明，在大域上，MLN确定的分布几乎将其概率质量集中在可能世界空间的一个完全不同部分，而不是均匀分配。

Result: 通过研究MLN在无限域上的概率分布特性，得出了关于随机结构极限行为的多项性质和结论。发现不同类型的MLN示例会导致随机结构不同的极限行为，MLN中的软约束权重可能影响极限行为。并推导出一种“δ-近似0-1定律”以及MLN和提升贝叶斯网络在大域上的不可比较性质。

Conclusion: 本文研究了Markov Logic Network（MLN）在无限域上的概率分布特性。研究了三种具体的MLN示例，并研究了域大小趋向无穷时随机结构的性质：（1）一个仅包含一个关系符号且具有arity 1的语言上的任意无量词MLNs。在这种情况下，我们对随机结构可能的极限行为进行了相当完整的描述。（2）偏好具有更少三角形（或更一般地，更少k-cliques）的图形的MLN。作为分析的推论，获得了一种“δ-近似0-1定律”对于一阶逻辑。(3) 偏好具有少于一个固定（但任意）数字的度高于固定数量的顶点的图形的MLN。分析表明，根据MLN使用的“软约束”不同，随机结构的极限行为可能会有很大不同，而软约束的权重可能会或可能不会对极限行为产生影响。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [35] [Evaluating Quality of Gaming Narratives Co-created with AI](https://arxiv.org/abs/2509.04239)
*Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 该论文提出结构化方法评估AI生成的游戏叙事质量，利用Delphi研究与专家意见，将其映射到Kano模型框架评估影响玩家满意度的维度。结果可指导开发者优先考虑质量方面合作创建游戏叙事。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于提出一种方法来评估由人工智能生成的游戏叙事的质量，并为游戏开发者提供指导，帮助其在与生成式人工智能共同创建游戏叙事时优先考虑质量方面。

Method: 本文采用Delphi研究结构与叙事设计专家小组相结合的方法，从文献和专家见解中综合故事质量维度，将其映射到Kano模型框架，以理解它们对玩家满意度的影响。

Result: 结果显示本研究可以为游戏开发者提供有关合作创建游戏叙事时如何优先考虑质量方面的信息，并为他们提供了一个有用的框架来评估AI生成的游戏叙事。

Conclusion: 该论文提出了一种结构化方法来评估由人工智能生成的游戏叙事，利用Delphi研究结构与叙事设计专家小组。研究综合了文献和专家见解中的故事质量维度，将它们映射到Kano模型框架中以了解它们对玩家满意度的影响。结果可以为游戏开发者提供指导，帮助其在与生成式人工智能共同创建游戏叙事时优先考虑质量方面。

Abstract: This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.

</details>


### [36] [EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation](https://arxiv.org/abs/2509.04310)
*Yunbo Long,Liming Xu,Lukas Beckenbauer,Yuhan Liu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 最近的研究表明，LLM代理在复杂、多轮谈判中能够展开Chain-of-Thought（CoT）推理，但现有代理忽视情绪在谈判中的作用，容易受操纵。因此，论文提出了EvoEmo框架，使用进化强化学习优化情绪表达。实验证明EvoEmo在谈判中表现优异，强调了自适应情绪表达对于提升LLM代理效果的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，LLM代理在复杂的、多轮谈判中能够展开Chain-of-Thought（CoT）推理，为存在主义人工智能开辟新的途径。然而，现有LLM代理在这样的谈判中往往忽视情绪的功能作用，而是生成被动的、偏好驱动的情绪反应，使其容易受到对立方的操纵和战略利用。因此，为了填补这一空白，提出了EvoEmo框架来优化动态情绪表达。

Method: 该论文使用进化强化学习框架EvoEmo来优化谈判中的动态情绪表达。情绪状态转换被建模为马尔可夫决策过程，并利用基于人口的遗传优化在不同谈判场景下进化高奖励情绪政策。提出了评估框架，包括两个基准方法：基本策略和固定情绪策略，用于衡量情绪感知谈判。

Result: EvoEmo在实验和消融研究中持续优于基线方法，在谈判中取得更高的成功率、更高效率和增加的买家储蓄。这些结果强调了在多轮谈判中采用自适应情绪表达的重要性。

Conclusion: 该论文提出了EvoEmo，一个基于进化强化学习的框架，用于优化谈判中的动态情绪表达。通过在多样化谈判场景中进化高奖励情绪策略，EvoEmo在实验和消融研究中表现出色，优于基线方法，包括高成功率、更高效率和增加的买家储蓄。结果突出了在多轮谈判中采用自适应情绪表达对于提升LLM代理的效果至关重要。

Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.

</details>


### [37] [Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes](https://arxiv.org/abs/2509.04317)
*Isidoro Tamassia,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 本文分析了如何在可能发生变化的测试环境中部署AlphaZero代理，并通过对标准框架进行简单修改来提高性能，代码已在GitHub上公开。


<details>
  <summary>Details</summary>
Motivation: AlphaZero通常假设神经网络训练时的环境在测试时不会改变，这限制了其适用性，因此需要解决在可能的变化测试环境中部署AlphaZero代理的问题。

Method: 结合Monte Carlo规划和先前训练的策略-价值神经网络提供的先验知识，通过修改AlphaZero框架的方式来适应可能发生变化的测试环境。

Result: 通过简单修改标准框架，可以显著提高AlphaZero代理的性能，即使在计划预算有限的情况下。

Conclusion: 本文分析了在可能发生变化的测试环境中部署AlphaZero代理的问题，并展示了如何通过对标准框架进行简单修改，显著提高性能，即使在可用计划预算较低的情况下。

Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.

</details>


### [38] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: 该论文介绍了一种名为MBTI-in-Thoughts的框架，通过心理学基础的个性化调节来增强大型语言模型代理的效果。个性化调节方法可以产生一致且可解释的行为偏好，在不同任务中表现出不同优势。通过结构化的多代理通信协议和自我反思，可以改善合作和推理质量。集成了16Personalities测试以确保特质的持久性，并展示了方法的泛化能力适用于其他心理框架。


<details>
  <summary>Details</summary>
Motivation: 本论文的动机在于将心理学理论与大型语言模型代理的行为设计相结合，构建一个无需精细调整即可提升代理效果的框架。借助MBTI等心理学工具，引导代理采纳不同人格特质，探索其对代理行为的影响，并验证其在多代理通信和自我反思方面的作用。通过实验验证验证框架的有效性，并展示方法的泛化能力。

Method: 该论文通过MBTI为基础的个性化调节方法，通过提示工程引导代理采纳不同的人格原型，从而控制人类心理学、认知和情绪这两个基础轴线上的行为。他们展示了这种个性化调节在不同任务中产生一致且可解释的行为偏好，并利用结构化的多代理通信协议进行实验。此外，他们集成了16Personalities测试以保证特质持久性，并展示了方法的泛化能力，可推广到其他心理框架。

Result: 通过该研究，展示了使用MBTI为基础的个性化调节方法可以在不同任务中产生一致且可解释的行为偏好，例如在叙事生成和博弈论情景中取得不同的优势。框架支持结构化的多代理通信协议，通过自我反思可以改善合作和推理质量。此外，集成了16Personalities测试以确保特质的持久性，并表明方法的泛化能力适用于其他心理框架。

Conclusion: 该论文介绍了MBTI-in-Thoughts框架，通过心理学基础的个性化调节来增强大型语言模型（LLM）代理的效果。他们表明，通过个性化调节，可以在各种任务中获得一致且可解释的行为偏好，例如在叙事生成方面情绪表达丰富的代理表现出色，在博弈论情景中，分析性调节的代理采取更稳定的策略。通过实验验证，他们的框架支持结构化的多代理通信协议，并揭示自我反思在交互前可以改善合作和推理质量。他们还集成了16Personalities测试以确保特质的持久性，同时展示了他们的方法可以顺利推广到其他心理框架，如大五人格、HEXACO或Enneagram。通过搭建心理学理论与LLM行为设计的桥梁，他们建立了一个基础，用于打造无需精细调整的心理增强人工智能代理。

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>


### [39] [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
*Matthew Ho,Chen Si,Zhaoxiang Feng,Fangxu Yu,Zhijian Liu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: 本研究提出了在推理过程中保留和利用发现的外部记忆的新方法。通过抽象概念作为记忆设计，实现了更好的性能并支持自我改进的解决方案。在ARC-AGI基准测试中，方法相对于基线表现显著增加，性能随推理计算的增加而提升。动态更新记忆在测试时间表现比固定记忆设置更好。


<details>
  <summary>Details</summary>
Motivation: 推理时缩放可以使LLMs执行更长、能力更强的推理轨迹，但一旦上下文窗口为新查询重置，揭示的模式和见解即被丢弃。外部记忆是保留这些发现的自然方式，最近的研究表明对于需要推理的任务具有明显的好处。将记忆从基于实例的内存条目（例如精确的查询/响应对或与原始问题上下文紧密耦合的摘要）转向基于概念的记忆，可以使这些记忆更广泛地可重用和可扩展。

Method: 提出了从推理轨迹中提取收获并为新查询检索条目的新策略，促进记忆的重复使用，并允许随着额外经验的积累而扩展记忆。通过引入抽象概念作为记忆设计，表明在所有测试的推理计算规模上优于基线表现。验证动态更新记忆在测试时间优于固定记忆设置，支持更多尝试的假设。

Result: 研究方法在ARC-AGI基准测试中取得了显著进展，在所有推理计算规模下都超过了基线表现。动态更新记忆在测试时间优于固定记忆设置。

Conclusion: 在ARC-AGI基准测试中，研究方法相对于强大的无记忆基准线表现出7.5%的相对增益，且性能随推理计算的增加而提升。通过动态更新记忆来支持假设，即解决更多问题并将更多模式抽象到记忆中能够进一步提高自我改进的解决方案形式。

Abstract: While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.

</details>
